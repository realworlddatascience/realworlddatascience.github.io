<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Real World Data Science</title>
<link>https://realworlddatascience.net/latest-content.html</link>
<atom:link href="https://realworlddatascience.net/latest-content.xml" rel="self" type="application/rss+xml"/>
<description></description>
<image>
<url>https://realworlddatascience.net/images/rwds-logo-150px.png</url>
<title>Real World Data Science</title>
<link>https://realworlddatascience.net/latest-content.html</link>
<height>83</height>
<width>144</width>
</image>
<generator>quarto-1.8.27</generator>
<lastBuildDate>Wed, 18 Feb 2026 00:00:00 GMT</lastBuildDate>
<item>
  <title>RWDS Big Questions: How do we balance innovation and regulation in the world of AI?</title>
  <dc:creator>Annie Flynn</dc:creator>
  <link>https://realworlddatascience.net/the-pulse/posts/2026/02/18/rwds_big_questions_ai_regulation.html</link>
  <description><![CDATA[ 





<p>AI development is accelerating, while regulation moves more deliberately. That tension creates a core challenge: how do we maintain momentum without breaking the things that matter? The aim isn’t to slow innovation unnecessarily, but to ensure progress happens at a pace that protects individuals and society. Responsible actors should not be disadvantaged — yet safeguards are essential to maintain trust.</p>
<p>For the latest video in our RWDS Big Questions series, our panel explores this delicate balance. From risk-based frameworks and transparency to global inequality in AI development, the conversation surfaces the tensions, trade-offs and practical realities facing policymakers, technologists and data scientists alike.</p>
<section id="watch-the-discussion" class="level2">
<h2 class="anchored" data-anchor-id="watch-the-discussion">Watch the discussion</h2>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/L69nxuy9caI" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="takeaways-at-a-glance" class="level2">
<h2 class="anchored" data-anchor-id="takeaways-at-a-glance">Takeaways at a glance</h2>
<ul>
<li><strong>Innovation and regulation are not opposites</strong> – both are essential, but difficult to balance.</li>
<li><strong>Responsible progress requires proportionality</strong> – not all AI applications carry the same level of risk.</li>
<li><strong>Transparency enables better governance</strong> – open dialogue between developers and regulators is key.</li>
<li><strong>Risk-based frameworks provide structure</strong> – distinguishing low-, high-, and unacceptable-risk uses helps focus oversight.</li>
<li><strong>Global disparities complicate regulation</strong> – some regions are regulating advanced AI systems, while others are still building foundational capacity.</li>
<li><strong>Innovation needs protected space</strong> – experimentation, iteration, and even failure are critical before formal standardisation.</li>
</ul>
</section>
<section id="key-themes-and-analysis" class="level2">
<h2 class="anchored" data-anchor-id="key-themes-and-analysis">Key themes and analysis</h2>
<p><strong>Proportional regulation through risk</strong></p>
<p>Not all AI systems pose the same level of harm. A risk-based approach — distinguishing low-, high-, and unacceptable-risk uses — offers a practical middle ground. It avoids blanket restrictions while ensuring stronger oversight where impact is greatest. The debate becomes less about whether to regulate, and more about how proportionate that regulation should be.</p>
<p><strong>Transparency as common ground</strong></p>
<p>Openness can bridge the gap between technologists and regulators. Clear communication about capabilities, limitations and risks enables more informed policy decisions. When innovation happens transparently and in dialogue with regulators, governance can evolve alongside technology rather than lagging behind it.</p>
<p><strong>The global unevenness of AI governance</strong></p>
<p>AI regulation is developing unevenly across regions. While parts of the West are formalising frameworks, many countries are still building foundational AI capacity. This raises difficult questions about sequencing: should regulation lead innovation, or follow it? A one-size-fits-all model may not reflect global realities.</p>
<p><strong>Protecting space to experiment</strong></p>
<p>Innovation requires room to test, iterate and occasionally fail. Early experimentation should not be overburdened with rigid controls — but successful, scalable systems must eventually transition into more standardised and regulated environments. The challenge is designing pathways that support both creativity and accountability.</p>
</section>
<section id="looking-ahead" class="level2">
<h2 class="anchored" data-anchor-id="looking-ahead">Looking ahead</h2>
<p>As AI continues to evolve, the balance between innovation and regulation will remain dynamic — and contested. This conversation opens up important questions, and we would love to hear our readers’ thoughts about how we move some of the principles mentioned in the video into practice.</p>
<ul>
<li>How do we facilitate transparent channels of communication between those developing AI and those designing the regulatory frameworks that will govern it?</li>
<li>What should determine whether an AI system is low, high, or unacceptable risk?</li>
<li>How do we define a “safe speed” for AI development — and who gets to decide?</li>
</ul>
<p>We are actively seeking submissions on these topics so, if you would like to be part of the conversation, <a href="mailto:rwds@rss.org.uk"><strong>get in touch</strong></a>.</p>
<div class="article-btn">
<p><a href="https://realworlddatascience.net/the-pulse/posts/2026/01/21/rwds-big-questions-challenges-today.html">Explore more videos in the series</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the author:</dt>
<dd>
<a href="https://www.linkedin.com/in/annieroseflynn/">Annie Flynn</a> is Head of Content at the <a href="rss.org.uk">Royal Statistical Society</a>.
</dd>
</dl>
<div class="g-col-12 g-col-md-6">
<p><strong>Copyright and licence</strong> : © 2026 Annie Flynn<br>
<a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"> <img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"> </a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">International licence</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<p><strong>How to cite</strong> :<br>
Flynn, Annie 2026. “<strong>RWDS Big Questions: What are the Key Challenges Facing Data Scientists Today?</strong>” <em>Real World Data Science</em>, 2026. <a href="https://realworlddatascience.net/the-pulse/posts/2026/01/rwds-big-questions-challenges-today.html">URL</a></p>
</div>
</div>
</div>


</div>
</section>

 ]]></description>
  <category>AI</category>
  <category>Governance</category>
  <category>Policy</category>
  <guid>https://realworlddatascience.net/the-pulse/posts/2026/02/18/rwds_big_questions_ai_regulation.html</guid>
  <pubDate>Wed, 18 Feb 2026 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/the-pulse/posts/2026/02/18/images/BQthumb.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Understanding and Addressing Algorithmic Bias: a Credit Scoring Case Study</title>
  <dc:creator>Devin Partida</dc:creator>
  <link>https://realworlddatascience.net/applied-insights/case-studies/posts/2026/02/11/algorithmic_bias_credit_scoring.html</link>
  <description><![CDATA[ 





<p>When you apply for a credit card or a loan, algorithms work in the background to determine financial worthiness. Despite increasing advancements, these are <a href="https://hai.stanford.edu/news/how-flawed-data-aggravates-inequality-credit">still imperfect</a> due to inherent biases. As data science students and professionals, you’ll inevitably face similar issues relating to biased data sets and should know how to combat them. What are some of the most effective techniques, and why do they matter?</p>
<section id="the-critical-issue-of-algorithmic-bias-in-credit-scoring-models" class="level2">
<h2 class="anchored" data-anchor-id="the-critical-issue-of-algorithmic-bias-in-credit-scoring-models">The Critical Issue of Algorithmic Bias in Credit Scoring Models</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/applied-insights/case-studies/posts/2026/02/11/images/thumbcredit.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>One of the most concerning aspects of algorithmic bias is the limited recourse available to those negatively impacted, leaving them vulnerable to opaque decision-making processes. This challenge underscores the need for increased transparency in <a href="https://www.psecu.com/learn/whats-in-a-credit-score">how credit scoring models are developed</a> and deployed. Ultimately, proactive mitigation strategies are essential to ensure fairness and equity in financial outcomes.</p>
<p>One of the biggest issues surrounding algorithmic bias in credit scoring is that adversely affected parties usually have little or no recourse for appealing unfavorable decisions. This problem happens because most widely used algorithms still can’t explain how they reached specific decisions, leaving people in the dark and forcing them to trust the technology, even as it potentially ruins lives.</p>
<p>A November 2025 academic review <a href="https://giesbusiness.illinois.edu/news/2025/11/12/new-research-reveals-widespread-bias--inefficiency-in-credit-scoring-and-mortgage-lending">revealed numerous flaws in financial algorithms</a> and confirmed various impacts. They included systematic disadvantages for minority groups and miscalibrated credit scores for individual borrowers. The researchers also discovered that these issues appeared despite the financial technology industry’s promises of superior efficiency.</p>
<p>One of the cited studies consistently showed that female applicants received credit scores six to eight points lower than their male counterparts. The researchers determined that the associated effects diminished economic welfare and that the ramifications continued for multiple borrowing cycles. Another investigation revealed persistent disparities across minority groups, despite the applicant’s chosen lender type.</p>
<p>Elsewhere, researchers examined the effects of using large language models to evaluate applicants’ loan data. This approach regularly <a href="https://news.lehigh.edu/ai-exhibits-racial-bias-in-mortgage-underwriting-decisions">recommended charging higher interest rates</a> to Black applicants or denying their applications. It did not make the same suggestions for identical white applicants. These examples demonstrate why data science professionals must remain constantly aware of the potential for bias and uphold fairness by reducing the issue whenever possible.</p>
</section>
<section id="practical-tips-for-bias-detection-and-mitigation" class="level2">
<h2 class="anchored" data-anchor-id="practical-tips-for-bias-detection-and-mitigation">Practical Tips for Bias Detection and Mitigation</h2>
<p>Sources of bias in credit scoring data and algorithms are more common than you might think. They can include:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/applied-insights/case-studies/posts/2026/02/11/images/infographic.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>One straightforward way to identify bias in training data is to be aware of the most common types and build algorithms to be less reliant on them when possible. Regularly reviewing the training data is similarly effective because it can catch biases before they have real-life effects.</p>
<p>You can also perform a disparate impact analysis to mitigate bias, which compares aggregate measurements attributed to fewer privileges or less representation to their counterparts. Dividing the proportion of one group that received an adverse outcome and comparing it with that of the other group identifies bias.</p>
<p>Creating a set of fairness metrics is another practical mitigation approach, as it encourages data scientists to understand the impacts of various aspects of a person’s background that they can and cannot control. The examined attributes could include someone’s employment history, income and debt, but also the extent to which they experienced equal opportunities.</p>
<p>Improvements in explainable artificial intelligence will further both bias detection and mitigation. Developers then see <a href="https://rehack.com/ai/explainable-ai/">the heavily weighted but irrelevant factors</a> that could lead to unfair outcomes and correct issues early.</p>
</section>
<section id="considerations-to-reduce-algorithmic-bias-in-credit-scoring" class="level2">
<h2 class="anchored" data-anchor-id="considerations-to-reduce-algorithmic-bias-in-credit-scoring">Considerations to Reduce Algorithmic Bias in Credit Scoring</h2>
<p>Reducing algorithmic bias in credit scoring requires cooperation across roles and departments and includes those who will use the tools containing the algorithms. Committing to specific postprocessing steps empowers people to become more familiar with an algorithm’s functionality and potential shortcomings rather than automatically trusting the results. Those developing the algorithms should prioritize transparency by designing explainable models when possible and making them accessible enough for the expected audience.</p>
<p>Staying abreast of recent research shapes data scientists’ efforts by helping them understand the possibilities. In a 2024 case study, MIT researchers created a new <a href="https://news.mit.edu/2024/researchers-reduce-bias-ai-models-while-preserving-improving-accuracy-1211">technique that identifies and eliminates</a> the specific attributes of training data that are the strongest contributors to a model’s biases about minority subgroups. This approach also preserves overall accuracy because it preserves more of the data compared to other options.</p>
<p>The developers confirmed that the technique can find hidden bias sources and training datasets with unlabeled information. This capability is significant because the data used by many applications lacks labels. They envision combining their technique with other approaches to improve fairness in high-stakes situations. This detail makes it well-suited for the financial industry because many of the associated decisions alter people’s lives and opportunities.</p>
<p>Maintaining responsible data science practices requires equipping professionals with the skills to detect and mitigate bias in an evolving technological landscape. A 2025 study involved a biased dataset that contained a <a href="https://www.psu.edu/news/bellisario-college-communications/story/most-users-cannot-identify-ai-bias-even-training-data">disproportionately high number</a> of white people with happy faces. This issue caused the AI algorithm to correlate race with emotional expressions.</p>
<p>The results of three experiments with human participants showed that most individuals did not notice the bias. This result shows why data scientists need ongoing education to spot less-obvious examples.</p>
</section>
<section id="stay-vigilant-to-maintain-fairness" class="level2">
<h2 class="anchored" data-anchor-id="stay-vigilant-to-maintain-fairness">Stay Vigilant to Maintain Fairness</h2>
<p>Your work on algorithms for credit scoring could adversely affect people’s lives and leave them with no way to contest unfavorable outcomes. Being a responsible data scientist means understanding the numerous risk factors and the controllable factors to minimize harm. Remaining aware of emerging AI applications in the financial industry and regularly meeting with colleagues to discuss ways forward increases fairness for everyone.</p>
<div class="article-btn">
<p><a href="../../../../../../applied-insights/index.html">Explore more data science ideas</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the author:</dt>
<dd>
<a href="https://devinpartida.com/">Devin Partida</a> is a data science and technology writer, as well as the Editor-in-Chief of ReHack.com. Her work has been featured on Hackernoon, TechTarget, DZone and others.
</dd>
</dl>
<div class="g-col-12 g-col-md-6">
<p><strong>Copyright and licence</strong> : © 2026 Devin Partida <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"> <img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"> </a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">International licence</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<p><strong>How to cite</strong> :<br>
Partida, Devin. 2026. “<strong>Understanding and Addressing Algorithmic Bias: a Credit Scoring Case Study</strong>.” <em>Real World Data Science</em>, 2026. <a href="https://realworlddatascience.net/applied-insights/case-studies/posts/2026/02/11/algorithmic-bias-credit-scoring.html">URL</a></p>
</div>
</div>
</div>


</div>
</section>

 ]]></description>
  <category>Ethics</category>
  <category>Algorithms</category>
  <guid>https://realworlddatascience.net/applied-insights/case-studies/posts/2026/02/11/algorithmic_bias_credit_scoring.html</guid>
  <pubDate>Wed, 11 Feb 2026 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/applied-insights/case-studies/posts/2026/02/11/images/thumbcredit.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Colorado’s AI Law Pause: What It Means for People Working in Data Science</title>
  <dc:creator>Dr. Stefani Langehennig, University of Denver Daniels College of Business</dc:creator>
  <link>https://realworlddatascience.net/the-pulse/posts/2026/02/02/colorado-AI.html</link>
  <description><![CDATA[ 





<p>In 2024, Colorado became the first U.S. state to pass a <a href="https://leg.colorado.gov/bills/sb24-205">comprehensive law</a> aimed at regulating “high-risk” artificial intelligence systems-models used in areas such as hiring, housing, credit, and healthcare. The law adopted a risk-based approach, placing additional obligations on systems that shape consequential decisions, including requirements around documentation, monitoring, and human oversight. Less than a year later, lawmakers delayed its implementation and began reconsidering key provisions, citing uncertainty about feasibility, cost, and enforcement.</p>
<p>Colorado’s approach drew explicitly on international models, most notably the <a href="https://artificialintelligenceact.eu/">European Union’s AI Act</a>, which similarly classifies AI systems by risk and ties higher-risk uses to stronger accountability requirements. Colorado’s experience is not only a story about state politics. It serves as a useful case study for a more practical question: what happens when ambitious AI governance principles meet the realities of building and maintaining production data systems?</p>
<p>For data scientists, analysts, machine learning engineers, and others responsible for real-world data products, this moment signals that AI governance is no longer a peripheral policy concern. It is becoming an operational constraint.</p>
<section id="from-governance-principles-to-technical-work" class="level2">
<h2 class="anchored" data-anchor-id="from-governance-principles-to-technical-work">From Governance Principles to Technical Work</h2>
<p>Colorado’s law followed a pattern increasingly visible in global AI governance, particularly the European Union’s AI Act. These frameworks share a risk-based logic in that systems that influence consequential decisions face higher expectations for transparency, oversight, and accountability.</p>
<p>At a high level, these expectations (fairness, consumer protection, responsible use) sound abstract. In practice, they translate directly into technical work:</p>
<ul>
<li>Clear documentation of model purpose, training data, and limitations</li>
<li>Records showing where data comes from and how it changes over time</li>
<li>Reproducible experiments and versioned artifacts</li>
<li>Ongoing monitoring for performance drift and unintended impacts</li>
<li>Defined processes for human review and intervention</li>
</ul>
<blockquote class="blockquote">
<p>None of this lives in legislation. It lives in scripts, workflows, dashboards, deployment systems, and operational infrastructure.</p>
</blockquote>
<p>Colorado’s stalled implementation of AI policy surfaced a familiar pattern, as many organizations are well equipped to optimize model performance, but far less prepared to operationalize accountability at scale. The friction emerged not because governance goals were controversial, but because the supporting technical infrastructure was uneven.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/the-pulse/posts/2026/02/02/images/thumb.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</section>
<section id="why-uncertainty-becomes-a-design-risk" class="level2">
<h2 class="anchored" data-anchor-id="why-uncertainty-becomes-a-design-risk">Why Uncertainty Becomes a Design Risk</h2>
<p>One challenge Colorado encountered was definitional ambiguity. For example, what qualifies as “high risk”, what safeguards are sufficient, and how should harms should be assessed? These questions are not merely legal - they are technical and context dependent.</p>
<p>Different data sources, deployment approaches, and users lead to different answers. For teams building data systems today, that uncertainty creates risk. When teams cannot easily see how data moves through a system, how models change over time, or how decisions are produced, adapting later becomes costly and disruptive.</p>
<p>Recent federal signals add another layer of complexity. President Trump’s executive order discouraging state-level AI regulation aims to reduce fragmented policy on AI, but it does not replace state experimentation with a concrete national policy. Teams now operate in a moving landscape shaped by state initiatives, evolving federal priorities, and international regimes like the EU AI Act. In this environment, aiming for minimal compliance is risky. Teams are better served by designing systems that are flexible and easy to observe from the start.</p>
</section>
<section id="responsibility-does-not-end-at-deployment" class="level2">
<h2 class="anchored" data-anchor-id="responsibility-does-not-end-at-deployment">Responsibility Does Not End at Deployment</h2>
<p>A lesson emerging from both policy debates and practice is that accountability does not stop when a model goes live. Responsibility shifts across teams over time, from data scientists to engineers, product owners, operators, and decision-makers.</p>
<p>This challenge is the focus of the <a href="https://senseaboutscience.org/responsible-handover-of-ai/">Responsible Handover of AI framework</a> developed by <a href="https://senseaboutscience.org/">Sense about Science</a>, which emphasizes the need for clear transitions of responsibility as AI systems move from development into real-world use. Rather than treating deployment as a handoff to “the business”, the framework highlights the risks that arise when assumptions, limitations, and responsibilities are not carried forward with the system.</p>
<p>For practitioners, this framing maps governance concerns onto familiar operational questions, such as who monitors systems after deployment, which development assumptions still matter in production, how limitations are communicated to users, and what happens when systems are updated or handed over to new teams.</p>
<p>Without explicit handover practices, accountability gaps emerge because responsibility becomes diffuse as systems evolve. From this perspective, many regulatory requirements are not adding entirely new work, rather they formalize practices teams already rely on. This includes documentation that travels with systems, monitoring in production, and clear escalation paths when something goes wrong.</p>
</section>
<section id="practical-steps-teams-can-take-now" class="level2">
<h2 class="anchored" data-anchor-id="practical-steps-teams-can-take-now">Practical Steps Teams Can Take Now</h2>
<p>Regardless of how U.S. and international regulation ultimately settles, many investments pay off immediately while reducing future risk, including:</p>
<ul>
<li><em>Standardizing documentation</em>. Ensure model summaries and data descriptions travel with systems as they move between teams</li>
<li><em>Build end-to-end visibility</em>. Version datasets, features, models, and configurations so results can be reproduced</li>
<li><em>Instrument monitoring early</em>. Track input drift, unstable predictions, performance decay, and downstream impacts once systems are in production</li>
<li><em>Clarify governance workflows</em>. Define who approves releases, who monitors systems, and how responsibility shifts over time</li>
<li><em>Translate risk for leadership</em>. Gaps in documentation and visibility tend to come back later as messy, expensive fixes; addressing them early saves time and pain</li>
</ul>
<blockquote class="blockquote">
<p>These practices are not limited to machine learning. Any system that informs decisions can create similar accountability challenges.</p>
</blockquote>
</section>
<section id="governance-lives-in-the-data-stack" class="level2">
<h2 class="anchored" data-anchor-id="governance-lives-in-the-data-stack">Governance Lives in the Data Stack</h2>
<p>There’s still no settled agreement on how AI should be governed. But for people building real-world data systems, its implications are already concrete. Accountability increasingly lives in the data stack in how workflows are instrumented, how models are monitored, and how decisions can be examined after the fact.</p>
<p>This is not simply about regulatory compliance. It is about building systems that are transparent, resilient, and trustworthy at scale. Organizations that treat governance as a core technical problem (rather than an external policy constraint imposed later) will be best positioned to navigate whatever regulatory balance ultimately emerges.</p>
<div class="article-btn">
<p><a href="../../../../../the-pulse/index.html">Explore more data science ideas</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the author:</dt>
<dd>
<a href="https://www.linkedin.com/in/stefani-langehennig-phd-418820144/">Dr.&nbsp;Stefani Langehennig</a> is an Assistant Professor of the Practice in the Business Information &amp; Analytics Department at the University of Denver’s Daniels College of Business. She is also the lead director for the Center for Analytics and Innovation with Data (CAID). As a former data scientist, she has worked with both academic and industry partners in the U.S. and abroad, helping organizations evaluate and implement data analytics and AI solutions. Her research focuses on computational social science methods, the impact of data transparency on political behavior, and legislative policy capacity.
</dd>
</dl>
<div class="g-col-12 g-col-md-6">
<p><strong>Copyright and licence</strong> : © 2026 Stefani Langehennig<br>
<a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"> <img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"> </a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">International licence</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<p><strong>How to cite</strong> :<br>
Langehennig, Stefani. 2026. “<strong>Colorado’s AI Law Pause: What It Means for People Working in Data Science</strong>.” <em>Real World Data Science</em>, 2026. <a href="https://realworlddatascience.net/the-pulse/posts/2026/02/02/colorado-AI.html">URL</a></p>
</div>
</div>
</div>


</div>
</section>

 ]]></description>
  <category>AI governance</category>
  <category>Applied data science</category>
  <category>Data engineering and MLOps</category>
  <category>Technology policy</category>
  <category>Operational risk</category>
  <guid>https://realworlddatascience.net/the-pulse/posts/2026/02/02/colorado-AI.html</guid>
  <pubDate>Fri, 06 Feb 2026 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/the-pulse/posts/2026/02/02/images/thumb.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Beyond Quantification: Interview with Professor Sylvie Delacroix on Navigating Uncertainty with AI</title>
  <dc:creator>Annie Flynn</dc:creator>
  <link>https://realworlddatascience.net/foundation-frontiers/posts/2026/01/29/beyond-quantification-delacroix-interview.html</link>
  <description><![CDATA[ 





<p>We recently published a <a href="https://realworlddatascience.net/foundation-frontiers/datasciencebites/posts/2025/11/21/uncertainty.html"><em>Data Science Bite</em></a> breaking down the first position paper of the newly launched journal, <a href="https://academic.oup.com/rssdat"><em>RSS: Data Science and Artificial Intelligence</em></a>. The paper, <a href="https://academic.oup.com/rssdat/article/1/1/udaf002/8317136"><em>Beyond Quantification: Navigating Uncertainty in Professional AI Systems</em></a>, argues that if AI is truly to support professional decision-making in high-stakes fields, we must move beyond probabilistic measures and use participatory approaches that allow experts to collectively express and navigate non-quantifiable forms of uncertainty.</p>
<p><em>Real World Data Science</em> recently had the opportunity to speak to the paper’s lead author, Professor Sylvie Delacroix, about how AI can better support human judgment, why it is crucial to recognise forms of uncertainty that can’t be reduced to numbers, and how participatory design can make AI a true partner, rather than a replacement, for professionals.</p>
<p>Watch the full interview below and scroll down for key takeaways and some analysis.</p>
<hr>
<section id="interview-beyond-quantification-and-uncertainty-in-ai" class="level2">
<h2 class="anchored" data-anchor-id="interview-beyond-quantification-and-uncertainty-in-ai">Interview: Beyond Quantification and Uncertainty in AI</h2>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/tJDy293oqPk" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<hr>
</section>
<section id="key-takeaways-at-a-glance" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways-at-a-glance">Key Takeaways at a Glance</h2>
<section id="not-all-uncertainty-is-measurable" class="level3">
<h3 class="anchored" data-anchor-id="not-all-uncertainty-is-measurable">Not all uncertainty is measurable</h3>
<p>AI often focuses on quantifiable uncertainty, like probabilities or confidence scores, but ethical and contextual uncertainties are equally important in professions like healthcare, education, and justice.</p>
<blockquote class="blockquote">
<p>“The problem is that if we design these systems in a way that means they’re only capable of communicating these quantifiable types of uncertainty, we risk systematically undermining the significance and importance of non-quantifiable types of uncertainty… which are fundamentally ethical and contextual.”</p>
</blockquote>
</section>
<section id="participatory-ai-matters" class="level3">
<h3 class="anchored" data-anchor-id="participatory-ai-matters">Participatory AI matters</h3>
<p>Systems should let professionals shape how uncertainty is expressed, supporting collaboration and collective judgment rather than replacing human decision-making.</p>
<blockquote class="blockquote">
<p>“The intervention that we want is ideally one that means the systems are mouldable by the users over time… that’s what we mean by participatory interfaces.”</p>
</blockquote>
</section>
<section id="the-goal-is-to-support-and-foster-human-intelligence-not-replace-it" class="level3">
<h3 class="anchored" data-anchor-id="the-goal-is-to-support-and-foster-human-intelligence-not-replace-it">The goal is to support and foster human intelligence, not replace it</h3>
<p>The most valuable AI tools help professionals reflect, reason, and intuitively navigate complex situations, rather than just process more data faster.</p>
</section>
<section id="real-world-ai-is-already-in-use" class="level3">
<h3 class="anchored" data-anchor-id="real-world-ai-is-already-in-use">Real-world AI is already in use</h3>
<p>GPs, teachers, and other professionals are using AI in sensitive ways, sometimes for informal “sense-making” conversations that influence moral judgments.</p>
</section>
<section id="small-refinements-have-big-impact" class="level3">
<h3 class="anchored" data-anchor-id="small-refinements-have-big-impact">Small refinements have big impact</h3>
<p>Features like expressing incompleteness, ethical uncertainty, or alternative perspectives can significantly strengthen professional agency when developed with participatory input.</p>
<blockquote class="blockquote">
<p>“You could imagine a GP flagging an output and saying… it turns out the output could have been very dangerous because it didn’t include key diagnostic tools… and you could then imagine an interesting conversation with other GPs to figure out together how incompleteness should be expressed.”</p>
</blockquote>
</section>
<section id="efficiency-should-not-undermine-judgment" class="level3">
<h3 class="anchored" data-anchor-id="efficiency-should-not-undermine-judgment">Efficiency should not undermine judgment</h3>
<p>AI can save time, but systems must preserve the dynamic, normative nature of the professional practices within which they are deployed to ensure long-term effectiveness.</p>
</section>
<section id="the-time-to-act-is-now" class="level3">
<h3 class="anchored" data-anchor-id="the-time-to-act-is-now">The time to act is now</h3>
<p>Professionals, designers, and regulators need to collectively shape AI tools before design choices are frozen, ensuring they support human-centred, ethical practice.</p>
<blockquote class="blockquote">
<p>“If professionals just wait for regulation to intervene, there’s a risk that regulation will arrive only when design choices are frozen… we all have agency in this; we can’t afford to be passive.”</p>
</blockquote>
<hr>
</section>
</section>
<section id="join-the-conversation" class="level2">
<h2 class="anchored" data-anchor-id="join-the-conversation">Join the conversation</h2>
<p><a href="https://academic.oup.com/rssdat"><em>RSS: Data Science and Artificial Intelligence</em></a> has an open <a href="https://academic.oup.com/rssdat/pages/call-for-papers-uncertainty-in-the-era-of-ai">call for submissions</a> responding to the paper.</p>
<p>Sylvie Delacroix’s work is a call to action for data scientists, designers, and professionals alike. We have a window of opportunity to shape AI systems that encourage humans to keep re-articulating the values they care about.</p>
<p>We want to hear from you. As AI tools become more integrated into high-stakes professions, how can we ensure that systems support human judgment in all its facets rather than simply optimising for efficiency?</p>
<p>Read the full paper <a href="https://academic.oup.com/rssdat/article/1/1/udaf002/8317136">here</a>, or our accessible digest <a href="https://realworlddatascience.net/foundation-frontiers/datasciencebites/posts/2025/11/21/uncertainty.html">here</a>, and join the conversation about building AI tools that truly serve people, not just processes.</p>
<div class="article-btn">
<p><a href="../../../../../foundation-frontiers/index.html">Explore more data science ideas</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the speaker:</dt>
<dd>
<a href="https://delacroix.uk/">Professor Sylvie Delacroix</a>is the Inaugural Jeff Price Chair in Digital Law at Kings College London. She is also the director of the <a href="https://www.kcl.ac.uk/research/centre-for-data-futures">Centre for Data Futures</a> and a visiting professor at Tohoku University. Her research focuses on the role played by habit within ethical agency, the social sustainability of the data ecosystem that makes generative AI possible and bottom-up data empowerment.
</dd>
</dl>
</div>
</div>


</div>
</section>

 ]]></description>
  <category>Interviews</category>
  <category>AI</category>
  <category>Ethics</category>
  <category>Uncertainty</category>
  <guid>https://realworlddatascience.net/foundation-frontiers/posts/2026/01/29/beyond-quantification-delacroix-interview.html</guid>
  <pubDate>Thu, 29 Jan 2026 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/foundation-frontiers/posts/2026/01/29/images/thumb.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Why Data Quality Is the New Competitive Edge For Data Scientists</title>
  <link>https://realworlddatascience.net/foundation-frontiers/posts/2026/01/27/data-qual-is-competitive-edge.html</link>
  <description><![CDATA[ 





<section id="discipline-of-data-science" class="level2">
<h2 class="anchored" data-anchor-id="discipline-of-data-science">Discipline of Data Science</h2>
<p>Data science is a relatively recent field compared to the disciplines that it consists of, namely statistics and computer science. New knowledge and disciplines often arise as combinations of existing fields and data science is no exception. Born into a world with increasingly large datasets, data science combined statistical methods with the tools of computer science to analyze large amounts of data.</p>
<p>As a discipline, data science gained popularity in industry and academia in the early 2000s and in part due to an influential paper published in the Harvard Business Review <span class="citation" data-cites="davenport2012sexiest">(1)</span>. This paper carried the provocative title of “Data Scientist: The Sexist Job of the Twenty-first Century”. After its publication, universities developed data science programs, governments poured funding into Big Data initiatives and organizations built data science teams to tackle problems. Powerful machine learning algorithms such as neural nets, random forests and ensemble methods provided ways to develop complex models that could be used on these large datasets.</p>
<p>As Hoerl noted in an earlier piece <span class="citation" data-cites="hoerl2025future">(2)</span>:</p>
<blockquote class="blockquote">
<p>“There was a hiring rush for data scientists, not just in technology companies, but in virtually all sectors of the economy. For example, GE hired a new Chief Digital Officer from Oracle, Bill Ruh, in 2011. Ruh opened a new Software Center (later renamed “GE Digital”) in San Ramon, California in 2012, and by 2016 had hired 1,400 data scientists there.”</p>
</blockquote>
</section>
<section id="current-state-of-play-with-generative-ai" class="level2">
<h2 class="anchored" data-anchor-id="current-state-of-play-with-generative-ai">Current State of Play with Generative AI</h2>
<p>While data analyses based on text data such as NLP (natural language processing) have been around for a while, a new type of approach has quickly become widespread with the rapid rise of generative AI methods. These models, known as LLMs (large language models), have entered the everyday vernacular as all organizations grapple with how to use these tools. With the debut of ChatGPT in 2023, these LLMs have become increasingly sophisticated, trained on a wider variety of data sources and optimized for a variety of different scenarios.</p>
<p>The accuracy of these models depends on the quality of the training data used. While LLMs are known to hallucinate or give inaccurate answers, they tend to perform better in situations where the training data is precise and exact, with less nuance than language often carries. As a result, LLMs can produce large amounts of computer code based on large amounts of training data based on accurate computer code. Some specialized LLMs (e.g.&nbsp;<a href="https://claude.ai/">Claude Code</a>) have been specifically designed to generate accurate code that can access, clean, combine, analyze and visualize data. While LLMs are not perfect in code generation, they can increase the efficiency of an experienced coder.</p>
<p>As a result of these LLMs, less knowledge is required to analyze and work with large datasets. A user can provide a specific prompt on the business question or research objective, upload the relevant data and have an LLM provide a relevant data analysis, complete with the underlying code used to generate that analysis.</p>
</section>
<section id="the-new-data-scientist" class="level2">
<h2 class="anchored" data-anchor-id="the-new-data-scientist">The “New” Data Scientist</h2>
<p>As LLMs improve in their ability to quickly generate vast amounts of accurate code, what does that mean for a discipline which has prided itself on its data wrangling skills? As Davenport and Patil noted “data scientists’ most basic, universal skill is the ability to write code” <span class="citation" data-cites="davenport2012sexiest">(1)</span>. Data science has seen coding as a viable career path.</p>
<p>When a skill becomes accessible to a wider variety of people and can be automated, how does one distinguish themselves in an organization? When coding can be done by AI tools, what happens to those who are known for their coding abilities? For a discipline to be recognized as a discipline, it must have some distinguishing characteristic that defines it as different from other disciplines. In addition, disciplines become more valuable and prominent as their contribution to society grows.</p>
<p>So, what are the skills that data scientists have that can’t be done well by AI? While AI capabilities are rapidly increasing, we do believe there are things that may be beyond the reach of AI for the time being.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/foundation-frontiers/posts/2026/01/27/images/infographic.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>We believe that the greatest factor limiting AI is data quality. There seems to be a growing consensus of the importance of data quality as noted in Davenport, Hoerl, and Redman <span class="citation" data-cites="davenport2025unstructured">(3)</span>, Davenport and Tiwari <span class="citation" data-cites="davenport2024generative">(4)</span>, and Redman <span class="citation" data-cites="redman2020source">(5)</span> — as well as <a href="https://realworlddatascience.net/foundation-frontiers/posts/2025/10/30/data-detectives.html">recent Real World Data Science pieces</a>. As the adage goes, “garbage in, garbage out”. With poor, inaccurate data used in training, the resulting AI output will also be poor and inaccurate. A related fear is that, as AI models start to use AI generated content as a data source, a recursive loop happens that degrades the quality of any AI output <span class="citation" data-cites="shumailov2024collapse">(6)</span>.</p>
<p>Discussions of data quality are limited in books, university courses and training programs. When they do occur, they are restricted to the question of “are the data right?” and discussions of data cleaning. Data cleaning is often focused on eliminating outliers or invalid points. While there is often a reason to remove invalid points, those outliers can sometimes be the source of valuable insights.</p>
<p>However, data quality is much more than data cleaning or checking to make sure the data are accurate. There is an element of contextual understanding and process knowledge that enables the data scientist to properly prepare the data for analysis. We are skeptical of AI’s ability to fully understand context and the nuances of assumptions that go into data analysis. In an earlier piece, Jensen provided some examples of the limitations of AI when it comes to proper data cleaning <span class="citation" data-cites="jensen2024cleaning">(7)</span>. For any set of data, subject matter knowledge of how the data were collected and what they represent is crucial to a proper analysis.</p>
<p>This creates an opportunity for data scientists to become more valuable. By employing probing questions to better understand the context of the data, they will be in a better position to identify data quality issues and ways to improve the data quality, thus leading to better model output.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>As coding becomes easier in an AI-enabled world - where anyone can code and analyze data - the skill set of a data scientist becomes less unique. Data scientists were once in high demand because they set themselves apart as coding wizards who could wrangle large datasets and extract insights. To remain successful and continue to deliver value, data scientists must now pivot their skillset. The real limiting factor in successful data science is data quality. A renewed focus on owning, improving and governing data quality will not only strengthen outcomes but also provide future job security and increase the value data scientists bring to organisations.</p>
<p><em>Note that this article is based on the following paper and contains some of the same ideas: Hoerl, Roger W. 2025. <a href="https://www.tandfonline.com/doi/full/10.1080/08982112.2025.2556222">“The Future of Statistics in an AI Era.”</a> Quality Engineering, published September 10, 2025.</em></p>
<p>You can find out more about the <em>Real World Data Science</em> stance on data quality from our article <a href="https://realworlddatascience.net/foundation-frontiers/posts/2025/10/30/data-detectives.html">‘Why We Should All Be Data Quality Detectives’</a>.</p>
<div class="article-btn">
<p><a href="../../../../../foundation-frontiers/index.html">Explore more data science ideas</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the authors:</dt>
<dd>
<a href="https://www.linkedin.com/in/roger-hoerl-6b6b3a5/">Roger Hoerl</a> is Brate-Peschel Professor of Statistics at Union College, after previously heading the Applied Statistics Laboratory at <a href="https://www.ge.com/news/reports/tag/ge%20global%20research">GE Global Research</a> for many years. He has been elected to the International Statistical Institute and the International Academy for Quality, recieved numerous statistic awards, and authored five books in the areas of statistics and business improvements.
</dd>
<dd>
<a href="https://www.linkedin.com/in/willis-jensen-305bba6/">Willis Jensen</a> is data and analytics expert, currently Senior Manager of People Analytics and Business Intelligence at <a href="https://chghealthcare.com/">CHG Healthcare</a>. He is an Adjunct Professor of Statistics at Brigham Young University, <a href="https://willisjensen.substack.com/">writes on Substack</a> and is a member of the Real World Data Science <a href="https://realworlddatascience.net/the-pulse/editors-blog/posts/2022/10/18/meet-the-team.html">editorial board</a>.
</dd>
</dl>
<div class="g-col-12 g-col-md-6">
<p><strong>Copyright and licence</strong> : © 2026 Roger Hoerl and Willis Jensen<br>
<a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"> <img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"> </a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">International licence</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<p><strong>How to cite</strong> :<br>
Hoerl, Roger. Jensen, Willis. 2026. “<strong>Why Data Quality Is the New Competitive Edge for Data Scientists</strong>.” <em>Real World Data Science</em>, 2026. <a href="https://realworlddatascience.net/foundation-frontiers/tutorials/posts/2026/12/data-qual-is-competetive-edge.html">URL</a></p>
</div>
</div>
</div>


</div>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body">
<div id="ref-davenport2012sexiest" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Davenport TH, Patil DJ. Data scientist: The sexiest job of the 21st century. Harvard Business Review. 2012 Oct;70–6. </div>
</div>
<div id="ref-hoerl2025future" class="csl-entry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Hoerl RW. <a href="https://doi.org/10.1080/08982112.2025.2556222">The future of statistics in an AI era</a>. Quality Engineering. 2025; </div>
</div>
<div id="ref-davenport2025unstructured" class="csl-entry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Davenport TH, Hoerl RW, Redman TC. To create value with AI, improve the quality of your unstructured data. Harvard Business Review [Internet]. 2025 May; Available from: <a href="https://hbr.org/2025/05/to-create-value-with-ai-improve-the-quality-of-your-unstructured-data">https://hbr.org/2025/05/to-create-value-with-ai-improve-the-quality-of-your-unstructured-data</a></div>
</div>
<div id="ref-davenport2024generative" class="csl-entry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Davenport TH, Tiwari P. Is your company’s data ready for generative AI? Harvard Business Review [Internet]. 2024 Mar; Available from: <a href="https://hbr.org/2024/03/is-your-companys-data-ready-for-generative-ai">https://hbr.org/2024/03/is-your-companys-data-ready-for-generative-ai</a></div>
</div>
<div id="ref-redman2020source" class="csl-entry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Redman TC. To improve data quality, start at the source. Harvard Business Review [Internet]. 2020 Feb; Available from: <a href="https://hbr.org/2020/02/to-improve-data-quality-start-at-the-source">https://hbr.org/2020/02/to-improve-data-quality-start-at-the-source</a></div>
</div>
<div id="ref-shumailov2024collapse" class="csl-entry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">Shumailov I, Shumaylov Z, Zhao Y, Papernot N, Anderson R, Gal Y. AI models collapse when trained on recursively generated data. Nature. 2024;631(8022):755–9. </div>
</div>
<div id="ref-jensen2024cleaning" class="csl-entry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">Jensen WA. Can data cleaning be automated? [Internet]. 2024. Available from: <a href="https://willisjensen.substack.com/p/can-data-cleaning-be-automated">https://willisjensen.substack.com/p/can-data-cleaning-be-automated</a></div>
</div>
</div></section></div> ]]></description>
  <category>Data quality</category>
  <guid>https://realworlddatascience.net/foundation-frontiers/posts/2026/01/27/data-qual-is-competitive-edge.html</guid>
  <pubDate>Tue, 27 Jan 2026 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/foundation-frontiers/posts/2026/01/27/images/thumb.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>RWDS Big Questions: What Are the Key Challenges Facing Data Scientists Today?</title>
  <dc:creator>Annie Flynn</dc:creator>
  <link>https://realworlddatascience.net/the-pulse/posts/2026/01/21/rwds-big-questions-challenges-today.html</link>
  <description><![CDATA[ 





<p>Data science is operating in a moment of paradox. We have more data, more tools, and more computational power than ever before — yet many of the core challenges feel stubbornly human.</p>
<p>In this video, experienced practitioners from varied backgrounds reflect on what they see as the biggest obstacles facing the profession today.</p>
<p>This video is part of our thought-leadership series, RWDS Big Questions, where members of our community answer one key question in multiple ways, offering diverse perspectives from across the industry.</p>
<p>Watch the video below to hear insights that span technical, organisational, and personal dimensions. Together, they reveal a set of deeply connected themes and, importantly, opportunities for the field to mature. Scroll down for analysis and practical takeaways.</p>
<hr>
<section id="video-what-are-the-key-challenges-facing-data-scientists-today" class="level2">
<h2 class="anchored" data-anchor-id="video-what-are-the-key-challenges-facing-data-scientists-today">Video: What Are the Key Challenges Facing Data Scientists Today?</h2>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/4WgCvhkTCiQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/the-pulse/posts/2026/01/21/images/challengesinfo1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</section>
<section id="the-patterns-behind-the-problems" class="level2">
<h2 class="anchored" data-anchor-id="the-patterns-behind-the-problems">The Patterns Behind the Problems</h2>
<p>Although the challenges raised span technical, organisational, and personal domains, they are connected by a small number of deeper themes that shape modern data science.</p>
<section id="the-gap-between-capability-and-understanding" class="level3">
<h3 class="anchored" data-anchor-id="the-gap-between-capability-and-understanding">The gap between capability and understanding</h3>
<p>Across multiple perspectives, there is a recurring mismatch between what our tools can do and how well we understand their limitations. From AI systems trained on poor-quality data to models built on artificial or incomplete datasets, technical capability is often outpacing validation, interpretation, and critical scrutiny.</p>
<p>This gap widens further as advanced tools become more accessible to non-specialists, increasing the risk of confident but flawed outputs.</p>
</section>
<section id="speed-amplifies-existing-weaknesses" class="level3">
<h3 class="anchored" data-anchor-id="speed-amplifies-existing-weaknesses">Speed amplifies existing weaknesses</h3>
<p>Pressure to move quickly doesn’t create new problems so much as it magnifies existing ones. Poor data quality, weak validation, and organisational silos become far more consequential when decisions must be made rapidly.</p>
<p>The demand for instant answers leaves little room for reflection, experimentation, or uncertainty — despite these being essential to good data science.</p>
</section>
<section id="data-science-is-constrained-by-its-environment" class="level3">
<h3 class="anchored" data-anchor-id="data-science-is-constrained-by-its-environment">Data science is constrained by its environment</h3>
<p>Many of the challenges raised point away from algorithms and towards the environments in which they are deployed. Organisational readiness, digital infrastructure, and especially incentive structures strongly shape how data science is practiced and whether it creates impact.</p>
<p>When teams are rewarded for control rather than collaboration, silos persist, data sharing becomes risky, and even the most robust models struggle to influence decisions.</p>
</section>
<section id="uncertainty-is-a-constant" class="level3">
<h3 class="anchored" data-anchor-id="uncertainty-is-a-constant">Uncertainty is a constant</h3>
<p>The personal experience of data scientists mirrors these structural challenges. In a field defined by rapid change, uncertainty about where to focus, what to learn, and how to stay relevant is common.</p>
<p>This is not just a skills issue, but a signal that data science is still evolving, without a single, stable definition of what “good” looks like.</p>
</section>
</section>
<section id="looking-ahead" class="level2">
<h2 class="anchored" data-anchor-id="looking-ahead">Looking Ahead</h2>
<p>Taken together, these themes suggest that the biggest challenges in data science are not isolated problems to be solved individually. They are interconnected tensions between speed and rigour, access and expertise, innovation and organisational inertia.</p>
<p>Addressing them requires interdisciplinary, systems-level thinking.</p>
<p>Which of these challenges resonates most with your own experience in data science? How can practitioners use these tensions as inflection points to actively shape the field, rather than simply react to it?</p>
<div class="article-btn">
<p><a href="../../../../../applied-insights/index.html">Explore more data science ideas</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the author:</dt>
<dd>
<a href="https://www.linkedin.com/in/annieroseflynn/">Annie Flynn</a> is Head of Content at the <a href="rss.org.uk">Royal Statistical Society</a>.
</dd>
</dl>
<div class="g-col-12 g-col-md-6">
<p><strong>Copyright and licence</strong> : © 2026 Annie Flynn<br>
<a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"> <img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"> </a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">International licence</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<p><strong>How to cite</strong> :<br>
Flynn, Annie 2026. “<strong>RWDS Big Questions: What are the Key Challenges Facing Data Scientists Today?</strong>” <em>Real World Data Science</em>, 2026. <a href="https://realworlddatascience.net/the-pulse/posts/2026/01/rwds-big-questions-challenges-today.html">URL</a></p>
</div>
</div>
</div>


</div>
</section>

 ]]></description>
  <category>Big Questions</category>
  <category>Data science</category>
  <category>Practice</category>
  <category>Careers</category>
  <guid>https://realworlddatascience.net/the-pulse/posts/2026/01/21/rwds-big-questions-challenges-today.html</guid>
  <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/the-pulse/posts/2026/01/21/images/thumb.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Why 95% Of AI Projects Fail and How to Change the Odds</title>
  <dc:creator>Lee Clewley</dc:creator>
  <link>https://realworlddatascience.net/applied-insights/case-studies/posts/2026/01/12/why-95-percent-of-ai-projects-fail.html</link>
  <description><![CDATA[ 





<p>Artificial intelligence is now capable of performing substantive work across scientific, medical, industrial and economic domains, yet organisational experience remains uneven. Most large firms have experimented with AI; very few report material gains. MIT’s NANDA study of enterprise generative AI estimates that only 5 percent of custom tools reach production with measurable impact on profit and loss <span class="citation" data-cites="mitnanda2025">(1)</span>. Early analysis from MIT’s Iceberg project points in the same direction at task level: current systems could already support far more work than they do today, but observed use remains shallow, concentrated in a narrow set of roles and often confined to standalone ‘copilot’ tools rather than embedded in core workflows <span class="citation" data-cites="chopra2025">(2)</span>.</p>
<p>For anyone who has sat through AI vendor demonstrations, the pattern is familiar: a procession of polished prototypes that rarely change how important decisions are made. As one Chief Information Officer put it <span class="citation" data-cites="mitnanda2025">(1)</span>: ‘We’ve seen dozens of demos this year. Maybe one or two are genuinely useful. The rest are wrappers or science projects.’</p>
<p>Two caveats matter here. Many pilots are exploratory by design, so failure to reach production may not necessarily be a failure in a scientific sense. Profit based metrics also miss scientific and operational learning, which often matters more in research intensive organisations <span class="citation" data-cites="ransbotham2020">(3)</span>; <span class="citation" data-cites="bcg2024">(4)</span>; <span class="citation" data-cites="deloitte2024">(5)</span>; <span class="citation" data-cites="schlegel2023">(6)</span>. Even allowing for those points, evidence across independent surveys is remarkably consistent: most organisations struggle to turn AI model capability into repeated value, and with an estimated 95% failure rate, the question becomes: how do we change the odds? <span class="citation" data-cites="mitnanda2025">(1)</span>; <span class="citation" data-cites="ransbotham2020">(3)</span>; <span class="citation" data-cites="bcg2024">(4)</span>; <span class="citation" data-cites="deloitte2024">(5)</span>; <span class="citation" data-cites="schlegel2023">(6)</span>.</p>
<section id="three-reasons-for-failure" class="level2">
<h2 class="anchored" data-anchor-id="three-reasons-for-failure">Three Reasons for Failure</h2>
<p>There are three important reasons why so many projects fail that are often overlooked in the literature. First, the problem is often mis-specified: it is framed by technologists or vendors rather than co-owned by the domain experts who understand the decision and bear the consequences. Second, leadership expectations are frequently misaligned, short time horizons and demands for certainty collide with a technology that improves through iteration and organisational learning. Third, many deployments are brittle: they assume stability in a domain defined by rapid model change and rising user expectations, when what is needed is an engineered system designed to adapt.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/applied-insights/case-studies/posts/2026/01/12/images/text1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>This article draws on two decades of work building AI systems for medicine discovery at GSK <span class="citation" data-cites="gskjules2024">(7)</span> and at Tangram <span class="citation" data-cites="tangram2025">(8)</span> to argue that success rests on three principles, in roughly this order:</p>
<ul>
<li>integrated subject matter expertise;</li>
<li>patient and informed executive leadership;</li>
<li>and building AI systems that learn with the organisation.</li>
</ul>
<p>The sections that follow develop each element in the context of drug discovery and show how an AI platform can help move real projects into the small minority that deliver value.</p>
</section>
<section id="essential-contexts-outside-our-focus" class="level2">
<h2 class="anchored" data-anchor-id="essential-contexts-outside-our-focus">Essential Contexts Outside Our Focus</h2>
<p>Before turning to the three principles developed below, it is worth acknowledging four adjacent domains that I will not treat in detail here, each of which now has a substantial literature of its own. First, organisational scholars have long shown that new information systems reshape power, status and discretion, making the politics of implementation as important as the technology itself <span class="citation" data-cites="markus1983">(9)</span>; <span class="citation" data-cites="orlikowski1992">(10)</span>. Second, governance, regulation and responsible AI practice which includes everything from model documentation and auditability to privacy, robustness and safety, have become central determinants of what can be deployed in practice, especially in regulated sectors <span class="citation" data-cites="paleyes2022">(11)</span>; <span class="citation" data-cites="ey2025">(12)</span>; <span class="citation" data-cites="capgemini2024">(13)</span>. Third, there is an emerging body of work on workforce transformation: how AI complements or displaces skills, how hybrid human–AI roles are designed, and how training, trust and professional bodies mediate adoption <span class="citation" data-cites="chopra2025">(2)</span>; <span class="citation" data-cites="ransbotham2020">(3)</span>; <span class="citation" data-cites="bcg2024">(4)</span>; <span class="citation" data-cites="deloitte2024">(5)</span>. Finally, the question of how to measure value and learn at portfolio scale with existing legacy IT systems (through experimentation, counterfactuals and disciplined comparisons between use cases) is itself a rich field that extends well beyond any single organisation. <span class="citation" data-cites="ransbotham2020">(3)</span>; <span class="citation" data-cites="bcg2024">(4)</span>; <span class="citation" data-cites="deloitte2024">(5)</span>; <span class="citation" data-cites="schlegel2023">(6)</span>; <span class="citation" data-cites="davenport2018">(14)</span>. Each of these strands is critical to understanding why AI succeeds, stalls or remains at a proof-of-concept level.</p>
<p>To maintain focus, I concentrate on three overlooked questions arising from direct experience: how to organise subject matter expertise such that the enterprise owns its AI; how to cultivate genuine leadership ownership; and how to engineer systems that learn and adapt rather than remain isolated demonstrations.</p>
</section>
<section id="principle-1-the-importance-of-building-with-subject-matter-experts" class="level2">
<h2 class="anchored" data-anchor-id="principle-1-the-importance-of-building-with-subject-matter-experts">Principle 1: The importance of building with subject matter experts</h2>
<p>The hardest part of building an AI platform is not the models or the engineers but assembling the subject matter experts (SMEs) who will frame and judge the work. Most commentary treats SMEs as validators, brought in at the end to bless a prototype. It rarely explains how to organise a molecular biologist, a clinician and a chemist so that they can state, in plain terms, what counts as an acceptable outcome.</p>
<p>Most commentators are not operators. They observe patterns across organisations but do not live with the consequences of poor SME integration. This distance between writing and practice shows up in the surveys. Foundry’s 2024 State of the CIO, summarised in MIT Sloan Management Review, reports that 85% of IT leaders see the CIO role as a driver of change, yet only 28% list leading transformation as their top priority <span class="citation" data-cites="foundry2024">(15)</span>]. The people commenting on AI often sit with strategy decks rather than with the unglamorous work of managing technological change and cross-functional coordination.</p>
<p>Drug discovery starkly exposes the gap. The relevant team is wide and requires exceptional coordination. Business and portfolio leaders understand how projects absorb capital and create value whereas molecular biologists and geneticists judge whether a gene is plausibly causal for a disease. Clinicians think through trial design and patient risk. Chemists know what can be made and delivered. Statisticians, AI engineers and data scientists understand models, data pipelines, experimental design and evaluation. This diversity is a strength but requires a lot more from leaders of such teams. When these groups work as separate silos, the result is a generic set of tools whose outputs are not trusted by the users and whose inputs are irrelevant. When these experts can operate as a single team, the conversation starts with a simple set of questions. Which decisions are we trying to improve? How will we know if we have succeeded? What data and statistical methods count as acceptable evidence? Which risks are we prepared to take and which are not negotiable?</p>
<p>At Tangram, that joint framing often collapses into one critical choice: which disease do we want to target for drug development, and which gene is driving it? That decision already embeds genetics, hepatocyte biology, chemistry, clinical feasibility and commercial context. The role of AI and engineering is then precise. It is to help the group search the vast hypothesis space, structure the evidence and quantify uncertainty, while leaving the final judgement with experts who feel they own the AI platform, can see why the AI has come to the conclusions it has, and also own the consequences.</p>
</section>
<section id="principle-2-patient-and-strategic-executive-leadership." class="level2">
<h2 class="anchored" data-anchor-id="principle-2-patient-and-strategic-executive-leadership.">Principle 2: Patient and strategic executive leadership.</h2>
<p>The second element is executive patience. Research from MIT Sloan shows that firms gaining value from AI tend to run more projects, over more years, with a sustained focus on learning how people and AI work together. <span class="citation" data-cites="ransbotham2020">(3)</span> Leaders in these organisations accept that early returns are small and uneven. They invest in a pipeline of use cases rather than a single bet. They resist what researchers have called the “last mile problem”: AI projects that reach technical proof of concept but never change how work is done. <span class="citation" data-cites="davenport2018">(14)</span> In life sciences this is acute. Discovery timelines are long, data are messy and early signals are faint. Leaders who expect quick, clean returns tend to cycle through pilots without ever building an asset that scientists trust.</p>
<p>Patience does not mean passivity; actually, the opposite is true. It means choosing a small number of important decisions, funding cross-functional teams to attack them, and holding the bar for quality high. It requires senior sponsorship to unblock data access, align incentives across discovery, clinical and commercial groups, and shield long term work from quarterly fashion cycles. When those conditions are in place, AI stops being a sequence of demonstrations and starts to become part of how the organisation thinks: the informed leader knows the difference.</p>
</section>
<section id="principle-3-building-ai-systems-that-learn-with-the-organisation" class="level2">
<h2 class="anchored" data-anchor-id="principle-3-building-ai-systems-that-learn-with-the-organisation">Principle 3: Building AI Systems that learn with the organisation</h2>
<p>The third element is the AI engineering. The MIT findings on the 95 percent figure are instructive: most generative AI projects fail not because the models are weak but because the systems around them are brittle. <span class="citation" data-cites="mitnanda2025">(1)</span>; <span class="citation" data-cites="paleyes2022">(11)</span> Foundation models are dropped into existing workflows with minimal adaptation. There is limited monitoring. Data quality is assumed rather than measured. When something breaks (as it inevitably will in non-deterministic systems) teams revert to manual work.</p>
<p>Modern AI engineering starts from the opposite assumption. Models and tools will change quickly. The surrounding stack must absorb that change without being rebuilt each time. The strategy must be built so that, when the product director hears a new technology is built or an LLM improved, this is always a good day.</p>
<p><strong>Build only what you must.</strong></p>
<p>The sensible principle is to only build the components where your domain expertise creates defensible value. Everything else should be bought. But buying is not effortless. Integration, monitoring, and vendor management drains teams unless you are staffed for it. Organisations that succeed at scale partner with vendors offering systems that learn and adapt; they focus on workflow integration; they deploy tools where process alignment is easiest.</p>
<p>So assuming you have the right staff who are working together, supportive leaders and good vendor relationships the next problem is how to create a platform itself.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/applied-insights/case-studies/posts/2026/01/12/images/text2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p><strong>A useful pattern for a modern AI platform has four parts:</strong></p>
<ul>
<li><strong>First, the data stack.</strong> Discovery teams need a small number of trusted stores for data with clear provenance and at least basic quality checks. For early target selection this means human genetics, expression data, interaction networks, preclinical phenotypes, clinical outcomes and internal experiments. Traceability and reproducibility is critical here. Any claim a model makes about a gene and disease pair should be traceable back to specific pieces of evidence.</li>
<li><strong>Second, the platform needs to be built on modular services rather than monoliths.</strong> Each service has a single responsibility and can be swapped when a better service appears. This keeps the cost of change low and allows teams to combine external tools with internal components in a controlled way.</li>
<li><strong>Third, the system needs to have continuous evaluation.</strong> Every component that answers questions is tested on held-out tasks, with simple metrics for accuracy, faithfulness, and recall, and monitored continually. There should be repeat measures and other tests of robustness. <span class="citation" data-cites="bolton2024">(16)</span> There is no reason not to report error bars in AI and yet they are rarely part of AI publications. Where this matters most is at the interface with non-determinism, inherent in large language models. A good medical AI assistant should give consistent answers even when questions are phrased differently. It should also say it does not know when the information is unclear or incomplete. <span class="citation" data-cites="bolton2024">(16)</span>; <span class="citation" data-cites="ji2023">(17)</span>; <span class="citation" data-cites="gskrambla2024">(18)</span></li>
<li><strong>Fourth, include memory and reinforcement learning so that the system learns.</strong> This is the most difficult component to implement and the one most often deferred. A system that cannot learn from use will make the same mistake repeatedly. Even the most patient users will lose trust and patience. But building memory into production systems, where the model retains context across sessions and improves from feedback, requires specialist expertise in reinforcement learning, retrieval-augmented generation with persistent stores, and the infrastructure to support online learning without catastrophic forgetting <span class="citation" data-cites="ouyang2022">(19)</span>. These skills are in high demand and short supply. The alternative is a system that feels potentially useful in demonstrations but frustrates users in daily work.</li>
</ul>
<p>For this to work, engineering teams need to stay in constant contact with biologists, geneticists, clinicians, chemists and portfolio managers. Together they decide what error rate is acceptable for a triage tool, what form of uncertainty estimate a portfolio board will respect and where human review is mandatory, for example before a new target enters serious preclinical work. Work on human–AI interaction design reinforces this point: systems should explain what they can and cannot do, expose their confidence and make it easy for users to correct them. The hardest part is that the first version is almost never right. Cross-functional teams need patience and ownership. They contribute real examples, refine prompts and evaluation sets, and expect the system to learn from its mistakes. The AI platform must be useful enough, early enough, that experts are willing to spend scarce attention improving it.</p>
</section>
<section id="a-worked-example-target-indication-pairing-in-sirna" class="level2">
<h2 class="anchored" data-anchor-id="a-worked-example-target-indication-pairing-in-sirna">A worked example: target-indication pairing in siRNA</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/applied-insights/case-studies/posts/2026/01/12/images/illu.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>At Tangram we built LLibra OS, an internal system designed to surface and assess new small interfering RNA targets <span class="citation" data-cites="tangram2025">(8)</span>. siRNA refers to short double-stranded RNA molecules that can silence a specific gene via the RNA interference pathway. The purpose is narrow: the AI needs to help scientists identify medicines worth taking forward.</p>
<p>In early discovery, hypothesis generation often reduces to a single question: which target and disease pair should we move into the siRNA pipeline? The question sounds simple. It is not.</p>
<p>A purely data-driven approach can surface millions of candidates. Genome-wide association, expression atlases, and protein interaction networks will produce statistical associations at scale. But association is not mechanism. Two things can correlate because they share upstream causes, because they sit in the same pathway without being rate-limiting, or because of confounding in the data.</p>
<p>Plausibility requires a different kind of evidence. If we modulate this target, what functional change should we observe at the cellular or tissue level? Does that functional phenotype connect credibly to the disease we care about? For our purposes, the chain of reasoning must pass through liver biology: does knockdown of this gene alter a measurable secretory or metabolic function, and does that function relate to the clinical phenotype we wish to treat? Is there a real unmet need for your research for patients? <span class="citation" data-cites="crooke2021">(20)</span></p>
<p>The AI assists in answering these questions. It helps the team hold multiple threads of conditional evidence in view simultaneously. It retrieves, reasons and summarises over tens of millions of papers in the literature, joins and flags inconsistencies between thousands of data sources and quantifies uncertainty where the evidence is thin. Whilst the AI platform does the work of thousands of researchers and continually learns on the job, the expert judgement remains with the SMEs. The SME is central and is given all the reasons why and how the AI found a piece of evidence. The AI structures and expands the space in which that judgement operates. When it works well the AI platform uncovers the non-obvious connections that researchers may never have found.</p>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>Seen from this angle, the 95 percent figure is not a verdict on AI technology but a statistic about organisational design: how rarely good questions, high quality data, diverse experts and committed leaders are brought together at the same time. The systems described in this essay matter, but they are secondary. The primary determinant of value is whether biologists, clinicians, chemists, data scientists and portfolio leaders sit together, own the same objectives and are backed by executives willing to invest over years rather than quarters. Where that integrated team is absent, even elegant architectures will fail. Where they are present, imperfect tools still move the needle.</p>
<p>Much commentary on AI spends its energy on model choice, technical detail and tooling. This article has argued that the more important work is organisational: deciding which decisions to improve, agreeing what counts as acceptable evidence, and creating cross-functional teams that can live with the consequences. The few organisations that succeed treat AI as an experiment in decision making rather than a procurement exercise. They expect the stack to change and the vendors to turn over, but they hold fast to the team, the questions and the discipline.</p>
<p>For Real World Data Science readers, the implication is direct. AI projects fail when nobody owns the estimand, the counterfactual and the error bars. AI doesn’t need to be perfect, but it needs to be good enough. As the statistician George E. P. Box famously observed: “All models are wrong, but some are useful”. Usefulness here depends on design, discipline and humility as much as model choice. Statisticians, data scientists and methodologists can reclaim the narrative by insisting not only that every AI project begins with a clear question, a credible experiment and a plan to learn, but also that these are held collectively by an integrated team with visible executive backing. That is how more organisations move into the 5 percent.</p>
<div class="article-btn">
<p><a href="../../../../../../applied-insights/index.html">Explore more data science ideas</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the author:</dt>
<dd>
<a href="https://www.linkedin.com/in/lee-clewley-988bbb18/">Lee Clewley (PhD)</a> is VP of Applied AI &amp; Informatics at <a href="https://tangramtx.com/">Tangram Therapeutics</a>, where he led the design and deployment of LLibra, a multi-LLM, agentic system for early discovery. Formerly Head of Applied AI at <a href="https://www.gsk.com/en-gb/">GSK</a>, he is a member of the Real World Data Science <a href="https://realworlddatascience.net/the-pulse/editors-blog/posts/2022/10/18/meet-the-team.html">editorial board</a>.
</dd>
</dl>
<div class="g-col-12 g-col-md-6">
<p><strong>Copyright and licence</strong> : © 2026 Lee Clewley<br>
<a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"> <img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"> </a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">International licence</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<p><strong>How to cite</strong> :<br>
Clewley, Lee. 2026. “<strong>Why 95% Of AI Projects Fail and How to Change the Odds</strong>.” <em>Real World Data Science</em>, 2026. <a href="https://realworlddatascience.net/applied-insights/tutorials/posts/2026/12/why-95-percent-of-ai-projects-fail.html">URL</a></p>
</div>
</div>
</div>


</div>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body">
<div id="ref-mitnanda2025" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">MIT NANDA. The GenAI divide: State of AI in business 2025. Massachusetts Institute of Technology; 2025. </div>
</div>
<div id="ref-chopra2025" class="csl-entry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Chopra A et al. Measuring skills-centered exposure in the AI economy. MIT Project Iceberg; Oak Ridge National Laboratory; 2025. </div>
</div>
<div id="ref-ransbotham2020" class="csl-entry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Ransbotham S, Khodabandeh S, Kiron D, Candelon F, Chu M, LaFountain B. Expanding AI’s impact with organizational learning. MIT Sloan Management Review. 2020; </div>
</div>
<div id="ref-bcg2024" class="csl-entry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Bellefonds N de et al. Where’s the value in AI? Boston Consulting Group; 2024. </div>
</div>
<div id="ref-deloitte2024" class="csl-entry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Deloitte AI Institute. The state of generative AI in the enterprise: Now decides next. Deloitte; 2024. </div>
</div>
<div id="ref-schlegel2023" class="csl-entry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">Schlegel D, Schuler K, Westenberger J. Failure factors of AI projects: Results from expert interviews. International Journal of Information Systems and Project Management. 2023;11(3):25–40. </div>
</div>
<div id="ref-gskjules2024" class="csl-entry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">GSK.ai. JulesOS: GSK’s agent-based operating system [Internet]. 2024. Available from: <a href="https://www.gsk.ai">https://www.gsk.ai</a></div>
</div>
<div id="ref-tangram2025" class="csl-entry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline">Tangram Therapeutics. LLibra OS: Identifying the right targets. 2025. </div>
</div>
<div id="ref-markus1983" class="csl-entry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">Markus ML. Power, politics, and MIS implementation. Communications of the ACM. 1983;26(6):430–44. </div>
</div>
<div id="ref-orlikowski1992" class="csl-entry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline">Orlikowski WJ. The duality of technology: Rethinking the concept of technology in organizations. Organization Science. 1992;3(3):398–427. </div>
</div>
<div id="ref-paleyes2022" class="csl-entry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline">Paleyes A, Urma RG, Lawrence ND. Challenges in deploying machine learning: A survey of case studies. ACM Computing Surveys. 2022;55(6). </div>
</div>
<div id="ref-ey2025" class="csl-entry">
<div class="csl-left-margin">12. </div><div class="csl-right-inline">EY. How responsible AI translates investment into impact. Ernst<br>
&amp; Young; 2025. </div>
</div>
<div id="ref-capgemini2024" class="csl-entry">
<div class="csl-left-margin">13. </div><div class="csl-right-inline">Capgemini Research Institute. Generative AI in organizations 2024. Capgemini; 2024. </div>
</div>
<div id="ref-davenport2018" class="csl-entry">
<div class="csl-left-margin">14. </div><div class="csl-right-inline">Davenport TH, Ronanki R. Artificial intelligence for the real world. Harvard Business Review. 2018;96(1):108–16. </div>
</div>
<div id="ref-foundry2024" class="csl-entry">
<div class="csl-left-margin">15. </div><div class="csl-right-inline">Foundry. State of the CIO survey 2024. Foundry; 2024. </div>
</div>
<div id="ref-bolton2024" class="csl-entry">
<div class="csl-left-margin">16. </div><div class="csl-right-inline">Bolton WJ, Poyiadzi R, Morrell ER, Bergen Gonzalez Bueno G van, Goetz L. RAmBLA: A framework for evaluating the reliability of LLMs as assistants in the biomedical domain. arXiv preprint. 2024; </div>
</div>
<div id="ref-ji2023" class="csl-entry">
<div class="csl-left-margin">17. </div><div class="csl-right-inline">Ji Z, Lee N, Frieske R, et al. Survey of hallucination in natural language generation. ACM Computing Surveys. 2023;55(12). </div>
</div>
<div id="ref-gskrambla2024" class="csl-entry">
<div class="csl-left-margin">18. </div><div class="csl-right-inline">GSK.ai. RAmBLA: Evaluating the reliability of LLMs as biomedical assistants. 2024. </div>
</div>
<div id="ref-ouyang2022" class="csl-entry">
<div class="csl-left-margin">19. </div><div class="csl-right-inline">Ouyang L, Wu J, Jiang X, et al. Training language models to follow instructions with human feedback. In: Advances in neural information processing systems. 2022. p. 27730–44. </div>
</div>
<div id="ref-crooke2021" class="csl-entry">
<div class="csl-left-margin">20. </div><div class="csl-right-inline">Crooke ST, Liang XH, Baker BF, Crooke RM. Antisense technology: A review. Journal of Biological Chemistry. 2021;296:100416. </div>
</div>
</div></section></div> ]]></description>
  <category>Viewpoints</category>
  <category>AI</category>
  <guid>https://realworlddatascience.net/applied-insights/case-studies/posts/2026/01/12/why-95-percent-of-ai-projects-fail.html</guid>
  <pubDate>Mon, 12 Jan 2026 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/applied-insights/case-studies/posts/2026/01/12/images/thumb95.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Deploying LLMs for Nonprofits: 10 Lessons from Knowbot</title>
  <dc:creator>Annie Flynn</dc:creator>
  <link>https://realworlddatascience.net/applied-insights/case-studies/posts/2025/12/17/deploying_llms_nonprofits.html</link>
  <description><![CDATA[ 





<p>*Based on <a href="https://realworlddatascience.net/foundation-frontiers/posts/2025/11/27/MHF-interview.html">our conversation</a> with <a href="https://www.mikehudsonfoundation.org/">Mike Hudson</a>, Founder of <a href="https://www.mikehudsonfoundation.org">MHF</a>, <a href="https://www.knowbot.uk/">Knowbot</a> and <a href="https://www.testramp.org/">TestRAMP.</a></p>
<p>Large language models (LLMs) are a form of generative artificial intelligence (AI) that offer transformative opportunities for organisations with complex information ecosystems. But deploying them responsibly requires technical pragmatism, cultural awareness, and a respect for context.</p>
<p>Specialist AI donor the <a href="https://www.mikehudsonfoundation.org/">Mike Hudson Foundation</a> has developed an LLM-powered ‘answer engine’ that sits on websites and answers users’ questions. Our recent conversation with MHF’s Foundersurfaced some valuable insights for data science practitioners working in this sphere.</p>
<section id="start-with-the-simplest-possible-use-case" class="level2">
<h2 class="anchored" data-anchor-id="start-with-the-simplest-possible-use-case">1. Start With the Simplest Possible Use Case</h2>
<p>One of Knowbot’s core design principles was <em>minimal friction</em>. MHF looked for a “gateway use case”: a low-risk, easy-to-understand tool that organisations could immediately see value in and adopt quickly.</p>
<p><strong>Practitioner takeaway:</strong> Don’t begin with the most ambitious AI project your organisation can imagine. Begin with an easy, low risk project that still delivers value and treat it as a learning experience..</p>
</section>
<section id="culture-matters-more-than-budget" class="level2">
<h2 class="anchored" data-anchor-id="culture-matters-more-than-budget">2. Culture Matters More Than Budget</h2>
<p>Hudson notes that AI readiness among nonprofits varies widely and isn’t correlated with organisational size. Some large charities are slow to innovate due to bureaucracy; some small ones are enthusiastic but unlikely to benefit.</p>
<p><strong>Practitioner takeaway:</strong> When planning an LLM deployment, assess <em>cultural readiness</em>, not just technical readiness. Ask:</p>
<ul>
<li><p>Who are the internal champions?</p></li>
<li><p>How much AI literacy exists?</p></li>
<li><p>How cautious is the organisation by default?</p></li>
</ul>
<p>This will drive adoption far more than infrastructure.</p>
</section>
<section id="build-for-trust-first-then-functionality" class="level2">
<h2 class="anchored" data-anchor-id="build-for-trust-first-then-functionality">3. Build for Trust First, Then Functionality</h2>
<p>The biggest obstacle MHF faced wasn’t the model, the infrastructure, or the code. It was <em>accessing the right decision-makers</em> and establishing trust with LLMs in general and Knowbot in particular.</p>
<p><strong>Practitioner takeaway:</strong> AI deployments in nonprofits are trust projects as much as technical ones. Practitioners should:</p>
<ul>
<li><p>Engage early with leadership.</p></li>
<li><p>Be explicit about risks and mitigations.</p></li>
<li><p>Provide clear, responsible documentation.</p></li>
<li><p>Avoid overclaiming what the model can do.</p></li>
</ul>
<p>The more transparent the process, the smoother the adoption.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/applied-insights/case-studies/posts/2025/12/17/images/LLM2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</section>
<section id="where-appropriate-restrict-the-models-knowledge-domain-to-reduce-risk" class="level2">
<h2 class="anchored" data-anchor-id="where-appropriate-restrict-the-models-knowledge-domain-to-reduce-risk">4. Where Appropriate, Restrict the Model’s Knowledge Domain to Reduce Risk</h2>
<p>Knowbot deliberately confines itself to the content on the host organisation’s website(s), plus general internal LLM knowledge. It doesn’t trawl the open internet. This dramatically limits opportunities for hallucinations, unsafe advice, or reputational risk.</p>
<p><strong>Practitioner takeaway:</strong> Whenever possible, design LLM answer engine systems that operate on <em>curated, organisation-owned content</em>. Domain restriction is one of the most effective forms of practical AI safety.</p>
</section>
<section id="expect-surprising-user-behaviour-and-design-for-it" class="level2">
<h2 class="anchored" data-anchor-id="expect-surprising-user-behaviour-and-design-for-it">5. Expect Surprising User Behaviour — And Design for It</h2>
<p>One of the unexpected patterns in early usage: people asked Knowbot, <em>“Who are you?”</em> This required the team to add a new prompt component and require every partner to host a <em>“What is Knowbot?”</em> page.</p>
<p><strong>Practitioner takeaway:</strong> Build processes for:</p>
<ul>
<li><p>Unexpected inputs</p></li>
<li><p>Prompt evolution</p></li>
<li><p>Iterative refinement</p></li>
</ul>
<p>LLM deployment is never “set and forget.”</p>
</section>
<section id="technological-timing-matters-and-keeps-improving" class="level2">
<h2 class="anchored" data-anchor-id="technological-timing-matters-and-keeps-improving">6. Technological Timing Matters — And Keeps Improving</h2>
<p>Hudson emphasised that many capabilities now considered standard (e.g.&nbsp;long context length, ring fenced access to specific types of knowledge, server deployment ease) would have been impossible even a year earlier. The tools needed to fulfil a nonprofit’s evolving needs often appear in the LLM ecosystem soon after the nonprofit requests new functionality - making the decision whether to ‘build custom’ or ‘wait’ a tricky one.</p>
<p><strong>Practitioner takeaway:</strong> Stay current. Model capabilities, guardrails, and hosting options evolve at high speed. What was impossible last quarter may be trivial today.</p>
</section>
<section id="value-impact-over-volume" class="level2">
<h2 class="anchored" data-anchor-id="value-impact-over-volume">7. Value Impact Over Volume</h2>
<p>Knowbot’s LLM processing costs MHF money, and soKnowbot’s team evaluates success not just by the number of questions answered but by the <em>relevance</em> of those questions to valuable decision-making. A tool that helps a policymaker or researcher retrieve something critical can have outsized impact.</p>
<p><strong>Practitioner takeaway:</strong>When measuring impact, develop metrics that capture qualitative value, not just quantitative usage. For example you might consider:</p>
<ul>
<li><p>Complexity of queries</p></li>
<li><p>Decision relevance</p></li>
<li><p>Equity of access</p></li>
<li><p>Whether the tool reduces burden on staff</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/applied-insights/case-studies/posts/2025/12/17/images/LLM3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</section>
<section id="in-fast-moving-environments-admit-what-you-dont-know" class="level2">
<h2 class="anchored" data-anchor-id="in-fast-moving-environments-admit-what-you-dont-know">8. In Fast-Moving Environments, Admit What You Don’t Know</h2>
<p>Both Knowbot and TestRAMP were built in contexts where knowledge was changing daily. Hudson emphasises the importance of asking “naïve” questions, learning quickly, and not pretending expertise where there is none.</p>
<p><strong>Practitioner takeaway:</strong> Cultivate humility. Curiosity and fast learning beats early certainty. Pair technical exploration with organisational openness about unknowns.</p>
</section>
<section id="relationships-and-partnerships-are-everything" class="level2">
<h2 class="anchored" data-anchor-id="relationships-and-partnerships-are-everything">9. Relationships and Partnerships Are Everything</h2>
<p>Across both initiatives, success depended less on algorithms and more on building new human relationships.</p>
<p><strong>Practitioner takeaway: </strong>AI for public good is a team sport. Map stakeholders. Share progress transparently. Community buy-in creates technical resilience.</p>
</section>
<section id="the-next-frontier-agentic-ai" class="level2">
<h2 class="anchored" data-anchor-id="the-next-frontier-agentic-ai">10. The Next Frontier: Agentic AI</h2>
<p>Hudson argues we’re at a turning point where AI will expand from being “retrieval engines” to becoming “agentic systems that can do things.” With that shift comes both opportunity and new categories of risk.</p>
<p><strong>Practitioner takeaway:</strong> Prepare now for agentic systems. Start with controlled automation, clear constraints, auditable logs, and robust governance. Retrieval is only the beginning.</p>
<p><a href="https://realworlddatascience.net/foundation-frontiers/posts/2025/11/27/MHF-interview.html">Read our full conversation with Mike Hudson here.</a></p>
<p><a href="https://www.mikehudsonfoundation.org/">Find out more about the Mike Hudson Foundation here.</a></p>
<p><em>Mike Hudson is an entrepreneur in technology &amp; electronic markets. He now uses his expertise to help solve social problems. Mike founded TestRAMP, a pandemic nonprofit social market described as a “major contribution to Covid PCR testing &amp; genomic sequencing” &amp; donated its £2.4mn profits for charity. Mike is a Fellow of ZSL &amp; adviser to its CEO. He is an honorary Research Fellow at City, University of London. Mike is a member of the Responsible AI Institute. He is a Foundation Fellow at St Antony’s College, University of Oxford.</em></p>
<div class="article-btn">
<p><a href="../../../../../../applied-insights/index.html">Explore more data science ideas</a></p>
</div>


</section>

 ]]></description>
  <guid>https://realworlddatascience.net/applied-insights/case-studies/posts/2025/12/17/deploying_llms_nonprofits.html</guid>
  <pubDate>Wed, 17 Dec 2025 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/applied-insights/case-studies/posts/2025/12/17/images/LLMthumb.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>​Testing Multi-Agent Systems in the LLM Age: A Practical Guide for Data Scientists</title>
  <dc:creator>Peter Capsalis</dc:creator>
  <link>https://realworlddatascience.net/applied-insights/tutorials/posts/2025/12/12/MAS-guide.html</link>
  <description><![CDATA[ 





<p>Agentic AI has gained recent popularity with the emergence of Large-Language Models (LLMs) and the rapid growth of the AI technology sector. The reinvention of technology companies entering the ‘agentic age’ mirrors a profound shift in how people interact with technology. In this article, I share a structured approach for testing Multi-Agent Systems (MAS) and provide some key questions to start you thinking critically about how your systems may be working.</p>
<section id="history-and-background-defining-mas" class="level2">
<h2 class="anchored" data-anchor-id="history-and-background-defining-mas">History and Background: Defining MAS</h2>
<p>There has been more than 30 years of research into intelligent agents. Traditionally, Multi-Agent Systems (MAS) referred to collections of autonomous software agents that could communicate, coordinate, and collaborate to solve complex tasks, often in domains like robotics, logistics, or distributed control systems.</p>
</section>
<section id="the-mas-llm-shift" class="level2">
<h2 class="anchored" data-anchor-id="the-mas-llm-shift">The MAS + LLM Shift</h2>
<p>What’s new today is the emergence of LLM-powered agents, where each agent is a Large Language Model (or a wrapper around one) capable of generating language, and calling external tools. This shift marks a new phase: MAS + LLM, where agents are not just rule-based or symbolic, but generative and language-driven.</p>
<p>​This distinction is crucial:</p>
<ul>
<li><p>Traditional MAS Example (Rule-Based): A fleet of warehouse robots coordinate to move packages using pre-programmed rules and message-passing protocols.</p></li>
<li><p>MAS + LLM Example (Generative &amp; Tool-Enabled): That same warehouse might now use a set of LLM agents to plan a delivery route, query traffic data via APIs, and negotiate with each other in natural language to optimise timing, all while calling tools like maps, databases, and calculators.</p></li>
</ul>
<p>This new architecture introduces challenges and opportunities: LLM agents can be more flexible and adaptive, but also more prone to errors like hallucinations or inconsistent tool usage.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/applied-insights/tutorials/posts/2025/12/12/images/MAS.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</section>
<section id="data-scientists-and-mas-a-new-mandate" class="level2">
<h2 class="anchored" data-anchor-id="data-scientists-and-mas-a-new-mandate">Data Scientists and MAS: A New Mandate</h2>
<p>​In traditional ML workflows, Data Scientists might train models and deploy them behind APIs for consumption by other services. In MAS + LLM setups, those models become tools that LLM agents can call as part of a broader reasoning process. Data Scientists may now be involved in designing these tools, defining agent roles, and testing how agents interact.</p>
<p>​Imagine, for example, that a Data Scientist is training a sentiment analysis model. Instead of embedding it in a web app, they expose it as a tool. A “Customer Feedback Agent” (LLM) calls this tool to analyse reviews, then passes results to a “Product Strategy Agent” to decide next steps. This new mandate means Data Scientists are uniquely positioned to ensure the reliability and responsible deployment of agentic systems by rigorously testing the individual tools and the complex communication logic.</p>
<p>​When using MAS + LLM, it is essential that Data Scientists ask the right questions to assess how well a system performs on a given task. Below is a proposed framework, based on established software testing hierarchies, adapted for MAS + LLM architectures:</p>
</section>
<section id="a-four-level-testing-approach" class="level2">
<h2 class="anchored" data-anchor-id="a-four-level-testing-approach">​A Four-Level Testing Approach</h2>
<section id="unit-level-checks-determinism-and-reproducibility" class="level3">
<h3 class="anchored" data-anchor-id="unit-level-checks-determinism-and-reproducibility">​1. Unit-Level Checks: Determinism and Reproducibility</h3>
<p>​These tests assess whether individual agents behave consistently when given identical inputs. This is foundational for debugging and validation.</p>
<p><strong>​Check</strong>: Does the agent produce the same output when given the same prompt?</p>
<p>Example: Recipe Planner Agent. A “Recipe Planner Agent” is asked: “Plan a healthy lunch under 500 calories.” If the response varies significantly each time, the agent may be hallucinating or poorly grounded.</p>
<p><strong>Check</strong>: Does the agent consistently call the same tool when prompted?</p>
<p>Example: Currency Conversion Tool.An agent is asked to convert $100 USD to GBP. Check: Does the agent consistently call the convert_currency(amount, from, to) tool with the correct parameters, and is the output reliably parsed?</p>
</section>
<section id="unit-integration-context-management-and-grounding" class="level3">
<h3 class="anchored" data-anchor-id="unit-integration-context-management-and-grounding">​2. Unit + Integration: Context Management and Grounding</h3>
<p>​These tests examine how well agents manage context and avoid hallucinations by properly utilizing their tools. Failures here often stem from insufficient or poorly structured information.</p>
<p><strong>​Check</strong>: Does the agent hallucinate outputs instead of calling tools?</p>
<p>​Example: Weather Forecast Agent. Prompt: “What’s the weather in London tomorrow?” If the agent guesses instead of calling the weather API, it may lack grounding or tool clarity.</p>
<p><strong>​Check</strong>: Does the agent fail due to context length limits?</p>
<p><strong>​Check</strong>: Does the agent fail to match the correct tool due to vague definitions?</p>
<p><strong>​Check</strong>: Does the agent handle tool errors gracefully?</p>
<p>​Example: Data Analysis Agent. Input: [“a”, “b”, “c”] provided to a calculate_mean() tool. If the agent fails to handle the non-numeric error output from the tool, it demonstrates poor context and error management.</p>
</section>
<section id="integration-testing-inter-agent-communication" class="level3">
<h3 class="anchored" data-anchor-id="integration-testing-inter-agent-communication">​3. Integration Testing: Inter-Agent Communication</h3>
<p>​These tests focus on how agents coordinate and hand off tasks. This layer is particularly tricky, as agents must operate independently while still collaborating effectively.</p>
<p><strong>​Check</strong>: Do agents successfully hand off tasks to one another?</p>
<p><strong>​Check</strong>: Does changing the prompt affect handover success (e.g., changing the tone or syntax)?</p>
<p><strong>​Check</strong>: Are agent roles and descriptions clear enough to support successful delegation?</p>
<p>​Example: Travel Planning Agents. A “Trip Planner Agent” delegates hotel booking to a “Hotel Booking Agent.” If the handover fails (e.g., the receiving agent doesn’t understand its input format), the receiving agent may be poorly defined or misnamed.</p>
</section>
<section id="system-level-validation-error-propagation-and-validation" class="level3">
<h3 class="anchored" data-anchor-id="system-level-validation-error-propagation-and-validation">​4. System-Level Validation: Error Propagation and Validation</h3>
<p>These tests assess how errors are surfaced, handled, and communicated across the entire system. They also include strategies for validating the final outputs.​</p>
<p><strong>Check:</strong> Do the underlying tools include error checking and format validation on their outputs?</p>
<p><strong>Check:</strong> Can agents detect and communicate null or failed outputs from other agents or tools?</p>
<p><strong>Check:</strong> Is there a mechanism (e.g., human-in-the-loop or a designated reviewer agent) to validate final results against expected metrics?</p>
<p>Example: Reviewer Agent for Expense Reports. A “Finance Agent” calculates total expenses, and a “Reviewer Agent” checks the result. If the Finance Agent returns £400 instead of £350 for the input “£100 travel, £200 meals, £50 misc.,” the Reviewer Agent can flag the discrepancy.</p>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>MAS + LLM is becoming an increasingly essential part of a Data Scientist’s toolkit. With the numerous agentic orchestration frameworks available (LangGraph, Autogen, CrewAI, etc.) and that number increasing over time, understanding how to assess MAS and actions to improve them is necessary for their development. I encourage you to use this framework as a starting point to establish robust testing pipelines and governance standards within your teams.</p>
<div class="article-btn">
<p><a href="../../../../../../applied-insights/index.html">Explore more data science ideas</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the author:</dt>
<dd>
<a href="https://www.linkedin.com/in/peter-capsalis-37795958/">Peter Capsalis (MBA, MSc, AdvDSP)</a> is an AI and Data Senior Manager at Ernst and Young where he leads teams of data professionals in the government and energy resources sectors to solve data challenges and deliver transformational change. He sits on the <a href="https://rss.org.uk/policy-campaigns/policy-groups/ai-task-force/">RSS AI Taskforce</a>, and the Society’s <a href="https://rss.org.uk/about/equity-diversity-and-inclusion-(edi)/">EDI committee</a>.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2025 Capsalis, Peter. “​Testing Multi-Agent Systems in the LLM Age: A Practical Guide for Data Scientists”, Real World Data Science, December 12, 2025. <a href="https://realworlddatascience.net/applied-insights/tutorials/posts/2025/12/12/MAS-guide.html">URL</a>
</dd>
</dl>
</div>


</div>
</div>
</section>

 ]]></description>
  <guid>https://realworlddatascience.net/applied-insights/tutorials/posts/2025/12/12/MAS-guide.html</guid>
  <pubDate>Fri, 12 Dec 2025 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/applied-insights/tutorials/posts/2025/12/12/images/MAS.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Keeping the Science in Data Science</title>
  <dc:creator>Willis Jensen, Fatemeh Torabi, Monnie McGee, Isabel Sassoon</dc:creator>
  <link>https://realworlddatascience.net/foundation-frontiers/posts/2025/12/03/scienceindatascience.html</link>
  <description><![CDATA[ 





<p>Have you ever run an elegant ML model that landed flat with those who were supposed to use the insights? Do you find yourself deep into building hundreds of features for your model without knowing exactly what they all mean? Do you spend the bulk of your time tweaking your algorithms while aiming for incremental improvements in accuracy? If so, you might be focused more on the “data” aspects of “data science” than the “science” aspects.</p>
<section id="two-foundational-elements-of-data-science" class="level2">
<h2 class="anchored" data-anchor-id="two-foundational-elements-of-data-science">Two Foundational Elements of Data Science</h2>
<p>“Data Science” contains two essential components, “data” and “science”. The field of Data Science requires holding both components in equilibrium. Data is the raw material molded in the service of Science. While Data is first, Science is no less important. Data is the foundation and Science gives it purpose.</p>
<p>What do we mean by Science? We’re referring specifically to the scientific method as an approach to gain knowledge. It is the process of formulating ideas and hypotheses about the world around us and collecting data to determine the validity of those ideas. By hypotheses we’re not limiting the definition to strict statistical hypothesis tests, but rather the general process of formulating a research question, gathering appropriate data and advancing human knowledge, regardless of the statistical techniques or machine learning algorithms employed. Science, at its core, is about using data to gain insights and understanding about the complex universe we inhabit.</p>
<p>The scientific method has a long history and is generally defined in terms of steps such as these <a href="https://en.wikipedia.org/wiki/Scientific_method?utm_source=chatgpt.com">noted in Wikipedia</a> (Scientific Method, 2025) as:</p>
<ol type="1">
<li><p>Characterizations (observations, definitions, and measurements of the subject of inquiry)</p></li>
<li><p>Hypotheses (theoretical, hypothetical explanations of observations and measurements of the subject)</p></li>
<li><p>Predictions (inductive and deductive reasoning from the hypothesis or theory)</p></li>
<li><p>Experiments (tests of all of the above)</p></li>
</ol>
<p>The relationship between Data and Science is cyclical. Performing good science requires gathering good data, informed by proper experimental design techniques, which in turn requires appropriate analysis and interpretation in the context of the science. And good research (science) often generates more questions than it answers, giving way to the need to gather more data, and so on. As such, data science is more than just confirming hypotheses or generating insights, it becomes the application of the scientific method.</p>
<p>The scientific method should be the scaffold supporting what data scientists do. While data scientists come from a variety of backgrounds, many have more training in computer science than statistical methodology, and have more experience in software tools than they do in executing the scientific method.</p>
<p>In an influential, relevant paper Shmueli (2010) described two major types of statistical modeling, I) explanatory models which attempt to determine causal effects, and II) predictive models which seek accurate predictions. While predictive models can lead to understanding and possible explanatory models, explanatory models tend to be preferred by those seeking more scientific explanations for phenomena.</p>
</section>
<section id="data-modeling-culture-versus-algorithmic-culture" class="level2">
<h2 class="anchored" data-anchor-id="data-modeling-culture-versus-algorithmic-culture">Data Modeling Culture Versus Algorithmic Culture</h2>
<p>Leo Breiman <a href="https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full">in a famous paper</a> described two paradigms: I) the data modelling approach, which assumes the model that generates the data, and II) the algorithm approach, which relies on flexible methods without making assumptions about an underlying data generating model or how the data are generated. Breiman(2001a) felt that the statistics discipline was missing out on opportunities by focusing more on data modeling approaches and not using algorithmic approaches. He practiced what he preached, developing new algorithmic approaches and encouraging the field to increase its focus on algorithms. For example, he introduced <a href="https://link.springer.com/article/10.1023/A:1010933404324">Random Forests</a>, starting a cascade of more algorithmic approaches to modeling (Breiman, 2001b).</p>
<p>This wise counsel from Breiman encouraged those working with data to build more expertise in algorithms, promoting the algorithmic culture as a way to harness the power of new computational techniques. We would equate Breiman’s algorithmic approach with a greater focus on the Data side of Data Science and the data modeling approach with a greater focus on the Science side of Data Science.</p>
</section>
<section id="balancing-data-and-science" class="level2">
<h2 class="anchored" data-anchor-id="balancing-data-and-science">Balancing Data and Science</h2>
<p>Just as pendulums slowly swing back and forth, so too has the pendulum swung too hard towards predictive accuracy (the Data side) at the expense of contextual interpretation (the Science side) . This pendulum swing is evidenced by the growing demand for explainable ML methods (see Alangariret al.&nbsp;2023 as an example) . One such method is the use of Shapley values to elicit and rank the most important features in a ML model (see Rozemberczkiet al 2022 for an introduction). It seems ironic that, in the rush to gain model accuracy with sophisticated models containing hundreds of features, end users of the models still want something they can understand and explain. In other words, they still want scientific knowledge and understanding of cause and effect, even for complicated problems.</p>
<p>So what is the best approach from a scientific perspective? Throw as many features into a model that you can think of and see which ones show up to be the most important? Or is there some thought and care that can go into feature selection, considering what might be important given your knowledge of the science behind a problem?</p>
<p>We’re not suggesting that it is bad to include many features in a model. We’re suggesting that considering the context of the problem can provide insight on features that might matter. Of course, we don’t want to jump to conclusions on what we think is important and miss opportunities to learn. We seek to maintain some balance between using our previous knowledge and experience while not increasing the risk of confirmation bias in the feature selection process.</p>
<p>In software engineering, there is a well-known warning: “premature optimisation is the root of all evil”. The same applies in Data Science. Too often, teams rush to optimise models, tuning hyper-parameters, stacking architectures, and searching for marginal gains, before clearly defining the scientific question or validating whether the data and assumptions are appropriate. This tendency leads to models that are mathematically elegant but scientifically ungrounded. Optimisation should follow understanding, not precede it. A model that captures the right question with moderate accuracy is far more valuable than one that optimises the wrong target to perfection. This limitation of models is reflected in the famous aphorism “All models are wrong, but some are useful,” most commonly associated with the British Statistician George Box, who wrote (Box 1976):</p>
<p><em>“Since all models are wrong, the scientist cannot obtain a”correct” one by excessive elaboration. On the contrary, following William of Occam, he should seek an economical description of natural phenomena. Just as the ability to devise simple but evocative models is the signature of the great scientist, so overelaboration and overparameterisation is often the mark of mediocrity.”</em></p>
<p>Is it okay to use black box models where the model accuracy is paramount and ignore the explainability of the model? Yes, for some problems. But should we use black box models for all problems? No.&nbsp;The key to being a good data scientist/statistician is to recognize when one provides more value than another and use the best approach for the problem at hand.</p>
<p>So how does one give more attention to the Science side of Data Science? It starts with more attention on the question of interest. It doesn’t matter so much the type of question - whether it is a research question, a business problem to solve, or something sparked by curiosity. And it is often more than a single question. Often, it is not a single question but a series of cascading questions, each one digging deeper to get at the root causes. To manage this complexity effectively, it helps to adopt a modular approach, structuring analytical work into well-defined, interlinked components that mirror the scientific process. Each module focuses on a specific purpose: formulating and refining hypotheses, understanding data provenance and quality, developing and validating models, and translating findings into meaningful actions. Such modularisations keep the process transparent and iterative, prevents premature optimisation, and ensures that model development remains anchored to the underlying scientific inquiry rather than drifting towards technical over-engineering. With this increased attention, we believe sampling methods and experimental design will continue to be fundamental.</p>
<p>Here’s one example loosely based on our work experience. A business executive has some reports that show an increase in turnover at their organization, which is driving up hiring and recruiting costs, making the company less profitable. We find that the turnover is higher for those who are newer to the company, which leads to the question: “Why these new ones?” This leads to an additional hypothesis that perhaps these newer employees are not getting the leadership support they need, which leads to questions about the effectiveness of leadership training programs, which in turn leads to questions around how we measure the effectiveness of training programs. By continuing to ask questions, we can get a more targeted effort at a root cause and thus increase the impact of our work.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/foundation-frontiers/posts/2025/12/03/images/businessexec.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>With the availability of many algorithms and approaches that are able to process large amounts of data, it can be tempting to gravitate towards them. When teaching analysis/ applied statistics courses, it is important to look beyond the methods and consider the overall aim. There are a set of frameworks that have been around for a while that can be helpful in finding the right balance. One example is PPDAC (Mckayet al.&nbsp;2000) that emphasises all the steps beyond the modelling part and the importance of considering them all. Using such frameworks can help decide whether a black box approach is suitable in the situation or whether this won’t achieve the overall intended aim.</p>
</section>
<section id="finding-balance" class="level2">
<h2 class="anchored" data-anchor-id="finding-balance">Finding Balance</h2>
<p>So how do we ensure a good balance between Data and Science in Data Science?</p>
<p>One way is to ask “so what” with any analysis that you do and any model that you build. Ideally, you would ask that at the beginning of a project to reduce wasted effort, but it should be clear how the analysis output will be used. And it would not be sufficient to say “so that we can publish the output in a paper”. You have to think about the impact of the analysis. Will it change a decision that is being made? Does it create a new insight that can be acted upon? Does it lead to a process improvement or a new product innovation? Does it lead to a new way of running an organization? Data science that doesn’t lead to some action or insight is just computation for computations sake. Vance et al.&nbsp;(2022) provides additional resources and advice for how to ask good questions.</p>
<p>A second way is to consider the potential explanations and meaning behind any model. Don’t become too enamored with the predictive accuracy of the model (which isn’t inherently a bad thing) at the expense of asking whether there are potential scientific explanations based on the features used in any model. Use a predictive model as a starting point for digging deeper and finding a smaller set of features that provide deeper insight on potential causal relationships to explore.Sometimes a simpler model that is easier to “explain” or one that uses trusted data provides more value than the latest and greatest algorithm.</p>
<p>A third way is continuing emphasis on the reproducibility of the results. Clean code, documentation of results, version control of analysis code and open sharing of the code with its underlying assumptions are best practices to ensure that others can replicate the findings of any data science output. <a href="https://realworlddatascience.net/foundation-frontiers/posts/2023/11/06/how-to-open-science.html">Sassoon (2023)</a> provides additional guidance for ensuring reproducibility and transparency of results.</p>
<p>A final way to find the balance is to better understand how the data are generated. Hoerl (2025) touched on this issue in calling for statisticians to focus more on data quality. <a href="https://realworlddatascience.net/foundation-frontiers/posts/2025/10/30/data-detectives.html">We believe this advice to be equally relevant for data scientists.</a> By recognizing the crucial importance of the data generation process, data scientists will better be able to use the right data that matches the problem of interest and push for changes as needed to ensure high quality data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/foundation-frontiers/posts/2025/12/03/images/balance.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>We encourage data scientists to live up to their name and become experts in both Data and Science. As they find a proper balance between those two areas, their impact and influence will increase and the quality of their rigorous work will stand up to scrutiny and advance human knowledge.</p>
<p><strong>References</strong></p>
<p>Alangari, N., Menai, M. E. B.,Mathkour, H., &amp;Almosallam, I. (2023).Exploring evaluation methods for interpretable machine learning: A survey.<em>Information</em>,<em>14</em>(8), 469.</p>
<p>Box, George (December 1976). “Science and Statistics”. Journal of the American Statistical Association. 71 (356): 791–799. doi:10.1080/01621459.1976.10480949. Retrieved 2025-11-28.</p>
<p>Breiman, L. (2001a). Statistical modeling: The two cultures (with comments and a rejoinder by the author).<em>Statistical Science</em>,<em>16</em>(3), 199-231.</p>
<p>Breiman, L. (2001b). Random forests.<em>Machine Learning</em>,<em>45</em>, 5-32.</p>
<p>Hoerl, R. W. (2025). The future of statistics in an AI era.<em>Quality Engineering</em>. Advance online publication.</p>
<p>Hyde, R. (2009). The fallacy of premature optimization.<em>Ubiquity</em>,<em>2009</em>(February).</p>
<p>MacKay, R. J., &amp; Oldford, R. W. (2000). Scientific method, statistical method and the speed of light.<em>Statistical Science</em>,<em>15</em>(3), 254-278.</p>
<p>Rozemberczki, B., Watson, L., Bayer, P., Yang, H. T., Kiss, O., Nilsson, S., &amp; Sarkar, R. (2022). Theshapleyvalue in machine learning. In<em>The 31st International Joint Conference on Artificial Intelligence and the 25th European Conference on Artificial Intelligence</em>(pp.&nbsp;5572-5579). International Joint Conferences on Artificial Intelligence Organization.</p>
<p>Sassoon, I. (2023, November 6). How to ‘open science’: A brief guide to principles and practices.<em>Real World Data Science</em>. https://realworlddatascience.net/foundation-frontiers/posts/2023/11/06/how-to-open-science.html</p>
<p>Scientific method. (2025, November). In<em>Wikipedia</em>.</p>
<p>Shmueli, G. (2010). To explain or to predict?<em>Statistical Science</em>,<em>25</em>(3), 289-310.</p>
<p>Vance, E. A., Trumble, I. M., Alzen, J. L., &amp; Smith, H. S. (2022). Asking great questions.<em>Stat</em>,<em>11</em>(1), e471.</p>
<div class="article-btn">
<p><a href="../../../../../foundation-frontiers/index.html">Explore more data science ideas</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the authors</dt>
<dd>
This article was authored by some of our editorial board members. You can find their bios <a href="https://realworlddatascience.net/the-pulse/editors-blog/posts/2022/10/18/meet-the-team.html">on our team page</a>.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2025 Willis Jensen, Fatemeh Torabi, Monnie McGee, Isabel Sassoon.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<p>: Jensen, Willis. Torabi, Fatemeh. McGee, Monnie. Sassoon, Isabel. “Keeping the Science in Data Science,” Real World Data Science, December 04, 2025. <a href="https://realworlddatascience.net/foundation-frontiers/posts/2025/11/27/scienceindatascience.html">URL</a></p>
</div>


</div>
</div>
</section>

 ]]></description>
  <guid>https://realworlddatascience.net/foundation-frontiers/posts/2025/12/03/scienceindatascience.html</guid>
  <pubDate>Thu, 04 Dec 2025 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/foundation-frontiers/posts/2025/12/03/images/thumbnail2.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>AI for Social Good: Interview with the Founder of Mike Hudson Foundation</title>
  <dc:creator>Annie Flynn</dc:creator>
  <link>https://realworlddatascience.net/foundation-frontiers/posts/2025/11/27/MHF-interview.html</link>
  <description><![CDATA[ 





<p>Since the emergence of generative AI such as OpenAI’s ChatGPT at the end of 2022, data science practitioners have watched large language models (LLMs) become both a transformative capability and a source of organisational anxiety. Few people sit closer to this intersection than <strong>Mike Hudson</strong>, a former fintech entrepreneur who now leads initiatives deploying AI for social good through the <a href="https://www.mikehudsonfoundation.org/">Mike Hudson Foundation</a>. MHF’s current projects include <a href="https://www.knowbot.uk/">Knowbot</a>, an LLM-powered tool designed to make complex websites more accessible. Knowbot uses LLMs to read and distill curated information to answer users’ complex questions.</p>
<p>Previously, MHF created <a href="https://www.testramp.org/">TestRAMP</a>, an ambitious effort during the COVID-19 pandemic to mobilise private lab PCR capacity for public benefit which also identified <a href="https://www.smf.co.uk/publications/testramp-marketplace-covid-testing/">potential lessons for future crisis situations</a>.</p>
<p>In this interview, Mike speaks candidly about the challenges and opportunities of deploying AI in nonprofit settings, and the lessons learned from building responsible, high-impact technology during moments of rapid change.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/foundation-frontiers/posts/2025/11/27/images/simplequestion.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<section id="the-appetite-for-ai-in-the-third-sector" class="level2">
<h2 class="anchored" data-anchor-id="the-appetite-for-ai-in-the-third-sector">The Appetite for AI in the Third Sector</h2>
<p><strong>Q: What led you to start working on Knowbot in the first place?</strong></p>
<p>I used to be a business entrepreneur with several tech and fintech businesses. That came to an end when I sold those businesses and then, when Covid started, it seemed like there was an opportunity to give something back. I founded TestRAMP, my first nonprofit, and that experience was so enjoyable and so productive. Once the pandemic eased, we had a foundation in place but no clear mission. Then ChatGPT launched, the world got excited about large language models, and we thought: could we use our tech backgrounds and some funding to build something for the social good?</p>
<p>Knowbot grew out of a simple question: Is there an appetite within the world of nonprofits for LLMs —and is there a use case for them? We needed a very simple, low-risk, easy-to-grasp AI use case that could act as a gateway for organisations cautiously exploring LLMs. Knowbot became that gateway.</p>
<p>We pushed it out to some of the non-profits we were already working with and it’s been interesting. There is some appetite for it, and the appetite is increasing. We use Knowbot as a jumping-off point to start a conversation about AI.</p>
<p><strong>Q: Are there patterns in which charities are more open to adopting AI?</strong></p>
<p>The biggest differentiator isn’t size or budget — it’s culture. We are finding it most productive to work with medium-to-large nonprofits with a scientific or research-oriented culture, where the internal decision-makers tend to “get” AI more quickly. Because our single biggest challenge, far and away above any technical or scaling challenges, or anything to do with IT or LLM, has been accessing the right decision-makers inside organisations.</p>
<p>The sector is understandably cautious and we are new kids on the block — Knowbot didn’t exist a year ago. And we have realised that word of mouth recommendations will be key to our growth: credibility has to be built one relationship at a time with the right nonprofit partners.</p>
</section>
<section id="designing-for-maximum-ease" class="level2">
<h2 class="anchored" data-anchor-id="designing-for-maximum-ease">Designing for Maximum Ease</h2>
<p><strong>Q: How did you approach technical deployment?</strong></p>
<p>From day one, we knew getting anything onto a nonprofit’s website would be difficult. So, technically speaking, we’ve made it as easy as we possibly can. Knowbot runs almost entirely on our servers. On a partner’s website, it appears as a javascript button in the corner of a user’s screen which when clicked loads a small Knowbot window, where they can ask questions. Knowbot loads its interface from our servers in Frankfurt.There’s no client-side coding or backend integration required. That was deliberately done to allow a very straightforward deployment, and it should only take new partners a couple of hours to implement. We’ve recently made an even simpler option: some partners now just link to a branded page that looks like their website but which actually exists on our servers.</p>
<p>Behind the scenes, Knowbot is written mostly in Python and uses LLMs from Anthropic, OpenAI, Meta, Google, and Perplexity. We don’t develop our own models — few organisations on Earth have the budget for that. Instead, we “build the car around the engine,” and it’s a slightly different car for each non-profit that we work with.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/foundation-frontiers/posts/2025/11/27/images/thumbnailsocial.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</section>
<section id="ethics-by-design-domain-restriction-as-a-safety-mechanism" class="level2">
<h2 class="anchored" data-anchor-id="ethics-by-design-domain-restriction-as-a-safety-mechanism">Ethics by Design: Domain Restriction as a Safety Mechanism</h2>
<p><strong>Q: As an AI-for-good nonprofit, how do you approach issues of risk, bias, and governance?</strong></p>
<p>With nonprofits, we are hyper-aware of these issues. Some of the organisations we work with handle sensitive information, so we need to be sure we’re not introducing new risks. With a non-profit that is working in healthcare, for example, if there is a possibility that Knowbot may be used for medical-adjacent questions, then we need to think carefully about whether that is something we should be doing and, if so, whether we can do it safely.</p>
<p>One major choice we made early on was domain restriction. Whereas most of the big answer engines out there, like ChatGPT, search the full public internet, Knowbot will only use its internal knowledge and specific website(s) upon which it’s based (i.e.&nbsp;the non-profit’s own website(s)). That means the knowledge is curated and the nonprofit knows exactly what information Knowbot can draw on. That dramatically reduces the risk of hallucination, misinformation, or unsafe advice.</p>
<p>We also adjust our prompts continually based on feedback. For example, we hadn’t anticipated that users would ask questions like “Who are you?” or “What is Knowbot?” Because the model had no context, it responded unpredictably. So we now require partners to include a “What is Knowbot?” page on their site, which Knowbow can reference.</p>
</section>
<section id="evolving-with-the-technology" class="level2">
<h2 class="anchored" data-anchor-id="evolving-with-the-technology">Evolving with the Technology</h2>
<p><strong>Q: What technical challenges have you encountered?</strong></p>
<p>I think we have been really lucky in terms of timing. To do what we’re doing now five years ago would have been completely impossible, because the LLMs weren’t there. From an infrastructure perspective, newer services such as cloud hosting tools like Render now let us deploy servers in minutes. That’s just a breath of fresh air. It takes away a lot of the operational heartache. And coding has become dramatically easier—AI assistance means we can build things now that would have taken us weeks before.</p>
<p>We’ve also been lucky in that LLM technology has improved fast enough that we’ve been able to incorporate new functionality almost as quickly as our nonprofit partners have requested it. For example, we now allow partners to restrict Knowbot to particular sections or topics within a website, or to sit across multiple websites. This has only become practical as models and retrieval tools have matured. Development has become faster, too: for example, we can now submit an entire codebase into our coding LLMs and ask questions about it. By comparison, when ChatGPT first launched we could only submit a small section of a program at a time.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/foundation-frontiers/posts/2025/11/27/images/impact.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</section>
<section id="impact-what-knowbot-is-changing" class="level2">
<h2 class="anchored" data-anchor-id="impact-what-knowbot-is-changing">Impact: What Knowbot Is Changing</h2>
<p><strong>What impact have you seen so far?</strong></p>
<p>There hasn’t been anybody who has stopped and decided that it’s not for them, which is encouraging. Most of our nonprofit partners have started small — monitoring answers, getting staff comfortable — and have then expanded deployment. Feedback has been invaluable and very positive.</p>
<p>We can measure impact partly by usage volume, but more importantly by value. Getting 1,000 questions about “where is the ice cream stall?” on a venue website is fine, but the real impact is when researchers or decision-makers can extract complex information from, say, a conservation charity or a healthcare resource site. That’s where AI becomes transformational.</p>
<p>We want to add more value with more nonprofits and other for-good actors. We think there are some obvious things that should be done more generally. Whether we do them through Knowbot or whether we flag other suitable tech partners isn’t clear yet, and probably isn’t that important. But, for example, long-term, we would like to see tools like Knowbot become standard on sites like NHS.uk, gov.uk, NICE, and others. These sites hold high-quality knowledge but can be very difficult to navigate due to the sheer volume of information they contain. Traditional on-website search tools simply aren’t up to the job and LLM-based retrieval is a natural upgrade. The sooner these sites start adopting tools such as Knowbot, the better information people are going to get. Whether that means members of the public, or whether it means professionals that are looking for technical advice, the biggest wins come from the right information helping people make valuable decisions.</p>
</section>
<section id="advice-for-nonprofits-considering-llms" class="level2">
<h2 class="anchored" data-anchor-id="advice-for-nonprofits-considering-llms">Advice for Nonprofits Considering LLMs</h2>
<p><strong>What advice would you give nonprofits thinking about adopting LLMs?</strong></p>
<p>Start small. Don’t try to design a global solution from day one. There is so much hype around AI — some justified, some not — that it feels like a high-risk decision. LLM AI is still new and evolving rapidly. Understand that you don’t know what you don’t know, and be prepared to experiment on a small scale, before building out.</p>
<p>Use low-impact tools first. Let different teams build familiarity. Learn what the models can and can’t do in your context. Build internal confidence gradually.</p>
<p>And for practitioners inside nonprofits facing resistance: the conversation is the same one we have externally. Be clear about risks, mitigation, and the fact that this is a learning process. Build trust.</p>
<p><strong>Q: Where do you see the next big opportunity for AI in the public interest?</strong></p>
<p>We’re at the beginning of another new chapter in generative AI. Until now, most LLMs have been about retrieval and synthesis. The next transformational phase is agentic AI: systems that can do things - take actions autonomously, or semi-autonomously. That will be incredibly consequential for society, with new risks and huge potential benefits. Getting that right is absolutely essential. Future technology aside, there remain enormous public interest opportunities even for today’s tech. There is always a lag between new technology and its adoption and implementation.</p>
<p><strong>Q: Anything you’d like readers to know about Knowbot?</strong></p>
<p>Yes! Come and talk to us. Every conversation teaches us something new, whether or not the organisation ends up using Knowbot. We’re eager to collaborate with nonprofits and with tech companies building the next generation of models.</p>
<p><em>Mike Hudson is an entrepreneur in technology &amp; electronic markets. He now uses his expertise to help solve social problems. Mike founded TestRAMP, a pandemic nonprofit social market described as a “major contribution to Covid PCR testing &amp; genomic sequencing” &amp; donated its £2.4mn profits for charity. Mike is a Fellow of ZSL &amp; adviser to its CEO. He is an honorary Research Fellow at City, University of London. Mike is a member of the Responsible AI Institute. He is a Foundation Fellow at St Antony’s College, University of Oxford.</em></p>
<div class="article-btn">
<p><a href="../../../../../foundation-frontiers/index.html">Explore more data science ideas</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the authors</dt>
<dd>
<strong>Annie Flynn</strong> is Head of Content at the RSS.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2025 Annie Flynn
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Flynn, Annie 2025. “AI for Social Good: Interview with the Founder of the Mike Hudson Foundation,” Real World Data Science, November 27, 2025. <a href="https://realworlddatascience.net/foundation-frontiers/posts/2025/11/27/MHF-interview.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>


</section>

 ]]></description>
  <guid>https://realworlddatascience.net/foundation-frontiers/posts/2025/11/27/MHF-interview.html</guid>
  <pubDate>Thu, 27 Nov 2025 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/foundation-frontiers/posts/2025/11/27/images/thumbnailsocial.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Beyond Quantification: Navigating Uncertainty in Professional AI Systems</title>
  <dc:creator>Annie Flynn</dc:creator>
  <link>https://realworlddatascience.net/foundation-frontiers/datasciencebites/posts/2025/11/21/uncertainty.html</link>
  <description><![CDATA[ 





<div class="callout callout-style-default callout-note callout-titled" style="margin-top: 0rem;">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>About the paper and this post
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p><strong>Title:</strong> Beyond Quantification: Navigating Uncertainty in Professional AI Systems</p>
<p><strong>Author(s) and year:</strong> Sylvie Delacroix, Diana Robinson, Umang Bhatt, Jacopo Domenicucci, Jessica Montgomery, Gaël Varoquaux, Carl Henrik Ek, Vincent Fortuin, Yulan He, Tom Diethe, Neill Campbell, Mennatallah El-Assady, Søren Hauberg, Ivana Dusparic12 and Neil D. Lawrence (2025)</p>
<p><strong>Status:</strong> Published in <em>RSS: Data Science and Artificial Intelligence</em>, open access: <a href="https://academic.oup.com/rssdat/article/1/1/udaf002/8317136">HTML</a></p>
</div>
</div>
</div>
<p>As artificial intelligence systems—especially large language models (LLMs)—become woven into everyday professional practice, they increasingly influence sensitive decisions in healthcare, education, and law. These tools can draft medical notes, comment on student essays, propose legal arguments, and summarise complex documents. But while AI can now answer many questions confidently, professionals know that confidence is not always what matters most.</p>
<p>Consider a doctor who suspects a patient may be experiencing domestic abuse, or a teacher trying to distinguish between a student’s misunderstanding and a culturally shaped interpretation of a text. These are situations where uncertainty isn’t just about missing data—it’s about interpretation, ethics, and human judgment.</p>
<p>Yet much current AI research focuses on quantifying uncertainty: assigning probability scores, confidence levels, or error bars. The authors of this paper argue that while such numbers help in some cases, they miss the forms of uncertainty that truly matter in professional decision-making. If AI systems rely only on numeric confidence, they risk eroding the very expertise they aim to support.</p>
<p>This paper asks a simple but transformative question: <strong>What if uncertainty isn’t always something to quantify, but something to communicate?</strong></p>
<section id="why-quantification-isnt-enough" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="why-quantification-isnt-enough">Why quantification isn’t enough</h2>
<p>The authors highlight a fundamental mismatch between the way today’s AI systems handle uncertainty and the way real professionals experience it. They distinguish between:</p>
<ul>
<li><p>Epistemic uncertainty – when we simply don’t know enough yet (e.g., missing data, incomplete measurements). <em>This can often be quantified.</em></p></li>
<li><p>Hermeneutic uncertainty – when a situation allows multiple legitimate interpretations, often shaped by culture, ethics, or context. <em>This cannot meaningfully be reduced to a percentage.</em></p></li>
</ul>
<div class="column-page">
<p><img src="https://realworlddatascience.net/foundation-frontiers/datasciencebites/posts/2025/11/21/images/uncertaintykinds.png" class="img-fluid"></p>
</div>
<p>Professional judgment often depends on this second kind. Teachers, doctors, and lawyers rely on tacit skills: subtle perceptions, ethical intuitions, and context-sensitive interpretation. AI systems trained on statistical patterns struggle to reflect this nuance.</p>
<p>When an AI model gives a probability score — “I’m 70% sure this infection is bacterial” — it communicates something useful. But if the real uncertainty stems from ethical or contextual complexity (e.g., whether asking a patient certain questions might put them at risk), probability scores offer a false sense of clarity.</p>
<p>The paper gives practical examples:</p>
<ul>
<li><p>A medical AI might be highly confident about symptoms but blind to the social dynamics suggesting abuse.</p></li>
<li><p>An educational AI may accurately flag grammar issues but miss culturally sensitive interpretations in a student essay.</p></li>
</ul>
<p>In both cases, the most important uncertainties are precisely the ones that cannot be captured by numbers.</p>
</section>
<section id="why-this-matters-now" class="level2">
<h2 class="anchored" data-anchor-id="why-this-matters-now">Why this matters now</h2>
<p>The authors warn that the problem becomes even more serious as we move toward agentic AI systems—multiple AI agents interacting and making decisions together. If one system miscommunicates uncertainty, the error may ripple through an entire network.</p>
<p>To address this, the authors propose shifting away from trying to algorithmically “solve” uncertainty, and instead enabling professionals themselves to shape how AI expresses it.</p>
</section>
<section id="takeaways-and-implications" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="takeaways-and-implications">Takeaways and implications</h2>
<p><strong>1. Uncertainty expression is part of professional expertise, not just a technical feature</strong></p>
<p>AI should not simply output probabilities. It should help preserve and enhance the ways professionals reason through complex, ambiguous situations. That means:</p>
<ul>
<li><p>highlighting when interpretation is required</p></li>
<li><p>surfacing multiple plausible perspectives</p></li>
<li><p>signalling when ethical judgment is involved</p></li>
<li><p>encouraging expanded inquiry rather than false certainty</p></li>
</ul>
<p>For example, instead of producing a diagnosis score, an AI assistant might say: “This pattern warrants attention to social context. Consider asking open-ended questions to understand the patient’s circumstances.”</p>
<p>This kind of prompting respects and supports professional judgment.</p>
<p><strong>2. Professionals—not engineers—must define how uncertainty is communicated</strong></p>
<p>The authors propose participatory refinement, a process where communities of practitioners (teachers, doctors, judges, etc.) collectively shape:</p>
<ul>
<li><p>the categories of uncertainty that matter in their field</p></li>
<li><p>the language and formats AI systems should use</p></li>
<li><p>how these systems should behave in ethically sensitive scenario</p></li>
</ul>
<p>This differs from typical user feedback loops. Instead of individuals clicking “thumbs down,” whole professions deliberate on what kinds of uncertainty an AI system should express and how.</p>
<p><strong>3. This requires new technical and organisational approaches</strong></p>
<div class="column-page">
<p><img src="https://realworlddatascience.net/foundation-frontiers/datasciencebites/posts/2025/11/21/images/futureai.png" class="img-fluid"></p>
</div>
<p>To make participatory refinement possible, future AI systems need:</p>
<ul>
<li><p>architectures that can incorporate community-defined uncertainty frameworks</p></li>
<li><p>interfaces designed for collective sense-making, not just individual use</p></li>
<li><p>institutional support (e.g., workshops, governance processes, professional committees)</p></li>
</ul>
<p>While this takes more time than simply deploying an AI system “out of the box,” the authors argue that in fields like healthcare or law, these deliberative processes are essential, not optional.</p>
<p><strong>4. Preserving “productive uncertainty” is key for ethical, adaptive professional practice</strong></p>
<p>If AI tools flatten complex uncertainty into simple numbers, they may unintentionally narrow the space for professional judgment and ethical debate. The authors suggest that sustained ambiguity—open questions, competing interpretations, ethical reflection—is not a flaw in human reasoning but a feature of high-quality professional work.</p>
<p>Well-designed AI should help maintain that reflective space, not close it down.</p>
<p><strong>Further reading</strong></p>
<p>For readers interested in exploring more:</p>
<ul>
<li><p>David Spiegelhalter – The Art of Uncertainty (accessible introduction to uncertainty in science)</p></li>
<li><p>Iris Murdoch – The Sovereignty of Good (on moral perception)</p></li>
<li><p>Participatory AI frameworks such as STELA (Bergman et al., 2024)</p></li>
<li><p>Visual analytics research on human-in-the-loop data interpretation</p></li>
<li><p>Discussions of agentic AI systems and coordinated AI in healthcare</p></li>
<li><p>Delacroix’s work on LLMs in ethical and legal decision-making</p></li>
</ul>
</section>
<section id="in-summary" class="level2">
<h2 class="anchored" data-anchor-id="in-summary">In summary</h2>
<p>This paper argues that if AI is to genuinely assist professionals, it must go beyond quantification. Numbers alone cannot capture the ethical, interpretive, and contextual uncertainties that define professional practice. Instead, AI should help preserve and enrich human judgment by communicating uncertainty in ways co-designed with the communities who rely on it. AI should not just be <em>accurate</em> —it should be <em>appropriately uncertain</em>.</p>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-6">
<dl>
<dt>About the author</dt>
<dd>
<strong>Annie Flynn</strong> is Head of Content at the RSS.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>About DataScienceBites</dt>
<dd>
<a href="../../../../../../foundation-frontiers/datasciencebites/index.html"><strong>DataScienceBites</strong></a> is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to <a href="../../../../../../contributor-docs/datasciencebites.html">become a contributor</a>.
</dd>
</dl>
</div>
</div>
</div>


</section>

 ]]></description>
  <guid>https://realworlddatascience.net/foundation-frontiers/datasciencebites/posts/2025/11/21/uncertainty.html</guid>
  <pubDate>Fri, 21 Nov 2025 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/foundation-frontiers/datasciencebites/posts/2025/11/21/images/thumb.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Why We Should All Be Data Quality Detectives</title>
  <dc:creator>A. Rosemary Tate and Roger Halliday</dc:creator>
  <link>https://realworlddatascience.net/foundation-frontiers/posts/2025/10/30/data-detectives.html</link>
  <description><![CDATA[ 





<p>At the <a href="https://rss.org.uk/news-publication/news-publications/2025/general-news/president-s-blog-reflections-on-a-record-breaking/">2025 Royal Statistical Society conference</a> in Edinburgh, a lively group of statisticians and data scientists gathered to tackle a quietly critical issue: data quality. Our workshop, titled “Why we should all be data quality detectives”, drew around 40 participants into a dynamic conversation about why data quality is often overlooked and what we can do to change that.</p>
<div id="thumbnail.png" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://realworlddatascience.net/foundation-frontiers/posts/2025/10/30/images/thumbnail.png" class="img-fluid figure-img" alt="The workshop drew 40 participants."></p>
<figcaption>The workshop drew 40 participants.</figcaption>
</figure>
</div>
<section id="the-case-for-data-quality" class="level2">
<h2 class="anchored" data-anchor-id="the-case-for-data-quality">The Case for Data Quality</h2>
<p>If you search for “data quality disasters” on any search engine, you will find many results. Similarly, literature on data quality measures offers abundant advice. But within the scientific research community, data quality is often ignored. For example, how often have you encountered the term “data quality” in the guidelines when submitting or reviewing an academic paper? We would venture to say, hardly ever (or never).</p>
<p>This is puzzling, because high-quality data (i.e., data that is fit for purpose) is essential; without it, results become almost meaningless. Data serves as the foundation of our work. So why isn’t its quality given the prominence it deserves? Why aren’t we, as statisticians and data scientists, advocating data quality more vocally?</p>
<p>Recent publications may shed some light: it appears that “Everyone wants to do the model work, not the data work” [1]<sup>1</sup>, and that statisticians may feel uneasy with elements that are not easily quantifiable [2]<sup>2</sup>. Or perhaps we are all guilty of “premature enumeration” (as Tim Harford puts it), rushing into data analysis without having a good look at the data first. Whatever the case, data quality work or “data cleaning/wrangling” is not seen as fun.</p>
<p>For us, as self-confessed “data quality detectives”, the reverse is true, and we began the workshop by reframing data quality not as a tedious chore, but as an empowering and even enjoyable part of the analytical process. We spend hours looking at the data, enjoying the delayed gratification of finally getting to (trustable) results.</p>
<p>In Rosemary’s case, her attitude was shaped by key experiences early in her statistical career. Her doctoral research focused on developing methods to automatically classify magnetic resonance spectra of human leg adipose tissue based on diet—specifically distinguishing between vegans and omnivores. The study recruited 33 vegans, while the control group included 34 omnivores and 8 vegetarians, primarily staff from the MRI unit at Hammersmith Hospital. With limited experience at the time, she began experimenting with various techniques, starting with k-means cluster analysis. Although she hoped the clusters would reflect dietary groups, the analysis instead produced two distinct clusters—one containing just two spectra and the other containing the rest. After consulting colleagues, she learned that the two outlier spectra had been acquired using a different protocol and were mistakenly included in the dataset. While she may have identified the error later, catching it early saved her several weeks of work — and won her some kudos with colleagues.</p>
<div id="kudos.png" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://realworlddatascience.net/foundation-frontiers/posts/2025/10/30/images/kudos.png" class="img-fluid figure-img" alt="Catching it early saved her several weeks of work."></p>
<figcaption>Catching it early saved her several weeks of work.</figcaption>
</figure>
</div>
</section>
<section id="detective-work-at-the-tables" class="level2">
<h2 class="anchored" data-anchor-id="detective-work-at-the-tables">Detective Work at the Tables</h2>
<p>During the workshop, we split into six groups to investigate two questions: Why does data quality get overlooked? What strategies can raise its profile?</p>
<p>The discussions were rich and revealing. Many pointed to organisational gaps — no clear strategy, limited training, and confusion over who is responsible for data quality. Others highlighted cultural issues: time pressures, lack of curiosity, and a tendency to assume someone else has already checked the data.</p>
<p>Simple Excel errors are also common. We heard an example case of a study comparing a new, advanced imaging machine with an older model. The results were presented in a spreadsheet, which included several measurements. As expected, the correlation matrix showed strong correlations between most columns—except for the first, which was the main measure of interest. It quickly became apparent that the sort function had been applied to that column, scrambling the values and rendering them effectively random. Unfortunately, the researcher had not kept a backup of the original data, meaning the entire experiment was compromised. During the COVID-19 pandemic, a similar technical mistake involving Excel led to <a href="https://www.bbc.co.uk/news/technology-54423988#:~:text=The%20badly%20thought%2Dout%20use,than%20a%20third%2Dparty%20contractor.">thousands of positive cases being omitted from the UK’s official daily figures</a>. These are the kinds of simple issues that could have been caught with a basic data check.</p>
<div id="excel-errors.png" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://realworlddatascience.net/foundation-frontiers/posts/2025/10/30/images/excel-errors.png" class="img-fluid figure-img" alt="Simple Excel errors are also common."></p>
<figcaption>Simple Excel errors are also common.</figcaption>
</figure>
</div>
<p>Other examples were given of data quality issues arising when datasets were used for a specific research focus, and the quality checks applied were tailored too narrowly to that focus. Additional problems only became apparent when the same data was later used for a different research purpose. Conclusion: you can’t be complacent about the quality of the data you’re using.</p>
</section>
<section id="strategies-for-change" class="level2">
<h2 class="anchored" data-anchor-id="strategies-for-change">Strategies for Change</h2>
<p>The second question sparked even more ideas. Suggestions ranged from embedding data quality education early (even at school level) to implementing cultural changes that lead to great transparency. Participants called for:</p>
<ul>
<li>Training and upskilling across roles</li>
<li>Transparent reporting of errors and limitations</li>
<li>Positive feedback loops for data collectors</li>
<li>Rewarding quality work and error detection</li>
<li>Modernising systems and improving interoperability</li>
<li>Using AI and automation to support quality checks</li>
<li>Publications including recommendations for more transparent reporting of “initial data analysis” in their guidelines.</li>
</ul>
<p>One standout idea: organisations could promote a “data amnesty” culture where errors can be acknowledged without blame. This is something Roger experienced during his time as Chief Statistician for the Scottish Government. There, he occasionally encountered serious data quality issues that required official statistics to be revised or delayed. Being transparent with users about these issues was a key principle of the Code of Practice for Official Statistics. A conscious effort was made — through training and through taking a certain approach to handling such situations — to foster a culture of openness and accountability. Staff were supported to create and implement plans to address the problems, learn from them, and communicate clearly with users. This transparency was essential to maintaining trust in both our processes and the statistics we produced.</p>
</section>
<section id="a-call-to-action" class="level2">
<h2 class="anchored" data-anchor-id="a-call-to-action">A Call to Action</h2>
<p>We walked away from the workshop with a clear conclusion: data quality needs a culture shift. It’s not enough to care — we need to prioritise and celebrate the work of those who keep our data trustworthy, while educating other stakeholders about what it involves.</p>
<p>Shaping the next steps will require keeping this conversation going within the data community and Real World Data Science can play an integral role in that. As a direct result of this piece, we have <a href="https://realworlddatascience.net/contributor-docs/call-for-contributions.html">updated our submission guidelines</a> to include recommendations for transparent data reporting and we would like to publish more stories of data disasters – or disasters averted through careful attention to data quality.</p>
<p>As one attendee put it, “We need to challenge the data and learn best practice from the get-go.” It’s time to embrace our inner data detectives; the integrity of our insights depends on it.</p>
<p>Please share your own data disaster stories in the comments, or in the <a href="mailto:rwds@rss.org.uk">Real World Data Science inbox</a>.</p>
<div class="article-btn">
<p><a href="../../../../../foundation-frontiers/index.html">Explore more data science ideas</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the authors</dt>
<dd>
<strong>Rosemary Tate</strong> is a Chartered Biostatistician and Computer Scientist with over 30 years of experience in medical research and statistical consulting. She has a BSC in mathematics and a DPhil in Computer Science and AI, and an MSc in Medical Statistics. She has been scientific manager of a large EU-funded project and held lectureships at the Institutes of Child Health and Psychiatry. An independent statistical consultant since 2016, she now spends most of her time as a “Data Quality Agent Provocateur”.
</dd>
<dd>
<strong>Roger Halliday</strong> CEO at Research Data Scotland, providing leadership to improving public wellbeing through transforming how data is used in research, innovation and insight. Roger was Scotland’s Chief Statistician from 2011 to 2022. During that time he was also Scottish Government Chief Data Officer (2017-20), and jointly led Scottish Government Covid Analytical Team during the pandemic. Before that, he worked in the Department of Health in England as a policy analyst managing evidence for decision making across NHS issues. He became an honorary Professor at the University of Glasgow in 2019.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2025 A. Rosemary Tate and Roger Halliday
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Tate, Rosemary A. and Halliday, Roger. 2025. “Why We Should All Be Data Quality Detectives” Real World Data Science, October 30, 20245. <a href="https://realworlddatascience.net/foundation-frontiers/posts/2024/10/31/data-detectives.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">References</h2>

<ol>
<li id="fn1"><p>Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. Everyone wants to do the model work, not the data work: Data cascades in High-Stakes AI. In proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1–15, 2021.↩︎</p></li>
<li id="fn2"><p>Thomas Redman and Roger Hoerl. Data quality and statistics: Perfect together? Quality Engineering, 35(1):152–159, 2023.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://realworlddatascience.net/foundation-frontiers/posts/2025/10/30/data-detectives.html</guid>
  <pubDate>Thu, 30 Oct 2025 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/foundation-frontiers/posts/2025/10/30/images/thumbnail.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Code, Calculate, Change - How Statistics Fuels AI’s Real World Impact: EICC Live</title>
  <link>https://realworlddatascience.net/foundation-frontiers/posts/2025/09/17/EICC_Live.html</link>
  <description><![CDATA[ 





<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/CeZpkZzWcuo" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Artificial Intelligence (AI) is transforming how we live, work, and make decisions every day – from the content we see on social media to how we’re hired, navigate to work, or how spam is filtered from our inboxes. But what exactly is AI? How does it work, where did it come from, and where is it taking us?</p>
<p>Dr Sophie Carr, chair of the <em>Real World Data Science</em> <a href="https://realworlddatascience.net/the-pulse/editors-blog/posts/2022/10/18/meet-the-team.html">board of editors</a>, was joined by a panel of expert speakers for a public lecture in Edinburgh at the beginning of the month. <em>Real World Data Science</em> <a href="https://realworlddatascience.net/the-pulse/posts/2025/07/28/NHS-foundation-AI.html">contributor</a> Will Browne delivered “a hitchhiker’s guide to the history of AI”, taking us from the first ever algorithm (coded by “poetical scientist” Ada Lovelace) to today’s large langugage models, via a counting horse and a US naval invention.</p>
<p>Parwez Diloo, a data scientist at <a href="https://baysconsulting.co.uk/">Bays Consulting</a>, talked about how to balance technology with a human touch in recruitment processes (and the difference between maths and magic!)</p>
<p>And Amy Wilson, a lecturer in industrial mathematics at the University of Edinburgh, spoke about graphical modelling for decision-making in criminal contexts, touching on the legal failures of probabilistic reasoning in high profile cases like that of Lucy Letby and Amanda Knox.</p>
<p>The talk was rounded off by a lively Q&amp;A session which covered the viability of AI-designed graphical models, remedies to the so-called inappropriate uses of AI, and collective action we can take to ensure AI bias does not entrench existing inequalities.</p>
<p>This talk was part of the <a href="https://www.eicc.co.uk/eicc-live/">EICC Live</a> programme, a series of free public talks held by the <a href="https://www.eicc.co.uk/">EICC</a> as part of a commitment to community engagement and quality education. The talk was filmed by EICC and is published here with thanks to them. It took place at the RSS 2025 International Conference.</p>
<div class="g-col-12 g-col-sm-6">
<div class="nav-btn">
<p><a href="../../../../../foundations-frontiers/index.qmd">Back to Foundations &amp; Frontiers</a></p>
</div>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">

</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2025 EICC
</dd>
</dl>
</div>


</div>
</div>

 ]]></description>
  <category>AI</category>
  <category>Communication</category>
  <category>Skills</category>
  <guid>https://realworlddatascience.net/foundation-frontiers/posts/2025/09/17/EICC_Live.html</guid>
  <pubDate>Wed, 17 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/foundation-frontiers/posts/2025/09/17/images/ELgraphic.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>All Creatures, Great, Small, and Artificial</title>
  <dc:creator>Robyn Lowe and Edward Rochead</dc:creator>
  <link>https://realworlddatascience.net/foundation-frontiers/posts/2025/08/26/new veterinary medicine.html</link>
  <description><![CDATA[ 





<p>This article had its genesis when co-author Ed’s dog, Sparkle, was treated for pneumonia in the summer of 2024. Ed, a mathematician and chair of the <a href="https://alliancefordatascienceprofessionals.com/">Alliance for Data Science Professionals</a>, was intrigued by the surgery’s use of data in Sparkle’s treatment and decided to find out more about the use of data and AI in veterinary medicine. His exploration led to a guest appearance on the <a href="https://www.vetvoices.co.uk/podcasts">Vet Voices on Air</a> podcast hosted by co-author Robyn. She is a registered veterinary nurse (RVN) and the director of <a href="https://www.vetvoices.co.uk/">Veterinary Voices UK</a>. Inspired by that conversation, this article explores the ways veterinary professionals are currently applying data science principles and how professions adapt and evolve in the face of these developments.</p>
<div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Sparkle. Credit: Edward Rochead." data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://realworlddatascience.net/foundation-frontiers/posts/2025/08/26/images/Sparkle.png" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="Sparkle. Credit: Edward Rochead.">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Sparkle. Credit: Edward Rochead
</figcaption>
</figure>
</div>
<p>The use of AI and the data science that underpins and enables it are growing in ubiquity, and one area that is embracing these approaches is veterinary medicine.</p>
<p>Unlike human medicine in the UK, veterinary medicine is not organised under a centralised system such as the National Health Service (NHS). Instead, veterinary care is delivered through a variety of business structures, including Joint Venture Practices, Independent, Corporate, and Charity. These structures differ not only in ownership and funding but also in the scope and services that the practices provide. In many cases,the availability of more specialised care may depend on the expertise of individual(s) within the practice. Broadly speaking, practices tend to include: farm animals; exotics; equine; small animal; and mixed. Some practices will cover zoological work, conservation work and invertebrate work among other specialties.</p>
<p>Veterinary surgeons and RVNs are also employed in academia, conducting applied research in industry or government, and as advisors in government agencies.</p>
<section id="data-in-the-veterinary-profession-challenges-and-opportunities" class="level2">
<h2 class="anchored" data-anchor-id="data-in-the-veterinary-profession-challenges-and-opportunities">Data in the Veterinary Profession: Challenges and Opportunities</h2>
<p>If Artificial Intelligence (AI) is to be used in any sphere, it needs to be trained on data. The data used to train should be relevant, complete, structured, accurate, consistently formatted, and labelled. Achieving this standard is a challenge not only in veterinary medicine but also in many other fields where data are fragmented and inconsistently recorded. In the veterinary profession, unlike centralised NHS data, the veterinary data are often stored in individual practices or farms. These may use different formats and scales (such as imperial or metric), US or UK date formats, and twelve or twenty-four hour clocks. These records may also fail to follow the animal if it is sold or moves to a new practice. Such inconsistencies mirror the difficulties faced in other domains, and can make the adoption of AI in veterinary medicine particularly complex..</p>
<p>On the other hand, animal data has fewer constraints than human data. Article 4 of the <a href="https://www.gov.uk/data-protection">UK General Data Protection Regulation</a> (GDPR) makes it clear that the act applies to ‘personal data’ and specifies that ‘an identifiable natural person is one who can be identified’, which means that there is potentially more freedom to use data related to animals than humans. (It is worth noting that the GDPR would apply to the farmer, pet owner, or veterinary staff involved, so some consideration might still be required.) Given this data is an asset, it is worth considering whether it is owned by the animal’s owner or the veterinary professional (or their employer) in any given circumstance.</p>
</section>
<section id="how-ai-is-already-transforming-veterinary-practice" class="level2">
<h2 class="anchored" data-anchor-id="how-ai-is-already-transforming-veterinary-practice">How AI is Already Transforming Veterinary Practice</h2>
<p>AI is becoming an affordable and widely used tool in veterinary medicine. It’s now commonly applied in areas like diagnostics, treatment, and disease monitoring and prediction, despite the misconception that it’s rarely used. Preventative healthcare has always been a key aim within veterinary medicine. The obligation to ensure that both animal health and welfare and public health are accounted for is reflected by point 6.1 of the <a href="https://www.rcvs.org.uk/setting-standards/advice-and-guidance/code-of-professional-conduct-for-veterinary-surgeons/#public">Code of Professional Conduct for Veterinary Surgeons</a> and <a href="https://www.rcvs.org.uk/setting-standards/advice-and-guidance/code-of-professional-conduct-for-veterinary-nurses/#public">RVNs</a>: ‘6.1 Veterinary surgeons must seek to ensure the protection of public health and animal health and welfare’.</p>
<p><strong>Diagnostics</strong><br>
Diagnosis and prediction of diseases is one key area where AI is being used in veterinary medicine in farm animals, companion animals and beyond.</p>
<p>For example, in companion animals AI has been used to assist in the diagnosis of canine <a href="https://pubmed.ncbi.nlm.nih.gov/32006871/">hypoadrenocorticism</a>, an endocrine disease. Additionally, machine learning algorithms have potential for improving the <a href="https://pubmed.ncbi.nlm.nih.gov/40440642/">prediction and diagnosis</a> of leptospirosis, an infectious zoonotic disease. Additionally, by combining MRI data with facial image analysis, an AI tool can assist in predicting the likelihood of <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/jvim.15621">chiari like malformation (CM) and syringomyelia (SM)</a> from images of the dog’s head obtained via an owner’s smartphone. And finally, AI can also assist with faecal analysis: images are analysed by proprietary <a href="https://dugganvet.ie/ovacyte/">Artificial Intelligence models</a> which reference the images against the Telenostic Reference Library, a company specialising in parasitology diagnostic solutions. The image recognition software identifies each specific parasite species and the number of parasitic eggs or oocysts present.</p>
<p>These are just a few examples of AI use in companion animals currently.</p>
<p><strong>Disease Monitoring and Prediction</strong> Disease monitoring and prediction are exciting because they can help us act earlier—sometimes even preventing illness. This not only improves animal health and welfare, but also supports <a href="https://www.skeptic.org.uk/2024/05/agr-tech-will-technology-help-or-hinder-food-production-and-animal-welfare/">antimicrobial stewardship</a> by reducing unnecessary treatments, helping to combat antimicrobial resistance—a serious global threat to both animals and humans.</p>
<p>An area that demonstrates compelling evidence of these positive outcomes is <a href="https://www.skeptic.org.uk/2024/05/agr-tech-will-technology-help-or-hinder-food-production-and-animal-welfare/">farming and agriculture</a>, where farmers are able to use AI to monitor herds, and act promptly to treat disease, before it would have been evident and notable by human monitoring. Examples, which will be explored in more detail below, include body condition technology, lameness technology, disease recognition, grazing, land and pasture management, biosensors and biochips and more.</p>
<div id="fig-2" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Technology that measures body condition each time the cow passes under the camera, reporting the changes in Body Condition Score directly to the farmer via app and online portal, helping to support individual cow treatment, group rationing and herd management." data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://realworlddatascience.net/foundation-frontiers/posts/2025/08/26/images/herdvision1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="Technology that measures body condition each time the cow passes under the camera, reporting the changes in Body Condition Score directly to the farmer via app and online portal, helping to support individual cow treatment, group rationing and herd management.">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Technology that measures body condition each time the cow passes under the camera, reporting the changes in Body Condition Score directly to the farmer via app and online portal, helping to support individual cow treatment, group rationing and herd management.
</figcaption>
</figure>
</div>
<p><strong>Body Condition Technology</strong> The agricultural industry typically relies on subjective visual observation, human recording and manual reporting of all the key health and welfare traits, including Body Condition Score (BCS). Despite these individuals being highly skilled professionals, there is inevitable human error, paired with the constraints of busy farm management which can lead to cases getting picked up later in their disease process. BCS is a major indicator of metabolic performance in dairy cows and directly related to fertility performance and health traits. Technologies such as <a href="https://herd.vision/">Herdvision</a> use a 2D and 3D camera system to monitor BCS, resulting in improvement in cattle heath and fertility, less premature culling, and savings on feeding costs.</p>
<p><strong>Lameness Technology</strong><br>
Lameness is considered one of the <a href="https://www.frontiersin.org/articles/10.3389/fvets.2019.00094/full">top cattle health and welfare challenges</a>. A <a href="https://www.sciencedirect.com/science/article/abs/pii/S1871141313001698">2013 study</a> noted that almost 70% of the dairy farmers expressed an intention to take action for improving dairy cow foot health. Cattle naturally mask the signs of pain, and as with body condition scoring we have relied on subjective visual observation, human recording and manual reporting of all the key health and welfare traits. Technology that can pick up lameness earlier, with more objectivity and with less labour intensity is hugely beneficial to both the animals’ health and welfare and the farm’s profitability.</p>
<div id="fig-3" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Images that produce prioritisation list for vets and hoof trimmers, ranking cows according to severity of immobility and identifying small changes in mobility and BCS before they are visible to the human eye." data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://realworlddatascience.net/foundation-frontiers/posts/2025/08/26/images/herdvision2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="Images that produce prioritisation list for vets and hoof trimmers, ranking cows according to severity of immobility and identifying small changes in mobility and BCS before they are visible to the human eye.">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Images that produce prioritisation list for vets and hoof trimmers, ranking cows according to severity of immobility and identifying small changes in mobility and BCS before they are visible to the human eye.
</figcaption>
</figure>
</div>
<p><strong>Disease Recognition</strong> As with lameness assessments, monitoring of pain in the UK pig industry relies on human observation, either in person or via video footage, to detect disease.</p>
<p>An interdisciplinary team at the Newcastle University have <a href="https://www.ukri.org/who-we-are/how-we-are-doing/research-outcomes-and-impact/bbsrc/ai-based-monitoring-aids-on-farm-disease-detection/">used artificial intelligence to develop automated systems</a> to analyse and monitor pig behaviour and health. The algorithm was tested in a controlled environment where infection and disease were present, assessing footage of pigs captured by cameras and pinpointing and quantifying changes in behaviours to identify links to disease.</p>
<p>Other computer vision and AI-based approaches have allowed the <a href="https://www.sciencedirect.com/science/article/abs/pii/S0168169920300673">automatic scoring</a> of pigs in relation to posture, aggressive episodes, tail-biting episodes, fouling, diarrhoea, stress prediction in piglets, weight estimation, and body size – all providing animal farmers increased insight into the health of their population.</p>
<p><strong>Grazing, Land and Pasture Management</strong> The use of AI has allowed more efficient pasture and grazing management, allowing movement of livestock onto new pastures when the grazing quality and quantity depletes below a certain threshold.</p>
<p>There are numerous methods of using Agri-Tech to monitor animals, such as the <a href="https://www.mdpi.com/1424-8220/19/3/603">SheepIT</a> project, an initiative where an automated IoT-based system controls grazing sheep. Typically, such solutions are split into two main groups: location monitoring and behaviour and activity monitoring. Location monitoring allows farmers to keep track of animals, inferring preferred pasturing areas and grazing times, and even detecting absent animals. Behaviour and activity monitoring focuses on detecting the type and duration of an animal’s activities – for example resting, eating or running - based on accelerometry and audiometry.</p>
<p><strong>Biosensors and Biochips</strong> In human medicine, advances in molecular medicine and cell biology have <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3270855/">driven the interest in electrochemical systems to detect disease biomarkers</a> and therapeutic compounds (medications for example). Currently in human literature, implantable biosensors have been noted in <a href="https://www.sciencedirect.com/science/article/abs/pii/S0956566305003544">glucose monitoring</a>, <a href="https://ieeexplore.ieee.org/abstract/document/4118162/">DNA detection</a> and <a href="https://www.sciencedirect.com/science/article/abs/pii/S0003269708007264">cultures</a>, among others. Microelectronic technology offers powerful circuits and systems to develop innovative and miniaturised biochips for sensing at the molecular level; these have numerous applications in veterinary medicine from hormone detection, pathogenic microorganism and infection monitoring and homeostatic mechanism surveillance (homeostasis being the bodies regulatory mechanisms that controls many functions and maintain stability) such as being applied to <a href="https://www.anl.gov/article/biochips-to-investigate-cattle-disease-win-entrepreneurial-challenge">pathogen detection</a> in cattle mastitis.</p>
<p>Paul Horwood, Farm Vet and Founder of <a href="https://www.bsas.org.uk/events/ailive/">AI(Live)</a>, a conference on the development of AI applications in the livestock industry, sees this as a time of opportunity for the profession:</p>
<p><em>“The farm vet’s role continues to evolve from”problem-solver after the fact” to “strategic advisor at the heart of herd health planning.” Technology is helping us get there by giving us earlier insights, better data, and stronger evidence for the decisions we make every day. We’re at a pivotal moment. The technology is here. The challenge is knowing how to use it and how to lead with it. As a farming nation, we have always been innovative; as farm animal veterinary surgeons, we can either wait to be brought in at the end of the conversation, or step forward now to shape how AI is used on UK farms. Let’s choose the latter.”</em></p>
<p><strong>Shared Frontiers: Common Threads in AI Adoption Across Sectors</strong></p>
<p>The veterinary sector, like every other industry, is on a journey when it comes to the use of artificial intelligence, and many of the themes that are emerging are common to other sectors.</p>
<p>For veterinary professionals this includes:</p>
<ul>
<li>The need to radically change training of new vets and RVNs to ensure that they are prepared to embrace the new opportunities that AI will bring.<br>
</li>
<li>The need to upskill existing vets and RVNs to enable them to use these new opportunities.</li>
<li>Working with stakeholders, such as in this case farmers and pet owners, to evolve the business model to ensure that all parties benefit from the change.</li>
<li>A change in the attitude to data, in which it becomes seen as a business asset when it is well managed, with the ultimate benefit in this sector of promoting the wellbeing of animals.</li>
</ul>
<p>These recurring patterns offer a blueprint for understanding how professions evolve in response to developments in the field, and a reminder that AI isn’t just transforming high-tech labs and Fortune 500 boardrooms – it is quietly revolutionising industries across every sector. By looking at how specific professions, like veterinary, are navigating this shift, we can better understand the broader dynamics at play when machine learning meets existing practice.</p>
</section>
<section id="bridging-disciplines-unlocking-value-through-interdisciplinary-collaboration" class="level2">
<h2 class="anchored" data-anchor-id="bridging-disciplines-unlocking-value-through-interdisciplinary-collaboration">Bridging Disciplines: Unlocking Value Through Interdisciplinary Collaboration</h2>
<p>This is a pivotal moment where the intersection of data science and veterinary medicine offers a unique opportunity for cross-sector collaboration, driving progress in both fields.</p>
<p>The field of data science has much to offer industries currently experiencing these inflection points. Although many data scientists come from ‘traditional’ backgrounds such as statistics, mathematics, or computer science graduates, many more diverse routes to data science roles now exist. These routes include people who wouldn’t necessarily call themselves data scientists who work in other professions who use data science in their working life, upskilling themselves through training, or even trial and error. The authors are already aware of veterinary professionals who are skilled data scientists, even if they may not identify as such, applying data science to veterinary research in academia or industry. The RSS, other professional bodies within the Alliance for Data Science Professionals, and Data Science departments in universities may find offering Continuous Professional Development opportunities to the veterinary profession worthwhile. Certainly, the certifications offered by the RSS of Data Science Practitioner and Advanced Data Science Practitioner are open to veterinary professionals who have developed such skills.</p>
<p>The veterinary profession may also provide benefits to the data science community, by providing data sets that can be applied in many ways without major GDPR issues, as well as opportunities to showcase the benefits of data science to society and animal health and welfare through examples similar to those above.</p>
<p>One vehicle for more cross-pollination could be joint conferences. A near-term opportunity is <a href="https://www.ailive.farm/">AI (Live)</a> in September 2025, which aims to start the debate and establish the principles by which AI and livestock farming can derive the maximum benefits, with a focus on education, governance and application.</p>
<p>By fostering collaboration across disciplines, we can ensure that the benefits of this data revolution are shared—by all creatures, great, small, and artificial. And we are happy to report that Sparkle, whose illness sparked this article, has made a full recovery and in fact recently celebrated her ninth birthday!</p>
<div class="article-btn">
<p><a href="../../../../../foundation-frontiers/index.html">Explore more data science ideas</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the author</dt>
<dd>
<a href="https://www.linkedin.com/in/robyn-lowe-7274a596/"><strong>Robyn Lowe, BSc (Hons), Dip AVN (Surgery, Medicine, Anaesthesia), Dip HE CVN, RVN</strong></a>, is a registered veterinary nurse and Director of <a href="https://www.vetvoices.co.uk/">Veterinary Voices UK</a>, a community of veterinary professionals fostering public understanding of veterinary and animal welfare issues. She hosts the organisation’s <a href="https://open.spotify.com/show/2DcdmAMJrwRf2RdgUPcYCP">Vet Voices on Air</a> podcast.
</dd>
<dd>
<a href="https://www.linkedin.com/in/prof-edward-r-17768847/"><strong>Professor Edward Rochead, M.Math (Hons), PGDip, CMath, FIMA</strong></a> is a mathematician employed by the government, currently leading work on STEM Skills and Data. Ed is chair of the Alliance for Data Science Professionals, a Visiting Professor at Loughborough University, an Honorary Professor at the University of Birmingham, Chartered Mathematician, and Fellow of the IMA and RSA.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2025 Robyn Lowe and Edward Rochead.
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a> Text, code, and figures are licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">International licence</a>, except where otherwise noted. Thumbnail image by <a href="https://www.shutterstock.com/image-photo/cattle-cow-animal-farm-veterinary-agriculture-1463752661">Shutterstock/g/fotopanorama360</a> <a href="https://creativecommons.org/licenses/by/4.0/">Licenced by CC-BY 4.0</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Lowe, Robyn and Rochead, Edward. 2025. “All Creatures, Great, Small, and Artificial” Real World Data Science, August 22nd, 2025. <a href="https://realworlddatascience.net/foundation-frontiers/posts/2025/08/22/veterinary-medicine.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>


</section>

 ]]></description>
  <category>AI</category>
  <category>Algorithms</category>
  <category>Machine Learning</category>
  <guid>https://realworlddatascience.net/foundation-frontiers/posts/2025/08/26/new veterinary medicine.html</guid>
  <pubDate>Tue, 26 Aug 2025 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/foundation-frontiers/posts/2025/08/26/images/vet.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Deploying Agentic AI - What Worked, What Broke, and What We Learned</title>
  <link>https://realworlddatascience.net/applied-insights/case-studies/posts/2025/08/12/deploying-agentic-ai.html</link>
  <description><![CDATA[ 





<section id="we-built-agentic-systems.-heres-what-broke." class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="we-built-agentic-systems.-heres-what-broke."><span class="header-section-number">1</span> We Built Agentic Systems. Here’s What Broke.</h2>
<p>When Agentic AI started dominating research papers, demos, and conference talks, I was curious but cautious. The idea of intelligent agents, autonomous systems powered by large language models that can plan, reason, and take actions using tools, sounded brilliant in theory. But I wanted to know what happened when you used them. Not in a toy notebook or a slick demo, but in real projects, with real constraints, where things needed to work reliably and repeatably.</p>
<p>In my role as Clinical AI &amp; Data Scientist at Bayezian Limited, I work at the intersection of data science, statistical modelling, and clinical AI governance, with a strong emphasis on regulatory-aligned standards such as CDISC. I have been directly involved in deploying agentic systems into environments where trust and reproducibility are not optional. These include real-time protocol compliance, CDISC mapping, and regulatory workflows. We gave agents real jobs. We let them loose on messy documents. And then we watched them work, fail, learn, and (sometimes) recover.</p>
<p>This article is not a critique of Agentic AI as a concept. I believe Agentic AI has potential value, but I also believe it demands more critical evaluation. That means assessing these systems in conditions that mirror the real world, not in benchmark papers filled with sanitised datasets. It means observing what happens when agents are under pressure, when they face ambiguity, and when their outputs have real consequences. What follows is not speculation about what Agentic AI might become a decade from now. It is a candid reflection on what it feels like to use these systems today. It is about watching a chain of prompts unravel or a multi-agent system drop the baton halfway through a task. If we want Agentic AI to be trustworthy, robust, and practical, then our standards for evaluating it must be shaped by lived experience rather than theoretical ideals.</p>
</section>
<section id="what-agentic-ai-looks-like-in-practice" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="what-agentic-ai-looks-like-in-practice"><span class="header-section-number">2</span> What Agentic AI Looks Like in Practice</h2>
<p>If you’re imagining robots in lab coats, that’s not quite what this is. It is more like releasing a highly motivated intern into a complex archive with partial instructions, limited supervision, and the freedom to decide which filing cabinets, databases, or tools to open next. It is messy. It is unpredictable. And it sometimes surprises you with just how resourceful or confused it can get. Agentic AI systems are purpose-built setups where a large language model is given a task and enough autonomy to decide how to approach it. That might mean choosing which tools to use, when to use them, and how to adapt when things go off-script. You are not just sending one prompt and getting an answer. You are watching a system reason, remember, call APIs, retry when things go wrong, and ideally, get to a useful result.</p>
<p>At Bayezian, we have explored this in several internal projects, including generating clinical codes from statistical analysis plans and study specifications, monitoring synthetic Electronic Health Records (EHRs) for rule violations, and running chained reasoning loops to validate document alignment. These efforts reflect the reality of building LLM agents into safety-critical and compliance-heavy workflows. Across these deployments, the question is never just “can it do the task” but “can it do the task reliably, interpretably, and safely in context”.</p>
<p>Broader research has followed similar directions. In clinical pharmacology and translational sciences, researchers have explored how AI agents can automate modelling and trial design while keeping a human in the loop, and offering blueprints for scalable, compliant agentic workflows link. In the context of patient-facing systems, agentic retrieval-augmented generation has improved the quality and safety of educational materials, with LLMs acting as both generators and validators of content link. Other teams have used multi-agent systems to simulate cross-disciplinary collaboration, where each AI agent brings a different scientific role to design and validate therapeutic molecules like SARS-CoV-2 nanobodies link.</p>
<p>Some of the systems we built used agent frameworks like LangChain or LlamaIndex. Others were bespoke combinations of APIs, function libraries, memory stores, and prompt stacks wired together to mimic workflow behavior. Regardless of the architecture, the core structure remained the same. The agent was given a task, a bit of autonomy, and access to tools, and then left to figure things out. Sometimes it worked. Sometimes it did not. That gap between intention and execution is where most of the interesting lessons sit.</p>
<p>In the next section, I describe one of those deployments in more detail: a multi-agent system used to monitor data flow in a simulated clinical trial setting.</p>
</section>
<section id="case-study-monitoring-protocol-deviations-with-agentic-ai" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="case-study-monitoring-protocol-deviations-with-agentic-ai"><span class="header-section-number">3</span> Case Study: Monitoring Protocol Deviations with Agentic AI</h2>
<p><strong>Why We Built It</strong></p>
<p>Clinical trials generate a stream of complex data, from scheduled lab results to adverse event logs. Hidden in that stream are subtle signs that something may be off: a visit occurred too late, a test was skipped, or a dose changed when it shouldn’t have. These are protocol deviations, and catching them quickly matters. They can affect safety, skew outcomes, and trigger regulatory scrutiny.</p>
<p>Traditionally, reviewing these events is a painstaking task. Study teams trawl through spreadsheets and timelines, cross-referencing against lengthy protocol documents. It is time-consuming, easy to miss context, and prone to delay. We wondered whether an AI-driven approach could act like a vigilant reviewer. Not to replace the team, but to help it focus on what truly needed attention.</p>
<p>Our motivation was twofold. First, to introduce earlier, more consistent detection without relying on rule-based systems that often buckle under real-world variability. Second, to test whether a group of coordinated language model agents, each with a clear focus, could carry out this work at scale while still being interpretable and auditable.</p>
<p>To do that, we built the system from the ground up. We designed a pipeline that could ingest clinical documents, extract key protocol elements, embed them for semantic search, and store them in structured form. That created the foundation for agents to work not just as readers of data, but as context-aware monitors. Understanding whether a missed Electrocardiogram (ECG) or a delayed Day 7 visit violated the protocol required more than lookup tables. It required reasoning. It required memory. It required agents built with intent.</p>
<p>What emerged was a system designed not just to scan data, but to think with constraints, assess context, and escalate issues when the boundaries of the trial were breached. The goal was not perfection, but partnership. A system that could flag what mattered, explain why, and stay open to human feedback.</p>
<p><strong>How It Was Set Up</strong></p>
<p>The system was built around a group of focused agents, each responsible for checking a specific type of protocol rule. Rather than relying on one large model to do everything, we broke the task into smaller parts. One agent reviewed visit timing. Another checked medication use. Others handled inclusion criteria, missed procedures, or serious adverse events. This made each agent easier to understand, easier to test, and less likely to be overwhelmed by conflicting information.</p>
<p>Before any agents could be activated, however, an early classifier was introduced to determine what type of document had arrived. Was it a screening form or a post-randomisation visit report? That initial decision shaped the downstream path. If it was a screening file, the system activated the inclusion and exclusion criteria checker. If it was a visit document, it was handed off to agents responsible for tracking timing, treatment exposure, scheduled procedures, and adverse events.</p>
<p>These agents did not operate in isolation. They worked on top of a pipeline that handled the messy reality of clinical data. Documents in different formats were extracted, cleaned, and converted into structured representations. Tables and free text were processed together. Key elements from study protocols were embedded and stored to allow flexible retrieval later. This gave the agents access to a searchable memory of what the trial actually required.</p>
<p>While many agentic systems today rely heavily on frameworks like LangChain or LlamaIndex, our system was built from the ground up to suit the demands of clinical oversight and regulatory traceability. We avoided packaged orchestration frameworks. Instead, we constructed a lightweight pipeline using well-tested Python tools, giving us more control over transparency and integration. For semantic memory and search, protocol content was indexed using FAISS, a vector store optimised for fast similarity-based retrieval. This allowed each agent to fetch relevant rules dynamically and reason through them with appropriate context.</p>
<p>When patient data flowed in, the classifier directed the document to the appropriate agents. If any agent spotted something unusual, it could escalate the case to a second agent responsible for suggesting possible actions. That might mean logging the issue, generating a report, or prompting a review from the study team. Throughout, a human remained involved to validate decisions and interpret edge cases that needed nuance.</p>
<p>We did not assume the agents would get everything right. The idea was to create a process where AI could handle the repetitive scanning and flagging, leaving people to focus on the work that demanded clinical judgement. The combination of structured memory, clear responsibilities, document classification, and human oversight formed the backbone of the system.</p>
<p>Figure 1 illustrates a two-phase agentic system architecture, where protocol documents are first parsed, structured, and embedded into a searchable memory (green), enabling real-time agents (orange) to classify incoming clinical data from the Clinical Trial Management System (CTMS), reason over protocol rules, detect deviations, and escalate issues with human oversight.</p>
<div id="fig-cde" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cde-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://realworlddatascience.net/applied-insights/case-studies/posts/2025/08/12/images/figure-1-sa.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cde-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: System Architecture and Agent Flow
</figcaption>
</figure>
</div>
<p><strong>Where It Got Complicated</strong></p>
<p>In early tests, the system did what it was built to do. It scanned incoming records, spotted missing data, flagged unexpected medication use, and pointed out deviations that might otherwise have slipped through. On structured examples, it handled the checks with speed and consistency.</p>
<p>But as we moved closer to real trial conditions, the gaps started to show. The agents were trained to recognise rules, but real-world data rarely plays by the book. Information arrived out of order. Visit dates overlapped. Exceptions buried in footnotes became critical. Suddenly, a task that looked simple in isolation became tangled in edge cases.</p>
<p>One of the most frequent problems was handover failure. A deviation might be correctly identified by the first agent, only to be lost or misunderstood by the next. A flagged issue would travel halfway through the chain and then disappear or be misclassified because the follow-up agent missed a piece of context. These were not coding errors. They were coordination breakdowns, small lapses in memory between steps that led to big differences in outcome.</p>
<p>We also found that decisions based on time windows were especially fragile. An agent could recognise that a visit was missing, but not always remember whether the protocol allowed a buffer. That kind of reasoning depended on holding specific details in working memory. Without it, the agents began to misfire, sometimes raising the alarm too early, other times not at all.</p>
<p>None of this was surprising. We had built the system to learn from its own limitations. But seeing those moments play out across agents, in ways that were subtle and sometimes difficult to trace, helped surface the exact places where autonomy met ambiguity and where structure gave way to noise.</p>
<p><strong>A Glimpse Into the Details</strong></p>
<p>One case brought the system’s limits into focus. A monitoring agent flagged a protocol deviation for a missing lab test on Day 14. On the surface, it looked like a valid call. The entry for that day was missing, and the protocol required a test at that visit. The alert was logged, and the case moved on to the next agent in the chain.</p>
<p>But there was a catch.</p>
<p>The protocol did call for a Day 14 lab, but it also allowed a two-day window either side. That detail had been extracted earlier and embedded in the system’s memory. However, at the moment of evaluation, that context was not carried through. The agent saw an empty cell for Day 14 and treated it as a breach. It did not recall that a test on Day 13, which had already been recorded, fulfilled the requirement.</p>
<p>This was not a failure of logic. It was a failure of coordination. The information the agent needed was available, but not in the right place at the right time. The memory had thinned just enough between steps to turn a routine variation into a false positive.</p>
<p>From a human perspective, the decision would have been easy. A reviewer would glance at the timeline, check the visit window, and move on. But for the agent, the absence of a test on the exact date triggered a response. It did not understand flexibility unless that flexibility was made explicit in the prompt it received.</p>
<p>That small oversight rippled through the process. It triggered an unnecessary escalation, pulled attention away from genuine issues, and reminded us that autonomy without memory is not the same as understanding.</p>
<p><strong>How We Measured Success</strong></p>
<p>To understand how well the system was performing, we needed something to compare it against. So we asked clinical reviewers to go through a set of patient records and mark any protocol deviations they spotted. This gave us a reference set, a gold standard, that we could use to test the agents.</p>
<p>We then ran the same data through the system and tracked how often it matched the human reviewers. When the agent flagged something that was also noted by a reviewer, we counted it as a hit. If it missed something important or raised a false alarm, we marked it accordingly. This gave us basic measures like sensitivity and specificity, in plain terms, how good the system was at picking up real issues and how well it avoided false ones.</p>
<p>But we also looked at the process itself. It was not just about whether a single agent made the right call, but whether the information made it through the chain. We tracked handovers between agents, how often a detected issue was correctly passed along, whether follow-up steps were triggered, and whether the right output was produced in the end.</p>
<p>This helped us see where the system worked as intended and where things broke down, even when the core detection was accurate. It was never just a question of getting the right answer. It was also about getting it to the right place.</p>
<p><strong>What We Changed Along the Way</strong></p>
<p>Once we understood where things were going wrong, we made a few targeted changes to steady the system.</p>
<p>First, we introduced structured memory snapshots. These acted like running notes that captured key protocol rules and exceptions at each stage. Rather than expecting every agent to remember what came before, we gave them a shared space to refer back to. This made it easier to hold onto details like visit windows or exemption clauses, even as the task moved between agents.</p>
<p>We also moved beyond rigid prompt templates. Early versions of the system leaned heavily on predefined phrasing, which limited the agents’ flexibility. Over time, we allowed the agents to generate their own sets of questions and reason through the answers independently. This gave them more space to interpret ambiguous situations and respond with a clearer sense of context, rather than relying on tightly scripted instructions. Alongside this, we rewrote prompts to be clearer and more grounded in the original trial language. Ambiguity in wording was often enough to derail performance, so small tweaks, phrasing things the way a study nurse might, made a noticeable difference. We then added stronger handoff signals. These were markers that told the next agent what had just happened, what context was essential, and what action was expected. It was a bit like writing a handover note for a colleague. Without that, agents sometimes acted without full context or missed the point altogether. Finally, we built in simple checks to track what happened after an alert was raised. Did the follow-up agent respond? Was the right report generated? If not, where did the thread break? These checks gave us better visibility into system behaviour and helped us spot patterns that weren’t obvious from the output alone.</p>
<p>None of these changes made the system perfect. But they helped close the loop. Errors became easier to trace. Fixes became faster to test. And confidence grew that when something went wrong, we would know where to look.</p>
<p><strong>What It Taught Us</strong></p>
<p>The system did not live up to the hype, and it was not flawless, but it proved genuinely useful. It spotted patterns early. It highlighted things we might have overlooked. And, just as importantly, it changed how people interacted with the data. Rather than spending hours checking every line, reviewers began focusing on the edge cases and thinking more critically about how to respond. The role shifted from manual detective work to something closer to intelligent triage.</p>
<p>What agentic AI brought to the table was not magic, but structure. It added pace to routine checks, consistency to decisions, and visibility into what had been flagged and why. Every alert came with a traceable rationale, every step with a record. That made it easier to explain what the system had done and why, which in turn made it easier to trust.</p>
<p>At the same time, it reminded us what agents still cannot do. They do not infer the way people do. They do not fill in blanks or read between the lines. But they do follow instructions. They do handle repetition. They do maintain logic across complex checks. And in clinical research, where consistency matters just as much as cleverness, that counts for a lot.</p>
<p>This experience did not make us think agentic systems were ready to run trials alone. But it did show us they could support the process in a way that was measurable, transparent, and worth building on.</p>
<p><strong>What This Taught Us About Evaluation</strong></p>
<p>Working with agentic systems made one thing especially clear. The way most people assess language models does not prepare you for what happens when those models are placed inside a real workflow.</p>
<p>It is easy enough to test for accuracy or coherence in response to a single prompt. But those surface checks do not reflect what it takes to complete a task that unfolds over time. When an agent is making decisions, juggling memory, switching between tools, and coordinating with others, a different kind of evaluation is needed.</p>
<p>We began paying attention to the sorts of things that rarely make it into research papers. Could the agent perform the same task consistently across repeated attempts? Did it remember what had just happened a few steps earlier? When one component passed information to another, did it land correctly? Did the agent use the right tool when the moment called for it, even without being told explicitly?</p>
<p>These were not academic concerns. They were practical indicators of whether the system would hold up under pressure. So we built simple ways to track them.</p>
<p>We looked at how stable the agent remained from one run to the next. We measured how often a person needed to step in. We checked whether the agent could retrieve details it had already encountered. And we monitored how information moved through the system, from one part to another, without being lost or altered along the way.</p>
<p>None of this required complex metrics. But each of these signals told us more about how the system behaved in real use than any benchmark ever did.</p>
</section>
<section id="a-call-for-practical-evaluation-standards" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="a-call-for-practical-evaluation-standards"><span class="header-section-number">4</span> A Call for Practical Evaluation Standards</h2>
<p>If we want reliable ways to judge these systems, we need to start from what happens when they are used in the real world. Much of the current thinking around evaluating agentic AI remains too abstract. It often focuses on what the system is supposed to do in principle, not what it manages to do in practice. But the most useful insights emerge when things fall apart. When an agent loses track of its task, forgets what just happened, or takes an unexpected turn under pressure.</p>
<p><a href="https://sakana.ai/ai-scientist/">A recent assessment of Sakana.ai’s AI Scientist</a> made this point sharply. The system promised end-to-end research automation, from forming hypotheses to writing up results. It was an ambitious step forward. But <a href="https://arxiv.org/html/2502.14297v1">when tested</a>, it fell short in important ways. It skimmed literature without depth, misunderstood experimental methods, and stitched together reports that looked complete but were riddled with basic errors. One reviewer said it read like something written in a hurry by a student who had not done the reading. The outcome was not a failure of intent, but a reminder that sophisticated language does not always reflect sound reasoning.</p>
<p>Instead of designing evaluation methods in isolation, we should begin with real scenarios. That means observing where agents stumble, how they recover, and whether they can carry through when steps are long and outcomes matter. It means showing the messy bits, not just polished results. Tools that help us retrace decisions, inspect memory, and understand what went wrong are just as important as the outputs themselves.</p>
<p>Only by starting from lived use with its uncertainty, complexity, and human oversight, can we build evaluation methods that truly reflect what it means for these systems to be useful.</p>
</section>
<section id="closing-thoughts-from-the-field" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="closing-thoughts-from-the-field"><span class="header-section-number">5</span> Closing Thoughts from the Field</h2>
<p>Agentic AI carries genuine promise, but even a single deployment can reveal how much distance there is between ambition and execution. These systems can be impressively capable in some moments and surprisingly brittle in others. And in domains where decisions must be precise and timelines matter, that brittleness is more than an inconvenience; it introduces real risk.</p>
<p>The lessons from our experience were not abstract. They came from watching one system try to handle a demanding, high-context task and seeing where it stumbled. It was not a matter of poor design or unrealistic expectations. The complexity was built in, the kind that only becomes visible once a system moves beyond isolated prompts and into continuous workflows.</p>
<p>That is why evaluation needs to begin with real use. With lived attempts, not controlled tests. With unexpected behaviours, not just benchmark scores. As practitioners, we have a front-row seat to what breaks, what improves with small tweaks, and what truly helps. That view should help shape how the field evolves.</p>
<p>If agentic systems are to mature, the stories of where they struggled and how we adapted cannot sit on the sidelines. They are part of how progress happens. And they may be the clearest indicators of what needs to change next.</p>
<div class="article-btn">
<p><a href="../../../../../../applied-insights/case-studies/index.html">Find more case studies</a></p>
</div>
<dl>
<dt>About the authors</dt>
<dd>
<a href="https://www.linkedin.com/in/francis-osei-b2b02116a/"><strong>Francis Osei</strong></a> is the Lead Clinical AI Scientist and Researcher at Bayezian Limited, where he designs and builds intelligent systems to support clinical trial automation, regulatory compliance, and the safe, transparent use of AI in healthcare. His work brings together data science, statistical modelling, and real-world clinical insight to help organisations adopt AI they can understand, trust, and act on.
</dd>
</dl>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2025 Francis Osei
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>. Thumbnail photo by <a href="https://www.shutterstock.com/g/donut8449">khunkornStudio</a> on <a href="https://www.shutterstock.com/image-photo/ai-chatbot-technology-virtual-assistant-customer-2582430481">Shutterstock</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Osei, F. (2025). “Deploying Agentic AI: What Worked, What Broke, and What We Learned”, Real World Data Science, August 12, 20245. <a href="https://realworlddatascience.net/applied-insights/case-studies/posts/2025/08/12/deploying-agentic-ai.html">URL</a>
</dd>
</dl>
</div>
<p>::: :::</p>


</section>

 ]]></description>
  <category>Reproducibility</category>
  <category>Data Analysis</category>
  <category>Machine learning</category>
  <category>Statistics</category>
  <guid>https://realworlddatascience.net/applied-insights/case-studies/posts/2025/08/12/deploying-agentic-ai.html</guid>
  <pubDate>Tue, 12 Aug 2025 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/applied-insights/case-studies/posts/2025/08/12/images/agentic-ai.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Book Review: AI in Business: Towards the Autonomous Enterprise by Sarah Burnett</title>
  <link>https://realworlddatascience.net/the-pulse/posts/2025/08/04/AI-in-Bus-Review.html</link>
  <description><![CDATA[ 





<center>
As AI continues to reshape industries, business leaders are increasingly seeking guidance on how to harness its potential responsibly and effectively. Keeping abreast of these conversations is crucial for data practitioners seeking to guide their non-technical stakeholders and foster cross-functional collaboration. The recently released second edition of <strong><a href="https://shop.bcs.org/page/detail/ai-in-business/?SF1=work_exact&amp;ST1=AIINBUSINESS2">AI in Business: Towards the Autonomous Enterprise</a></strong> (BCS, 2024) aims to help decision-makers understand the strategic opportunities and challenges of AI in a business context. We asked Ed Rochead, Chair of the <a href="https://alliancefordatascienceprofessionals.com/">Alliance for Data Science Professionals</a>, to reflect on the themes, frameworks and case studies it offers.
</center>
<div id="fig-cde" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cde-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://realworlddatascience.net/the-pulse/posts/2025/08/04/Images/bookcover.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cde-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: AI in Business: Towards the Autonomous Enterprise
</figcaption>
</figure>
</div>
<p>The key words in the title of the book are ‘autonomous enterprise’: this is a book about using AI to make enterprises more effective with autonomy (AI), rather than (as the cover picture of a robot might suggest) embed autonomous robotic systems in a business.</p>
<p>The volume is in three main parts. The first introduces AI, with useful explanations of terms like “generative AI” and gives some thoughts about how AI might be used for innovation and efficiency in a business. The second part gives some case studies on how real, recognisable organisations have successfully used AI to automate their operations with some success. The third part reflects upon the future, focusing on how an organisation might start the journey towards autonomy, as well as some thoughts on the impact of autonomy on society.</p>
<p>A chapter I found particularly helpful is ‘What You Need to Know About AI.’ It seeks to explain the relevant terms at the level required by an industry or business leader, rather than giving an in-depth technical account of the concepts, and in this the author achieved their intent.</p>
<p>At the heart of the book are the case studies, involving organisations that include international companies, an NHS Trust, and a district council. The reader is likely to have some personal experience of these types of organisations, for instance as a patient or resident. This made the case studies even more engaging, at least to me, as not only could I put myself in the shoes of a leader in the organisation concerned, it was also possible to empathise with those involved in the system. This knowledge and interest really brought the content to life, and this made the author’s choice of case studies inspired.</p>
<p>The closing section is intriguing. The chapter introducing the first steps an organisation might take towards autonomy is helpful, as it illustrates example of the stages of autonomy as applied to buying a car (spoiler alert – in the last stage it is manufactured and then drives itself to the consumer’s home!) The second chapter in this section, looking towards the future, gets the reader thinking more broadly about the impact of automation on society. This includes the thorny issues of ethics and impact on things like (un)employment; both areas were covered engagingly and thought provokingly.</p>
<p>Although impressed with the content of the book, I found the typeface very cramped and small, and joked with friends that 200 pages of material is crammed into 160. This makes the contents a harder read than they might otherwise be.</p>
<p>In one sense, each of the three sections would make a good, if short, book but, read together in the sequence provided, they become more than the sum of their parts, with the first part informative, the second part engaging, and the third thought provoking. <em>AI in Business</em> is an excellent read for an organisational leader seeking inspiration to automate. It gives enough language and concept familiarity to enable such a reader to ask sensible questions of technical experts, and an idea of the art of the possible. Someone with an interest in how AI might change the world around us could also find this a fascinating and informative read.</p>
<div class="article-btn">
<p><a href="../../../../../the-pulse/index.html">Discover more The Pulse</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-6">
<dl>
<dt>About the author</dt>
<dd>
<a href="https://www.linkedin.com/in/prof-edward-r-17768847/">Professor Edward Rochead, M.Math (Hons), PGDip, CMath, FIMA</a> is a mathematician employed by the government, currently leading work on STEM Skills and Data. Ed is chair of the <a href="https://alliancefordatascienceprofessionals.com/">Alliance for Data Science Professionals</a>, a Visiting Professor at Loughborough University, an Honorary Professor at the University of Birmingham, Chartered Mathematician, and Fellow of the IMA and RSA. Copyright and licence
</dd>
<dd>
© 2025 Royal Statistical Society
</dd>
<dd>
Thumbnail image by <a href="https://www.bcs.org/">BCS</a> <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a> This article is licensed under a Creative Commons Attribution (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">International licence</a>.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">

</div>
</div>
</div>



 ]]></description>
  <category>AI</category>
  <category>Data Science</category>
  <category>Machine learning</category>
  <category>Collaboration</category>
  <guid>https://realworlddatascience.net/the-pulse/posts/2025/08/04/AI-in-Bus-Review.html</guid>
  <pubDate>Mon, 04 Aug 2025 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/the-pulse/posts/2025/08/04/Images/bookcover2.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>From medical history to medical foresight: why the NHS needs its own foundation AI model for prevention</title>
  <link>https://realworlddatascience.net/the-pulse/posts/2025/07/28/NHS-foundation-AI.html</link>
  <description><![CDATA[ 





<p>The point of this article is to, in a mildly entertaining way, persuade you that developing a sovereign foundation AI model should be a priority for the NHS, professional bodies and patients but we need to get the research right.</p>
<p><strong>Risk is personal</strong></p>
<p>How do we move from treating disease to preventing disease? The traditional approach has been to publicise well evidenced public health interventions; don’t smoke, drink less, eat vegetables, exercise, vaccinate, wear sunscreen. This is all very good advice at the population level but for the individual it’s hard to know what to worry about and what to prioritise. I, being a clumsy man with bad ankles and a lack of spatial awareness, am at risk of going to A&amp;E with (another) concussion. You will be different.</p>
<p><strong>A little bit of history</strong></p>
<p>Individualised risk models in healthcare are not new. Traditional statistical approaches have used tabular data to predict healthcare events and have done a good job. These models are converted into questionnaires that clinicians can use to make decisions based on your risk. If you have had the NHS health check; a clinician will have measured blood pressure, cholesterol, height, weight and a few questions on your medical history. They will then feed this into a model and the output is the risk of you having a heart attack or stroke over the next ten years (1). There are also automated approaches built into the systems your GP uses that help stratify the population based on individual risk of things like frailty (2).</p>
<p>These kind of models are usually based on a snapshot of data and require bespoke data pipelines and engineering to massage the data into the right shape for the model, wonderful news for data scientists and statisticians as it leads to a proliferation of finely tuned models which can keep us in gainful employment for many years. However, each one has significant costs to develop, test, validate, deploy and integrate into clinical practice.</p>
<p>Another issue with these traditional models is that they squash a medical history into a single row of data for each patient, losing the chronology of health. Intuitively, we would expect that the sequence of events matters in predicting healthcare outcomes and traditional approaches struggle to capture this.</p>
<p><strong>Using a sequence of events to predict a sequence of events</strong></p>
<p>Sequences of events are easier for data engineers too. It’s much simpler to join together all the data into a sequence than perform a series of complex aggregations and transformations for every model. The simpler the data engineering needed to create the inputs the easier it is to scale as you are making fewer assumptions about the data.</p>
<p>So, if they are easier to engineer, and they capture more information why are they not the standard way of predicting health outcomes? Because modelling sequences is harder than modelling a row of data. As the model sees more of a sequence it has to hold that memory somewhere so that the model can accumulate the appropriate information. Models that could do this started appearing in machine learning literature in the early 1990’s (3) but for a long time we had neither the data, the computing power nor quite the right kind of algorithms to make them useful. Today they have become feasible in healthcare due to the rise in electronic healthcare records, standardised codes for classifying events and the rise of the transformer model. Transformer models combine the ability to hold an internal “memory” of the sequence with the capacity to pay attention to different aspects of the sequence, which basically make them magic.</p>
<p>These models have demonstrated state of the art accuracy in predicting future events using electronic patient histories. Examples for those interested in reading more include BEHRT (4), Med-Bert (5), TransformerEHR (6) and the more recent generative transformer model ETHOS (7). These can be used for a range of healthcare prediction tasks whilst delivering state of the art predictive accuracy, again, magic.</p>
<p>A recent preprint (8) from Microsoft has also demonstrated that these EHR models act in a similar way to the large language models like those backing ChatGPT; their performance scales predictably with processing power, data and the size of the model. This means that more data will probably lead to a better model and we can optimise this model performance to a given computational budget.</p>
<p><strong>So what?</strong></p>
<p>Why should you care about this? If we can take these architectures and train them on data at the scale of the NHS then each individual patient could have a relatively accurate prediction of their most likely next healthcare events(9). It would be your medical history projected forward, providing a narrative that is easier to understand than a page of risk scores. It’s your potential medical future. This could help with changing behaviour to reduce future risk, something we all struggle with. I think of it like the medical version of the ghost of Christmas future but using a chain of events rather than clinking ghost chains.</p>
<p>We are already seeing heavy usage of publicly available large language models for healthcare. 10% of a representative sample of Australians used ChatGPT for medical advice rising to 26% of 25-34 year olds (10) , I assume the UK is similar. It seems that the public is much more ready than the health system to use these models and regulation is struggling to keep up, and for good reason, they may not actually help.</p>
<p><strong>The underwhelming evidence</strong></p>
<p>As of August 2024 there were 950 AI models approved by the FDA, with a significant proportion of those for clinical decision support, but only 2.4% of these are supported by randomised controlled trials (11).</p>
<p>This is important, as what works on a machine learning researcher’s infrastructure may not work in a clinical setting. In 2018, a comprehensive health economic evaluation of a risk prediction model for identifying people at risk of hospital admission found that those in the treatment arm had a higher healthcare cost and there was no significant impact on the number of people being admitted to hospital, despite accurate predictions (12). Some prediction models even cause harmful self-fulfilling prophecies when used for decision making (the paper is well worth a read) (13).</p>
<p><strong>The prize</strong></p>
<p>The UK government is clear about the ambition to be an “AI maker” not an “AI taker”. Given the expected improvement in accuracy from scaling these EHR models, there is an opportunity for the UK to leverage what should be one of its greatest data assets (decades of longitudinal electronic healthcare records from cradle to grave) and create a sovereign foundational model that supports patient care. These are being developed now in the US and elsewhere. A meta-analysis in 2023 found over 80 foundational healthcare models, there are many more today and there is concern that at some point it will be cheaper for the NHS to bring one in and pay for it than to train its own.</p>
<p><strong>Foresight</strong></p>
<p>Fortunately we have made some progress in the UK with NHS data. Foresight (14), a transformer model developed in London on data from 1.4 million patients has demonstrated impressive results . This model has been taken on for covid research to see if the same approach can better predict disease/COVID-19 onset, hospitalisation and death, for all individuals, across all backgrounds and diseases using national data made available during the pandemic for research specifically on covid. This is being done through the British heart foundation’s collaboration with NHS England’s secure data environment (15).</p>
<p>However, just because we can do this, it does not mean that we should. Researchers need to be careful to stay within the bounds of their project and make extraordinary efforts to engage with the public. We have to ensure that our data is not being exploited inappropriately for commercial gain. The Royal College of General Practitioners has raised concerns that this model goes beyond what they agreed to, Professor Kamila Hawthorne, Chair of the Royal College of GPs, said “As data controllers, GPs take the management of their patients’ medical data very seriously, and we want to be sure data isn’t being used beyond its scope, in this case to train an AI programme.” The project has been paused for the time being despite being approved and specifically targeted at covid for research.</p>
<p>The best model for predicting outcomes from covid or the risk factors involved in covid is likely to be a population scale generative transformer model. This research will determine whether that hypothesis is true and whether this kind of data could provide more accurate predictions for patients. The NHS data and the model are kept inside a secure data environment with personal identifiers stripped out. No patient details are passed to researchers and no data or code leaves that environment without explicit permission. This research seems like something we should do.</p>
<p>Despite the potential of AI assisted clinicians for differential diagnosis (with recent evidence that they perform better than both clinicians alone and clinicians using search (16) and the attractiveness of having your medical history and your medical future in your pocket, we are a way off this reality. The gap between research and demonstrating the cost-effectiveness of AI solutions in the real world is significant but all the component parts needed to close this gap exist; the data, the models, the research capability and the political will.</p>
<p>We will get there. Foundational models in healthcare are no longer a theoretical possibility, but an imminent reality. The UK has a rare opportunity to lead, not follow, by building a sovereign AI model trained on NHS data to accelerate the transition from treating disease to preventing disease. To get there, we must confront hard questions about patient engagement and real-world benefit. But to stop research based solely on the sophistication of the method is to misunderstand the moment. I think patients expect us to do better.</p>
<div class="keyline">
<hr>
</div>
<p><strong>References</strong></p>
<ol type="1">
<li><p>Hippisley-Cox, J., Coupland, C.A.C., Bafadhel, M. et al.&nbsp;Development and validation of a new algorithm for improved cardiovascular risk prediction. Nat Med 30, 1440–1447 (2024). https://doi.org/10.1038/s41591-024-02905-y</p></li>
<li><p>Clegg A, Bates C, Young J, Ryan R, Nichols L, Ann Teale E, Mohammed MA, Parry J, Marshall T. Development and validation of an electronic frailty index using routine primary care electronic health record data. Age Ageing, May;45(3):353-60, (2016) https://doi.org/10.1093/ageing/afw039.</p></li>
<li><p>Jeffrey L. Elman,Finding structure in time,Cognitive Science,Volume 14, 179-211, (1990). https://doi.org/10.1016/0364-0213(90)90002-E</p></li>
<li><p>Li, Y., Rao, S., Solares, J.R.A. et al.&nbsp;BEHRT: Transformer for Electronic Health Records. Sci Rep 10, 7155 (2020). https://doi.org/10.1038/s41598-020-62922-y</p></li>
<li><p>Rasmy, L., Xiang, Y., Xie, Z. et al.&nbsp;Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. npj Digit. Med. 4, 86 (2021). https://doi.org/10.1038/s41746-021-00455-y</p></li>
<li><p>Yang, Z., Mitra, A., Liu, W. et al.&nbsp;TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records. Nat Commun 14, 7857 (2023). https://doi.org/10.1038/s41467-023-43715-z</p></li>
<li><p>Renc, P., Jia, Y., Samir, A.E. et al.&nbsp;Zero shot health trajectory prediction using transformer. npj Digit. Med. 7, 256 (2024). https://doi.org/10.1038/s41746-024-01235-0</p></li>
<li><p>Grout R, Gupta R, Bryant R, Elmahgoub MA, Li Y, Irfanullah K, Patel RF, Fawkes J, Inness C. Predicting disease onset from electronic health records for population health management: a scalable and explainable Deep Learning approach. Front Artif Intell. 2024 Jan 8;6:1287541. doi: 10.3389/frai.2023.1287541.</p></li>
<li><p>Sheng Zhang et al.&nbsp;Exploring Scaling Laws for EHR Foundation Models (2025) arXiv:2505.22964v1</p></li>
<li><p>Julie Ayre, Erin Cvejic and Kirsten J McCaffery. Use of ChatGPT to obtain health information in Australia, 2024: insights from a nationally representative survey Med J Aust (2025). doi: 10.5694/mja2.52598</p></li>
<li><p>Windecker D, Baj G, Shiri I, Kazaj PM, Kaesmacher J, Gräni C, Siontis GCM. Generalizability of FDA-Approved AI-Enabled Medical Devices for Clinical Use. JAMA Netw Open. 2025 Apr 1;8(4):e258052. doi: 10.1001</p></li>
<li><p>Snooks H et al.&nbsp;Predictive risk stratification model: a randomised stepped-wedge trial in primary care (PRISMATIC). Southampton (UK): NIHR Journals Library; 2018 Jan.&nbsp;PMID: 29356470.</p></li>
<li><p>van Amsterdam WAC, van Geloven N, Krijthe JH, Ranganath R, Cinà G. When accurate prediction models yield harmful self-fulfilling prophecies. Patterns (N Y). 2025 Apr 11;6(4):101229. doi: 10.1016/j.patter.2025.101229.</p></li>
<li><p>Kraljevic, Zeljko et al.&nbsp;Foresight—a generative pretrained transformer for modelling of patient timelines using electronic health records: a retrospective modelling study. The Lancet Digital Health, Volume 6, Issue 4, e281 - e290</p></li>
<li><p>CVD-COVID-UK/COVID-IMPACT: Projects CCU078: Foresight: a generative AI model of patient trajectories across the COVID-19 pandemic https://bhfdatasciencecentre.org/projects/ccu078/</p></li>
<li><p>McDuff, D., Schaekermann, M., Tu, T. et al.&nbsp;Towards accurate differential diagnosis with large language models. Nature 642, 451–457 (2025). https://doi.org/10.1038/s41586-025-08869-4</p></li>
</ol>
<div class="article-btn">
<p><a href="../../../../../the-pulse/index.html">Discover more The Pulse</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-6">
<dl>
<dt>About the author</dt>
<dd>
<a href="https://www.linkedin.com/in/will-browne-391b1930/">Will Browne</a> is co-founder of healthcare technology company <a href="https://www.emrys.health/">Emrys Health</a>, where he works on the development of infrastructure for transformative, equitable and accessible healthcare. He is Events Secretary of the <a href="https://rss.org.uk/membership/rss-groups-and-committees/sections/data-science-section/">RSS Data Science and AI section</a> and a member of the <a href="https://rss.org.uk/policy-campaigns/policy-groups/ai-task-force/">RSS AI Taskforce</a>. Copyright and licence
</dd>
<dd>
© 2025 Royal Statistical Society
</dd>
<dd>
Thumbnail image by <a href="https://unsplash.com/@tugcegungormezler?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Tugce Gungormezler</a> / on Unsplash. <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a> This article is licensed under a Creative Commons Attribution (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">International licence</a>.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">

</div>
</div>
</div>



 ]]></description>
  <category>AI</category>
  <category>Data Science</category>
  <category>Machine learning</category>
  <category>Deep learning</category>
  <category>Econometrics</category>
  <guid>https://realworlddatascience.net/the-pulse/posts/2025/07/28/NHS-foundation-AI.html</guid>
  <pubDate>Mon, 28 Jul 2025 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/the-pulse/posts/2025/07/28/Images/NHS.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>We’re Back: Real World Data Science Relaunches</title>
  <dc:creator>Editorial Board</dc:creator>
  <link>https://realworlddatascience.net/the-pulse/editors-blog/posts/2025/07/07/editors-relaunch.html</link>
  <description><![CDATA[ 





<p>You may have noticed our brief hiatus. Since publishing our series on AI - which covered <a href="https://realworlddatascience.net/foundation-frontiers/posts/2024/04/29/gen-ai-human-intel.html">the quest for human-level intelligence</a>, <a href="https://realworlddatascience.net/foundation-frontiers/posts/2024/05/07/ai-series-3.html">data-set risks</a>, <a href="https://realworlddatascience.net/foundation-frontiers/posts/2024/05/14/ai-series-2.html">ethical considerations</a> and much more - the ongoing deluge of content and commentary on AI in the wider world has continued to accelerate. This year has seen a surge in developments that sit at the intersection of data science and AI: from the growing use of synthetic data to overcome privacy and bias challenges, to the rise of multi-modal models that demand increasingly sophisticated data engineering and integration techniques. The emergence of Agentic AI has sparked new conversations around data provenance, model interpretability, and the reproducibility crisis in machine learning. Meanwhile, the meteoric rise of open-source disruptor DeepSeek triggered stock-market ruptures and industry panic, before <a href="https://www.theguardian.com/technology/2025/jan/27/deepseek-cyberattack-ai">cyber-attacks</a>, <a href="https://www.wiz.io/blog/wiz-research-uncovers-exposed-deepseek-database-leak">data leaks</a> and a <a href="https://gizmodo.com/deepseek-gets-an-f-in-safety-from-researchers-2000558645?utm_source=pocket_shared">failed safety test</a> complicated its standing - a parable for the volatility of the space, where data governance failures and safety oversights can rapidly derail innovation. Meanwhile, governments worldwide are <a href="https://www.aa.com.tr/en/europe/macron-announces-112b-in-ai-investment-over-coming-years/3477218?utm_source=pocket_saves">investing heavily</a> in <a href="https://assets.publishing.service.gov.uk/media/67851771f0528401055d2329/ai_opportunities_action_plan.pdf?utm_source=substack&amp;utm_medium=email">national data infrastructure</a> and advanced analytics capabilities, while grappling with how best to regulate a field that is evolving faster than policy can keep up.</p>
<p>The world of data science has been a dizzying place over the last few months, so we took a moment to pause and take stock. In the face of rapid change and constant noise, it felt important to reflect with intention on the role Real World Data Science can and should play in this evolving landscape. Now we’re back - ready to rejoin the conversation with renewed clarity and purpose.</p>
<p>As a project from the <a href="https://rss.org.uk/">Royal Statistical Society</a>, in partnership with the <a href="https://www.amstat.org/">American Statistical Association</a>, we are backed by organisations with nearly two centuries of history in championing sound evidence, rigorous methodology and ethical data use. These values form the foundation of our next phase - distilled into the essential pillars: data, evidence and decision. With an esteemed editorial board representing the cutting-edge of industry and academia, and an international network of practitioners working at the coalface of modern data science, we are uniquely placed to navigate the pace and complexity of today’s data-driven world. Real World Data Science will meet that world in real time with the RSS’s trademark steadying presence, bridging the gap between rigorous analysis and real-time relevance.</p>
<p>We are now returning with a slightly refreshed site, encompassing four editorial sections:<br>
<a href="https://realworlddatascience.net/the-pulse/">The Pulse</a> - covering news, updates and real-time commentary<br>
<a href="https://realworlddatascience.net/applied-insights/">Applied Insights</a> - exploring how data science is used to solve real-world problems in business, public policy and beyond<br>
<a href="https://realworlddatascience.net/foundation-frontiers/">Foundations &amp; Frontiers</a> - unpicking the ideas behind the impact: the concepts, tools and methods that make data science possible<br>
<a href="https://realworlddatascience.net/people-paths/">People &amp; Paths</a> - offering strategic reflections on careers, leadership and professional evolution in data science.</p>
<p>You can find the full details of these sections, plus guidance around submitting to them, in our new <a href="https://realworlddatascience.net/the-pulse/editors-blog/posts/2025/07/08/relaunch-CFS.html">Call for Submissions</a>.</p>
<p>Despite these updates, we remain committed to providing content that is useful and relevant for practicing data scientists seeking to learn good practices in the field and new potential applications.</p>
<p>The choices we make now will shape how data and AI serve society for years to come. If you’re working on the front lines of these changes, whether through research, practice, or critical reflection, we invite you to share your insights and help us build a future for data science that is thoughtful, transparent and grounded in real world understanding.</p>
<p><a href="https://realworlddatascience.net/the-pulse/editors-blog/posts/2022/10/18/meet-the-team.html">Meet the Team</a></p>
<div class="article-btn">
<p><a href="../../../../../../the-pulse/editors-blog/index.html">Back to Editors’ blog</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2025 Royal Statistical Society
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Real World Data Science Editorial Board. 2025. “We’re Back: Real World Data Science Relaunches” Real World Data Science, July 7, 2025. <a href="https://realworlddatascience.net/the-pulse/editors-blog/posts/2025/07/07/editors-relaunch.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>



 ]]></description>
  <category>Call for contributions</category>
  <category>Updates</category>
  <guid>https://realworlddatascience.net/the-pulse/editors-blog/posts/2025/07/07/editors-relaunch.html</guid>
  <pubDate>Mon, 07 Jul 2025 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/the-pulse/editors-blog/posts/2025/07/07/images/team.png" medium="image" type="image/png" height="77" width="144"/>
</item>
<item>
  <title>Call for Submissions</title>
  <dc:creator>Editorial Board</dc:creator>
  <link>https://realworlddatascience.net/the-pulse/editors-blog/posts/2025/07/08/relaunch-CFS.html</link>
  <description><![CDATA[ 





<p>Get ready to engage with Real World Data Science as we unveil an exciting editorial refresh! We’re thrilled to announce that submissions are now open across our four dynamic sections: The Pulse, Applied Insights, Foundations &amp; Frontiers, and People &amp; Pathways. Join us as we redefine the conversation in data science with fresh perspectives and insights. Real World Data Science is relaunching to meet the pace and complexity of today’s data-driven world in real time, with the RSS’s trademark steadying presence. We will be publishing high-quality case-studies, tutorials and think-pieces that bridge the gap between rigorous analysis and real-time relevance, and that speak directly to latest events and emerging trends.</p>
<p>All submissions will be peer-reviewed by members of the Real World Data Science <a href="https://realworlddatascience.net/the-pulse/editors-blog/posts/2022/10/18/meet-the-team.html">Editorial Board</a>.</p>
<section id="our-audience" class="level2">
<h2 class="anchored" data-anchor-id="our-audience">Our Audience:</h2>
<p>People working in data science looking for practical insights, methodological rigour and thought-leadership that informs their work and decision-making.</p>
</section>
<section id="our-voice" class="level2">
<h2 class="anchored" data-anchor-id="our-voice">Our Voice:</h2>
<p>Authoritative, Trustworthy, Cutting Edge</p>
</section>
<section id="our-editorial-sections" class="level2">
<h2 class="anchored" data-anchor-id="our-editorial-sections">Our Editorial Sections:</h2>
<p>Real World Data Science has four editorial sections. Please read through and consider where your piece would fit best. Each piece we publish needs to be tailored towards the focus of one of these sections.</p>
<p><a href="https://realworlddatascience.net/the-pulse/">THE PULSE</a></p>
<p>News, updates and real time commentary.</p>
<p>Purpose: To respond to current events, trends and debates in the data science world with rigour, insight and relevance.</p>
<p>Content Types: Articles that speak directly to current events/trends/launches</p>
<p>Example Call To Action: Invite readers to share your commentary with their networks as a trusted voice in the space. Invite engagement, discussion and debate over the topics.</p>
<p><a href="https://realworlddatascience.net/applied-insights/">APPLIED INSIGHTS</a><br>
How data science is used to solve real-world problems in business, public policy and beyond.</p>
<p>Purpose: To showcase real-world applications of data science, including hands-on tutorials, project walk-throughs, and case studies from industry, academia, or public service.</p>
<p>Content Types:</p>
<ul>
<li>High-quality step-by-step tutorials with code<br>
</li>
<li>Case studies detailing a problem, approach, and outcome<br>
</li>
<li>Lessons learned from real-world deployments</li>
</ul>
<p>Example Call To Action: Readers should walk away with something to try.</p>
<p><a href="https://realworlddatascience.net/foundation-frontiers/">FOUNDATIONS &amp; FRONTIERS</a><br>
The ideas behind the impact: the concepts, tools and methods that make data science possible.</p>
<p>Purpose: To deepen understanding of the theoretical and ethical foundations of data science, and to spotlight thought leadership and emerging ideas.</p>
<p>Content Types:</p>
<ul>
<li>Think-piece style articles with an engaging angle on methodology, ethics and standards<br>
</li>
<li>Interviews with thought-leaders<br>
</li>
<li><a href="https://realworlddatascience.net/foundation-frontiers/datasciencebites/">Data Science Bites</a> - our handy summaries/explainers of academic papers</li>
</ul>
<p>Example Call To Action: Invite discussion and engagement – pose questions and challenges to the reader.</p>
<p><a href="https://realworlddatascience.net/people-paths/">PEOPLE &amp; PATHS</a><br>
Strategic reflections on careers, leadership and professional evolution in data science.</p>
<p>Purpose: To explore the evolving nature of data science careers through the lens of experience, leadership, and long-term impact. This section highlights how professionals shape and are shaped by the field—through roles, decisions, and philosophies.</p>
<p>Content Types:</p>
<ul>
<li>Profiles of/interviews with senior professionals reflecting on career philosophy and leadership<br>
</li>
<li>Roundtables with experts on hiring, mentoring, or organisational design</li>
<li>Commentary on career-defining trends, such as the rise of AI governance or the shift toward interdisciplinary teams</li>
</ul>
<p>Example Call To Action: Encourage readers to share our strategic insights with their community.</p>
</section>
<section id="use-of-ai-in-submissions" class="level2">
<h2 class="anchored" data-anchor-id="use-of-ai-in-submissions">Use of AI in Submissions</h2>
<p>We recognise that LLMs and other generative AI tools are increasingly part of the data science workflow, from code generation and data cleaning to drafting documentation and shaping analysis. We welcome a transparent approach in submissions that have made use of these tools, and ask that authors include a declaration outlining where and how AI was used in the development of their submission. This helps us maintain transparency, uphold standards of reproducibility, and better understand the evolving role of AI in real-world data science practice.</p>
<p>To make your submission, please review our <a href="https://realworlddatascience.net/contributor-docs/contributor-guidelines.html">contributor guidelines</a> and email us at rwds@rss.org.uk</p>
<div class="article-btn">
<p><a href="../../../../../../the-pulse/editors-blog/index.html">Back to Editors’ blog</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2025 Royal Statistical Society
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>. Thumbnail photo by <a href="https://unsplash.com/@johnsonvr?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Virgina Johnson</a> on <a href="https://unsplash.com/photos/turned-on-red-open-neon-sigange-QmNnZj_Ok-M?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Unsplash</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Real World Data Science Editorial Board. 2025. “Call for Submissions” Real World Data Science, July 7, 2025. <a href="https://realworlddatascience.net/the-pulse/editors-blog/posts/2025/07/07/relaunch-CFS.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>


</section>

 ]]></description>
  <category>Call for contributions</category>
  <category>Updates</category>
  <guid>https://realworlddatascience.net/the-pulse/editors-blog/posts/2025/07/08/relaunch-CFS.html</guid>
  <pubDate>Mon, 07 Jul 2025 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/the-pulse/editors-blog/posts/2025/07/08/Images/open.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
