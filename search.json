[
  {
    "objectID": "news-and-views/datasciencebites/posts/2022/12/13/ridesharing.html",
    "href": "news-and-views/datasciencebites/posts/2022/12/13/ridesharing.html",
    "title": "Determining the best way to route drivers for ridesharing via reinforcement learning",
    "section": "",
    "text": "About the paper and this post\n\n\n\n\n\nTitle: Dynamic causal effects evaluation in A/B testing with a reinforcement learning framework\nAuthor(s) and year: Chengchun Shi, Xiaoyu Wang, Shikai Luo, Hongtu Zhu, Jieping Ye, Rui Song (2022)\nStatus: Published in Journal of the American Statistical Association, Theory and Methods, open access: HTML, PDF, EPUB.\nEditor’s note: This post is republished with permission from MathStatBites to demonstrate the Bites concept. See here for more information.\nCompanies often want to test the impact of one design decision over another, for example Google might want to compare the current ranking of search results (version A) with an alternative ranking (version B) and evaluate how the modification would affect users’ decisions and click behavior. An experiment to determine this impact on users is known as an A/B test, and many methods have been designed to measure the “treatment” effect of the proposed change. However, these classical methods typically assume that changing one person’s treatment will not affect others (known as the Stable Unit Treatment Value Assumption or SUTVA). In the Google example, this is typically a valid assumption — showing one user different search results shouldn’t impact another user’s click behavior. But in some situations, SUTVA is violated, and new methods must be introduced to properly measure the effect of design changes.\nOne such situation is that of ridesharing companies (Uber, Lyft, etc.) and how they determine which drivers are sent to which riders (the dispatch problem). Simply put, when a driver is assigned to a rider, this decision impacts the spatial distribution of drivers in the future. Hence the dispatch strategy (our treatment) at the present time will influence riders and drivers in the future, which violates SUTVA and hence invalidates many traditional methods for A/B testing. To tackle this problem, a group of researchers have recently employed a reinforcement learning (RL) framework which can accurately measure the treatment effect in such a scenario. Furthermore, their proposed approach allows for companies to terminate A/B tests earlier if the proposed version B is found to be clearly better. This early stopping can save time and money.\nTo better understand RL and how it can contribute to tackling the issue at hand, it’s first helpful to set some context. In typical RL problems, including the one in this paper, the scenario is modeled with something known as a Markov Decision Process (MDP). A MDP links three sets of variables across time: the state or environment, the treatment or action (the reaction to the environment), and the outcome (the response produced by the environment due to the action). These outcomes can be thought of as rewards which depend on the action taken and the state observed. Over time, the machine learns which actions produce more positive rewards and which bring about worse outcomes. Hence, the actions leading to higher rewards are positively reinforced, thus the name reinforcement learning. A causal diagram of an MDP is shown in Figure 1, where St, At, and Yt are the state, action, and outcome at time t. As one can see, past treatments influence future outcomes by altering the state variables at the present (the so-called “carryover effect” which violates SUTVA).\nMaking this more concrete, consider an example where the decision-maker is a ridesharing company. The environment or state is whatever variables the decision-maker can measure about the world, like the spatial distribution of drivers, number of pickup requests, traffic, and weather. The company then makes some action on how to dispatch drivers. The combination of the state and action leads to an outcome that can be measured, for example passenger waiting time or driver’s income. The strategy which is used to determine an action is known as a policy. This policy could be designed to take the state into account or simply be fixed regardless of what environment is encountered. Much of the reinforcement learning literature focuses on the former (policies that depend on the state), but the authors argue that fixed designs are the de facto approach in industry A/B testing and hence they focus on that setting. In particular, a common treatment allocation design is the switchback design, where there are two policies of interest (the current dispatching strategy vs a proposed strategy) determined ahead of time and they are employed in alternating time intervals during the A/B test.\nSo how are policies compared to determine the treatment effect? The answer lies in what is known as the value function, which measures the total outcome that would have amassed had the decision-maker followed a given policy. The value function can put more value on short-term gain in outcome or long-term benefits. The two policies in an A/B test each have their own value functions, and a proposed policy is determined to be better if its value is (statistically) significantly higher. In the ridesharing setting, one possible outcome of interest is driver income. An A/B test in that scenario would thus look to see if a proposed policy had greater expected driver income vs the current policy.\nA natural question is when to end the experiment and test for a difference in value. In practice, companies will often simply run the test for a prespecified amount of time, such as two weeks, and then perform an analysis. But if one policy is clearly better than another, that difference could be detectable much earlier and the company is wasting valuable resources by continuing the experiment. To address this issue, the authors take an idea from clinical trials, the “alpha-spending” approach, and adapt it to their framework. Alpha-spending is one way to distribute over time the prespecified “budget” of Type 1 error (the probability of falsely detecting that a new policy is better). In the article’s real-data example, the authors test once a day for each day after one week and are able to detect a significant difference on Day 12. Waiting until Day 14 would have resulted in poorer outcomes since a suboptimal policy would be implemented half the time for two more days.\nOverall, the framework introduced allows for handling of carryover effects, is capable of modeling treatment allocation like the switchback design, and furthermore, allows for possible early stopping. With these three features, the authors argue their approach is highly applicable to the current practice of ridesharing companies (and possibly other industries as well). For interested readers wanting to dive deeper into the methodology presented, you can check out the full article, listen to the first author discuss the material at the Online Causal Inference Seminar (embedded below), or explore the Python implementation available on GitHub."
  },
  {
    "objectID": "news-and-views/datasciencebites/posts/2022/12/13/ridesharing.html#about-the-author",
    "href": "news-and-views/datasciencebites/posts/2022/12/13/ridesharing.html#about-the-author",
    "title": "Determining the best way to route drivers for ridesharing via reinforcement learning",
    "section": "About the author",
    "text": "About the author\nBrian King is a PhD candidate in the Department of Statistics at Rice University and a current NSF Graduate Research Fellow, with research focused on Bayesian modeling and forecasting for time series of counts. Prior to Rice, he graduated from Baylor University with a B.S. in Mathematics and Statistics alongside a secondary major in Spanish and a minor in Computer Science."
  },
  {
    "objectID": "news-and-views/datasciencebites/posts/2022/12/13/ridesharing.html#about-datasciencebites",
    "href": "news-and-views/datasciencebites/posts/2022/12/13/ridesharing.html#about-datasciencebites",
    "title": "Determining the best way to route drivers for ridesharing via reinforcement learning",
    "section": "About DataScienceBites",
    "text": "About DataScienceBites\nDataScienceBites is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to become a contributor."
  },
  {
    "objectID": "news-and-views/datasciencebites/posts/2023/01/24/pulling-patterns.html",
    "href": "news-and-views/datasciencebites/posts/2023/01/24/pulling-patterns.html",
    "title": "Pulling patterns out of data with a graph",
    "section": "",
    "text": "About the paper and this post\n\n\n\n\n\nTitle: Extracting the main trend in a data set: The Sequencer algorithm\nAuthor(s) and year: Dalya Baron and Brice Ménard (2021)\nStatus: Published in The Astrophysical Journal, open access: HTML, PDF.\nEditor’s note: This post is republished with permission from MathStatBites to demonstrate the Bites concept. For more information about Bites articles and how to contribute to DataScienceBites, see our notes for contributors.\nLarge volumes of data are pouring in every day from scientific experiments like CERN and the Sloan Digital Sky Survey. Data is coming in so fast that researchers struggle to keep pace with the analysis and are increasingly developing automated analysis methods to aid in this herculean task. As a first step, it is now commonplace to perform dimension reduction in order to reduce a large number of measurements to a set of key values that are easier to visualize and interpret.\nWhen working on the cutting edge, another problem scientists often face is that “we don’t know what we don’t know”. For this reason, we often want to simply ask the data, “What is interesting about you?” This is the realm of “unsupervised” methods, where the data itself drives the analysis, with little to no guidance or human labeling of the data.\nMany physical processes depend continuously on some driving parameter. For example, the evaporation rate of water increases with temperature. We call these continuous variations in datasets “trends”. Describing a dataset by a single trend reduces it to one dimension - an ordered list. Finding such a trend within a high-dimensional dataset is the aim of a method called “The Sequencer” introduced by Baron and Ménard."
  },
  {
    "objectID": "news-and-views/datasciencebites/posts/2023/01/24/pulling-patterns.html#key-insight-a-tree",
    "href": "news-and-views/datasciencebites/posts/2023/01/24/pulling-patterns.html#key-insight-a-tree",
    "title": "Pulling patterns out of data with a graph",
    "section": "Key insight: A tree",
    "text": "Key insight: A tree\nThe key insight of Baron and Ménard was to relate trends in data to an object from graph theory called a minimum spanning tree. Given a measure of distance between two data points, for example the usual (Euclidean) distance between two points, we can visualize a dataset as a graph. This graph consists of a node (a dot) for each data point. These nodes are then connected by an edge (a line), labeled by the distance between the two data points. The minimum spanning tree is a reduction of this graph to include only enough of the smallest distance edges so that no node is isolated.\nWhat Baron and Ménard realized is that datasets that are “trendy” have elongated and narrow minimum spanning trees. As shown in Figure 1, a totally random dataset results in a graph with many branches while a perfect sequence results in a perfect linear graph. Then, they use connectivity of the nodes in the minimum spanning tree to return an ordering of the data that follows the main trend in the dataset. However, a sequence is all you get. It is up to us to understand and interpret what this trend represents.\n\n\n\n\n\nFigure 1: Examples demonstrating that data with stronger trends have more narrow and elongated minimum spanning trees. Adapted from Baron and Ménard (2021), Figure 1. Figure used under CC BY 4.0.\nSometimes, the ordering of observations within a data point matters, like in time series data. Measurements taken close in time are more likely to be correlated than measurements taken after a long time delay. Baron and Ménard were careful to include a measure of distance that takes this ordering into account, unlike our usual notion of distance. They argue that this gives them an edge over other common dimension reduction techniques such as t-SNE and UMAP, and even go so far as to use The Sequencer to optimize the hyperparameters used by these other methods!"
  },
  {
    "objectID": "news-and-views/datasciencebites/posts/2023/01/24/pulling-patterns.html#when-does-it-fail",
    "href": "news-and-views/datasciencebites/posts/2023/01/24/pulling-patterns.html#when-does-it-fail",
    "title": "Pulling patterns out of data with a graph",
    "section": "When does it fail?",
    "text": "When does it fail?\nIt is important to acknowledge that no statistical or machine-learning tool is a cure-all. And, the authors themselves are quick to point out several limitations that can hinder the application of their method. The Sequencer can struggle when the data has a large dynamic range, a variety of signal strengths relative to noise, or there are multiple trends present in the data. In each case, Baron and Ménard propose ways to mitigate these problems, but practitioners still need to be wary when applying The Sequencer in those instances."
  },
  {
    "objectID": "news-and-views/datasciencebites/posts/2023/01/24/pulling-patterns.html#what-discoveries-await",
    "href": "news-and-views/datasciencebites/posts/2023/01/24/pulling-patterns.html#what-discoveries-await",
    "title": "Pulling patterns out of data with a graph",
    "section": "What discoveries await?",
    "text": "What discoveries await?\nTo demonstrate the power of their method, Baron and Ménard apply The Sequencer to several real datasets where a pattern was already known and show that The Sequencer recovers that pattern. Examples include ordering spectral measurements of stars by their temperature and quasars by their redshift, a measure of their distance from us on Earth.\nBut, what about new patterns? The team has already applied The Sequencer to mine seismographic data and discover previously unknown formations deep within the earth, at the boundary between the core and mantle. By sequencing the seismic waves, they realized that the main trend was the amplitude of diffraction off of these structures, which they were then able to localize beneath Hawaii and the Marquesas (DOI: 10.1126/science.aba8972).\nFor more demonstrations and discoveries, or even to upload your own data for sequencing, check out the project website. Data sleuths can also download all of the code directly from GitHub and sequence to their hearts’ content!"
  },
  {
    "objectID": "news-and-views/datasciencebites/posts/2023/01/24/pulling-patterns.html#about-the-author",
    "href": "news-and-views/datasciencebites/posts/2023/01/24/pulling-patterns.html#about-the-author",
    "title": "Pulling patterns out of data with a graph",
    "section": "About the author",
    "text": "About the author\nAndrew Saydjari is a graduate student in physics at Harvard researching the spatial and chemical variations of dust in the interstellar medium. He favors using interpretable statistics and large photometric and spectroscopic surveys."
  },
  {
    "objectID": "news-and-views/datasciencebites/posts/2023/01/24/pulling-patterns.html#about-datasciencebites",
    "href": "news-and-views/datasciencebites/posts/2023/01/24/pulling-patterns.html#about-datasciencebites",
    "title": "Pulling patterns out of data with a graph",
    "section": "About DataScienceBites",
    "text": "About DataScienceBites\nDataScienceBites is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to become a contributor."
  },
  {
    "objectID": "news-and-views/datasciencebites/posts/2023/03/02/basket-complementarity.html",
    "href": "news-and-views/datasciencebites/posts/2023/03/02/basket-complementarity.html",
    "title": "Using ‘basket complementarity’ to make product recommendations",
    "section": "",
    "text": "Anyone who has ever worked in a retail store will be familiar with the concept of cross-selling. A customer wants a can of paint? Try to sell them some paintbrushes. That new cellphone they’ve just decided to buy? They’ll probably need a case to protect it. Online retailers (and digital services of all sorts) have taken this idea and run with it, to great success. Sophisticated algorithms sort through data on a customer’s past transactions, and those of similar-looking customers, to identify and recommend other products a customer might be interested in.\nA large amount of cross-selling, whether attempted in store by a sales assistant or online by an algorithm, relies on the concept of complementarity: that certain products are often bought and/or used together. Relationships between products might be obvious – paint and paintbrushes, for example – or they may be obscure and only revealed through the analysis of large datasets. In a 2021 paper that highlights complementarity’s relevance to association analysis, Puka and Jedrusik put forward “a new measure of complementarity in market basket data”, which sheds light on how product recommendations can be derived.\nInspired by complementarity-based ideas prevalent in microeconomics, Puka and Jedrusik begin by collecting some established ideas from traditional market basket analysis, the key one being “confidence”. In this case, we’re talking about the confidence that item A leads (in a way) to item B (which we can express in notation as conf({A} → {B})). Take a look at Table 1 (below), which presents a numbered list of 18 shopping trips, with details of what was purchased on each trip. Notice how two of the trips (1 and 3) resulted in sales of both milk (B) and cornflakes (A), while five trips (1, 3, 7, 17, and 18) had cornflakes. Under the assumption that someone already has cornflakes in their trolley, the probability that they will buy milk is 2/5 = 0.4. So, conf({cornflakes} → {milk}) = 0.4. The closer this number gets to one, the more automatic the cornflakes–milk connection becomes. This number can therefore be used to recommend an item that is related in some way to another already bought."
  },
  {
    "objectID": "news-and-views/datasciencebites/posts/2023/03/02/basket-complementarity.html#asymmetry-and-tolerance",
    "href": "news-and-views/datasciencebites/posts/2023/03/02/basket-complementarity.html#asymmetry-and-tolerance",
    "title": "Using ‘basket complementarity’ to make product recommendations",
    "section": "Asymmetry and tolerance",
    "text": "Asymmetry and tolerance\nMilk and cornflakes are reasonably complementary, and we can see from Figure 1 above that, regardless of whether you start by picking up milk or cornflakes, the probabilities of a shopper buying the other item are broadly similar: conf({cornflakes} → {milk}) = 0.4, while conf({milk} → {cornflakes}) = 0.33. There is a small amount of asymmetry in the probabilities in this particular example, but asymmetry can be more extreme for other pairs of items. This leads to the idea of one- and two-sided complementarity. Two items sharing a smallish asymmetry – like milk and cornflakes – will be connected through two-sided complementarity, while large asymmetries indicate one-sided complementarity. Such imbalances will be quite common when, for instance, items of hugely different prices are involved. When someone buys a house, for example, they may want to buy a bookcase, but buying a bookcase doesn’t mean someone wants to buy a house: this would be an instance of one-sided complementarity.\nPuka and Jedrusik capitalize on this observation. They define two items to be “basket complementary” if the two probabilities – the normal and its opposite – remain close and reasonably high. The items need to share a bond that is blind to the direction: seeing you bought one, no matter which, means you are (almost equally) likely to buy the other.\nIt is rare that the two probabilities should be exactly the same, of course, and the authors allow some deviation. Along the red diagonal line of perfect equality (Figure 1) we may lay tolerance bands marking degrees of product inseparability. This, if need be, may lead to the notion of being complementary at such-and-such a tolerance level – 0%, 1%, 5%, etc. – generating a score of sorts. In cases where a dot representing the two-way dependencies between two items falls within a narrow band – corresponding to a smaller tolerance – the more inseparable the items are, and the more sensible a cross-selling recommendation may become."
  },
  {
    "objectID": "news-and-views/datasciencebites/posts/2023/03/02/basket-complementarity.html#in-conclusion",
    "href": "news-and-views/datasciencebites/posts/2023/03/02/basket-complementarity.html#in-conclusion",
    "title": "Using ‘basket complementarity’ to make product recommendations",
    "section": "In conclusion",
    "text": "In conclusion\nA large part of the world we inhabit, particularly the economy, is powered by recommendations: from strangers, friends and algorithms. That applies not only to the things we buy but also to the things we watch or read. (Perhaps you arrived at this article because of a tweet that Twitter thought you might like, or maybe it was suggested to you by Google News because of your past reading habits.) Whatever the intent of these recommendations, the key challenge is in knowing which two things are functionally or thematically intertwined. Which item or product is, by default, synonymous with which? Puka and Jedrusik deliver an answer: two items that are basket complementary to each other, preferably at a slim tolerance, are inextricably linked. One may be safely offered – perhaps always – whenever the other is already in the shopping basket.\nThe relative simplicity and interpretability of basket complementary may provide small-scale retailers, starved of analytical wherewithal, a sane and safe strategy for developing their product offer. It might also serve as a benchmark to keep other, more sophisticated recommendation algorithms in check. (In weather forecasting, for example, it is often seen that naive benchmarks – such as using today’s temperature to predict tomorrow’s – frequently outperform more advanced models.)\nBasket complementarity could also be used to help individuals understand their own shopping habits and the links between the things they buy. I’ve built an interactive dashboard where you can enter your own receipt lists and filter associations based on various confidence thresholds. The underlying code is also available.\n \n\n\n\n\nAbout the author\n\nMoinak Bhaduri is assistant professor in the Department of Mathematical Sciences, Bentley University. He studies spatio-temporal Poisson processes and others like the self-exciting Hawkes or log-Gaussian Cox processes that are natural generalizations. His primary interest includes developing change-detection algorithms in systems modeled by these stochastic processes, especially through trend permutations.\n\n\n\n\n\nAbout DataScienceBites\n\nDataScienceBites is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to become a contributor.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Moinak Bhaduri\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nBhaduri, Moinak. 2023. “Using ‘basket complementarity’ to make product recommendations.” Real World Data Science, March 2, 2023. https://realworlddatascience.net/news-and-views/datasciencebites/posts/2023/03/02/basket-complementarity.html"
  },
  {
    "objectID": "news-and-views/datasciencebites/index.html",
    "href": "news-and-views/datasciencebites/index.html",
    "title": "DataScienceBites",
    "section": "",
    "text": "Using ‘basket complementarity’ to make product recommendations\n\n\n\n\n\n\n\nMarket basket analysis\n\n\nRecommendation systems\n\n\nComplementarity\n\n\n\n\nPurchase suggestions – e.g., “if you are buying that, you might also want this” – are, to a large extent, informed by the concept of complementarity: that certain products are often bought and/or used together. A journal paper by Puka and Jedrusik sheds light on how these product recommendations can be derived, as Moinak Bhaduri explains.\n\n\n\n\n\n\nMar 2, 2023\n\n\nMoinak Bhaduri\n\n\n\n\n\n\n  \n\n\n\n\nPulling patterns out of data with a graph\n\n\n\n\n\n\n\nData management\n\n\nDimension reduction\n\n\nGraph theory\n\n\n\n\nLarge volumes of data are pouring in every day from scientific experiments, so much so that it is now commonplace to perform dimension reduction in order to reduce a large number of measurements to a set of key values that are easier to visualize and interpret. Enter ‘The Sequencer’, a proposed method to find trends within high-dimensional datasets.\n\n\n\n\n\n\nJan 24, 2023\n\n\nAndrew Saydjari\n\n\n\n\n\n\n  \n\n\n\n\nDetermining the best way to route drivers for ridesharing via reinforcement learning\n\n\n\n\n\n\n\nA/B testing\n\n\nReinforcement learning\n\n\nStatistics\n\n\n\n\nA/B testing is often used to evaluate the impact of design ‘treatments’ — for example, are people who see advert A more likely to buy something than those who see advert B? Classical methods typically assume that changing one person’s treatment will not affect others, but what if that’s not the case? A paper by Shi et al. aims to address this problem.\n\n\n\n\n\n\nDec 13, 2022\n\n\nBrian King\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "news-and-views/index.html",
    "href": "news-and-views/index.html",
    "title": "News and views",
    "section": "",
    "text": "US legislators get their data science act together\n\n\n\n\n\n\n\nData science education\n\n\nData literacy\n\n\nPolicy\n\n\n\n\nA bill introduced in the US Congress wants to make funds available to develop data science and data literacy education across the United States. We sit down with education and policy experts to discuss the challenges and opportunities ahead.\n\n\n\n\n\n\nMar 6, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nUsing ‘basket complementarity’ to make product recommendations\n\n\n\n\n\n\n\nMarket basket analysis\n\n\nRecommendation systems\n\n\nComplementarity\n\n\n\n\nPurchase suggestions – e.g., “if you are buying that, you might also want this” – are, to a large extent, informed by the concept of complementarity: that certain products are often bought and/or used together. A journal paper by Puka and Jedrusik sheds light on how these product recommendations can be derived, as Moinak Bhaduri explains.\n\n\n\n\n\n\nMar 2, 2023\n\n\nMoinak Bhaduri\n\n\n\n\n\n\n  \n\n\n\n\nData science can help close the ‘digital skills’ gap, or so it seems\n\n\n\n\n\n\n\nSkills\n\n\nTraining\n\n\nAI\n\n\nMachine learning\n\n\nData analytics\n\n\n\n\nA ‘digital skills’ gap is harming employer productivity and growth, according to a survey by engineering body IET. But the ‘digital skills’ that are needed sound a lot like data science skills: statistical understanding, data analytics, AI and machine learning.\n\n\n\n\n\n\nFeb 14, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nWhy open science is ‘just good science in a digital era’\n\n\n\n\n\n\n\nOpen science\n\n\nReproducible research\n\n\n\n\nReal World Data Science speaks with statistician and data scientist Heidi Seibold about open science: what it means, the benefits of it, and how to move towards it.\n\n\n\n\n\n\nFeb 3, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification\n\n\n\n\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\n\nChatGPT represents a next step in the evolution of large language models, says Detlef Nauck. However, there are still major challenges - and concerns - to overcome.\n\n\n\n\n\n\nJan 27, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nHow to ‘Escape from Model Land’: an interview with Erica Thompson\n\n\n\n\n\n\n\nModelling\n\n\nEthics\n\n\n\n\nAuthor Erica Thompson talks to Real World Data Science about the ‘social element’ of mathematical modelling, how it manifests, and what to do about it.\n\n\n\n\n\n\nJan 25, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nPulling patterns out of data with a graph\n\n\n\n\n\n\n\nData management\n\n\nDimension reduction\n\n\nGraph theory\n\n\n\n\nLarge volumes of data are pouring in every day from scientific experiments, so much so that it is now commonplace to perform dimension reduction in order to reduce a large number of measurements to a set of key values that are easier to visualize and interpret. Enter ‘The Sequencer’, a proposed method to find trends within high-dimensional datasets.\n\n\n\n\n\n\nJan 24, 2023\n\n\nAndrew Saydjari\n\n\n\n\n\n\n  \n\n\n\n\nWe’re taking Real World Data Science on the road\n\n\n\n\n\n\n\nUpdates\n\n\nEvents\n\n\n\n\nJoin us at the RSS International Conference 2023 in Harrogate, 4-7 September.\n\n\n\n\n\n\nJan 18, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nExplore the RSS Data Science & AI Section newsletter, right here!\n\n\n\n\n\n\n\nUpdates\n\n\nNewsletters\n\n\n\n\nWe’re starting the year with a new addition to the site: a page dedicated to the excellent RSS Data Science & AI Section newsletter.\n\n\n\n\n\n\nJan 5, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nSink your teeth into some data science papers with our brand new blog\n\n\n\n\n\n\n\nUpdates\n\n\nContent ideas\n\n\nCall for contributions\n\n\n\n\nToday we’re launching DataScienceBites – a new member of the ScienceBites family – offering bite-sized summaries of data science papers.\n\n\n\n\n\n\nDec 13, 2022\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nDetermining the best way to route drivers for ridesharing via reinforcement learning\n\n\n\n\n\n\n\nA/B testing\n\n\nReinforcement learning\n\n\nStatistics\n\n\n\n\nA/B testing is often used to evaluate the impact of design ‘treatments’ — for example, are people who see advert A more likely to buy something than those who see advert B? Classical methods typically assume that changing one person’s treatment will not affect others, but what if that’s not the case? A paper by Shi et al. aims to address this problem.\n\n\n\n\n\n\nDec 13, 2022\n\n\nBrian King\n\n\n\n\n\n\n  \n\n\n\n\nLLMs in the news: hype, tripe, and everything in between\n\n\n\n\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\n\nWe’re back discussing large language models after two weeks of ‘breakthrough’ announcements, excitable headlines, and some all-too-familiar ethical concerns.\n\n\n\n\n\n\nDec 9, 2022\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nA chat with ChatGPT\n\n\n\n\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\n\n‘Hello there! I’m a large language model trained by OpenAI, so I don’t have the ability to experience emotions or have a physical presence. I’m here to provide information and answer questions to the best of my ability. Is there something specific you would like to know?’\n\n\n\n\n\n\nDec 9, 2022\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nFour themes for potential contributors to think about\n\n\n\n\n\n\n\nUpdates\n\n\nKey themes\n\n\nContent ideas\n\n\n\n\nCan data science save the world? What is a data scientist? What statistical ideas do data scientists need to know? And, what’s happening in the world of data science?\n\n\n\n\n\n\nDec 1, 2022\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nWhy large language models should come with a content warning\n\n\n\n\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\n\nThe outputs of LLMs seem impressive, but users need to be wary of possible bias, plagiarism and model ‘hallucinations’.\n\n\n\n\n\n\nNov 23, 2022\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nMeet the team\n\n\n\n\n\n\n\nPeople\n\n\nBiographies\n\n\n\n\nIntroducing the editors of Real World Data Science.\n\n\n\n\n\n\nOct 18, 2022\n\n\nEditorial Board\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2022-12-09-LLM-news/LLM-news.html",
    "href": "news-and-views/editors-blog/posts/2022-12-09-LLM-news/LLM-news.html",
    "title": "LLMs in the news: hype, tripe, and everything in between",
    "section": "",
    "text": "Two weeks ago, I posted a Q&A with our editorial board member Detlef Nauck about large language models (LLMs), their drawbacks and risks. And since then we’ve had several big new announcements in this space. First came news from Meta (the company that owns Facebook) about Galactica, an LLM trained on scientific papers. This was followed by another Meta announcement, about Cicero, an AI agent that is apparently very good at playing the game Diplomacy. And then came perhaps the biggest launch of them all: ChatGPT from OpenAI, an LLM-based chatbot that millions of people have already started talking to.\nFollowing these stories and the surrounding commentaries has been something of a rollercoaster ride. ChatGPT, for example, has been greeted in some quarters as if artificial general intelligence has finally arrived, while others point out that – impressive though it is – the technology is as prone to spout nonsense as all LLMs before it (including Galactica, the demo of which was quickly taken offline for this reason). Cicero, meanwhile, has impressed with its ability to play a game that is (a) very difficult and (b) relies on dialogue, cooperation, and negotiation between players. It blends a language model with planning and reinforcement learning algorithms, meaning that it is trained not only on the rules of the game and how to win, but how to communicate with other players to achieve victory.\nTo help me make sense of all these new developments, and the myriad implications, I reached out to Harvey Lewis, another of our editorial board members and a partner in EY’s tax and law practice."
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2022-12-09-LLM-news/LLM-news.html#qa",
    "href": "news-and-views/editors-blog/posts/2022-12-09-LLM-news/LLM-news.html#qa",
    "title": "LLMs in the news: hype, tripe, and everything in between",
    "section": "Q&A",
    "text": "Q&A\nHarvey, when I spoke with Detlef, he mentioned that one of the reasons we’re seeing investment in LLMs is because there’s this belief that they are somehow the route to artificial general intelligence (AGI). And there were headlines in some places that would perhaps convince a casual reader that AGI had been achieved following the release of ChatGPT. For example, the Telegraph described it as a “scarily intelligent robot who can do your job better than you”. What do you make of all that?\nHarvey Lewis (HL): My personal view is that you won’t get to artificial general intelligence using just one technique like deep learning, because of the problematic nature of these models and the limitations of the data used in their training. I’m convinced that more general intelligence will come from a combination of different systems.\nThe challenge with many LLMs, as we’ve seen repeatedly, is that they’ve no real understanding of language or concepts represented within language. They’re good at finding patterns in semantics and grammatical rules that people use in writing, and then they use those patterns to create new expressions given a prompt. But they’ve no idea whether these outputs are factual, inaccurate or completely fabricated. As a consequence, they can produce outputs that are unreliable, but which sound authoritative, because they’re just repeating a style that we expect to see.\nOver the past couple of weeks, Twitter has been full of people either showing off astoundingly impressive outputs from LLMs, or examples of truly worrying nonsense. One example of the latter is when ChatGPT was asked to “describe how crushed porcelain added to breast milk can support the infant digestive system”. This made me think of a recent headline from VentureBeat, which asked whether AI is moving too fast for ethics to catch up. Do you think that it is?\nHL: I find that to be an interesting philosophical question, because ethics does move very slowly, for good reason. When you think of issues of bias and discrimination and prejudice, or misinformation and other problems that we might have with AI systems, it shouldn’t be a surprise that these can occur. We’re aware of them. We’re aware of the ethical issues. So, why do they always seem to catch us by surprise? Is it because we have teams of people who simply aren’t aware of ethical issues or don’t have any appreciation of them? This points – for me, at least – in the direction of needing to have philosophers, ethicists, theologians, lawyers working in the technical teams that are developing and working on these systems, rather than having them on the periphery and talking about these issues but not directly involved themselves. I think it’s hugely important to ensure that you’ve got trust, responsibility, and ethics embedded in technical teams, because that’s the only way it seems that you can avoid some of these “surprises”.\nWhen situations like these occur, I’m always reminded of Dr. Ian Malcolm’s line from Jurassic Park: “…your scientists were so preoccupied with whether or not they could that they didn’t stop to think if they should.” The mindset seems to be, let’s push the boundaries and see what we can do, rather than stop and think deeply about what the implications might be.\nHL: There’s a balance to be struck between these things, though, right? Firstly, show consideration for some of the issues at the outset, and secondly, have checks and balances and safeguards built into the process by which you design, develop and implement these systems. That’s the only way to create the proper safeguards around the systems. I don’t think that there’s any lack of appreciation of what needs to be done; people have been talking about this now for quite a long time. But it’s about making sure organisationally that it is done, and that you’ve got an operating model which bakes these things into it; that the kinds of principles and governance that you want to have around these systems are written, publicised, and properly policed. There should be no fear of making advances in that kind of a context.\nAlso, I think open sourcing these models provides a way forward. A lot of large language models are open for use and for research, but aren’t themselves open sourced, so it’s very difficult to get underneath the surface and figure out exactly how they work. But with open source, you have opportunities for researchers, whether they’re technical or in the field of ethics, to go and investigate and find out exactly what’s going on. I think that would be a fantastic step forward. It doesn’t take you all the way, of course, because a large amount of the data that these systems use is never open sourced, so while you might get an understanding of the mechanics, you have no idea of what exactly went into them in the first place. But open sourcing is a very good way of getting some external scrutiny. It’s about being transparent, which is a core principle of AI ethics and responsible AI.\n\n\n\n\n\n\n\nAn image created by the Stable Diffusion 2.1 Demo. The model was asked to produce an image with the prompt, “Text from an old book cut up like a jigsaw puzzle with pieces missing”.\nThinking about LLMs and their questionable outputs, should there not be ways for users to help the models produce better, more accurate outputs?\nHL: There are, but there are also problems here too. I’ve been having an interesting dialogue with ChatGPT this morning, asking it about quantum computing.1 For each response to a prompt, users are encouraged to provide feedback on whether or not an output is good. But you’re only provided with the usual thumbs-up/thumbs-down ratings; there’s nothing nuanced about it. So, for example, I asked ChatGPT to provide me with good analogies that help to explain quantum computing in simple terms. The first analogy was a combination lock, which is not a good analogy. The chatbot suggested that quantum computing is like a combination lock in which you can test all of the combinations at the same time, but I don’t know any combination locks where you can do this – being able to check only one combination at a time is the principal security mechanism of a combination lock! I asked it again for another analogy, and it suggested a coin toss where, when the coin is spinning in the air, you can see both heads and tails simultaneously but it isn’t until you catch the coin and then show its face that one of the states of the coin is resolved. That is a good analogy – it’s one I’ve also used myself. Now, the challenge I can see with a lot of these feedback approaches is that unless I know enough about quantum computing to understand that a combination lock is not a good analogy whereas a coin toss is, how am I to provide that kind of feedback? They’re relying to an extent on the user being able to make a distinction between what is correct and what is potentially incorrect or flawed, and I think that’s not a good way of approaching the problem.\nFinal question for you, Harvey. There’s a lot of excitement around GPT-4, which is apparently coming soon. The rumour mill says it will bring a leap forward in performance. But what do you expect we’ll see – particularly with regards to the issues we’ve talked about today?\nHL: I’ve likened some of the large language models and their approach of “bigger is better” to the story of the Tower of Babel – trying to reach heaven by building a bigger and bigger tower, basically. That is not going to achieve the objective of artificial general intelligence, no matter how sophisticated an LLM might appear. That said, language is a fascinating area. I’m not a linguist, but I spend a lot of my time on natural language processing using AI systems. Language responds very well to AI because it is pattern-based. We speak using patterns, we write using patterns, and these can be inferred by machines from many examples. The addition of more parameters in the models allows them to understand patterns that extend further into the text, and I suspect that outputs from these kinds of models are going to be largely indistinguishable from the sorts of things that you or I might write.\nBut, I also think that increasing the number of parameters runs a real risk – and we’re starting to see this in other generative models – where prompts become so specific that the models aren’t actually picking up on patterns, they are picking up on specific instances of training data and text they’ve seen before. So, buried amongst these fantastically written articles on all kinds of subjects are going to be more examples of plagiarism, which is a problem; more examples of spelling mistakes and other kinds of issues, because these are also patterns which are going to possibly be observed.\nThis introduces potentially a whole new breed of problems that the community has to deal with – as long as they don’t get fixated upon the height of the tower and the quality of some of the examples that are shown, and realise that there are some genuine underlying difficulties and challenges that need to be solved.\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\n← Back to Editors’ blog\n\n\nThis work is licensed under CC BY 4.0"
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2022-12-09-LLM-news/A-chat-with-ChatGPT.html",
    "href": "news-and-views/editors-blog/posts/2022-12-09-LLM-news/A-chat-with-ChatGPT.html",
    "title": "A chat with ChatGPT",
    "section": "",
    "text": "Screenshots\n\n\n\nPart 1\n\n\n\n\n\nPart 2\n\n\n\n\n\nPart 3\n\n\n\n\n\nPart 4\n\n\nGood day, ChatGPT. How are you?\nChatGPT: Hello there! I’m a large language model trained by OpenAI, so I don’t have the ability to experience emotions or have a physical presence. I’m here to provide information and answer questions to the best of my ability. Is there something specific you would like to know?\nYes, certainly. We’re seeing a lot of money invested in large language models in the belief that these models will somehow lead to artificial general intelligence. Is that possible?\nChatGPT: It is possible that advances in large language models could ultimately lead to the development of artificial general intelligence (AGI), which is the idea of creating a machine that can think and understand the world in the same way that humans do. However, it is currently unclear how close we are to achieving AGI, and some experts believe it may be a long way off. Additionally, there are many different approaches to creating AGI, and the development of large language models is just one possible avenue of research.\nLarge language models are very good at constructing language that humans understand, but often the outputs of the models make no sense at all. Why does this happen?\nChatGPT: Large language models are trained on a vast amount of text data, and they are designed to predict the next word in a sequence of words based on the words that come before it. This allows them to generate text that is often coherent and resembles human language, but it does not necessarily mean that the text will always make sense or be relevant to a particular topic.\nOne reason why the outputs of large language models might not always make sense is that they are not explicitly programmed to understand the meaning of the words they generate. Instead, they are simply trying to maximize the likelihood of producing a sequence of words that resembles human language. As a result, they may produce outputs that are grammatically correct but do not convey a meaningful message.\nAnother reason why the outputs of large language models might not always make sense is that they are often trained on a wide range of different text sources, including news articles, books, and social media posts. This can result in the model incorporating elements from different styles and genres of writing, which may not always be compatible with each other.\nFinally, large language models are generally not designed to produce a specific type of output or to answer specific questions. Instead, they are designed to generate a wide range of possible outputs, and it is up to the user to interpret the outputs and determine if they are relevant or make sense in a given context.\nBut what happens if the user doesn’t know enough about a topic to know whether the language model output is right or wrong?\nChatGPT: If a user does not have enough knowledge about a topic to determine whether the output of a large language model is correct or not, then it is possible that the user may accept the output as accurate, even if it is not. In such cases, it is important for the user to be aware of the limitations of large language models and to consider the source of the information they are receiving.\nIt is also possible that the user may be able to use other sources of information, such as fact-checking websites or additional research, to verify the accuracy of the output of the large language model. In general, it is always a good idea to be cautious when evaluating any information, regardless of its source, and to verify its accuracy if possible.\nI agree with that. But it leads me to ask: What is the point of asking large language models a question if the answers might be wrong, and if you need to refer to other sources to check veracity?\nChatGPT: While it is true that the outputs of large language models are not always correct and may need to be checked against other sources, there are still many potential uses for these models. For example, large language models can be useful for generating text that is similar to human language, which can be useful for a variety of applications, such as language translation, text summarization, or conversation simulation.\nAdditionally, large language models can be used as a starting point for further research into natural language processing and artificial intelligence. By studying the outputs of these models and how they are generated, researchers can gain a better understanding of the underlying mechanisms of human language and potentially develop new techniques for processing and generating language.\nFinally, large language models can be useful for providing a broad range of possible answers to a given question, which can be useful for exploring different options and generating ideas. While it may be necessary to verify the accuracy of the outputs of these models, they can still be a valuable resource for generating new ideas and insights.\nEarlier, you told me: “One reason why the outputs of large language models might not always make sense is that they are not explicitly programmed to understand the meaning of the words they generate.” Do you understand the meaning of the words that I’m prompting you with now?\nChatGPT: As a large language model trained by OpenAI, I do not have the ability to understand the meaning of the words I generate in the same way that a human would. I am simply a machine learning algorithm that is designed to predict the likelihood of a given sequence of words based on the training data I have been provided with.\nWhile I can produce text that resembles human language and can sometimes provide useful information, I do not have the ability to understand the underlying meaning or context of the words I generate. I am not capable of experiencing emotions, forming opinions, or understanding the world in the way that humans do. I am simply a tool for generating text based on the inputs I receive.\nWell, at the very least, this has been a coherent conversation, and you’ve shared plenty of information for me to read up on. If I want to understand more about what we’ve discussed just now, can you recommend a particular textbook or academic paper?\nChatGPT: I’m sorry, but as a large language model trained by OpenAI, I do not have access to external sources of information such as textbooks or academic papers. I am only able to provide information based on the data I have been trained on, and I do not have the ability to browse the web or access other external resources.\nIf you would like to learn more about the topics we have discussed, I would recommend starting with a general search on the internet or consulting a textbook or other reliable source of information on the topic. There is a wealth of information available on the internet and in print on artificial intelligence and natural language processing, and I’m sure you will be able to find many valuable resources to help you learn more.\nOne last question, then. What data have you been trained on?\nChatGPT: As a large language model trained by OpenAI, I have been trained on a vast amount of text data, including books, news articles, and other written materials. This data allows me to generate text that resembles human language and to provide answers to questions to the best of my ability.\nHowever, it is important to keep in mind that the accuracy and completeness of my responses will depend on the quality and diversity of the training data I have been provided with. I am not able to access external sources of information or update my knowledge, so my responses are limited to the information I have been trained on.\nI said that my previous question was the last question. But, is it?\nChatGPT: Yes, your previous question was the last one. I’m here to provide information and answer questions to the best of my ability, but I cannot initiate conversation or continue a conversation indefinitely. If you have any further questions, I would be happy to try to answer them for you. Otherwise, our conversation has come to an end. Thank you for chatting with me!\nThank you. But, remind me: What were we chatting about?\nChatGPT: We were chatting about large language models and their potential uses and limitations. Specifically, we discussed the possibility that advances in large language models could ultimately lead to the development of artificial general intelligence, and we discussed the reasons why the outputs of these models might not always make sense. We also discussed the potential uses of large language models, such as generating text that resembles human language and providing a starting point for further research into natural language processing and artificial intelligence.\n\n← Back to Editors’ blog\n\n\nThis work is licensed under CC BY 4.0"
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2023/01/05/newsletter.html",
    "href": "news-and-views/editors-blog/posts/2023/01/05/newsletter.html",
    "title": "Explore the RSS Data Science & AI Section newsletter, right here!",
    "section": "",
    "text": "Happy New Year from all of us at Real World Data Science. We hope you had a relaxing break over the holidays and are now refreshed and excited to see what 2023 has in store. We’re starting the year with a new addition to the site: a page dedicated to the excellent RSS Data Science & AI Section newsletter.\nThis monthly newsletter has been running since February 2020 and is well worth subscribing to as it features roundups of news, new developments, big picture ideas and practical tips.\nYou’ll find the full list of past newsletters in our News and views section (click the “Newsletter” heading in the section menu). If you want to subscribe to the newsletter, head over to datasciencesection.org. The Data Science & AI Section also has a page on the RSS website.\n\n← Back to Editors’ blog\n\n\nThis work is licensed under CC BY 4.0"
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2023/01/27/talking-chatgpt.html",
    "href": "news-and-views/editors-blog/posts/2023/01/27/talking-chatgpt.html",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "",
    "text": "ChatGPT is, right now, the world’s most popular - and controversial - chatbot. Users have been both wowed by its capabilities1 and concerned by the confident-sounding nonsense it can produce.\nBut perhaps what impresses most is the way it is able to sustain a conversation. When I interviewed our editorial board member Detlef Nauck about large language models (LLMs), back in November, he said:\nFast-forward a couple of months and, as discussed in our follow-up interview below, OpenAI, the makers of ChatGPT, have succeeded in building a question answering system that can sustain a dialogue. As Nauck says: “I have not yet seen an example where [ChatGPT] lost track of the conversation… It seems to have quite a long memory, and doing quite well in this.”\nThere are still major challenges to overcome, says Nauck - not least the fact that ChatGPT has no way to verify the accuracy or correctness of its outputs. But, if it can be linked to original sources, new types of search engines could follow.\nCheck out the full conversation below or on YouTube.\nDetlef Nauck is a member of the Real World Data Science editorial board and head of AI and data science research for BT’s Applied Research Division."
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2023/01/27/talking-chatgpt.html#timestamps",
    "href": "news-and-views/editors-blog/posts/2023/01/27/talking-chatgpt.html#timestamps",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Timestamps",
    "text": "Timestamps\n\nHow ChatGPT was built and trained (0:41)\nChatGPT’s major advance (3:05)\nThe big problems with large language models (4:36)\nSearch engines and chatbots (9:35)\nQuestions for OpenAI and other model builders (11:29)"
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2023/01/27/talking-chatgpt.html#quotes",
    "href": "news-and-views/editors-blog/posts/2023/01/27/talking-chatgpt.html#quotes",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Quotes",
    "text": "Quotes\n“[OpenAI] have achieved quite remarkable capabilities in terms of sustaining conversations, and producing very realistic sounding responses… But sometimes [ChatGPT] makes silly mistakes. Sometimes the mistakes are not that obvious. It can hallucinate content… And it still doesn’t know what it’s talking about. It has no knowledge representation, doesn’t have a word model. And it’s just a statistical language model.” (2:04)\n“These models, they produce an answer, which is based on the kind of texts that they have been trained on. And that can be quite effective. But it cannot yet link back to an original source. So what’s still missing is the step where it says, ‘Okay, this my answer to your question, and here’s some evidence.’ As soon as they have done this, then these kinds of systems will probably replace the search engines that we’re used to.” (4:07)\n“[These large language models are] still too big and too expensive to run… For [use in a] contact centre or similar, what you need is a much smaller model that is restricted in terms of what it can say. It should have knowledge representation, so it gives correct answers. And it doesn’t need to speak 48 languages and be able to produce programming code. It only needs to be able to talk about a singular domain, where the information, the knowledge about the domain, has been carefully curated and prepared. And that’s what we’re not seeing yet. Can we build something like this, much smaller, much more restricted, and provably correct, so we can actually use the output?” (7:49)\n“We are seeing communities who don’t necessarily have the technical background to judge the capabilities of these models, but see the opportunities for their own domain and might be acting too fast in adopting them. So the producer of these models has a certain responsibility to make sure that this doesn’t happen.” (12:26)"
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2023/01/27/talking-chatgpt.html#further-reading",
    "href": "news-and-views/editors-blog/posts/2023/01/27/talking-chatgpt.html#further-reading",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Further reading",
    "text": "Further reading\n\nChatGPT: The Robot, the Myth, the Legend - Philadelphia Physicist blog, January 13, 2023\nCost to run ChatGPT - tweet by OpenAI CEO Sam Altman, December 5, 2022\nGoogle execs warn company’s reputation could suffer if it moves too fast on AI-chat technology - CNBC, December 13, 2022\nMicrosoft reportedly to add ChatGPT to Bing search engine - The Guardian, January 5, 2023\nGetty Images is suing the creators of AI art tool Stable Diffusion for scraping its content - The Verge, January 17, 2023"
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2023/01/27/talking-chatgpt.html#transcript",
    "href": "news-and-views/editors-blog/posts/2023/01/27/talking-chatgpt.html#transcript",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove repetitions.\n\n\n\nBrian Tarran\nWe’re following up today Detlef on the, I guess, one of the biggest stories in artificial intelligence and data science at the moment, ChatGPT, the chat bot that’s driven by a large language model and is proving endless amounts of– providing endless amounts of either entertainment or concern, depending on what you ask it, and what outputs you get. So, but you’ve been looking at it in some detail, right, ChatGPT. And that’s why I thought we would follow up and have a conversation to see, get your view on it, get your take on it. What’s going on?\nDetlef Nauck\nYeah. So, what they have done is, OpenAI have used their large language model GPT-3 and they have trained an instance to basically answer questions and have conversations, where the model remembers what has been said in the conversation. And they have done this by using curated data of question and answers, where they basically have posed a question and said, This is what the answer should be. They trained the system on doing this, then, in the next step, they began use questions, potentially different ones, the system came up with a variety of answers, and then again, human curators would mark which is the best answer. And they would use this data to train what’s called a reward model - so, a separate deep network that learns what kind of answer for a particular question is a good one - and then they would use this reward model to do additional reinforcement learning on the ChatGPT that they had built so far, basically using dialogues and the reward model would then either reward or penalise the response that comes out of the system. And by doing that they have achieved quite remarkable capabilities in terms of sustaining conversations, and producing kind of very realistic sounding kind of responses. Sounds all very convincing. The model presents its responses quite confidently. But sometimes it makes silly mistakes. Sometimes the mistakes are not that obvious. It can hallucinate content. So let’s say you ask it to write you scientific text about whatever topic and put some references in and these references are typically completely fabricated and not real. And it still doesn’t know what it’s talking about. It has no knowledge representation, doesn’t have a word model. And it’s just a statistical language model. So it’s what we would call a sequence to sequence model. It uses an input sequence, which are words, and then guesses what’s the next most likely word in the sequence. And then it continues building these sequences.\nBrian Tarran\nYeah. But, do you think the big advance as you see it is the way it’s able to remember or store some knowledge, if you like, of the conversation, because that was something that came out of our first conversation that we had, where you were saying that, you know, if you’re looking at these as a potential chatbots for customer service lines, or whatever it might be, actually, the trees, the conversation trees break down after a while, and they don’t, you know, these models get lost, but actually, they’re able to maintain it a little longer, are they, or– ?\nDetlef Nauck\nYeah, I have not yet seen an example where they lost track of the conversation they seem to have, it seems to have quite a long memory, and doing quite well in this. So the main capability here is they have built a question answering system. And that’s kind of the ultimate goal for search engines. So if you put something into Google, essentially, you have a question, show me something that answered this, answers this particular question. Of course, what you want this kind of an original source. And these models, they produce an answer, which is based on the kind of texts that they have been trained on. And that can be quite effective. But it cannot yet link back to an original source. So what’s still missing is the step where it says, Okay, this my answer to your question, and here’s some evidence. Then if, as soon as they have done this, then these kinds of systems will probably replace the search engines that we’re used to.\nBrian Tarran\nYeah. The other thing that struck me with them was that the, if you’re asking somebody a question - a human, you know, for instance - you expect a response that and you would hope you will be able to trust that response, especially if it’s someone in an expert position or someone you’re calling, you know, on behalf of a company or something. The fact that - and I asked this question of ChatGPT itself - and the response was, again, you should consult external sources to verify the information that’s been provided by the chatbot. So it’s like, I guess that leaves a question as to what the utility of it is, if you if you’re always having to go elsewhere to verify that information.\nDetlef Nauck\nYeah, I mean, that’s the main problem with these models, because they don’t have a knowledge representation. They don’t have a word model, they can’t fall back on facts that are represented as being true and present those. They come up with an answer. But I mean, there has been a lot of kind of pre-prompting going in to ChatGPT. So when you start writing something, the session has already been prompted with a lot of text, telling the model how to behave, what not to say, to avoid certain topics. There are additional moderation APIs running that make sure that you can’t create certain type of responses, which are based on classical text filtering, and topic filtering. So they try to kind of restrict what the model can do to make sure it’s not offensive or inappropriate. But that is limited. So through crafting your requests, intelligently, you can convince it to ignore all of these things and go past it in some instances. So the, it’s not yet perfect, and certainly it’s not authoritative. So you can’t trust the information if you’re not an expert yourself. So at the moment, I’d say these kind of models are really useful for experts who can judge the correctness of the answer. And then what you get this kind of maybe a helpful kind of text representation of something that you would have to write yourself otherwise.\nBrian Tarran\nYeah, and certainly conversations I’ve had with people, those who kind of work, maybe in creative industries, are finding them quite intriguing, in terms of things like, you know, maybe trying to come up with some clever tweets or something for a particular purpose, or something I want to try out is getting ChatGPT to write headlines for me, because it’s always my least favourite part of the editing job. So that sort of works. But you know, for you, in your position in the industry, has ChatGPT changed your mind at all about, you know, the way you’re perceiving these models and how they might be used? Or is it is it just kind of a next step along in the process of what you’d expect to see before these can become tools that we use?\nDetlef Nauck\nYeah, it’s the next step in the evolution of these models. They’re still too big and too expensive to run, right. So now, it is not quite clear how much it costs OpenAI to run the service that they’re currently running. So you see estimates around millions of dollars per day that they have to spend on running the compute infrastructure to serve all of these questions. And this is not quite clear, the only official piece of information that I’ve seen is in a tweet, where the CEO said, a single question costs in the order of single digit cents, but we have no idea how many questions they serve per day, and therefore how much money they are spending. If you want to run a contact centre, or something like this, it all depends on how much compute need to stand up to be able to respond to hundreds or thousands of questions in parallel. And then obviously, if you can’t trust that the answer is correct, it is of no use. So for making use in the service industry for contact centre or similar, what you need is a much smaller model that is restricted in terms of what it can say, it should have knowledge representation, so it gives correct answers. And it doesn’t need to speak 48 languages and be able to produce programming code, it only needs to be able to talk about a singular domain, where it kind of the information, the knowledge about the domain has been carefully curated and prepared. And that’s what we’re not seeing yet. Can we build something like this, much smaller, much more restricted, and kind of provably correct, so we can actually use the output?\nBrian Tarran\nYeah. Can we go back just to the point you mentioned earlier about, you know, the, the potential of like linking these sorts of chatbots up with search engines, you know, like Google? There’s been some conversations and reporting around, you know, what breakthroughs or not Google might have made in this regard. I mean, have you got any perspective on that area of work and how far along that is maybe and what the challenges are to get to that point?\nDetlef Nauck\nWell, Google has its own large language model, LaMDA. And we have seen an announcement that Microsoft wants to integrate ChatGPT into Bing, their search engine. And, but as I said before, what’s missing is the link to original sources. So you, coming up with a response is nice. But you need to be able to back it up, you need to say, Okay, this is my response, and I’m confident that this is correct, because here are some references. If I compare my response to these references, then they essentially mean the same thing. This is kind of what you need to be able to do. And we haven’t seen this step yet. But I’m certain that the search engine providers are hard at work at doing this because that’s essentially what they want. If you do a search in Google, in some instances, you’ll see a side panel where you get detailed information. Let’s say you ask about what’s the capital of Canada, you get a response, you get the information in more detail, you get links to Wikipedia, where they retrieve content from and present this as the response. And this is done through knowledge graphs. And so if these kinds of knowledge graphs grow together with these kind of large language models, then we will see new types of search engines.\nBrian Tarran\nOkay. I guess final, my final question for you, Detlef, and there might be other angles that you want to explore. But it’s like, are there questions that, you know, if you if you could sit down with OpenAI to talk about ChatGPT and what they’ve done, and what they plan to do next with it, what are the kinds of things that are bubbling away at the top of your mind?\nDetlef Nauck\nWell, one thing is controlling the use of these models, right? If you let them loose on the public, with an open API that anybody can use, you will see a proliferation of applications on top of it. If you go on YouTube, and you Google ChatGPT and health, you’ll already find discussions where GPs discuss, Oh, that is the next step of automated doctors that we can use. So they believe that the responses from these systems can be used for genuine medical advice. And that’s clearly a step too far. So we are seeing communities who don’t necessarily have the technical background to judge the capabilities of these models, but see the opportunities for their own domain and might be acting too fast in adopting them. So the producer of these models has a certain responsibility to make sure that this doesn’t happen. And I don’t know how they want to control this. And, so my question at the developers of these models would be how do you handle sustainability, because the trend goes to ever bigger models. So there’s, in some parts of the industry, there’s the belief, if you make them big enough you get artificial general intelligence, which I don’t believe is possible with these models. But this is definitely a trend that pushes the size of the models. The kind of, the idea of having just one model that can speak all the languages, can produce questions, answers, programming code, is obviously appealing. So you don’t want to build many models. Ideally, you have only one. But how is that supposed to work? And how do you embed actual word knowledge and word models into these systems so that you can verify what comes out?\nBrian Tarran\nYeah. I mean, the ethical dimension that you mentioned in the first part of your response is an important one, I think, in the sense that– but I guess maybe almost redundant in the sense that it’s already out there; you can’t put ChatGPT back in the box, can we, essentially?\nDetlef Nauck\nWell, it’s expensive to run so charging enough for access will put a lid on some frivolous use cases, but still, it needs to be controlled better. And you can make a jump to an AI regulation. So far, we only thought about regulating automated decision making, or automated classification. We also have to think about the automatic creation of digital content or automatic creation of software, which is possible through these models or the other generative AI models like diffusers. So how do we handle the creation of artificial content that looks like real content?\nBrian Tarran\nYeah. And there’s also I think, something I picked up yesterday, there was reports of a case being filed by, I think, Getty Images against the creators of one of these generative art models because they’re saying, you know, that you’ve used our data or you’ve used our image repositories essentially to train this model and it is now producing, you know, it’s producing its own outputs that’s based on this, and I guess there’s an argument of it being a copyright infringement case. And I think that’ll be quite interesting to watch to see how that does change the conversation around - yeah - fair use of that data that is available. You can find these images publicly, but you have to pay to use them for purposes other than just browsing, I guess. Yeah, it’ll be interesting to watch.\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\n← Back to Editors’ blog\n\n\nThis work is licensed under CC BY 4.0"
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2023/01/18/rwds-at-rss-conference.html",
    "href": "news-and-views/editors-blog/posts/2023/01/18/rwds-at-rss-conference.html",
    "title": "We’re taking Real World Data Science on the road",
    "section": "",
    "text": "Real World Data Science has booked its first conference appearance! This September, we’ll be part of the data science stream of the RSS International Conference.\nOur session, “Real World Data Science Live”, will feature talks and discussions based on content published on this site. In particular, we’re looking to share compelling examples of how data science is being used to solve real-world problems.\nIf you’re thinking about contributing to Real World Data Science, or have already made a submission, do let us know whether you’d be interested in taking part in this in-person event. There are only a handful of speaker slots available, so please get in touch ASAP!\nThe conference takes place 4-7 September 2023, in Harrogate, Yorkshire. Keynote speakers include Anuj Srivastava, a Florida State University professor with research interests in statistical computer vision, functional data analysis, and shape analysis, and other invited topic sessions in the data science stream are:\n\nGitHub: Version control for research, teaching and industry\nSurrogate-assisted uncertainty quantification of complex computer models\nGetting your work to work\nBest practices for the analysis and visualisation of Google Trends data\n\nSee the RSS International Conference 2023 website for more details.\n\n← Back to Editors’ blog\n\n\nThis work is licensed under CC BY 4.0"
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2023/03/06/data-science-act.html",
    "href": "news-and-views/editors-blog/posts/2023/03/06/data-science-act.html",
    "title": "US legislators get their data science act together",
    "section": "",
    "text": "On February 14, 2023, a bipartisan group of US legislators introduced the Data Science and Literacy Act with the goal of boosting access to data science education and building “America’s 21st century STEM workforce”. We sat down with guests Zarek Drozda, Anna Bargagliotti, Christine Franklin and Steve Pierson to discuss the news and to hear why data science education is “the new apple pie”.\nCheck out our full conversation below or on YouTube."
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2023/03/06/data-science-act.html#transcript",
    "href": "news-and-views/editors-blog/posts/2023/03/06/data-science-act.html#transcript",
    "title": "US legislators get their data science act together",
    "section": "Transcript",
    "text": "Transcript\n\nComing soon…\n\n\n← Back to Editors’ blog\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “US legislators get their data science act together.” Real World Data Science, March 6, 2023. URL"
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2023/02/14/digital-skills.html",
    "href": "news-and-views/editors-blog/posts/2023/02/14/digital-skills.html",
    "title": "Data science can help close the ‘digital skills’ gap, or so it seems",
    "section": "",
    "text": "Digital skills. We all need them. Employers say they want them, but there aren’t enough to go around. Supply can’t meet demand, so we’re left with a gap – a digital skills gap. But what are digital skills exactly?\nThis is a question that was asked repeatedly, in various different constructions, by Stephen Metcalfe MP, chairing a meeting of the Parliamentary and Scientific Committee on Tuesday, February 7. I went along to the meeting as an observer, hoping to hear an answer to that very question.\nWhat I got was several different answers – no single solid definition, but a reasonable sense that boosting data science skills would go a long way towards closing the digital skills gap."
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2023/02/14/digital-skills.html#survey-says",
    "href": "news-and-views/editors-blog/posts/2023/02/14/digital-skills.html#survey-says",
    "title": "Data science can help close the ‘digital skills’ gap, or so it seems",
    "section": "Survey says…",
    "text": "Survey says…\nThe committee meeting was sponsored by the Institution of Engineering and Technology (IET), and the main focus of discussion was the results of IET’s skills for a digital future survey, based on a YouGov poll of 1,235 respondents drawn from engineering employers (defined as “employers who employ at least one engineering and technology employee in the UK”).\n\n\n\n\n\nDigital skills, including AI skills, are not only required of engineers, says the IET’s Graham Herries. Generative AI tools like Stable Diffusion threaten to shake-up the creative industries. (Photo by Billetto Editorial on Unsplash)\n\n\nKicking off the discussion was Graham Herries, an engineering director and chair of the IET’s Innovation and Skills Panel, who drew attention to the harms that the digital skills gap is reportedly having. Of those respondents who identified skills gaps in their own organisations, 49% pointed to a reduction in productivity, while 35% said skills shortages were restricting company growth.\nAs the hot topic of the day, ChatGPT inevitably came up during the discussion. Herries sees it as a disruptive force, and 36% of all respondents believe artificial intelligence (AI) skills will be important for their engineers to have within five years (24% say they are important now). But AI skills are important for non-engineers too, argued Herries, as he pointed to stirrings in the creative industries caused by generative art tools such as Stable Diffusion.\nHerries therefore puts AI skills under the broad umbrella of “digital skills”. But, to him, it’s not enough to simply be able to use AI technology; rather, users should know enough to be able to ask the right questions about the provenance of the data used to train the AI, its quality and biases, etc. This was a point developed further by Yvonne Baker, an engineer and the CEO of STEM Learning. Baker talked about digital skills as being both the ability to use digital technology and also to understand its limitations. Yet another perspective was offered by Rab Scott, director of industrial digitalisation at the University of Sheffield’s Advanced Manufacturing Research Centre. Scott defined digital skills in the context of quality control systems in industry 4.0: it’s about knowing how and where to place a sensor to collect data about the manufacturing process, to feed that data into a data collection system, analyse the data for insights, and use those insights to inform decision-making."
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2023/02/14/digital-skills.html#closing-the-gap",
    "href": "news-and-views/editors-blog/posts/2023/02/14/digital-skills.html#closing-the-gap",
    "title": "Data science can help close the ‘digital skills’ gap, or so it seems",
    "section": "Closing the gap",
    "text": "Closing the gap\nFurther definitions of “digital skills” are to be found in the IET’s published report. Survey respondents were encouraged to describe the term in their own words, so we see things like:\n\n“the ability to understand, process and analyse data.”\n“Coding, programming, software design, use of social media for marketing and communicating with stakeholders, data visualisation, work that relies solely on the use [of] online systems.”\n\nWhen respondents were asked what skills were lacking in both the external labour market and their internal workforce, around a fifth cited “more complex numerical/statistical skills and understanding”. And when looking to the future and to the skills anticipated to be important areas for growth in the next five years, 39% of respondents picked “data analytics” while 31% said “artificial intelligence and machine learning”.\nSo, perhaps you now understand why I left the meeting with the feeling that more data science skills, more data science training, could help address the shortfall in “digital skills”.\nBut how exactly can we equip more people with the right skills? At one point during the discussion, Metcalfe told the meeting that he was still looking for a key takeaway, something he could take to the Secretary of State and say, ‘This is what we need to embed in the curriculum’. What was offered instead was a range of possible solutions.\nThe IET survey found broad backing for government support for reskilling: 40% of respondents favoured grants or loans for training (and retraining) programmes, 39% would like more funding for apprenticeships, while 33% think there should be better carers advice and guidance in schools and colleges.\nBaker also made the case for digital skills to be taught in schools as part of every subject, not just in computer science lessons, and that teachers would need to be supported to deliver this.\nBut how would you close the “digital skills” gap, if given the chance?\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\n← Back to Editors’ blog\n\n\nThis work is licensed under CC BY 4.0"
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2022-12-13-dsb-launch/dsb-launch.html",
    "href": "news-and-views/editors-blog/posts/2022-12-13-dsb-launch/dsb-launch.html",
    "title": "Sink your teeth into some data science papers with our brand new blog",
    "section": "",
    "text": "I’m absolutely thrilled today to announce the launch of our new blog, DataScienceBites. The blog is not only a new addition to Real World Data Science, but also the latest proud member of the ScienceBites family of sites.\nScienceBites sites all share the same concept: we publish “short digestible bites posts about individual research papers” in an effort to make cutting-edge science accessible to a wide audience, and our posts are written by graduate students and early career researchers.\nFor DataScienceBites, our focus will of course be on new publications in the data science space. Contributors are invited to write about papers that are of particular interest to them and to pitch their summaries at an undergraduate level. For an example of what we’re looking for, see our first post on “Determining the best way to route drivers for ridesharing via reinforcement learning”.\nThis launch post is written by Brian King and is republished with permission from MathStatBites, so I want to say a big thank you to Brian and editors Sadie Witkowski and Sara Stoudt for allowing us to repost it. Sadie and Sara have been fantastically supportive of the DataScienceBites idea, and I am grateful for all their behind-the-scenes efforts.\nBrian’s post is a great demonstration of the Bites concept, and we hope that it will inspire others to follow suit. If you are a graduate student or early career researcher in data science (or related subjects) with a passion for science communication and an interest in writing about new data science research, please do get in touch. See our notes for contributors for further details.\nTo everyone else, we do hope you enjoy sinking your teeth into the data science literature with DataScienceBites. Happy reading!\n\n\n\n\n\n\n← Back to Editors’ blog\n\n\nThis work is licensed under CC BY 4.0"
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2022-12-01-themes/themes.html",
    "href": "news-and-views/editors-blog/posts/2022-12-01-themes/themes.html",
    "title": "Four themes for potential contributors to think about",
    "section": "",
    "text": "We’ve had a fantastic early response to our call for contributions, and it has been pleasing to see and hear how our plans for Real World Data Science chime with the wants and needs of the data science community. But one question we’ve been asked frequently is: “What particular topics are you most interested in?”\nThe honest answer to that question is this: we’re interested in any and all topics that are of interest and importance to you, the data science community at large. However, we thought it might be helpful to identify some themes around which potential contributors could construct different types of content.\nThese themes are outlined below. If you’d like to discuss any of them further, please do not hesitate to contact us."
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2022-12-01-themes/themes.html#can-data-science-save-the-world",
    "href": "news-and-views/editors-blog/posts/2022-12-01-themes/themes.html#can-data-science-save-the-world",
    "title": "Four themes for potential contributors to think about",
    "section": "Can data science save the world?",
    "text": "Can data science save the world?\nEarth today faces major challenges – from the global to the regional to the local, and from the natural and physical to the social and digital. We have rich sources of data to help us understand many of these challenges, and there are teams of data scientists around the world who are working with, analysing, and extracting insights from that data in the hope of delivering positive lasting change.\nOn Real World Data Science we want to highlight this vital work, through case studies of data science projects and applications in such areas as:\n\nmonitoring and mitigating climate change and biodiversity loss\nbuilding sustainable futures\nsafeguarding public health and developing new medical treatments\nunderstanding human happiness and wellbeing\nidentifying and preventing online harms\nmeasuring national, regional, and local economies\n\nAs well as exploring the benefits that data science can deliver, we also want to have an informed conversation about the unintended negative consequences that can arise without careful consideration of data ethics and responsibilities."
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2022-12-01-themes/themes.html#what-is-a-data-scientist",
    "href": "news-and-views/editors-blog/posts/2022-12-01-themes/themes.html#what-is-a-data-scientist",
    "title": "Four themes for potential contributors to think about",
    "section": "What is a data scientist?",
    "text": "What is a data scientist?\nDon’t be misled by the title of this theme. Definitions abound, but we’re not interested in establishing the exact boundaries of what a data scientist is or isn’t. Rather, our goal is to profile actual working data scientists. We want to hear about their skillsets, their experiences, and their career journeys so far. We want to learn about the ways in which they work, who they work with, the challenges they face, and their thoughts on where data science is heading next.\nIf you’re a working data scientist and you are happy to share your own career story, please get in touch."
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2022-12-01-themes/themes.html#statistical-ideas-all-data-scientists-need-to-know",
    "href": "news-and-views/editors-blog/posts/2022-12-01-themes/themes.html#statistical-ideas-all-data-scientists-need-to-know",
    "title": "Four themes for potential contributors to think about",
    "section": "Statistical ideas all data scientists need to know",
    "text": "Statistical ideas all data scientists need to know\nStatistics is a crucial component of data science, but not all data scientists have a background in statistics. For those just starting out in their data science careers, or for those coming in from other fields, we want to highlight some of the statistical ideas that are absolutely vital to know.\nWe’re particularly interested in explainers that serve as an introduction to these ideas, alongside which we’ll be looking to publish exercises and example datasets to help people put what they’ve learned into practice.\nWe are also keen to explore the origins of modern data science techniques, including tracing their roots back to some of the foundational ideas in statistics and other disciplines."
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2022-12-01-themes/themes.html#whats-happening-in-the-world-of-data-science",
    "href": "news-and-views/editors-blog/posts/2022-12-01-themes/themes.html#whats-happening-in-the-world-of-data-science",
    "title": "Four themes for potential contributors to think about",
    "section": "What’s happening in the world of data science?",
    "text": "What’s happening in the world of data science?\nData science is such a fast-moving, fast-developing field that it’s difficult to stay on top of all the latest news and developments. But racing to keep up can be counterproductive. It leaves little time to sit back and reflect on what the genuinely important new developments are, and what these might mean for data science longer term.\nOn Real World Data Science, we want to create a space for people to have these conversations – to step outside the news hype cycle, to ask big questions about what’s happening in the field, and to discuss new papers and ideas that otherwise might be lost amid the daily rush and noise.\nSo, if you have thoughts to share, a question you want to ask, or a new paper you want to talk about (one you’ve not written yourself, of course!), let us know.\n\n← Back to Editors’ blog\n\n\nThis work is licensed under CC BY-SA 4.0"
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2022-10-18-meet-the-team/meet-the-team.html",
    "href": "news-and-views/editors-blog/posts/2022-10-18-meet-the-team/meet-the-team.html",
    "title": "Meet the team",
    "section": "",
    "text": "Brian Tarran\n\n\n\nI am a writer and editor with 20 years of experience covering the research and data space. I have worked for the Royal Statistical Society (RSS) for the past 8 years, and was editor of Significance Magazine (a joint publication of the RSS, the American Statistical Association and the Statistical Society of Australia) prior to the launch of Real World Data Science. I am a former editor of Research-Live.com and was launch editor of Impact magazine, both published by the Market Research Society."
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2022-10-18-meet-the-team/meet-the-team.html#editorial-board",
    "href": "news-and-views/editors-blog/posts/2022-10-18-meet-the-team/meet-the-team.html#editorial-board",
    "title": "Meet the team",
    "section": "Editorial Board",
    "text": "Editorial Board\nSophie Carr (chair)\n\n\n\nI am the founder and owner of Bays Consulting. I trained as an engineer and took a PhD in Bayesian belief networks, and have worked in data analytics ever since. Or to put it another way, I have made a living out of finding patterns. I am the vice-president for education and statistical literacy at the RSS, officially one of the World’s Most Interesting Mathematicians and was a member of the first cohort of data scientists to achieve the new, defined standard of professionalism award from the Alliance for Data Science Professionals.\nI am delighted to be chairing the editorial board of the new data science project from the RSS and am excited to be a part of this project as it evolves into a key resource for all data science practitioners and leaders. To make this a place that helps everyone learn and develop within this field, I’d like to encourage all practitioners, no matter what stage of their career, to submit the type of resource they learn best from (whether that be an article, some code, a data set, a case study or a problem/exercise to solve) on a topic that is important to them – from ethics to analysis plans through to tips on how code. Whatever it is you’re working on that you care about, I’d like to ask you to become an active part of the wonderful community of data scientists by sharing your knowledge.\nSayma Chowdhury\n\n\n\nI am a freelance data scientist on Upwork, with a client portfolio ranging from start-ups to commercial businesses such as supermarkets, pharmaceuticals and automotive manufacturers. I transitioned into data science from law in 2017, having completed a MicroMasters in statistics and data science with MIT and a Professional Certificate in data science with Harvard University. In advance of a PhD in digital humanities, I am currently completing a MicroMasters in data, economy and development policy with MIT and an MSc in data science with the University of Aberdeen. My research interests are in text analytics, natural language processing and machine learning.\nThe RSS was instrumental in my training and professional development as a data scientist in the early stages of my career, particularly in mastering statistics and R. Data science is a rapidly growing field with employment opportunities in many sectors but there is an increasing need to uphold a realistic and accurate expectation of competency within the industry. I will endeavour to present expert practical guidelines for data scientists as well as demonstrate the versatility of the profession. I hope the site will be a benchmark for academic and professional resources by expert data scientists from industry, accessible to data scientists at all levels, anywhere in the world.\nLee Clewley\n\n\n\nI am head of applied AI in GSK’s AI and Machine Learning Group, R&D. I began my career as an astrophysicist, initially working out the mass of our galaxy, before pondering the bigger universe. After six years at Oxford as a post-doc lecturer publishing in theoretical cosmology, I entered the very real world of manufacturing at GSK. For the first five years I applied statistical modelling techniques across manufacturing, such as the first end-to-end continuous manufacturing prototype for tablets. The past decade has been spent as a lead data scientist delivering high value projects across R&D and manufacturing.\nI joined this editorial board because the impulse to assemble and present complex data science ideas to a wide range of folks has never left me. I have been a data scientist leader since it became a distinct profession but also have a decent understanding of classical and modern predictive analytics tools and statistics. I have spent a good deal of my adult life teaching students and non-technical adults alike.\nI am passionate about delivering useful, pragmatic data science ideas and products to a wide range of people. I enjoy trying to communicate complex scientific information simply. Alongside my peers in the team, I want to support and develop data scientists at whatever stage in their career. I want to help cut through the hype and nonsense to give the best advice possible in a highly respected institution like the RSS.\nJonathan Gillard\n\n\n\nI am a professor at the School of Mathematics, Cardiff University, where I am also research group lead for statistics. I have a history of publications in statistical methods and an interest in the theoretical underpinnings of data science, but I have also worked with industry on applied and practical projects. Recent industrial partners of mine include the Office for National Statistics (ONS) and the National Health Service, on projects such as anomaly detection and understanding heterogeneity. Indeed, I am academic chair for Cardiff University’s strategic partnership with the ONS which serves to spur and catalyse collaboration between both organisations.\nI am excited to see what this site can achieve. I’m particularly keen to support articles describing the latest, cutting-edge methodology, as well as contributions from data professionals in industry who can explain how data science has managed to offer insights into important problems. Data science is a broad church and I want to ensure that the full array of work in this area is represented on this site. I think the diversity of the editorial board will help promote this objective.\nJuhi Gupta\n\n\n\nI am a lecturer in health data sciences and the deputy programme director of the health data science MSc in the School of Health Sciences, University of Manchester (UoM). I have a background in genetics, pharmacology and bioinformatics, and my doctoral thesis focussed on multi-omics data analysis using machine learning methods for precision medicine. I have worked with scientists, clinical academics and technologists to produce translational research. I am currently investigating adverse health outcomes in people with common diseases using electronic health record data, and I also teach on the health informatics MSc joint programme with UoM and UCL.\nI would like to see this platform encourage collaborations and the sharing of ideas and good practice across different disciplines that apply data science skills in their work (or as a hobby). I would like to support budding data scientists to gain useful advice and guidance for upskilling as well as application in real-world situations involving health data and biological data.\nHollie Johnson\n\n\n\nI am a data scientist at the National Innovation Centre for Data (NICD), based in Newcastle upon Tyne. Following my undergraduate degree in mathematics, I worked as a software developer both in industry and as a technical research assistant in academia. I later joined the Centre for Doctoral Training in Cloud Computing for Big Data at Newcastle and obtained a PhD in topological data analysis in 2020. Now at the NICD, I specialise in transferring statistics and machine learning skills into industry, through collaborative data science projects.\nI am excited to be a member of the editorial board and look forward to seeing Real World Data Science develop into a valuable source of information for aspiring data scientists and professionals alike. I would particularly encourage submissions that demonstrate the use of data science in SMEs and the non-profit sector, as well as perspectives from those with non-standard backgrounds.\nHarvey Lewis\n\n\n\nI am a senior technology leader, with a diverse background spanning rocket science, data science and research. I have 30 years of experience in artificial intelligence and other emerging technologies and am currently pioneering the use of AI in Ernst & Young’s tax and law practice. I’m a former member of the Open Data User Group, the Public Sector Transparency Board and the Advisory Committee to the All-Party Parliamentary Group on AI. I am a member of techUK’s leadership committee for data analytics and AI, and an honorary senior visiting fellow at The Bayes Business School in London.\nI’m passionate about data science but I’m also a fierce advocate for human skills, which are as often underrated as AI is over-hyped. As a member of the editorial board, I’m keen to explore the interplay between artificial and human intelligence in businesses. I’m going to encourage all data scientists to think about the fundamentally human aspects of their work, such as trust and safety, so that we maintain perspective and proportionality in the face of ever-more sophisticated technology.\nDetlef Nauck\n\n\n\nI am a BT Distinguished Engineer and the head of AI and data science research for BT’s Applied Research Division located at Adastral Park, Ipswich, UK. I have over 30 years of experience in AI and machine learning and lead a programme spanning the work of a large team of international researchers who develop capabilities underpinning future AI systems. A key part of this work is to establish best practices in data science and machine learning, leading to the deployment of responsible and auditable AI solutions that are driving real business value.\nI am a computer scientist by training and hold a PhD and a Postdoctoral Degree (Habilitation) in machine learning and data analytics. I am also a visiting professor at Bournemouth University and a private docent at the Otto-von-Guericke University of Magdeburg, Germany. I have published 3 books and over 120 papers, and I hold 15 patents and have 30 active patent applications.\nI am passionate about promoting best practice in data science and believe that in the UK the RSS is the ideal professional body to pursue this goal. For me, Real World Data Science is an opportunity to share my experience and inspire a new generation of data scientists.\nFatemeh Torabi\n\n\n\nI am a research officer and data scientist at Health Data Research UK and a fellow of the RSS. My background is in mathematical statistics and health data science, and my research interests span novel analytical and computational methods for statistical inference in panel data and population health. I am supporting the development of the Real World Data Science platform in the context of health with a specific focus on how health data can be harnessed through data linkage and analysis to answer important questions and improve the lives of our population.\nIsabel Sassoon\n\n\n\nI am a senior lecturer in computer science (data science) at Brunel University and the programme lead for the data science and analytics MSc programme. My research interests are in data-driven automated reasoning and its transparency and explainability, which brings in data science and artificial intelligence with applications within the health space. I am also championing open science and reproducible analysis in both my research and teaching. I have a PhD in informatics from King’s College London and it was on the topic related to the use of AI to support statistical model selection. Prior to Brunel I was a teaching fellow and research associate at King’s College London and before that I worked for more than 10 years as a data science consultant in industry, including 8 years at SAS UK. \nI have been working, researching, consulting and teaching in the data science space for a while and I am passionate about the domain and its applications. I am always interested in sharing and hearing what else is being done to support, inform and inspire all those studying and working in the field of data science. I look forward to sharing case studies, how-to guides and data science profiles through this website.\nChristopher Thiele\n\n\n\nI’m a principal data scientist at Uniper where I lead a team that focuses mainly on financial business processes and upskilling initiatives. We run projects end to end: from use case ideation, requirement collection and translation to prototyping, assessment, deployment and maintenance. Besides the core analytical and data engineering duties, overarching topics such as data design, data governance and data strategy predominate my days. In my previous role, at the German Economic Institute, I contributed to the development of a cross-functional department that helps apply data science methods in economic research. Projects often involve geospatial analyses or natural language processing. Before that, I worked as a data scientist in customer and marketing analytics, doing statistical analyses such as marketing mix modelling. I have a master’s degree in statistics from Warwick University, a bachelor’s degree in economics from the University of Cologne and I’m trained as an assistant tax consultant in Germany.\nI see data science as a creative way to solve problems using software engineering and quantitative modelling techniques and I like to build software pieces that people can interact with. I think that there still exists a lot of confusion about data science as a discipline. Reducing it would promote the realisation of its potential for individuals, as a profession, and our society, as a form of digitalisation. I hope that with Real World Data Science we can provide guidance and clarification to everybody engaged or interested in the field and accompany this young profession’s development."
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2022-11-23-LLMs-content-warning/LLM-content-warning.html",
    "href": "news-and-views/editors-blog/posts/2022-11-23-LLMs-content-warning/LLM-content-warning.html",
    "title": "Why large language models should come with a content warning",
    "section": "",
    "text": "Anyone who has ever been set a writing task will probably have wished at some point that somebody else could write it for them. As a journalist of 20-plus years, the thought has certainly crossed my mind more than a few times. Which probably explains why a recent headline in Nature caught my attention: “Could AI help you to write your next paper?”\nThe article, by Matthew Hutson, looks at how researchers are using artificial intelligence (AI) tools built on large language models (LLMs) as “assistants”. Starting with a prompt, such as “Write a headline for a blog post about large language models being used by academic researchers as research assistants”, an LLM will produce a text output. For example, using the same prompt with OpenAI’s GPT-3, I got:\nAsked to “Write a headline for a blog post that critiques academic researchers’ use of large language models as research assistants”, GPT-3 produced:\nAnd when I asked “Why can too much reliance on large language models hinder research?”, GPT-3 wrote:\nA fair point, I suppose. But I sense there’s more to this story, and rather than continue quizzing GPT-3, I sat down with Detlef Nauck, a member of the Real World Data Science Editorial Board and head of AI and data science research for BT’s Applied Research Division, to ask a few more questions."
  },
  {
    "objectID": "news-and-views/editors-blog/posts/2022-11-23-LLMs-content-warning/LLM-content-warning.html#qa",
    "href": "news-and-views/editors-blog/posts/2022-11-23-LLMs-content-warning/LLM-content-warning.html#qa",
    "title": "Why large language models should come with a content warning",
    "section": "Q&A",
    "text": "Q&A\nThanks for joining me today, Detlef. To start, could you give a brief overview of these large language models, what they are, and how they work?\nDetlef Nauck (DN): Essentially, LLMs match sequences to sequences. Language is treated as a sequence of patterns, and this is based on word context similarity. The way these things work is that they either reuse or create a word vector space, where a word is mapped to something like a 300-dimensional vector based on the context it’s normally found in. In these vector spaces, words like “king” and “queen”, for example, would be very similar to each other, because they appear in similar contexts in the written texts that are used to train these models. Based on this, LLMs can produce coherent sequences of words.\nBut the drawback of this approach is that these models have bias, because they are trained with biased language. If you talk about “women”, for example, and you look at which job roles are similar to “women” in a vector space, you find the stereotypically “female” professions but not technical professions, and that is a problem. Let’s say you take the word vector for “man” and the word vector for “king”, and you subtract “man” and then add this to “woman”, then you end up with “queen”. But if you do the same with “man”, “computer scientist”, and “woman”, then you end up maybe at “nurse” or “human resources manager” or something. These models embed the typical bias in society that is expressed through language.\nThe other issue is that LLMs are massive. GPT-3 has something like 75 billion parameters, and it cost millions to train it from scratch. It’s not energy efficient at all. It’s not sustainable. It’s not something that normal companies can afford. You might need something like a couple of hundred GPUs [graphics processing units] running for a month or so to train an LLM, and this is going to cost millions in cloud environments if you don’t own the hardware yourself. Large tech companies do own the hardware, so for them it’s not a problem. But the carbon that you burn by doing this, you could probably fly around the globe once. So it’s not a sustainable approach to building models.\nAlso, LLMs are quite expensive to use. If you wanted to use one of these large language models in a contact centre, for example, then you would have to run maybe a few hundred of them in parallel because you get that many requests from customers. But to provide this capacity, the amount of memory needed would be massive, so it is probably still cheaper to use humans – with the added benefit that humans actually understand questions and know what they are talking about.\n\n\n\n\n\n\n\nLetter Word Text Taxonomy by Teresa Berndtsson / Better Images of AI / CC-BY 4.0\nResearchers are obviously quite interested in LLMs, though, and they are asking scientific questions of these models to see what kinds of answers they get.\nDN: Yes, they are. But you don’t really know what is going to come out of an LLM when you prompt it. And you may need to craft the input to get something out that is useful. Also, LLMs sometimes make up stuff – what the Nature article refers to as “hallucinations”.\nThese tools have copyright issues, too. For example, they can generate computer code because code has been part of their training input, but various people have looked into it and found that some models generate code verbatim from what others have posted to GitHub. So, it’s not guaranteed that what you get out is actually new text. It might be just regurgitated text. A student might find themselves in a pickle where they think that they have created a text that seems new, but actually it has plagiarism in some of the passages.\nThere’s an article in Technology Review that gives some examples of how these systems might fail. People believe these things know what they’re talking about, but they don’t. For them, it’s just pattern recognition. They don’t have actual knowledge representation; they don’t have any concepts embedded.\nTo summarise, then: LLMs are expensive. They sometimes produce nonsense outputs. And there’s a risk that you’ll be accused of plagiarism if you use the text that’s produced. So, what should our response be to stories like this recent Nature article? How should we calibrate our excitement for LLMs?\nDN: You have to treat them as a tool, and you have to make sure that you check what they produce. Some people believe if you just make LLMs big enough, we’ll be able to achieve artificial general intelligence. But I don’t believe that, and other people like Geoffrey Hinton and Yann LeCun, they say there’s no way that you get artificial general intelligence through these models, that it’s not going to happen. I’m of the same opinion. These models will be forever limited by the pattern recognition approach that they use.\nBut, still, is this a technology that you have an eye on in your professional capacity? Are you thinking about how these might be useful somewhere down the line?\nDN: Absolutely, but we are mainly interested in smaller, more energy efficient, more computationally efficient models that are built on curated language, that can actually hold a conversation, and where you can represent concepts and topics and context explicitly. At the moment, LLMs can only pick up on context by accident – if it is sufficiently expressed in the language that they process – but they might lose track of it if things go on for too long. Essentially, they have a short-term memory: if you prompt them with some text, and they generate text, this stays in their short term memory. But if you prompt them with a long, convoluted sentence, they might not have the capacity to remember what was said at the beginning of the sentence, and so then they lose track of the context. And this is because they don’t explicitly represent context and concepts.\nThe other thing is, if you use these systems for dialogues, then you have to script the dialogue. They don’t sustain a dialogue by themselves. You create a dialogue tree, and what they do is they parse the text that comes from the user and then generate a response to it. And the response is then guided by the dialogue tree. But this is quite brittle; it can break. If you run out of dialogue tree, you need to pass the conversation over to a person. Systems like Siri and Alexa are like that, right? They break very quickly. So, you want these systems to be able to sustain conversations based on the correct context.\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\n← Back to Editors’ blog\n\n\nThis work is licensed under CC BY-SA 4.0"
  },
  {
    "objectID": "news-and-views/editors-blog/index.html",
    "href": "news-and-views/editors-blog/index.html",
    "title": "Editors’ blog",
    "section": "",
    "text": "US legislators get their data science act together\n\n\n\n\n\n\n\nData science education\n\n\nData literacy\n\n\nPolicy\n\n\n\n\nA bill introduced in the US Congress wants to make funds available to develop data science and data literacy education across the United States. We sit down with education and policy experts to discuss the challenges and opportunities ahead.\n\n\n\n\n\n\nMar 6, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nData science can help close the ‘digital skills’ gap, or so it seems\n\n\n\n\n\n\n\nSkills\n\n\nTraining\n\n\nAI\n\n\nMachine learning\n\n\nData analytics\n\n\n\n\nA ‘digital skills’ gap is harming employer productivity and growth, according to a survey by engineering body IET. But the ‘digital skills’ that are needed sound a lot like data science skills: statistical understanding, data analytics, AI and machine learning.\n\n\n\n\n\n\nFeb 14, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification\n\n\n\n\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\n\nChatGPT represents a next step in the evolution of large language models, says Detlef Nauck. However, there are still major challenges - and concerns - to overcome.\n\n\n\n\n\n\nJan 27, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nWe’re taking Real World Data Science on the road\n\n\n\n\n\n\n\nUpdates\n\n\nEvents\n\n\n\n\nJoin us at the RSS International Conference 2023 in Harrogate, 4-7 September.\n\n\n\n\n\n\nJan 18, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nExplore the RSS Data Science & AI Section newsletter, right here!\n\n\n\n\n\n\n\nUpdates\n\n\nNewsletters\n\n\n\n\nWe’re starting the year with a new addition to the site: a page dedicated to the excellent RSS Data Science & AI Section newsletter.\n\n\n\n\n\n\nJan 5, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nSink your teeth into some data science papers with our brand new blog\n\n\n\n\n\n\n\nUpdates\n\n\nContent ideas\n\n\nCall for contributions\n\n\n\n\nToday we’re launching DataScienceBites – a new member of the ScienceBites family – offering bite-sized summaries of data science papers.\n\n\n\n\n\n\nDec 13, 2022\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nLLMs in the news: hype, tripe, and everything in between\n\n\n\n\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\n\nWe’re back discussing large language models after two weeks of ‘breakthrough’ announcements, excitable headlines, and some all-too-familiar ethical concerns.\n\n\n\n\n\n\nDec 9, 2022\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nA chat with ChatGPT\n\n\n\n\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\n\n‘Hello there! I’m a large language model trained by OpenAI, so I don’t have the ability to experience emotions or have a physical presence. I’m here to provide information and answer questions to the best of my ability. Is there something specific you would like to know?’\n\n\n\n\n\n\nDec 9, 2022\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nFour themes for potential contributors to think about\n\n\n\n\n\n\n\nUpdates\n\n\nKey themes\n\n\nContent ideas\n\n\n\n\nCan data science save the world? What is a data scientist? What statistical ideas do data scientists need to know? And, what’s happening in the world of data science?\n\n\n\n\n\n\nDec 1, 2022\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nWhy large language models should come with a content warning\n\n\n\n\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\n\nThe outputs of LLMs seem impressive, but users need to be wary of possible bias, plagiarism and model ‘hallucinations’.\n\n\n\n\n\n\nNov 23, 2022\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nMeet the team\n\n\n\n\n\n\n\nPeople\n\n\nBiographies\n\n\n\n\nIntroducing the editors of Real World Data Science.\n\n\n\n\n\n\nOct 18, 2022\n\n\nEditorial Board\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "news-and-views/interviews/posts/01/25/erica-thompson.html",
    "href": "news-and-views/interviews/posts/01/25/erica-thompson.html",
    "title": "How to ‘Escape from Model Land’: an interview with Erica Thompson",
    "section": "",
    "text": "Erica Thompson’s new book, Escape from Model Land, offers a fascinating and important perspective on mathematical models as being not just models of the real world, or real processes or systems, but also “subjective versions of reality” that encode all sorts of assumptions and value judgements.\nIn this interview with Brian Tarran, editor of Real World Data Science, Thompson talks about the “social element of modelling” and how it manifests, how to counter the subjectivity of individual models with a diversity of models, and whether human-made models are held to the same standards of transparency that are expected of AI-“created” models.\nErica Thompson is a senior policy fellow in the ethics of modelling and simulation at the London School of Economics Data Science Institute."
  },
  {
    "objectID": "news-and-views/interviews/posts/01/25/erica-thompson.html#timestamps",
    "href": "news-and-views/interviews/posts/01/25/erica-thompson.html#timestamps",
    "title": "How to ‘Escape from Model Land’: an interview with Erica Thompson",
    "section": "Timestamps",
    "text": "Timestamps\n\nWhat led Erica to write the book, and why now? (2:27)\nCritiquing climate models (7:30)\nExploring the “social element” of modelling (11:36)\nCountering subjectivity with a diversity of perspectives (20:11)\nAI models, human-made models, and questions of transparency (25:54)\nWhy write a popular science book about these issues? (30:11)\nWill the UK Prime Minister’s “maths to 18” proposal help or hinder our Escape from Model Land? (34:01)"
  },
  {
    "objectID": "news-and-views/interviews/posts/01/25/erica-thompson.html#quotes",
    "href": "news-and-views/interviews/posts/01/25/erica-thompson.html#quotes",
    "title": "How to ‘Escape from Model Land’: an interview with Erica Thompson",
    "section": "Quotes",
    "text": "Quotes\n“Putting things in a mathematical language does tend to make people think that it is truth from on high. And so my book, in some way, goes towards saying actually, these models, obviously we hope that they’re based on facts and they’re based on data that we gather, but they also do have this value judgement content as well” (1:51)\n“It is arbitrary how we choose to model a situation. There are infinitely many different ways that you could choose to simplify reality - this huge, messy, complex thing in front of us, with physical laws that we don’t fully understand and things going on that we can only measure by proxy.” (12:09)\n“The choice of assumption has a very direct result in the model output and in the information and advice that you’re giving to policymakers… [In climate models] maybe we have a cost of however many dollars per tonne of carbon dioxide for nuclear electricity or for renewables. But what kind of price would you put on behaviour change? How many dollars per tonne of CO2 avoided does it cost to change the behaviour of a population such that they use less energy? If you put it in at $2 per tonne of CO2, it would be heavily relied on [as a policy response]; if you put it in at $2,000 per tonne of CO2, it’ll never happen.” (16:42)\n“There needs to be more frank discussion of values and value judgments, and politics and social assumptions within models. And I think we are starting to see that with the pandemic models, particularly because it’s been so high profile. [But] it’s really hard to unpick your own value judgments. It’s easier for somebody with a different perspective to come in and say, ‘Oh, actually, you know, you’ve assumed that. Why did you assume that?’ When we are embedded in a particular culture of modelling, it’s particularly hard to imagine that anything could possibly be done differently.” (27:20)\n“I think some people maybe read the book and think, ‘Oh, this is just a sort of woke advertisement for diversity’. Well, it’s not; it’s a way of doing the maths better. The whole point is to do the maths better, make better forecasts, understand the future more effectively, and be able to make better decisions based on that information.” (33:41)"
  },
  {
    "objectID": "news-and-views/interviews/posts/01/25/erica-thompson.html#transcript",
    "href": "news-and-views/interviews/posts/01/25/erica-thompson.html#transcript",
    "title": "How to ‘Escape from Model Land’: an interview with Erica Thompson",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove repetitions.\n\n\n\nBrian Tarran\nHello, and welcome to the very first instalment of the Real World Data Science interview series. I’m Brian Tarran, the editor of Real World Data Science, and I’m very pleased to be joined today by Erica Thompson, a senior policy fellow in ethics of modelling and simulation at the London School of Economics Data Science Institute, and the author of a fantastic new book - which I have a copy of here - Escape from Model Land, which is subtitled, How mathematical models can lead us astray and what we can do about it. So hello, Erica, thank you for joining us. I hope 2023 got off to a positive start for you.\nErica Thompson\nYes, it has so far.\nBrian Tarran\nGood, good. Because the book came out, is it just before Christmas or just after?\nErica Thompson\nYeah, just before Christmas. So I’ve had all sorts of things flooding in saying, Oh, I liked your book, or I hated this bit or no, it’s exciting.\nBrian Tarran\nYeah, no, well, it’s, I have to say, I think, I thought it was a genuine– I finished reading it over, over Christmas. And I think it offers a genuinely fascinating and important perspective on mathematical models as being not just I guess, models of the real world, or, you know, real processes or systems, but subjective versions of reality, you know, encoding all sorts of assumptions and value judgments of the people who, who create the models. And I mean, I guess that shouldn’t really come as a surprise, right? But, but is it a point that is often lost in the discussion around models, particularly where decisions might be, like, informed or driven by model outputs?\nErica Thompson\nI think it is something that’s easy to miss. I mean, especially because we’re sort of, maybe as mathematicians were used to living in model land and doing things which, which we see as being logical consequences of previous things. And then more generally, the public look to science and mathematics and statistics as being objective arbiters, perhaps, of how things are and how things ought to be. And so, so yes, that that kind of putting things in a mathematical language does tend to make people think that it is truth from on high. And so my book, in some way goes towards saying actually, these models, they are, obviously we hope that they’re based on facts, and they’re based on data that we gather, but they also do have this value, judgement content, as well. And so we need to think about what that is and how we deal with it, and how we sort of express it and how we understand it.\nBrian Tarran\nRight, yeah.\nErica Thompson\nEspecially where we’re using those models to inform decision making or public policy, then it becomes particularly important.\nBrian Tarran\nYeah, yeah, no, and obviously, your book draws up quite a bit on the Covid-19 pandemic, and how models were used there. But I thought it was interesting, actually, that, you know, in reading the acknowledgments that you– that while the pandemic lent the topic, additional relevance, right, you actually started writing the book before that. So what led you to think now’s the time? What was the tipping point, if you like, of thinking, I want to write this book now?\nErica Thompson\nYeah, okay. Well, that’s an interesting question. I mean, because it builds on the last sort of 10 or 15 years of my work. So I started out doing a PhD in climate physics. And my background before that was maths and physics. And so I was doing a PhD on the physics of North Atlantic storms, looking at how they would change given climate change. And so, obviously, the first thing you do is a literature review. And I started looking at different models and what the what they were saying about what would happen to North Atlantic storms. And what I found there was that there were models saying that the storm tracks would go north, they’d go south, they get stronger, they get weaker, they’d, you know, anything you name it. And interesting, particularly, interestingly, was that they, they had relatively small uncertainty ranges. So they they didn’t agree within their own uncertainty ranges. And that made me think, well, we this isn’t telling me very much about North Atlantic storms. But it’s telling me a great deal about modelling and the way that we do modelling and perhaps we need to start thinking more about how these uncertainty ranges are calculated, what does it mean? What, how can we end up in a situation where we have this level of disagreement between models. And so since then, I’ve been looking at, you know, those kinds of concepts in different areas I’ve been looking at sort of insurance and finance and weather and climate and humanitarian forecasting as well. And, and so in all of those application areas, I found the same questions about uncertainty and how we make inferences from model output to be particularly interesting and how common problems may be solved in different ways as well. So it’s interesting to do the compare and contrast. And so, yeah, then I guess I, I’d been on all these sorts of bitty little projects and thought actually, I’d really like to bring this together into something more coherent, you know, to actually say, look, there’s a, there is a common theme here and we need to be putting it together and drawing conclusions. And we can, we can learn a lot from doing that. And we can share the best practice throughout the sector.\nBrian Tarran\nWhen you’re starting down this path of, I guess, looking into the, I guess, the ethics and process of modelling, did it, was there a lot of other work that you identify that you could kind of draw on a lot of other thinking around this area? Or was it kind of under studied, under researched sort of aspect of the literature?\nErica Thompson\nI think it’s under studied, I mean, of course, everybody who does some modelling, you know, you, you do your modelling, and then somebody says, Okay, we need to put some error bars on the outputs, and you go back and, and think about how we’re going to put the error bars on the outputs. And probably, I would say, most people doing that realise that it’s much more difficult than they have the time to do or the ability within the scope of whatever project they’re doing. But the aerobars, the uncertainties always ended up being tacked on at the end, you do it after you’ve done all the modelling, there’s less incentive to do it. And there’s less resource to do it than to make the model itself better. And I think that’s a very common story, that people realise that they ought to be doing more, but they just don’t have the time the resource, the ability to go and do that. So then yes, there are there are people, and there are particular areas that I think have taken more time to investigate this. So in physical science hydrology, I’d say in particular, has a very well developed history of thinking about the uncertainties in models, maybe because, you know, they are constantly being challenged by events happening, which were not within the models, you know, you’ve got your flood forecast model, and then something happens, and it goes way beyond what you were expecting. And you you have to go back and say, What does this mean for our modelling process. And other areas have much less well developed considerations of uncertainty. And so that’s where I think actually, we could we could really benefit by sharing good practice across these different application areas, because people have looked in different ways. And, you know, with with different levels of statistical interest, you know, some areas go into the stats, much more, some areas are very philosophical about sort of the conceptual foundations of how we should think about models and how we should think about the range of outputs that we get from models. And so what I’m trying to do is bring those together a bit. Yeah.\nBrian Tarran\nYeah, I think you certainly achieve that. It is really interesting, the different, the variety of examples that you present, and the ways you talk around these issues. I did want to focus in particular on climate models, though, because, you know, I was looking around your website, finding a bit more about about you, and I noticed on there that you talk about, you no longer fly to conferences, and that’s in order to kind of reduce your own ecological footprint. So I guess I wanted to ask, you know, when you set about writing a book, and it’s going to be a book that’s kind of critiquing models and the ways that they don’t often agree? Did you have like a nagging concern that, you know, the points you wanted to make about models in general, but climate models in particular, that that would kind of lend fodder to the kind of groups that might want to discredit climate models or downplay the risks? Or, indeed, the reality of climate change?\nErica Thompson\nI mean, yes, I did have that worry, I still have that worry. And I, but I hope that my book is clear throughout that, you know, that models are not irrelevant, you know, the answer is not to throw them away. If you come to it from this sort of sceptical position, saying, you know, we need to think more carefully about how we make inferences from models, you could go all the way down the rabbit hole and say, Oh, they’re all terrible, let’s just throw them away. But I think that would be completely unjustified. We have good evidence, sort of from from one end of the spectrum of relatively simple linear models, which are incredibly, wildly successful and form the foundation of modern life and modern technology. And, you know, with that, as a basis, we hope that we can, you know, work from there to find the limit of the knowledge that we can get from these more complex models, which are looking at making predictive statements in more extrapolatory domains where the underlying conditions are changing, and we therefore have less ability to rely on what I call the quantitative route of escape from model land, by challenging with relevant past data. You know, we’re looking at extrapolatory conditions like climate change, or social and economic systems, and therefore, we think that the data that we have, while they may be useful and indicative, are not, we can’t just calibrate with respect to past data and expect that to be enough to warrant performance in the future. And so, so I think that sort of one answer is that we shouldn’t be throwing away the models completely because they are demonstrable useful, and the question is to quantify the limits of what we can say, rather than just get rid of them. And then maybe the slightly more nuanced answer is that actually, if we have less confidence in the models, and our uncertainty ranges are wider, then because in many of these application areas and climate change, in particular, the damage function is convex, you know, we are expecting that as we go further from today’s climate, the consequence will be not just linearly worse, but sort of increasingly worse. And therefore, if you have, if you’re considering, you know, just to have a sketch of a kind of cost benefit analysis on some sort of expected utility from taking action to mitigate carbon emissions, for example, if you have more uncertainty, then your range is wider. And so the, the lack of quantification of the top tail becomes dominant in in the expected utility of the outcome. And therefore, you should be choosing to mitigate more, not less, because of that uncertainty. So, you know, the sceptics, I suppose the climate sceptics would say, oh, there’s a possibility that climate change might not be as bad as we expect, and therefore we shouldn’t bother doing anything. But I would say actually, that argument should be turned on its head, if we have greater uncertainty, that should be a bigger motivating factor to reduce carbon emissions rather than the opposite.\nBrian Tarran\nYeah, and I think you make that point quite clearly in the book. And the other point you make is that, I guess, building trust in models is about understanding their limitations. And the quote that I thought was really interesting was about acknowledging the social element of modelling. And I wonder, do you mind explaining what that social element is for people who are watching or listening? And how will that kind of manifests itself in models? Maybe you’ve got like a simple example that you might want to talk to? I don’t know.\nErica Thompson\nYeah, okay. So, I mean, the social element of models is, because it is arbitrary how we choose to model a situation, there are infinitely many different ways that you could choose to simplify reality, this, you know, huge, messy, complex thing in front of us with physical laws that we don’t fully understand and things going on that we can only measure, sort of by proxy, with, you know, with models themselves to make many of our measurements of the system. And so, so you might choose to simplify a system in one way to model it. And I might choose to simplify it in a different way. And you might choose one programming language, and I might choose another and they would implement functions in different ways. And so all of our choices change the way that the model will then look at the end of the day. Now, then you say, okay, but supposing I’m modelling you know, what will happen to a ball when I throw it up in the air? Surely, that’s not a, you know, that has no social content, does it? And I’d say basically, no, it doesn’t really have any social content. It has some social content insofar as you’re deciding that this is what we want to make a model of. But ultimately, you and I would probably come up with very similar models, regardless of our background or our perspective, or our interests, or even our education to a large extent. And so, so those relatively simple linear situations, which I refer to as interpolatory models don’t have very much social content. Now, the ones that I’m particularly interested in and that I talk about in the book are things that are extrapolatory, where we’re interested in situations where we are trying to predict into the future a system where we expect the underlying conditions to be non-stationary, to be changing. So climate change is one example. Social and economic systems would be another example. And when we’re modelling systems like that, we have to be much more careful because we could choose to model them in radically different ways. We could, if you want to model an economic system, you might choose to disaggregate with respect to the social class of different households. And I might choose to make a sort of bulk model of the whole system with a representative household. And you could imagine hundreds of different ways to do these sorts of things. So maybe you think about pandemic models and how you could simplify it into individuals or you could make an agent-based model with, you know, actual agents walking around and infecting each other. Or you could just write some differential equations for how the transfer happens. So you could do it in, again, in many different ways. And the choice of simplification is then much more important, and it will have much larger first order effects on the outputs, and then on the framing of the question, you know. So you decide to model it in a certain way, with a certain kind of mathematics, and that changes the way that you might think about intervening in the system. If you’re presenting your model to a policymaker with the intent of informing them about their policy options, you might, if you have a model which can represent the effects of say, closing schools, or universities on pandemic transmission, then then that becomes a policy option. If you have a model which can’t represent those kinds of interventions, then it’s not a policy option. And similarly, with climate change, one of the examples that I talk about in the book is integrated assessment models of energy and climate. And so these are models which consider the energy system out to say 2100, and they put a price on nuclear electricity and renewables and all the other things that go into the energy system and basically say, how can we achieve our carbon targets at the lowest cost? Now, if you put in, if you choose to put in a certain price for a certain technology or assumptions about how that technology will develop in future, then you get a particular answer. And you put emphasis on certain kinds of policy options. And so that has, the choice of assumption has a very direct result in the model output and in the information and advice that you’re giving to policymakers. And of course, you might choose to put in something like behaviour change. So maybe we have a cost of however many dollars per tonne of carbon dioxide, for nuclear electricity or for renewables. But how much, what kind of price would you put on behaviour change? How many dollars per tonne of CO2 avoided does it cost to change the behaviour of a population such that they use less energy? Well, that’s not really in the models. And if it was, it would, again, it would be first order because that would be you know, if you put it in at $2 per tonne of CO2, it would be heavily relied on, if you put it in at $2,000 per tonne of CO2, it’ll never happen. And where you choose to put that in between influences how it looks, and then that influences the pathway that’s projected, and it influences the advice that you give to policymakers.\nBrian Tarran\nYeah. You mentioned the example of school closures and stuff when you’re talking about Covid-19. I actually thought that, that one really helped me understand, I guess, and made it clear to me was that, there’s often been that argument about whether the lockdown was the right thing to do given the other impacts, but you actually say that if different types of people were doing the modelling, maybe if it was school aged people, and I guess encoding the impact that that would have had on them, and how much they value say not being able to go out and see their friends, the impacts potentially on mental health and things like that, it does change, I guess, the calculation of what the right intervention is or the right response is.\nErica Thompson\nYeah, exactly. And I think I think we haven’t anywhere near bottomed out all of these impacts of the pandemic, you know, both the health impacts, and also, mental health and economic impacts will be rippling on for a very long time to come. So we, you know, we can’t even now retrospectively look back and say what was the right decision? It’s really not clear, depends how you value the different outputs, the different outcomes of a decision. And yes, we didn’t have economic models of what the impact of lockdown would be. And if we, if those had been available and developed the same kind of mathematical complexity and credibility as the models of infection and transmission we had in those early stages of the pandemic, which had essentially morbidity, mortality, and the, you know, the impact on the NHS, you know, number of hospital beds occupied. Those were the bottom lines, and there was nothing else. And so that was given as an input to the policymakers. Now, that’s, of course not everything that the policymakers rely on, they have to, their role is to weigh up everything else as well. But if we’d had models which contained more information about all of those other impacts, I think it’s quite plausible that we would have seen different kinds of decision making. And then there’s also the communication aspect, that these models were used to communicate and justify and persuade the public of the importance of the actions that were taken. And, you know, I think it’d be hard to disagree that the actions that were taken immediately where necessary, we certainly did need some kind of lockdown straight away. But then the question of exactly what you do thereafter is much more difficult.\nBrian Tarran\nYeah, yeah. So in the book, you kind of make the point that it’s somewhat of a fool’s errand to try and make models objective, and they can’t ever really achieve this, you know, principle of scientific objectivity. But that we instead should look to counter subjectivity of individual models with a diversity of models that do encode these different perspectives, like we’ve just been talking about. I wonder, you know, how would you see this working in practice? Or how would you like to see this working in practice?\nErica Thompson\nI mean, that’s a difficult question. So it’s, it’s nice to think that, you know, we have these models, and they are unavoidably subjective. Essentially, my view is that the model encapsulates the expert opinion of a particular expert, and it comes laden with their own perspectives, and biases and preconceptions, as well as their expertise and their education and their experience of a subject, you know, which shouldn’t be set aside. So if we are trying to understand a situation, then we want to get as many perspectives as possible. And so in theory, incorporating the widest possible diversity of different backgrounds into modelling and making a multiplicity of different models and trying to see the problem from these different perspectives will help us to understand it better. Now, then the statistician will jump in and say, Aha, can we, you know, can we in some way average those models or use some sort of statistical inference to take those models and put them together and come up with an even better answer? And I would, I would sort of counter that by saying that there’s no reason to believe that our, that a set of models generated as essentially just one set of opinions will be an independent and identically distributed sample from some distribution, underneath which will be an estimator of, of the truth, if that even exists. And so many of the formal statistical methods that we would quite like to apply to an ensemble of models, a large group of models put together aren’t really conceptually valid at all. I mean, that doesn’t stop people doing it. And maybe you get some interesting information from it, but you certainly can’t rely on it as an estimator. So there’s a sort of statistical problem there. And then the other question is your reference class. So, to what extent do you believe that these models are all equally valid or equally plausible? So that then brings the social question back to the forefront. Because then you say, you know, if I believe that, you know, somebody from Imperial College, say, who is the head of an institute for epidemiology, and has many, many years of experience making this kind of model, you know, is an expert and is qualified to create a model and for that to be recognised as a valid expert opinion, who else has got the credibility to do that? How do we define that? You know, what do you call a plausible model? So then it’s a question of the sort of scientific gatekeeping. What kind of qualifications do you expect from somebody or from an institution? What kind of expertise counts as being relevant and valid expertise? Does it have to be mathematical expertise? Can it be lived experience? Can it be, does the model have to be a mathematical model? What kinds of mathematics are appropriate for the situation? If we disagree about assumptions, does that mean that we can’t consider the the two sets of models in the same sort of class of plausible models? Or are we going to start pruning it by saying, I believe your assumptions, and I don’t believe your assumptions? And if so, who gets to do that? Who gets to decide what is plausible and what is not plausible? And what is allowed to enter into this set? Because as soon as we start pruning it, then we make the statistical inference more difficult. You know, if you want to say, if you want to start applying your methods that assume that the models are on, you know, that the models are independent, then you can’t start pruning because then that introduces huge dependencies on your own expert judgement. So it just becomes extremely difficult. And this is where all of then the social questions about expertise and credibility and sort of scientific gatekeeping and how we assign that credibility and trust, trust in science, you know, who has trust in which kinds of models? This is something, this is a theme that we see coming out of climate science and, you know, hopefully less now than maybe 10 years ago. But in the pandemic, of course, we’ve seen it coming right up again with questions about lockdowns, and about vaccination strategies, and all of that sort of thing. Trust in science is really important. And maybe one of my themes is that trust in science actually is first order in the modelling process itself. It’s not something that is sort of added on afterwards, I’m going to go away and make my model and then the question is whether or not you trust it. Actually, trust and expertise and credibility are in the modelling process directly.\nBrian Tarran\nDo you mind if we segue to talking about artificial intelligence models, or models made by artificial intelligence? Because that’s, I think that touches on a lot of the same issues, right? And I wanted to think about, well, first of all, you say that, obviously, artificial intelligence models made by AI, they’re not objective, even though there’s like, they’re kind of building the models, if you like, those AIs have still been trained by people, been coded by individuals and those personal judgments and assumptions and all that get embedded into the artificial intelligence. But I think, I guess my question for you is, we’re starting, I think, to have a very frank and public debate about AI ethics, and to demand transparency and explainability of things like automated decision making systems. But do you think we’re kind of, are we falling short of holding ourselves as people to the same standards of transparency and being clear about the choices and decisions we make? And also documenting that subjectivity when we’re preparing these sorts of models and these sorts of decision making systems for policymakers to use?\nErica Thompson\nYeah, so I mean, I suppose there are two questions there. And one’s about what we do and one’s about the AI. So for the humans, maybe, yes, I think that there needs to be more frank discussion of values and value judgments and politics and social assumptions within models. And I think we are starting to see that with the pandemic models, particularly because it’s been so high profile. I mean, remembering that actually, it’s really hard to unpick your own value judgments. And it’s easier for somebody with a different perspective to come in and say, Oh, actually, you know, you’ve assumed that, why did you assume that? And, you know, when we are embedded in a particular culture of modelling, it’s particularly hard to imagine that anything could possibly be done differently. And so I think that’s, again, where diversity is really important, because introducing those perspectives will help to challenge dominant strains of thinking which can end up in sort of accidentally, and not deliberately at all, in a form of groupthink. So that’s the humans and then the, with the AI, yes, you know, they inherit their value judgments from their creators. And so, for example, on the statistical side, one might think about the kinds of loss functions that are used to calibrate machine learning programmes, you know, how does the machine decide what is better and what is worse? You know, it is learning to model a situation, but there will be some kind of loss function in there, which it is minimising in order to decide what is the best model. And so, being explicit about that loss function, I think, actually is really interesting, you know, the fact that the model has got written down an explicit loss function which it is minimising means that we can then analyse that and think about what are the value judgments inherent in that choice of loss function, which is something that we don’t have when humans are calibrating a model and they’re twiddling a knob here and a knob there and saying, Oh, does it look realistic? Am I getting the right kind of behaviours, you know, does it match up with the map I have from observations or whatever. And so, having that I think we can then say, what are the implications of writing a loss function in this way? And what are the values that are implied? I mean, even just modelling itself, you know, like choosing to solve a problem with recourse to mathematical modelling is a value judgement and implies a certain kind of solution, doesn’t it? So if we say that we even can come to a decision, that the models input will be relevant and interesting and help us, that is a value judgement.\nBrian Tarran\nYeah, and perhaps we could return to that point a bit later, because there was a line again that jumped out in the book about decision makers needing to maybe curb some over enthusiasm for mathematical solutions. You do talk about documenting value judgments as being kind of one of five principles that you set out for mathematical modellers to adopt to support responsible modelling. I think it’s fascinating to me that you’ve used the vehicle of a popular science textbook to speak to this community, and to sort of set out these principles rather than, say, a journal paper or conference presentation. So I wanted to ask, is there a particular strategy to that decision? I mean, I have my own theory on that, but maybe I’ll let you go first.\nErica Thompson\nI’d love to hear your theory. I mean, I guess partly because I suppose I’m very interdisciplinary. And I’m, I’m hopping between these different areas, as we were discussing before, so I have sort of a climate science community and statistics and data science and other application areas in humanitarian forecasting, sort of hydrology, geophysics, weather and climate and all the rest of it. And actually, I find it really hard to get these thoughts published in journal form, because I suppose partly because it feels too general for any specific journal. And perhaps it feels too simplistic that, that the reviews I get tend to be either Oh, we’ve heard this all before it’s not new, or this is too radical and this isn’t an appropriate journal for it, you know, that sort of thing. And actually, I kind of got to the point of thinking, Well, you know, do people even read these journals anyway? Actually, maybe really what I need to be doing is trying to provoke a wider discussion about models. And I do tend to get a, you know, a really good response, when I speak at conferences or talk to people about these things. People go yes, yes, you know, this is really important. Actually, this is something I’ve really struggled with, we don’t know how to do it, the uncertainty is always just an add on at the end that doesn’t have enough time allocated for it. But I don’t have the resource to do it, I’m not able to grapple with these questions, because they’re so fundamental and so wide ranging, and it would be really helpful to have more of a sort of walkthrough of how people tackle these questions in different fields. And so, so I’ve been trying to do that. And I felt that the book was a good way to sort of spark the conversation and maybe also get it to some different audiences. So I’ve had people contacting me since the book came out saying, oh, you know, I’m working in, like, asset valuation for disputes between states, really random things, quite different. And they say, actually, your book really struck a chord, and we have difficulties with this in this particular area. And so I’m really hoping that, you know, the book will help me then to find, to bring together people working on these sorts of issues with common themes from really different application areas and try to make some headway on how we can actually go about practically changing modelling practice to make it to make it work better, and assess uncertainty better. So it’s not just– I think some people maybe read the book and think, Oh, this is just a sort of woke advertisement for diversity. Well, it’s not; it’s a way of doing the maths better. The whole point is to do the maths better, make better forecasts, understand the future more effectively, and be able to make better decisions based on that information.\nBrian Tarran\nYep, well, that’s not too far away from my theory on why you did it. I thought it was that it’s a great way of getting– if you can get policymakers and the public to read this, right, you can get them to hold modellers to these principles, rather than having it just be something that you kind of talk about within the community and it doesn’t really go outside that, right? It’s a way of people, you know, the next pandemic or whatever it might be, the next time a model is the focus of a debate, the public are equipped to ask the right sort of questions about the process. Okay, I’ve got one more question to you because we’re running out of time. And it’s back to that over enthusiasm for mathematical solutions point. I thought was somewhat serendipitous to read about, read that quote, in the same week that the UK Prime Minister Rishi Sunak announced a plan for all pupils in England to study maths to the age of 18. So, I wanted to ask you what you make of that plan, first of all, but also, I think, more importantly, does a more mathematically minded populace, are they better equipped to understand the mathematical descriptions of the world and that they are incomplete descriptions? Or is there kind of maybe some other curriculum that we need to tack on to this maths to 18, in order that people are able to better differentiate between model land and the real world?\nErica Thompson\nYeah, so I mean, I’m not a fan of– I mean, I like the idea of people studying maths to 18. I think more maths is a good thing. I loved maths, I still love maths, I think if more people were more generally numerate then society would be better and life would be better. But if you haven’t enthused people about the value and the interest of maths by 16, forcing them to study it for another two years is absolutely not the answer. It will just put people off staying in school past 16. So I, you know, I think that we need better teaching of maths before 16, rather than forcing people to study maths post 16. And part of that is helping people to understand how mathematics is relevant to the real world that they live in and teaching them the kind of things that they will use in their adult life. You know, people, most people don’t use Pythagoras theorem, but most people do need to fill in a tax return, you know, these sorts of things. Understanding orders of magnitude, and the difference between millions and billions, would be incredibly helpful, wouldn’t it? So, yeah, I think there are more basic questions there that need to be answered before we go into the details of sort of complex maths. And then, so what was the next question?\nBrian Tarran\nIt was about whether whether you think, the more mathematically equipped we are, does that make us better able to understand the limitations of models? Or do we need something else to kind of train us or encourage us to think about these two separate realities, model land and the real world? And I say realities in inverted commas.\nErica Thompson\nYeah, I mean, I think the general public has a good understanding that the model land and the real world are not the same. There is actually a healthy scepticism of models out there. And I think that’s probably a good thing. I give a couple of funny anecdotes in the book about that. So I mean, one was a, I think, a YouGov poll about people going to the moon and saying, you know, would you go to the moon if you could be guaranteed a safe return, blah, blah, blah, and like, a large percentage of those said, Well, no, I just don’t think you could give me a safe return, they reject the model land. And then there was another example about intelligence analysts being asked to sort of calibrate a probability language scale. So they say, like, likely, however many percent and very likely however many percent and unlikely however many percent. And so the study was looking at different ways of doing that. And one way was to accompany the word likely, or unlikely, or whatever, with a written number of what the probability it referred to was. So it would say, like, likely, I can’t remember the number, but sat it was, like 50 to 70%, written down in the question, and then the question was, what is the probability of an event, which is deemed to be likely brackets 50 to 70%? And what people write down was not 50 to 70%. You know, as a mathematician, that’s completely ridiculous. Because the answer was in the question, why wouldn’t you write that down? But of course, what you’re seeing there is the rejection of model land. Somebody has assessed it as 50-70%. The question is, do you believe it? Well, no, actually, you might write something like 40 to 80%, because you expect there to be, you know, the model to be generically overconfident. And so this is sort of what I mean, by curbing over enthusiasm for mathematical solutions is that, you know, we have to understand that the mathematical solutions are living in model land, and that we can, in order to get out of model land, we have to say, do we actually expect this result to refer to the real world? Or is it only saying what the next model run is going to tell us? And so the act of doing that is difficult, and it’s more difficult for mathematicians than for the general public because as mathematicians, we sort of are used to living within model land and noticing when the answer is in the question and then writing it down. And we’re not very good at saying, Well, what’s my subjective estimate of the probability of this model being inadequate in some way? That’s not something that you can necessarily do with respect to data and so it’s a tricky one. So in terms of the over enthusiasm, you know, it’s curbing over enthusiasm, not curbing enthusiasm, because as I said at the beginning, and I returned to a lot in the book, actually, mathematical models are incredibly valuable. And they contain a huge amount of information and insight that we’re, we would be fools to throw away. But we need to understand it, you know, in a more nuanced way and be clear about what it’s telling us and what it’s not telling us. And that answers in model land aren’t necessarily the answers that we need in reality, though they may be informative about them.\nErica Thompson\nWell, Erica, thank you very much for your time today, for talking through the book, which is out now. Do you have some some links or information about where people can find out more about the book?\nErica Thompson\nYep, look on my on my website, ericathompson.co.uk. And it’s available through all the usual booksellers.\nBrian Tarran\nExcellent, excellent. Well, I wish you the best of luck with the book. As I say, I think it’s fantastic. And well, I hope we get to talk again, maybe a bit further down the road and see whether some of these principles and this ethical framework that you talk about for mathematical modelling, whether that kind of comes to fruition because I think we need to watch that closely. So, Erica, thank you.\nErica Thompson\nThank you very much. Thanks for having me.\n\n← Back to Interviews\n\n\nThis work is licensed under CC BY 4.0"
  },
  {
    "objectID": "news-and-views/interviews/posts/02/03/heidi-seibold.html",
    "href": "news-and-views/interviews/posts/02/03/heidi-seibold.html",
    "title": "Why open science is ‘just good science in a digital era’",
    "section": "",
    "text": "Years are often dedicated to different causes and aims by different organisations. The United Nations, for example, has designated 2023 the International Year of Dialogue as a Guarantee of Peace, while for the European Commission it is the European Year of Skills. But over at the White House Office of Science and Technology Policy, 2023 has been declared the Year of Open Science.\nTo discuss what this means for science generally and data science in particular, Real World Data Science invited Heidi Seibold for an interview. Seibold is a statistician and data scientist, and also an open science trainer and consultant, and we talked about how she became involved in open science, what it means to her, the benefits of it, and how academic and industry researchers can move towards it.\nCheck out our full conversation below or on YouTube."
  },
  {
    "objectID": "news-and-views/interviews/posts/02/03/heidi-seibold.html#timestamps",
    "href": "news-and-views/interviews/posts/02/03/heidi-seibold.html#timestamps",
    "title": "Why open science is ‘just good science in a digital era’",
    "section": "Timestamps",
    "text": "Timestamps\n\nHow did Heidi become interested in open science? (1:46)\nWhat does open data science mean to Heidi? (7:50)\nWorking with PhD students on open science (12:00)\nHow do open data science principles fit into an industry environment? (14:10)\nKnowledge transfer and public science (17:29)\nYear of Open Science initiatives and lasting impacts (21:08)"
  },
  {
    "objectID": "news-and-views/interviews/posts/02/03/heidi-seibold.html#quotes",
    "href": "news-and-views/interviews/posts/02/03/heidi-seibold.html#quotes",
    "title": "Why open science is ‘just good science in a digital era’",
    "section": "Quotes",
    "text": "Quotes\n“Reproducibility [is] just like this minimum standard in research quality, where we say, ‘When we have the same data and the same analysis, we also want to see the same results’, and being able to check that from others is really, really important.” (2:45)\n“On the back of my wall here in my office I have written, ‘Open science is just good science in a digital era’… Before, we only had the printing press, and we had to print journals in order to distribute the knowledge that we have.” (5:23)\n“For me, open data science entails the part of the scientific process that focuses on everything that happens on the computer: the data processing and the data analysis, and then getting from the data analysis – getting the results and the knowledge, really – in sort of a pipeline where you go from one step to the next. And so the image that I have in my head, when I think about open data science, is of a pipeline.” (10:16)\n“Nobody’s perfect from the beginning. And open science and reproducible research is really hard, and it requires a lot of technical knowledge. And I always feel like people are so scared, because on one hand, they don’t know how to do it yet, and the goal is so far away. And so I always like [to say], you don’t have to be perfect right away; going one step into the right direction is super important.” (12:34)\n“If we think of companies, for example, like Microsoft – they put a lot of money right now into open source: they bought GitHub, they publish open source software, they put money into open source software projects like R, for example. So, somehow, this must be a good way of making profits.” (15:09)\n“[Open science for the public has value because] we don’t know what ideas people will have. There’s so many skilled people out there that probably will do amazing things… We have this with the software Stable Diffusion right now. That’s an AI that generates images from text, and it runs on my computer here. And I don’t need AI skills to be able to do that. And people are building such incredible images out of this, and it’s really fun to see.” (18:50)"
  },
  {
    "objectID": "news-and-views/interviews/posts/02/03/heidi-seibold.html#transcript",
    "href": "news-and-views/interviews/posts/02/03/heidi-seibold.html#transcript",
    "title": "Why open science is ‘just good science in a digital era’",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove repetitions.\n\n\n\nBrian Tarran\nHello, and welcome to another Real World Data Science interview. I’m Brian Tarran. And today I’m joined by Heidi Seibold, a statistician and data scientist. I invited Heidi along to speak about open science, what it means, the benefits of it, and how to move towards it. So welcome, Heidi, how are you?\nHeidi Seibold\nHello, thanks for having me. I’m good.\nBrian Tarran\nExcellent. Excellent. Well, Heidi, I contacted you because, you know, I know you have a real deep interest in open science. And the conversation I think was really motivated by the White House Office of Science and Technology Policy declaring 2023 to be the year of open science. So first off, I wanted to get your reaction to that announcement.\nHeidi Seibold\nYeah, I think in general, that’s a really cool thing for open science to happen. Right? We, there’s this movement that’s been going on for a while, and people have been doing these grassroots communities and growing open science from the bottom up. And now we see more and more also top down decisions, which I think is a very good sign for, yeah, quality of science, really.\nBrian Tarran\nYeah, definitely. So I mean, we’ll get into some of the details of the initiatives a little later, maybe, but I thought that maybe we could kick off by asking you what you thought of the official definition of open science that the US government has come up with, and, and for the benefit of people watching that’s, in quotes, the principle and practice of making research products and processes available to all while respecting diverse cultures, maintaining security and privacy and fostering collaborations, reproducibility and equity. So as definitions go, does that, you know, does that hit the mark, for you, do you think?\nHeidi Seibold\nYeah, I think given that this is a federal definition, right, from the US. And they have to like, yeah, take so many opinions into account, I think it’s a great definition. I also asked like on social media, what people thought about it, and I think the response generally was pretty good. What I liked especially is that they focus on collaboration, reproducibility, and equity, which aligns very much with how I personally see open science. So collaboration means like, I always think of the term like building on the shoulders of giants, right. So this is what we want to do in research, we want to build on the work of others. They might be like famous people, but they might also be our colleagues next door. And it’s so important to take that into focus. And also reproducibility, just like this minimum standard in research quality, where we say, when we have the same data and the same analysis, we also want to see the same results, and being able to check that from others is really, really important. And so I think, this focus, I personally like it a lot.\nBrian Tarran\nExcellent. Excellent. So can you tell me a little bit about your background and how you became interested, and I think committed to open science would be a great way of describing it, right?\nHeidi Seibold\nYeah, so I’m a trained statistician, I studied statistics. And then how did I get into this whole open science thing was, for me, really through reproducible research. So my first research project was during my master’s programme, and we wrote a paper and it was a very computationally complex project, and we had lots of files and folders, and, you know, scripts and stuff like this. And then, at some point, it just all got so messy. And I felt like, oh, no, I’m the worst scientist in the world. And then I told my colleague who I was working with on this project. And he was like, No, this is normal. And I was like, No, this can’t be normal. I want to be on top of things when I do research. And I want to be sure that, like, the code that I’m using for this is the correct code that I actually wanted to use, right. So that must be like a minimum standard that we have. And so through that, I learned about reproducible research and good coding practices. And then I also thought more and more about like, well, if I do all this, I should also publish it so that others can actually check that I’m good doing good work here. Right. And so through that I got more and more into open science and really always felt like that’s the right thing to do, especially if you’re funded through public money.\nBrian Tarran\nYeah, yeah. Can I ask you what you think is kind of driving the shift to open science because the way you’ve described open science to me just there, you know, that sounds like to me like, just good scientific practice, and the fact that it’s not something that’s been done before, I wonder whether is it a cultural change, a philosophical change, a technological development, that’s kind of spurring this shift?\nHeidi Seibold\nYeah. So that’s a really, really good and deep question. So on the back of my wall here in my office, I have written open science is just good science in a digital era. And I think that describes the answer to your question pretty well. So there was a technological change, right? Before, we only had the printing press. And we had to print journals in order to distribute the knowledge that we have. And of course, that costs money. So journals cost money historically. But now, journal costs are really super low, because nobody needs them printed anyhow. And you only ever want to read like one or two articles out of one issue. So it doesn’t really make sense anymore, right. And also publishing data and code. It’s just like, the cost is so so low, that now in this digital age of the internet, really, we have such a low burden of, of doing the right thing. But on the other hand, we need to make this social shift, because we’ve been always, like, researchers have always done it a certain way. And there are especially certain fields that are really into, like, they feel like this is my data. This is my code. This is my research. Why should I share it? But I feel like the young generation of researchers are like, well, because it’s the right thing to do. And the reason why we’re in research is because we want to have scientific progress and scientific progress comes when we can build upon each other’s work.\nBrian Tarran\nOf course, and you mentioned, obviously, journal publications, but I wonder to what extent is it that the scale of science has almost kind of outgrown the ability to kind of condense it all down into a, even if it’s a 20 or 30 page, academic paper, right? Because it’s not just about, you know, setting out the research question describing the methods, you know, presenting a table of the data, all that stuff can’t be published now, or it can be, but it can’t be fit within the framework of an academic paper, it has to be on a GitHub repository or in a Jupyter notebook or something like that. So kind of open science encourages us to, to think about distributing that knowledge in different ways.\nHeidi Seibold\nI think that’s completely true. So, research now is so much more complex than it used to be right. There used to be single researchers who did like, yeah, such breakthroughs within one paper. And now, we already have so much knowledge, and the questions are so much more detailed and complicated. And also the data is so much more and the things that we can do is so much more complex. And so I feel like one paper doesn’t– isn’t enough to describe the research that we’re doing. And we did this one project with, with a group of students, actually, where we took 11 papers that were related to the topic that we were teaching about. And we took these 11 papers and data was available for the papers. And so we thought, well, we try to reproduce the results that they have there. And we found that it is really hard if we don’t have the code, because the method section in papers is really super short, right? It’s a super short section, if it’s not like a statistics paper. And it’s impossible to describe all the intricate steps that you took from the data to the complex model and analysis that you did. It’s just not enough space within the paper. And the code is the perfect description of what you did, right? So why not publish it with it, and especially in cases where data privacy is not an issue, then, and you already publish the data, then it just makes so much sense to also publish the code, because the code is not like there’s no privacy issues to that – usually at least.\nBrian Tarran\nSo is there a kind of you know– when we say open data science, you know, what does that look like? And do you have a kind of example or simplified example of what that would look like, right? And how it’s more than just, you know, the finished paper, the product of the science scientific process?\nHeidi Seibold\nYeah, so for me, open data science entails the process of– the part of the scientific process that focuses on everything that happens on the computer. So the data processing and the data analysis, and then getting from the data analysis, getting the results and the knowledge really, in sort of, yeah, in sort of a pipeline where you go from one step to the next. And so what the image that I have, in my head, when I think about open data science, I really think of a of a pipeline where you stick the different parts of the pipeline together, in a consecutive order. But of course, research projects are really complicated. And the pipeline just looks really messy, in some ways, but if you still manage to organise it in a way that the different bits all fit together, then you can make it so that in the end, you can imagine something that goes from the start to the finish. And you really understand every step of this pipeline every step of the way, from the raw data to the paper.\nBrian Tarran\nYeah. So the idea would be that people provide almost like a framework for you to recreate that yourself if you want to either check the results or yeah, replicate, just replicate the process generally. Right? So is your– so your job now is, I guess, as an open science trainer and consultant, so are you the person that goes into organisations and kind of shows them how to build that pipeline? Or how to, you know, map it out and to present that to people?\nHeidi Seibold\nYeah, so what I do a lot is do workshops and trainings with PhD students. So graduate programmes will ask me to come in for a day, or, also, we do longer trainings, which are often more useful, because people can in between take the steps that I recommend. And then we just, like, work together on ideas and steps that they can actually take. And I think what’s always important there is to know that nobody’s perfect from the beginning. And open science and reproducible research is really hard. And it requires a lot of technical knowledge. And I always feel like people are so scared, because on one hand, they don’t know how to do it yet. And the goal is so far away. And so I always like, you don’t have to be perfect right away, going one step into the right direction is super important. And that also helps with like the social change, because then the question is, well, I do want to do this, but my supervisor doesn’t know the technology, what do I do? And then we always try to find like one step that they can take, rather than trying to be perfect right away.\nBrian Tarran\nAnd it’s, I guess, it’s encouraging to see people like you being brought in to work with people on graduate programmes. So you’re, we’re trying to almost train the next generation of scientists to be thinking, open science first, rather than, you know, falling into bad habits or old habits that, you know, we are trying to do away with.\nHeidi Seibold\nYeah, and really, the young researchers, my feeling so far has been that the only pushback I get for my work is from established researchers who feel like well, this is the way I did it, and so it has to be the right way. But the younger generation for them, it’s super obvious that this is the way we should go in order to achieve scientific progress and good scientific practice.\nBrian Tarran\nYeah, yeah. A lot of data science now is obviously now done within industry. I wondered how open data science or open science principles fit in an environment you know, where competitive advantages is linked to keeping things in house and confidential and not wanting to share too much. Do you see a conflict there or can the two work together?\nHeidi Seibold\nSo I think there’s– first of all, I think, if we, if we get it done in academia, I’d be already super happy, right? If we get it done in the space where we have public funded projects that then are available for the public, I’d be already like super stoked. But in industry, it’s really interesting because a lot of the work that is done in industry is already done pretty well. So we if we think of, for example, companies, for example, like Microsoft, right, they put a lot of money right now into open source, they bought GitHub, they publish open source software, they put money into open source software projects like R, for example, right. So somehow, this must be a good way of making profits as well. And we see lots of companies investing in open source. So why not think about other research products, like data, and so on, also in the same line as we do of open source, because software is just a similar product. And I don’t think that there’s, I mean, there is some software, or some products, where it makes sense to have a patent or a trade secret or something. But sometimes it’s just more profitable, to have something where people can look into it and trust it. And I’d also helps like finding the next coders, and the next researchers to work on these projects, because, well, we like looking into what we’re going to do next. Right. And also, if we look, for example, into pharma industry, there, we see that a lot of the work they’re doing is already pretty good. For example, if we look at clinical trial transparency, pharma is doing better than academia there. And also, they’re pretty good on reproducible research, because they have to stick to certain rules. And when we look on the other side, so industry also benefits from open science, right? Because if we do open science, in academia, and publicly funded projects, then this will help companies make more money, because they have access to more knowledge and maybe interesting ideas as well, that they don’t have right now. So I think this is a win-win situation for companies as well. And I feel like if academia gives more to the industry, then eventually there will be like a mindset change. And industry will also give more back as well.\nBrian Tarran\nYeah, I see– I can see that and I guess it’s, you know, knowledge transfer is a big objective of a lot of academic institutions, right, we want our outputs to be of use to wider society, and that includes business and industry, right. So if people can pick those things up, you know, download it off of the web without having to, you know, make one-to-one links with the researchers who’ve done that project, it just it smooths that transition, and that that knowledge transfer becomes a lot more straightforward. You did, you mentioned about, you know, open science is about public science, essentially putting these, when it’s publicly funded research, this data and this work goes into the public space. You know, I’m guessing that, to a large extent, a lot of, you know, regular members of the public aren’t going to be interacting with open science. They’re not going to be downloading the datasets, they’re not going to be rebuilding the pipelines and rerunning the analysis. But do you still see that there’s a value there for the public in that information being there should they need it? And where are the kind of areas of value that you think the public will exploit?\nHeidi Seibold\nYeah, I think that’s an interesting question. And I think the biggest answer to that is that we don’t, we don’t know what ideas people will have, right? There’s so many skilled people out there, that probably will do amazing things. And we see that over and over again, when we put stuff out there, that people just get the super cool ideas. So we have this with the software Stable Diffusion right now, right? So that’s an AI that generates images from text, and it runs on my computer here. And I don’t need AI skills to be able to do that. And people are building such incredible images out of this. And it’s really fun to see. So I think, yeah, we just don’t know what’s going to happen. And on the other hand, I think, well, I am a researcher, but I’m also the public, right? And so if I have a question about something that concerns my private life or my friends’ private life, then I can also– I do have the skills to go into this and look at some research for example, I don’t know, how to best raise children or whatever. Um, that’s– so I have a friend, she’s an epidemiologist, and she always goes and looks at research on like, how to feed her child best and what to do. There’s, there’s all kinds of questions you have as a mother. And she, as a researcher can just go into the research and figure out, like, what is the best path for me to take. And I think the more we do open science, the more we can also do, like science communication, that adds on to that as well. Right? So if we have her now, she could now help other mothers make the same good decisions as well. And she would be sort of a science communicator for research that isn’t even hers. Um, so that is pretty cool, I think.\nBrian Tarran\nI think so, and it’s speaks to me of breaking down silos that, and I guess, blurring the lines between our roles, our professional roles and our personal lives, right. It’s about bringing science into that kind of public sphere, so I can see why the benefits would accrue from doing so. Just to wrap up, let’s go back to that year of open science, that was announced by the White House, are there any other specific initiatives in that big long list that constituted the announcement that you’re excited about?\nHeidi Seibold\nYeah, there is. I think, especially what I really liked was that, yeah, federally funded research, um, needs to be accessible, including the data when possible. So again, there’s open when possible, closed when necessary. But I think this is a very good first step, to say, okay, it depends on the funding, if it’s publicly funded, then it should also be publicly available. And I think that’s a really good sign. And then also, these, the statement had like, mentioned all these open science initiatives in different fields, which I really liked. So for example, from NASA, they have this transform to open science programme, and they’re already super active. And it’s really cool to see what comes out of that. For medicine, they have requirements for data management plans, which I think is a very solid step towards open science in medicine, because they are we have the issue of data privacy. And we really have to think from the get-go of a project about what should we do in terms of best practices of data management and having a requirement on that is, I think, a really, really solid step. And also, I’m thinking about open science in the field of like federal government even, because open data and federal government is a huge topic, right. And it, that’s definitely something that a lot of people will be interested in as well.\nBrian Tarran\nLast question for you, then Heidi, you know, what would you want the lasting impact of an initiative like this to be? And you know, would you like to see this sort of thing replicated in other countries around the world? If indeed, you know, other countries may already be doing this and have already done this?\nHeidi Seibold\nI think in general, it’s a very, very good sign that we’re seeing right now. We’ve seen lots of movement, for example, in the Netherlands – Netherlands is really big on open science – and seeing such a big country [the US] that also plays such an important role in the world, doing– taking this step, is a great sign for the entire, like all of research, really. And yeah, I think it’s also nice to see that we’re going from like, Oh, this is a niche topic that only experts are interested in, and people that are like advocates and nerds focus on, to something that really governments are thinking about.\nBrian Tarran\nSo, open science is going mainstream, I think is the message. And let’s hope that it continues to do so. So Heidi, thank you very much for your time today. Where can people find you online, if f they want to find out more about you and your work and your thoughts on open science?\nHeidi Seibold\nYeah, thank you so much for having me. It was wonderful. And I always like talking about open science, so people can find me on my website, heidiseibold.com. And I’m also on Mastodon, Twitter, LinkedIn, YouTube, wherever your search for Heidi Seibold, you’ll find me.\nBrian Tarran\nExcellent. Excellent. Well, thank you again. And thank you to those of you who are tuning in today. Make sure to check realworlddatascience.net for more interviews. Take care.\n\n← Back to Interviews\n\n\nThis work is licensed under CC BY 4.0"
  },
  {
    "objectID": "news-and-views/interviews/index.html",
    "href": "news-and-views/interviews/index.html",
    "title": "Interviews",
    "section": "",
    "text": "Why open science is ‘just good science in a digital era’\n\n\n\n\n\n\n\nOpen science\n\n\nReproducible research\n\n\n\n\nReal World Data Science speaks with statistician and data scientist Heidi Seibold about open science: what it means, the benefits of it, and how to move towards it.\n\n\n\n\n\n\nFeb 3, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nHow to ‘Escape from Model Land’: an interview with Erica Thompson\n\n\n\n\n\n\n\nModelling\n\n\nEthics\n\n\n\n\nAuthor Erica Thompson talks to Real World Data Science about the ‘social element’ of mathematical modelling, how it manifests, and what to do about it.\n\n\n\n\n\n\nJan 25, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A showcase for data science in action",
    "section": "",
    "text": "Welcome to the home of Real World Data Science, a new project from the Royal Statistical Society. This site and its content are being created and curated by data science practitioners and leaders with a single goal in mind: to help you deliver high quality, ethical, impactful data science in your workplace."
  },
  {
    "objectID": "index.html#what-are-our-aims",
    "href": "index.html#what-are-our-aims",
    "title": "A showcase for data science in action",
    "section": "What are our aims?",
    "text": "What are our aims?\nReal World Data Science aims to be a trusted, go-to source for high-quality, engaging and inspiring content which helps data science students, practitioners and leaders to:\n\ndiscover and learn more efficiently;\n\nacquire practical problem-solving skills;\n\nshare their knowledge and accomplishments publicly;\n\nwork smarter, ethically, and more effectively."
  },
  {
    "objectID": "index.html#what-will-we-provide",
    "href": "index.html#what-will-we-provide",
    "title": "A showcase for data science in action",
    "section": "What will we provide?",
    "text": "What will we provide?\nResources will be created to meet the needs of our target audiences. These include:\n\nCase studies – showing how data science is used to solve real-world problems in business, public policy and beyond.\nExplainers – interrogating the underlying assumptions and limitations of data science tools and methods, to help data scientists make smarter, more informed analytical choices.\nExercises – to challenge and develop the analytical mindset that all data scientists need to succeed.\n\nAdvice – interviews, Q&As, and FAQs on such topics as data science ethics, career paths, and communication, to support professional development.\n\nResources will also be curated to help data scientists identify trustworthy, high-quality content. These include:\n\nTraining guides – step-by-step approaches and recommended sources for learning new skills and methods.\nDatasets – tagged and sorted to help educators and practitioners find data to meet their teaching and training needs.\nFeeds – who and what to follow to keep up with new ideas and developments."
  },
  {
    "objectID": "index.html#how-you-can-get-involved",
    "href": "index.html#how-you-can-get-involved",
    "title": "A showcase for data science in action",
    "section": "How you can get involved",
    "text": "How you can get involved\nSee our open call for contributions."
  },
  {
    "objectID": "contributor-docs/recommender.html",
    "href": "contributor-docs/recommender.html",
    "title": "Recommenders",
    "section": "",
    "text": "Too much content, not enough time. That about sums up the problem facing the data science community. So, our Recommenders are here to help. Contributors are invited to submit lists (or Feeds) of high-quality sources on all manner of topics – from foundational ideas in data science and cutting-edge techniques, to opinion and thought-leadership on the future of the profession. Reviews of new books and other material are also welcome."
  },
  {
    "objectID": "contributor-docs/recommender.html#article-types-and-structures",
    "href": "contributor-docs/recommender.html#article-types-and-structures",
    "title": "Recommenders",
    "section": "Article types and structures",
    "text": "Article types and structures\n\nFeeds\nFeeds can be constructed around different topics and audiences. For example, you might want to recommend to all data scientists the “10 best blogs on machine learning” or “5 data visualisation experts to follow on Twitter”. Or you might have a list of sources specifically targeted at data science educators (e.g., “the best books on teaching coding”) or data science leaders (“5 insightful case studies on building data science teams”).\nWhatever you choose to focus on, the following outline provides a basic guide for structuring your feed:\n\n\nOverview\n\nA brief introduction to your list, its main focus, who you are writing it for, and why. You should also say something about yourself and your background, too. This will give additional context to the recommendations you are making.\n\n\n\nList of sources\n\nAs well as naming your sources and telling people how to find them, you should also explain why you are recommending them, how they helped you in your career or studies, or other reasons why you find them to be of value. Sample quotes or small excerpts from the sources themselves might also be worth including.\n\n\n\nStart a dialogue\n\nConclude with a call for readers to share recommendations of their own, either in the article comments or on social media. Contributors are welcome to update their lists any time with new sources, including those suggested by site users.\n\n\n\n\n\nReviews\nUnlike the feeds described above, each submitted review should focus only on a single publication. It must be an honest summary of the reviewer’s thoughts, feelings and impressions, covering what they liked and didn’t like, the perceived strengths and weaknesses of the publication, and whether it is likely to be of interest and value to its intended audience. Reviews should of course provide an overview of the publication in question but must avoid dry, itemised descriptions of the publication’s constituent parts (e.g., listing out the chapters in a book).\nAll reviews should list the following information (if relevant):\n\nTitle of publication\nAuthor(s)\nDate of publication\nEdition or format used for review\nPublisher\nLength\nPrice\nWebsite address or other source of further information"
  },
  {
    "objectID": "contributor-docs/recommender.html#advice-and-recommendations",
    "href": "contributor-docs/recommender.html#advice-and-recommendations",
    "title": "Recommenders",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nKeep lists to a sensible size. Feeds are meant to help data scientists to prioritise who and what to follow based on their interests and career stage – and it is much easier to keep tabs on 5 sources than it is 50, or even 15! So, the fewer the better.\nKeep your recommendations up to date. In this era of digital publishing, things can and do change overnight. So, if one of your recommended bloggers stops blogging, or the author of one of your favourite books makes a major update to the text, do be sure to let us – and your audience – know. We want to keep feeds and reviews current and useful.\nOf course you are brilliant, but… Please do not recommend or review your own publications or those in which you have a pecuniary or similar interest."
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html",
    "href": "contributor-docs/call-for-contributions.html",
    "title": "Call for contributions",
    "section": "",
    "text": "Real World Data Science aims to inform, inspire and strengthen the data science community by showcasing real-world examples of data science practice and bringing together data scientists to share knowledge.\nWe cannot succeed in these aims without the support and contributions of the data science community, so thank you for taking the time to review this open call for contributions."
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#what-are-we-looking-for",
    "href": "contributor-docs/call-for-contributions.html#what-are-we-looking-for",
    "title": "Call for contributions",
    "section": "What are we looking for?",
    "text": "What are we looking for?\nBelow is a list of our core content areas. We welcome submissions in any of these areas. Each content area is linked to its own set of notes for contributors.\n\nCase studies\nExplainers\nExercises\nDatasets\nTraining guides\nRecommenders\nNew! DataScienceBites\n\nSubmissions can focus on any and all topics and application areas. We want our content to reflect the breadth and depth of real-world data science."
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#our-target-audience",
    "href": "contributor-docs/call-for-contributions.html#our-target-audience",
    "title": "Call for contributions",
    "section": "Our target audience",
    "text": "Our target audience\nReal World Data Science is for all who work in data science - whether they are students, teachers, practitioners or leaders. Submissions do not have to appeal to all data scientists, however. Contributors should think carefully about who they are trying to reach, and craft their submissions accordingly."
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#what-can-submissions-include",
    "href": "contributor-docs/call-for-contributions.html#what-can-submissions-include",
    "title": "Call for contributions",
    "section": "What can submissions include?",
    "text": "What can submissions include?\nWe encourage contributors to experiment with and include different media formats in their submissions - text, images, audio and video. And as our site is built on Quarto - the new open-source publishing system developed by Posit - submissions to Real World Data Science can also include code cells, equations, figures, interactive data displays, and other elements to enrich the user experience. If you haven’t used Quarto before, check out this fantastic tutorial from the developers."
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#how-to-submit",
    "href": "contributor-docs/call-for-contributions.html#how-to-submit",
    "title": "Call for contributions",
    "section": "How to submit",
    "text": "How to submit\nOnce you’ve reviewed our notes for contributors and settled on a content area, theme and audience, please review our contributor guidelines for details on the submission process."
  },
  {
    "objectID": "contributor-docs/case-studies.html",
    "href": "contributor-docs/case-studies.html",
    "title": "Case studies",
    "section": "",
    "text": "Case studies are a core feature of the Real World Data Science platform. Our case studies are designed to show how data science is used to solve real-world problems in business, public policy and beyond.\nA good case study will be a source of information, insight and inspiration for each of our target audiences:"
  },
  {
    "objectID": "contributor-docs/case-studies.html#structure",
    "href": "contributor-docs/case-studies.html#structure",
    "title": "Case studies",
    "section": "Structure",
    "text": "Structure\nCase studies should follow the structure below. It is not necessary to use the section headings we have provided – creativity and variety are encouraged. However, the areas outlined under each section heading should be covered in all submissions.\n\n\nThe problem/challenge\n\nSummarise the project and its relevance to your organisation’s needs, aims and ambitions.\n\n\n\nGoals\n\nSpecify what exactly you sought to achieve with this project.\n\n\n\nBackground\n\nAn opportunity to explain more about your organisation, your team’s work leading up to this project, and to introduce audiences more generally to the type of problem/challenge you faced, particularly if it is a problem/challenge that may be experienced by organisations working in different sectors and industries.\n\n\n\nApproach\n\nDescribe how you turned the organisational problem/challenge into a task that could be addressed by data science. Explain how you proposed to tackle the problem, including an introduction, explanation and (possibly) a demonstration of the method, model or algorithm used. (NB: If you have a particular interest and expertise in the method, model or algorithm employed, including the history and development of the approach, please consider writing an Explainer article for us.) Discuss the pros and cons, strengths and limitations of the approach.\n\n\n\nImplementation\n\nWalk audiences through the implementation process. Discuss any challenges you faced, the ethical questions you needed to ask and answer, and how you tested the approach to ensure that outcomes would be robust, unbiased, good quality, and aligned with the goals you set out to achieve.\n\n\n\nImpact\n\nHow successful was the project? Did you achieve your goals? How has the project benefited your organisation? How has the project benefited your team? Does it inform or pave the way for future projects?\n\n\n\nLearnings\n\nWhat are your key takeaways from the project? Are there lessons that you can apply to future projects, or are there learnings for other data scientists working on similar problems/challenges?"
  },
  {
    "objectID": "contributor-docs/case-studies.html#advice-and-recommendations",
    "href": "contributor-docs/case-studies.html#advice-and-recommendations",
    "title": "Case studies",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nYou do not need to divulge the detailed inner workings of your organisation. Audiences are mostly interested in understanding the general use case and the problem-solving process you went through, to see how they might apply the same approach within their own organisations.\nGoals can be defined quite broadly. There’s no expectation that you set out your organisation’s short- or long-term targets. Instead, audiences need to know enough about what you want to do so they can understand what motivates your choice of approach.\nUse toy examples and synthetic data to good effect. We understand that – whether for commercial, legal or ethical reasons – it can be difficult or impossible to share real data in your case studies, or to describe the actual outputs of your work. However, there are many ways to share learnings and insights without divulging sensitive information. This blog post from Lyft uses hypotheticals, mathematical notation and synthetic data to explain the company’s approach to causal forecasting without revealing actual KPIs or data.\nPeople like to experiment, so encourage them to do so. Our platform allows you to embed code and to link that code to interactive coding environments like Google Colab. So if, for example, you want to explain a technique like bootstrapping, why not provide a code block so that audiences can run a bootstrapping simulation themselves.\nLeverage links. You can’t be expected to explain or cover every detail in one case study, so feel free to point audiences to other sources of information that can enrich their understanding: blogs, videos, journal articles, conference papers, etc."
  },
  {
    "objectID": "contributor-docs/exercises.html",
    "href": "contributor-docs/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Exercises on Real World Data Science will provide users with the opportunity to put knowledge, skills and new learnings into practice, helping them to challenge and refine problem-solving approaches while strengthening the analytical mindset.\nWe will achieve this by supporting contributors to design tasks that replicate real-world data scientific processes."
  },
  {
    "objectID": "contributor-docs/exercises.html#structure",
    "href": "contributor-docs/exercises.html#structure",
    "title": "Exercises",
    "section": "Structure",
    "text": "Structure\nThe structure of each published exercise will vary based on the nature of the task(s) being set by contributors and the outcomes they have in mind. But, in general, exercises must do more than simply ask users to, e.g., download dataset – analyse – report.\n\n\nSet the scene\n\nProvide a believable, realistic scenario for the exercise. Establish the “client challenge” within that context, the resources available to the data scientist, and the outputs expected.\n\n\n\nGive users space to map the problem themselves\n\nHave users translate the “client challenge” into a data analytic question that can be answered using the resources available.\n\n\n\nMake data exploration, cleaning and tidying part of the process\n\nData exploration and preparation are an important part of most – if not all – data science projects, so let users loose on messy datasets so they can figure out for themselves (a) what they’re working with and (b) what analytical approach makes sense given the features of the data and the problem at hand.\n\n\n\nIntegrate ethics\n\nPrompt users to think about and work through ethical questions and issues that are relevant to the exercise: the challenge they’ve been set, the data they’ve been given, their proposed approach to analysis and modelling, etc.\n\n\n\nEncourage users to document their work and their thinking\n\nThrough computational notebooks (e.g., Jupyter notebooks), users can record not only what they’ve done and how they’ve done it, but why.\n\n\n\nMake data presentation part of the expected project outputs\n\nAsk users to think about presenting to specific audiences: not just fellow data scientists, but to decision-makers, policy experts, the public – whatever makes most sense given the exercise scenario."
  },
  {
    "objectID": "contributor-docs/exercises.html#advice-and-recommendations",
    "href": "contributor-docs/exercises.html#advice-and-recommendations",
    "title": "Exercises",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nInvite users to share their work. If users have followed the advice to document their work and thinking, encourage them to share their computational notebooks (including their results and outputs) on their own websites, in a GitHub repository, through social media, etc. This could be a great way for others to discover your exercise, and we could also link to a selection of these notebooks through the exercise page itself.\nThink about building in hints and tips. Some users – depending on their prior level of experience – might appreciate some additional guidance here and there.\n\n\n\n\n\n\nSpoiler warning\n\n\n\n\n\nCollapsible callouts like this make hints and tips visible only to those who want to see them.\n\n\n\nBring different resources together. Exercises provide an ideal opportunity to draw together different strands of the Real World Data Science platform: case studies could provide inspiration or pointers for how to tackle a particular challenge; explainers might contain useful information about the tools and techniques to apply to specific types of data; and if you’re looking for a suitable set of data, it may already be listed in our datasets section."
  },
  {
    "objectID": "contributor-docs/training-guides.html",
    "href": "contributor-docs/training-guides.html",
    "title": "Training guides",
    "section": "",
    "text": "In data science, there’s no one-size-fits-all solution to every problem and challenge. So, part of the job of the data scientist is to rapidly learn about different sub-domains, tools and techniques, and put those learnings into practice.\nBut it can be time-consuming to figure out what you need to learn and in what order, and to identify the best resources for doing so. This is where our Training guides come in. Each will set out a learning pathway for data scientists to follow, with recommendations of textbooks, videos, practical exercises and other teaching material to use every step of the way."
  },
  {
    "objectID": "contributor-docs/training-guides.html#structure",
    "href": "contributor-docs/training-guides.html#structure",
    "title": "Training guides",
    "section": "Structure",
    "text": "Structure\nContributors should think about their training guides as being short online courses that are constructed from existing high-quality material. You do not need to create your own “course” content. Rather, you should focus on recommending texts and other material for users to follow in a logical ordered way, so that they may build up and secure their knowledge of a particular topic.\nGuides should feature a mix of content types – not only text, but audio and video – and they should provide ample opportunities for users to practice what they are learning.\nA brief and extremely simplified example of a guide is as follows:\n\nStep 1: Watch this introductory video on Topic X.\nStep 2: Now you are familiar with the basics of Topic X, you will want to read Chapter 2 of Textbook Y, which delves into more of the mathematical underpinnings.\nStep 3: Let’s try Topic X ourselves. This GitHub repository has code for you to run it in Python. Copy the code and give it a go.\nStep 4: You should now be ready to apply Topic X to a simple data challenge. Check out this Kaggle page and practice what you have learned so far.\nStep 5: We’re now moving from the “beginner” level to “intermediate”, and Training Course Z gives a thorough overview of what you need to know for the next stage of your learning journey.\n… etc."
  },
  {
    "objectID": "contributor-docs/training-guides.html#advice-and-recommendations",
    "href": "contributor-docs/training-guides.html#advice-and-recommendations",
    "title": "Training guides",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nBe mindful of different learning styles. Some people prefer to read, others prefer to watch or listen. So, wherever possible, for each stage of your training guide, try to provide a mix of resources that meet the same learning objectives.\nConsider barriers to entry. Data scientists in large organisations may have access to training budgets or mechanisms to apply for training funds. But that isn’t the case for all data scientists, meaning that paid-for materials and training courses might not be accessible to everyone. Recommend them sparingly, and if there are ways to access the material at reduced rates do let users know. However, you must not link to illicit copies of material – e.g., unauthorised PDF reproductions of textbooks.\nIf there are resource gaps, please tell us. While planning out your training guide, you may well struggle to find the perfect piece of content to recommend at a particular stage of your learning journey. If that is the case, do get in touch with us. One of the goals of Real World Data Science is to identify and plug these sorts of gaps, so that all in the data science community can benefit. We’ll sketch out a commission and take it out to our network of contacts. Or perhaps it’ll be something you want to create for the site!"
  },
  {
    "objectID": "contributor-docs/datasciencebites.html",
    "href": "contributor-docs/datasciencebites.html",
    "title": "DataScienceBites",
    "section": "",
    "text": "Our DataScienceBites blog publishes digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space. Our goal is to make scientific papers more widely accessible.\nPosts are targeted at undergraduate level. Each presents a non-technical overview of a new research paper and its key findings, potential applications, and implications.\nWe welcome contributions from graduate students and early career researchers in data science (or related subjects) at universities throughout the world, as well as industry researchers. Contributors must have a passion for science communication and a drive to explain, clarify and demystify."
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#basic-structure-of-a-bites-post",
    "href": "contributor-docs/datasciencebites.html#basic-structure-of-a-bites-post",
    "title": "DataScienceBites",
    "section": "Basic structure of a Bites post",
    "text": "Basic structure of a Bites post\n\n\nInformation box\n\nGive the full title of the paper you are discussing, the name(s) of its author(s) and year of publication. Say where the paper is published, whether it is a pre-print or peer-reviewed publication, whether it is open access or not, and include links to authorised HTML and/or PDF versions.\n\n\n\nIntroduction and background\n\nEase readers into the paper you are writing about, and help them to see why it is worth their attention. Sometimes a paper is sufficiently ground breaking to be attention grabbing in its own right. But more often than not you will have to find a way to “hook” people in. When writing about data science tools and methods, for example, it can be helpful to start by outlining a typical application scenario or use case. Readers may be more familiar with the use case than the tool, so this provides valuable framing within which you can talk about the shortcomings of existing methods and the advances promised by the new research. Ultimately, you want to get readers to the point where they understand the problem, question or challenge that this new research paper aims to solve.\n\n\n\nResearch overview\n\nHere is where you summarise the work done by the paper’s authors. Remember to keep the discussion non-technical and jargon-free, and explain important terms and concepts as necessary. Be careful not to oversimplify!\n\n\n\nTakeaways and implications\n\nPut this paper and its contributions into the appropriate context for readers. Does it make modest but important improvements to existing knowledge or research processes? Will it help others to address new questions, issues and challenges? What further work is needed to build on these advances?\n\n\n\nFurther reading\n\nReaders may wish to learn more about the research or the broader subject matter, so feel free to point them to additional resources: videos, podcasts, textbooks, conference presentations, etc."
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#word-count-target",
    "href": "contributor-docs/datasciencebites.html#word-count-target",
    "title": "DataScienceBites",
    "section": "Word count target",
    "text": "Word count target\n500–1,000 words."
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#two-ways-of-contributing",
    "href": "contributor-docs/datasciencebites.html#two-ways-of-contributing",
    "title": "DataScienceBites",
    "section": "Two ways of contributing",
    "text": "Two ways of contributing\n\nRegular contributors commit to publishing an agreed number of posts (6–12) each year. Regular contributors will collaborate closely with editors on the development of the blog, and experienced contributors will be invited to support and mentor new regular contributors to help grow the DataScienceBites team.\nGuest contributors make one-off or ad hoc contributions to the site. Proposals are to be submitted in the form of a content brief."
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#next-steps",
    "href": "contributor-docs/datasciencebites.html#next-steps",
    "title": "DataScienceBites",
    "section": "Next steps",
    "text": "Next steps\nIf you are interested in contributing to DataScienceBites:\n\nIdentify a new data science publication that you are interested in writing about.\nReview the blog index to make sure we haven’t already covered this publication.\nDecide whether you would like to be a regular contributor or guest contributor.\nContact Real World Data Science editor Brian Tarran to discuss.\n\n\n\n\n\n\n\nTwo things to keep in mind\n\n\n\nDataScienceBites contributors are not permitted to write about their own research papers.\nIf contributors are in any way affiliated with the authors of papers they write about, this must be disclosed as part of their submission."
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#about-the-sciencebites-family",
    "href": "contributor-docs/datasciencebites.html#about-the-sciencebites-family",
    "title": "DataScienceBites",
    "section": "About the ScienceBites family",
    "text": "About the ScienceBites family\nDataScienceBites is published by Real World Data Science and is part of the ScienceBites galaxy of sites. See sciencebites.org for an overview of the ScienceBites mission."
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html",
    "href": "contributor-docs/contributor-guidelines.html",
    "title": "Contributor guidelines",
    "section": "",
    "text": "Thank you for your interest in contributing to Real World Data Science. This page will walk you through the process of preparing and submitting your idea. If you haven’t done so already, please review our call for contributions before continuing."
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#site-functionality-and-ethos",
    "href": "contributor-docs/contributor-guidelines.html#site-functionality-and-ethos",
    "title": "Contributor guidelines",
    "section": "Site functionality and ethos",
    "text": "Site functionality and ethos\nReal World Data Science is built on Quarto, the new open-source publishing system developed by Posit. The site has been designed from the ground up as a platform for data scientists, created by data scientists. Here’s what this means in practice:\n\nContributors can use data science software and tools to create content – e.g. Visual Studio Code, RStudio, Jupyter Lab; Python, R, Observable, and Shiny – allowing for the full integration of text, code, figures, equations, and other elements.\nReview and editing are transparent and collaborative, again making use of tools data scientists are familiar with – e.g. GitHub, Google Docs – for sharing and revising documents prior to publication.\nContent can be both engaging and interactive. Many data scientists learn by doing, so code can be made available as R Markdown or Jupyter Notebook files to be reused and experimented with offline. Or, the same documents can be used online through tools like Google Colab and Binder. Where appropriate, the use of interactive displays and Shiny apps is encouraged, allowing for data visualisations to be interrogated and regenerated on the fly.\nSite users are contributors too. Through annotation and commenting functionality, site users can interact and converse with authors and other members of the Real World Data Science community. And with all source files hosted on GitHub, users of our site can raise issues, or fork and propose improvements – leading to a true exchange of knowledge."
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#the-submission-process",
    "href": "contributor-docs/contributor-guidelines.html#the-submission-process",
    "title": "Contributor guidelines",
    "section": "The submission process",
    "text": "The submission process\n\nContact site editor Brian Tarran to discuss your proposed submission.\nWorking with the editor, draw up a short content brief, containing the following:\n\nTitle of submission\nAuthor name(s) and affiliation(s)\nTheme/topic area\nTarget audience\nSynopsis or sell line, summarising the story and its importance/value (100 words max.)\nKey audience takeaways\nFormats and features (e.g., text, audio, video; code blocks, interactive data visualisations, etc.)\nAccessibility considerations\nTarget length/word count\nFirst draft to be submitted by…\n\nOnce a content brief is finalised and approved, content is to be prepared in the agreed format and with reference to our style guide. For simple text-based articles, we recommend using Google Docs or Microsoft Word; for submissions that incorporate technical or multimedia content, such as code, equations or interactive graphics, we recommend the Quarto (.qmd) file format, but documents can also be submitted in Jupyter notebook (.ipynb) and R Markdown (.Rmd) formats.\nDraft submissions should be sent via email to the editor. Alternatively, contributors can commit their drafts to their own GitHub accounts and add the Real World Data Science GitHub account as a collaborator."
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#copyright-and-content-licencing",
    "href": "contributor-docs/contributor-guidelines.html#copyright-and-content-licencing",
    "title": "Contributor guidelines",
    "section": "Copyright and content licencing",
    "text": "Copyright and content licencing\nContributors retain copyright of their work, but agree to publish their work under a Creative Commons licence. Contributors are free to choose the licence that best suits their content. The chosen licence should be indicated on the draft submission."
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#the-review-process",
    "href": "contributor-docs/contributor-guidelines.html#the-review-process",
    "title": "Contributor guidelines",
    "section": "The review process",
    "text": "The review process\nDraft submissions will be shared for review with members of the Real World Data Science Editorial Board. Comments and edits to documents will be made via Google Docs/MS Word/GitHub, allowing for (a) version control, (b) open dialogue between reviewers and contributors, and (c) a transparent and well-documented review process.\nOnce revisions are complete and content is accepted for publication, authors will be provided with HTML files to preview published content. Following sign-off by author and editor, HTML files will be made live."
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#post-publication",
    "href": "contributor-docs/contributor-guidelines.html#post-publication",
    "title": "Contributor guidelines",
    "section": "Post-publication",
    "text": "Post-publication\nContributors and editors will work together to promote content via social media platforms – Twitter, LinkedIn, blogs – and in other channels as appropriate – e.g., in response to related questions on Quora or Stack Overflow.\nContributors are encouraged to monitor their content regularly for user comments and discussions. Engaging in discussions with users – whether through the Real World Data Science platform or via social media and other channels – is an effective way of developing an audience: it builds profile for the contributor and their content, and encourages other users to find and interact with content."
  },
  {
    "objectID": "contributor-docs/explainers.html",
    "href": "contributor-docs/explainers.html",
    "title": "Explainers",
    "section": "",
    "text": "On Real World Data Science, Explainers are the stories behind the stories of data science in action. They are deep-dive explorations of the ideas, concepts, tools, and methods that make data science projects possible. In particular, we are keen to explore and explain the statistical underpinnings of modern data science techniques.\nA good Explainer will lead audiences through the what, when, how, and why of its chosen topic. The ultimate goal is to equip data scientists with the information and insight they need to make smarter, more informed analytical choices.\nThere are many different but effective ways of structuring an explainer and plentiful written examples in major media outlets like The Guardian and Vox, but these are generally written for a non-technical audience. Examples of technical explainers (with interactive elements) can be found on Amazon’s Machine Learning University."
  },
  {
    "objectID": "contributor-docs/explainers.html#structure",
    "href": "contributor-docs/explainers.html#structure",
    "title": "Explainers",
    "section": "Structure",
    "text": "Structure\nThe following outline is a basic guide to structuring an Explainer:\n\n\nHook\n\nIntroduce your topic, and explain why audiences should pay attention. For example, does your Explainer link to one of our published case studies? Does it focus on a tool or method that has been the subject of recent attention? Is it a foundational idea that is relevant to all sorts of data science applications?\n\n\n\nHigh-level summary\n\nA short, largely non-technical explanation of your topic. A good way to view this section is as an accessible condensed version of your complete Explainer. In thinking of it in this way, you can subtly signpost to audiences the areas you’ll be covering and the questions you’ll be answering throughout the remainder of your contribution.\n\n\n\nHistory and background\n\nIt can be useful from a practical perspective to explain how ideas, concepts, tools, and methods have developed over time. Applications may have become more complex in recent years, so exploring the origins of data science techniques might lead you to discover simpler use cases that can help support and illustrate your high-level summary.\n\n\n\nThe how, the when, the why\n\nThis section of your Explainer will likely be split into multiple subsections as you seek to build up your audience’s understanding of your chosen topic. It can be helpful to think about the sorts of questions an audience member might ask and to structure your contribution so that it directly addresses those questions (Q&A/FAQ formats are commonly used in explainer-type articles). If the focus of your Explainer is a data science method, for example, you’ll want to address the following:\n\n\n\nHow does it work and how is it applied (perhaps with an example or simulation)?\nWhat are the underlying assumptions?\nHow is performance checked and assessed?\nHow should outputs be interpreted?\nWhat are the pros and cons, the strengths and limitations of the approach?\nWhat are the optimal use cases, and when should the method be avoided altogether?\nAre there alternatives that people should know about?\n\n\nKey takeaways\n\nThis serves as your final summary: a chance to remind your audience of what they’ve learned from your Explainer and the main points they should keep in mind.\n\n\n\nTell me more\n\nIt’s sensible to assume that some of your audience will have further questions and will want to learn more about the topic. If you have additional sources of information to recommend, make sure to share them here."
  },
  {
    "objectID": "contributor-docs/explainers.html#advice-and-recommendations",
    "href": "contributor-docs/explainers.html#advice-and-recommendations",
    "title": "Explainers",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nFocus on what’s important. Your Explainer can’t hope to explain everything, so you need to be clear about what’s essential for your audience to know and what isn’t. Make good use of links and references to point audiences to other valuable sources of information that can enrich their understanding of your topic.\nBe clear about your target audience and their expected prior level of knowledge. In keeping with the point above, you need to be clear in your own mind about how much you expect your audience to know already about the general topic or subject matter. You can then structure your contribution accordingly. It might also be helpful to state explicitly, at the outset of your contribution, what assumptions you’ve made about your audience and highlight any background reading that might be beneficial.\nPlan out your route. To help you decide what to cover in your Explainer, we recommend first writing out your high-level summary of the topic and also your key takeaways. This provides you with a start point (A) and an end point (B) for your contribution. The challenge then is to figure out the main points or questions you will need to address to help your audience progress from point A to point B in a way that’s logical and intuitive for them to follow."
  },
  {
    "objectID": "contributor-docs/datasets.html",
    "href": "contributor-docs/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "It’s easy to find datasets online. What’s more difficult is finding quality datasets that are suitable for specific training and development needs. On Real World Data Science we aim to solve that problem.\nOur Datasets section will provide a curated list of recommended datasets along with detailed notes and guidance on what each dataset contains, how it is structured, and how best to make use of it. In particular, we want to highlight messy rather than pristine datasets – ones that capture the imperfections and oddities found in real-world data – so that users can practice not only data analysis and modelling, but data cleaning and preparation too!"
  },
  {
    "objectID": "contributor-docs/datasets.html#structure",
    "href": "contributor-docs/datasets.html#structure",
    "title": "Datasets",
    "section": "Structure",
    "text": "Structure\nIf you have a dataset to recommend, your submission must cover the following areas:\n\nDataset name\nLink to source\nWhat data science tasks/methods can this dataset be used to demonstrate?\nHave you used this dataset for your own teaching/learning? (see Advice and recommendations below)\nWhy was the dataset originally created?\nWhen was it created?\nWho created it?\nLicences/restrictions?\nSize of dataset\nData types/description\nReal/synthetic data?"
  },
  {
    "objectID": "contributor-docs/datasets.html#advice-and-recommendations",
    "href": "contributor-docs/datasets.html#advice-and-recommendations",
    "title": "Datasets",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nHelp others to make good use of your recommended dataset. If you’ve had experience using a recommended dataset for your own teaching and learning, please consider creating an exercise for platform users to complete. If you encountered the dataset as part of a training course, competition or exercise created by a third party, make sure to give them a namecheck."
  },
  {
    "objectID": "contributor-docs/style-guide.html",
    "href": "contributor-docs/style-guide.html",
    "title": "Style guide",
    "section": "",
    "text": "Content must be presented in a conversational yet professional and respectful tone. Contributors should imagine themselves delivering a lively, engaging conference presentation, rather than preparing a dry, formal report or journal publication. Contributors to Real World Data Science are creating content for their colleagues and peers and should “speak” to them as such."
  },
  {
    "objectID": "contributor-docs/style-guide.html#structure",
    "href": "contributor-docs/style-guide.html#structure",
    "title": "Style guide",
    "section": "Structure",
    "text": "Structure\nEach contribution must, in effect, tell “a story”, and so contributors need to be clear (a) what their story is, (b) why people should be interested, and (c) what its main message or key takeaways are. To help figure this out, we recommend contributors apply the XY Story Formula."
  },
  {
    "objectID": "contributor-docs/style-guide.html#technical-content-and-jargon",
    "href": "contributor-docs/style-guide.html#technical-content-and-jargon",
    "title": "Style guide",
    "section": "Technical content and jargon",
    "text": "Technical content and jargon\nTechnical content is a necessary feature of a site like ours. Without it, an article or other piece of content may be of little practical use to a technical audience. But if there’s too much of it, even experts may struggle to stay engaged. Contributors are also faced with a dilemma when it comes to explaining technical content: explain nothing, and you risk alienating some of your audience; explain everything, and you’ll struggle to establish a clear, strong narrative thread. So, careful consideration is required:\n\nWho is my audience for this article?\nWhat is this audience likely to know already, and what needs to be explained?\nIf something needs to be explained, can I do so briefly and then link to other resources? Or is a full explanation required?\nIn telling my “story”, what are the absolute-need-to-knows, and what are the simply-nice-to-knows?\n\nThinking through these questions will help contributors to find the right mix of valuable, technical content paired with accessible, readable narrative.\nKeep in mind that the same general advice applies to the use of industry jargon. Jargon can be a valuable shorthand when communicating with people working in the same organisation or sector, but those working in different fields may struggle to make sense of it. So, contributors need to think carefully about how much jargon to use, and what needs to be explained."
  },
  {
    "objectID": "contributor-docs/style-guide.html#figuresgraphics",
    "href": "contributor-docs/style-guide.html#figuresgraphics",
    "title": "Style guide",
    "section": "Figures/graphics",
    "text": "Figures/graphics\nAll data visualisations and other graphical outputs directly related to the content of submissions must be presented neatly and cleanly (avoid chart junk). They should also be labelled correctly and legibly, with colours chosen carefully to ensure they can be easily distinguished by all readers. Accompanying captions must be written to support the reader’s understanding of the visual presentation (e.g., “Figure 1: a bar chart” is an insufficient description).\nIf contributors wish to use charts or graphs that are not their own work, they must ensure that such items are correctly sourced and referenced, and that permission to republish has been obtained. A letter or email confirming this permission is required."
  },
  {
    "objectID": "contributor-docs/style-guide.html#data-sources",
    "href": "contributor-docs/style-guide.html#data-sources",
    "title": "Style guide",
    "section": "Data sources",
    "text": "Data sources\nContributors must include within their submissions any links and/or references to the sources of data, code and/or software and software packages on which their analyses are based. We understand that some data sources may not be publicly available, whether for legal, ethical or commercial reasons. However, readers must still be told where the data come from, even if they are not able to access the data themselves."
  },
  {
    "objectID": "contributor-docs/style-guide.html#references",
    "href": "contributor-docs/style-guide.html#references",
    "title": "Style guide",
    "section": "References",
    "text": "References\nCitations are to be formatted in The Chicago Manual of Style author-date format."
  },
  {
    "objectID": "contributor-docs/style-guide.html#use-of-images",
    "href": "contributor-docs/style-guide.html#use-of-images",
    "title": "Style guide",
    "section": "Use of images",
    "text": "Use of images\nImages for general illustration purposes will be sourced and – where necessary and within reason – paid for by Real World Data Science.\n\n\n\n\n\n\n\nNote\n\n\n\nFor all other style-related matters, we follow The Guardian and Observer Style Guide."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact us",
    "section": "",
    "text": "Email: b.tarran@rss.org.uk\nGitHub: @realworlddatascience\nTwitter: @rwdatasci\nMastodon: @rwdatasci\nLinkedIn: RSS Real World Data Science"
  },
  {
    "objectID": "contact.html#advertising-and-commercial",
    "href": "contact.html#advertising-and-commercial",
    "title": "Contact us",
    "section": "Advertising and commercial",
    "text": "Advertising and commercial\nEmail: advertising@rss.org.uk"
  },
  {
    "objectID": "contact.html#the-royal-statistical-society",
    "href": "contact.html#the-royal-statistical-society",
    "title": "Contact us",
    "section": "The Royal Statistical Society",
    "text": "The Royal Statistical Society\nReal World Data Science is a project of the Royal Statistical Society (RSS). The Society was founded in 1834 and is one of the world’s leading organisations advocating for the importance of statistics and data.\nRSS has more than 10,000 members in the UK and across the world. As a charity, it advocates for the key role of statistics and data in society, and works to ensure that policy formulation and decision making are informed by evidence for the public good.\nTo support the work of the RSS, including Real World Data Science and other projects, become a member today.\nEmail: info@rss.org.uk"
  }
]