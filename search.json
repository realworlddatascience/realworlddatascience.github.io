[
  {
    "objectID": "ts-and-cs.html",
    "href": "ts-and-cs.html",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of Real World Data Science (RWDS), its editors, the Royal Statistical Society (RSS), or other partners and funders.\nRWDS has prepared the content of this website responsibly and carefully. However, RWDS, its editors, the RSS, or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nRWDS, its editors, the RSS, or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. RWDS, its editors, the RSS, or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nRWDS, its editors, the RSS, or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use."
  },
  {
    "objectID": "ts-and-cs.html#legal-disclaimer",
    "href": "ts-and-cs.html#legal-disclaimer",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of Real World Data Science (RWDS), its editors, the Royal Statistical Society (RSS), or other partners and funders.\nRWDS has prepared the content of this website responsibly and carefully. However, RWDS, its editors, the RSS, or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nRWDS, its editors, the RSS, or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. RWDS, its editors, the RSS, or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nRWDS, its editors, the RSS, or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use."
  },
  {
    "objectID": "ts-and-cs.html#site-content",
    "href": "ts-and-cs.html#site-content",
    "title": "Terms and conditions",
    "section": "Site content",
    "text": "Site content\nThis site and the “Real World Data Science” and “RWDS” brands and logos are copyright © The Royal Statistical Society.\nCopyright and licence terms for published articles and any associated videos, images, or other material can be found at the end of each article page. We make every reasonable effort to locate, contact and acknowledge copyright owners and wish to be informed by any copyright owners who are not properly identified and acknowledged on this website so that we may make any necessary corrections.\nWhere licence terms for individual articles, videos, images and other published content permit republication, you may do so in accordance with the stated terms of the respective licence(s).\nYou are not permitted to republish this site in its entirety."
  },
  {
    "objectID": "ts-and-cs.html#what-websites-do-we-link-to",
    "href": "ts-and-cs.html#what-websites-do-we-link-to",
    "title": "Terms and conditions",
    "section": "What websites do we link to?",
    "text": "What websites do we link to?\nRWDS editors and contributors recommend external web links on the basis of their suitability and usefulness for our users. Selection and addition of links to our website is entirely a matter for RWDS and for RWDS alone.\nIt is not our policy to enter into agreements for reciprocal links.\nThe inclusion of a link to an organisation’s or individual’s website does not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders of any product, service, policy or opinion of the organisation or individual. RWDS, its editors, the RSS, or other partners and funders are not responsible for the content of external websites."
  },
  {
    "objectID": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "href": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "title": "Terms and conditions",
    "section": "What websites will we not link to?",
    "text": "What websites will we not link to?\nWe will not link to websites that contain racist, sexual or misleading content; that promote violence; that are in breach of any UK law; which are otherwise offensive to individuals or to groups of people.\nThe decision of RWDS is final and no correspondence will be entered into.\nIf you wish to report a concern, please email b.tarran@rss.org.uk"
  },
  {
    "objectID": "ts-and-cs.html#software-and-services",
    "href": "ts-and-cs.html#software-and-services",
    "title": "Terms and conditions",
    "section": "Software and services",
    "text": "Software and services\nSource code and files for this site are available from GitHub. Use of our GitHub repository is governed by the Contributor Covenant Code of Conduct.\nThis site is built using Quarto, an open-source scientific and technical publishing system developed by Posit. Quarto source code and software licences are available from GitHub.\nReal World Data Science is hosted by GitHub Pages.\nThis site uses Google Analytics 4 for web analytics reporting.\nUser comments and reaction functionality is provided by giscus, a comments system powered by GitHub Discussions. Use of this comment functionality is governed by the Contributor Covenant Code of Conduct."
  },
  {
    "objectID": "ts-and-cs.html#notice-and-takedown-policy",
    "href": "ts-and-cs.html#notice-and-takedown-policy",
    "title": "Terms and conditions",
    "section": "Notice and Takedown policy",
    "text": "Notice and Takedown policy\nIf you are a rights holder and are concerned that you have found material on our site for which you have not given permission, or is not covered by a limitation or exception in national law, please contact us in writing stating the following:\n\nYour contact details.\nThe full bibliographic details of the material.\nThe exact and full url where you found the material.\nProof that you are the rights holder and a statement that, under penalty of perjury, you are the rights holder or are an authorised representative.\n\nContact details:\nNotice and Takedown,\nLicensing,\n12 Errol Street,\nLondon EC1Y 8LX\nweb@rss.org.uk\nUpon receipt of notification, the ‘Notice and Takedown’ procedure is then invoked as follows:\n\nWe will acknowledge receipt of your complaint by email or letter and will make an initial assessment of the validity and plausibility of the complaint.\nUpon receipt of a valid complaint the material will be temporarily removed from our website pending an agreed solution.\nWe will contact the contributor who deposited the material, if relevant. The contributor will be notified that the material is subject to a complaint, under what allegations, and will be encouraged to assuage the complaints concerned.\nThe complainant and the contributor will be encouraged to resolve the issue swiftly and amicably and to the satisfaction of both parties, with the following possible outcomes:\n\nThe material is replaced on our website unchanged.\nThe material is replaced on our website with changes.\nThe material is permanently removed from our website.\n\n\nIf the contributor and the complainant are unable to agree a solution, the material will remain unavailable through the website until a time when a resolution has been reached."
  },
  {
    "objectID": "ts-and-cs.html#contributor-covenant-code-of-conduct",
    "href": "ts-and-cs.html#contributor-covenant-code-of-conduct",
    "title": "Terms and conditions",
    "section": "Contributor Covenant Code of Conduct",
    "text": "Contributor Covenant Code of Conduct\n\nOur pledge\nWe as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\nOur standards\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\nEnforcement responsibilities\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\nScope\nThis Code of Conduct applies within all community spaces (encompassing this site, our GitHub repository, our social media channels, and any RWDS-organised online and offline events). It also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\nNote that unless prior permission is agreed in writing with the editor of RWDS, only the editor and editorial board of RWDS may officially represent the community. Comment to the media must only be given by appointed representatives and must be approved by the RSS press office.\n\n\nEnforcement\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at b.tarran@rss.org.uk. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\nEnforcement guidelines\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n1. Correction\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n2. Warning\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n3. Temporary Ban\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n4. Permanent Ban\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\nAttribution\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "about-rwds.html",
    "href": "about-rwds.html",
    "title": "Welcome to Real World Data Science",
    "section": "",
    "text": "Welcome to the home of Real World Data Science, a new project from the Royal Statistical Society. This site and its content are being developed by data science practitioners and leaders with a single goal in mind: to help you deliver high quality, ethical, impactful data science in your workplace."
  },
  {
    "objectID": "about-rwds.html#what-are-our-aims",
    "href": "about-rwds.html#what-are-our-aims",
    "title": "Welcome to Real World Data Science",
    "section": "What are our aims?",
    "text": "What are our aims?\nReal World Data Science aims to be a trusted, go-to source for high-quality, engaging and inspiring content which helps data science students, practitioners and leaders to:\n\ndiscover and learn more efficiently;\n\nacquire practical problem-solving skills;\n\nshare their knowledge and accomplishments publicly;\n\nwork smarter, ethically, and more effectively."
  },
  {
    "objectID": "about-rwds.html#what-we-provide",
    "href": "about-rwds.html#what-we-provide",
    "title": "Welcome to Real World Data Science",
    "section": "What we provide",
    "text": "What we provide\nResources are created to meet the needs of our target audiences. These include:\n\nCase studies – showing how data science is used to solve real-world problems in business, public policy and beyond.\nExplainers – interrogating the underlying assumptions and limitations of data science tools and methods, to help data scientists make smarter, more informed analytical choices.\nExercises – to challenge and develop the analytical mindset that all data scientists need to succeed.\n\nAdvice – interviews, Q&As, and FAQs on such topics as data science ethics, career paths, and communication, to support professional development.\n\nWe are also curating resources to help data scientists identify trustworthy, high-quality content. These include:\n\nTraining guides – step-by-step approaches and recommended sources for learning new skills and methods.\nDatasets – tagged and sorted to help educators and practitioners find data to meet their teaching and training needs.\nFeeds – who and what to follow to keep up with new ideas and developments."
  },
  {
    "objectID": "about-rwds.html#how-you-can-get-involved",
    "href": "about-rwds.html#how-you-can-get-involved",
    "title": "Welcome to Real World Data Science",
    "section": "How you can get involved",
    "text": "How you can get involved\nSee our open call for contributions."
  },
  {
    "objectID": "case-studies/posts/2023/06/15/road-to-reproducible-research.html",
    "href": "case-studies/posts/2023/06/15/road-to-reproducible-research.html",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "",
    "text": "Reproducibility, or “the ability of a researcher to duplicate the results of a prior study using the same materials as the original investigator”, is critical for sharing and building upon scientific findings. Reproducibility not only verifies the correctness of processes leading to results but also serves as a prerequisite for assessing generalisability to other datasets or contexts. This we refer to as replicability, or “the ability of a researcher to duplicate the results of a prior study if the same procedures are followed but new data are collected”. Reproducibility, which is the focus of our work here, can be challenging – especially in the context of deep learning. This article, and associated material, aims to provide practical advice for overcoming these challenges.\nOur story begins with Davit Svanidze, a master’s degree student in economics at the London School of Economics (LSE). Davit’s efforts to make his bachelor’s thesis reproducible are what inspires this article, and we hope that readers will be able to learn from Davit’s experience and apply those learnings to their own work. Davit will demonstrate the use of Jupyter notebooks, GitHub, and other relevant tools to ensure reproducibility. He will walk us through code documentation, data management, and version control with Git. And, he will share best practices for collaboration, peer review, and dissemination of results.\nDavit’s story starts here, but there is much more for the interested reader to discover. At certain points in this article, we will direct readers to other resources, namely a Jupyter notebook and GitHub repository which contain all the instructions, data and code necessary to reproduce Davit’s research. Together, these components offer a comprehensive overview of the thought process and technical implementation required for reproducibility. While there is no one-size-fits-all approach, the principles remain consistent."
  },
  {
    "objectID": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#davits-journey-towards-reproducibility",
    "href": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#davits-journey-towards-reproducibility",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "Davit’s journey towards reproducibility",
    "text": "Davit’s journey towards reproducibility\n\nMore power, please\nThe focus of my bachelor’s thesis was to better understand the initial spread of Covid-19 in China using deep learning algorithms. I was keen to make my work reproducible, but not only for my own sake. The “reproducibility crisis” is a well-documented problem in science as a whole,1 2 3 4 with studies suggesting that around one-third of social science studies published between the years 2010 and 2015 in top journals like Nature and Science could not be reproduced.5 Results that cannot be reproduced are not necessarily “wrong”. But, if findings cannot be reproduced, we cannot be sure of their validity.\nFor my own research project, I gathered all data and started working on my computer. After I built the algorithms to train the data, my first challenge to reproducibility was computational. I realised that training models on my local computer was taking far too long, and I needed a faster, more powerful solution to be able to submit my thesis in time. Fortunately, I could access the university server to train the algorithms. Once the training was complete, I could generate the results on my local computer, since producing maps and tables was not so demanding. However…\n\n\nBloody paths!\nIn switching between machines and computing environments, I soon encountered an issue with my code: the paths, or file directory locations, for the trained algorithms had been hardcoded! As I quickly discovered, hardcoding a path can lead to issues when the code is run in a different environment, as the path might not exist in the new environment.\nAs my code became longer, I overlooked the path names linked to algorithms that were generating the results. This mistake – which would have been easily corrected if spotted earlier – resulted in incorrect outputs. Such errors could have enormous (negative) implications in a public health context, where evidence-based decisions have real impacts on human lives. It was at this point that I realised that my code is the fundamental pillar of the validity of my empirical work. How can someone trust my work if they are not able to verify it?\nThe following dummy code demonstrates the hardcoding issue:\n```{python}\n# Hardcoded path\nfile_path = \"/user/notebooks/toydata.csv\"\ntry:\n    with open(file_path) as file:\n        data = file.read()\n        print(data)\nexcept FileNotFoundError:\n    print(\"File not found\")\n```\n\nIn the code above, a dummy file (toydata.csv) is used. The dummy file contains data on the prices of three different toys, but only the path of the file is relevant to this example. If the hardcoded file path – \"/user/notebooks/toydata.csv\" – exists on the machine being used, the code will run just fine. But, when run in a different environment without said path, the code will result in a \"File not found error\". Better code that uses relative paths can be written as:\n```{python}\n# Relative path\nimport os\n\nfile_path = os.path.join(os.getcwd(), \"toydata.csv\")\ntry:\n    with open(file_path) as file:\n        data = file.read()\n        print(data)\nexcept FileNotFoundError:\n    print(\"File not found\")\n```\n\nYou can see that this code has successfully imported data from the dataset toydata.csv and printed its two columns (toy and price) and three rows.\nThe following example is a simplified version of what happened when I wrote code to train several models, store the results and run a procedure to compare results with the predictive performance of a benchmark model:\n```{python}\n# Set an arbitrary predictive performance value of a benchmark model\n# and accept/reject models if the results are above/below the value.\nbenchmark = 50\n# Set the model details in one place for a better overview\nmodel = {\n    \"model1\": {\"name\": \"model1\", \"type\": \"simple\"}, \n    \"model2\": {\"name\": \"model2\", \"type\": \"complex\"}\n}\n# Set the current model to \"model1\" to use it for training and check its results\ncurrent_model = model[\"model1\"]\n# Train a simple model for \"model1\" and a complex model for \"model2\"\n# Training result of the \"model1\" is 30 and for \"model2\" is 70\nmodel_structure = train(current_model[\"type\"])\n# Save the model and its result in a .csv file\nmodel_structure.to_csv('/all/notebooks/results-of-model1.csv', index=False)\n```\n```{python}\n# Load the model result and compare with benchmark\nprint(\"Model name: {}\".format(current_model[\"name\"]))\nprint(\"Model type: {}\".format(current_model[\"type\"]))\n# Load the result of the current model\nresult = pd.read_csv('/all/notebooks/results-of-model2.csv').iloc[0, 0]\nprint(\"Result: {}\".format(result))\n\nif result &gt; benchmark:\n    print(\"\\033[3;32m&gt;&gt;&gt; Result is better than the benchmark -&gt; Accept the model and use it for calculations\")\nelse:\n    print(\"\\033[3;31m&gt;&gt;&gt; Result is NOT better than the benchmark -&gt; Reject the model as it is not optimal\")\n```\n\nEverything looks fine at a glance. But, if you examine the code carefully, you may spot the problem. Initially, when I coded the procedure (training the model, saving and loading the results), I hardcoded the paths and had to change them for each tested model. First, I trained model2, a complex model, and tested it against the benchmark (70 &gt; 50 → accepted). I repeated the procedure for model1 (a simple model). Its result was identical to model2, therefore I kept model1 following the parsimony principle.\nHowever, for the code line loading the result for the current model (line 5, second cell), I forgot to amend the path and so mistakenly loaded the result of model2. As a consequence, I accepted a model which should have been rejected. These wrong results were then spread further in the code, including all charts and maps and the conclusions of my analysis.\nA small coding error like this can therefore be fatal to an analysis. Below is the corrected code:\n```{python}\nimport os\n# Set an arbitrary predictive performance value of a benchmark model\n# and accept/reject models if the results are above/below the value.\nbenchmark = 50\n# Set the model details (INCLUDING PATHS) in one place for a better overview\nmodel = {\n    \"model1\": {\"name\": \"model1\", \"type\": \"simple\", \"path\": os.path.join(os.getcwd(), \"results-of-model1.csv\")}, \n    \"model2\": {\"name\": \"model2\", \"type\": \"complex\", \"path\": os.path.join(os.getcwd(), \"results-of-model2.csv\")}\n}\n# Set the current model to \"model1\" to use it for training and check its results\ncurrent_model = model[\"model1\"]\n# Train a simple model for \"model1\" and a complex model for \"model2\"\n# Training result of the \"model1\" is 30 and for \"model2\" is 70\nmodel_structure = train(current_model[\"type\"])\n# Save the model and its result in a .csv file\nmodel_structure.to_csv(current_model[\"path\"], index=False)\n```\n```{python}\n# Get the model result and compare with the benchmark\nprint(\"Model name: {}\".format(current_model[\"name\"]))\nprint(\"Model type: {}\".format(current_model[\"type\"]))\n# Load the result of the current model WITH a VARIABLE PATH\nresult = pd.read_csv(current_model[\"path\"]).iloc[0, 0]\nprint(\"Result: {}\".format(result))\n\nif result &gt; benchmark:\n    print(\"\\033[3;32m&gt;&gt;&gt; Result is better than the benchmark -&gt; Accept the model and use it for calculations\")\nelse:\n    print(\"\\033[3;31m&gt;&gt;&gt; Result is NOT better than the benchmark -&gt; Reject the model as it is not optimal\")\n```\n\nHere, the paths are stored with other model details (line 7–8, first cell). Therefore, we can use them as variables when we need them (e.g., line 16, first cell, and line 5, second cell). Now, when the current model is set to model1 (line 11, first cell), everything is automatically adjusted. Also, if the path details need to be changed, we only need to change them once and everything else is automatically adjusted and updated. The code now correctly states that model1 performs worse than the benchmark and is therefore rejected and we should keep model2, which performs best.\nI managed to catch this error in time, but it often can be difficult to spot our own mistakes. That is why making code available to others is crucial. A code review by a second (or third) pair of eyes can save everyone a lot of time and avoid spreading incorrect results and conclusions.\n\n\nSolving compatibility chaos with Docker\nOne might think that it would be easy to copy code from one computer to another and run it without difficulties, but it turns out to be a real headache. Different operating systems on my local computer and the university server caused multiple compatibility issues and it was very time-consuming to try to solve them. The university server was running on Ubuntu, a Linux distribution, which was not compatible with my macOS-based code editor. Moreover, the server did not support the Python programming language – and all the deep learning algorithm packages that I needed – in the same way as my macOS computer did.\nAs a remedy, I used Docker containers, which allowed me to create a virtual environment with all the necessary packages and dependencies installed. This way, I could integrate them with different hardware and use the processing power of that hardware. To get started with Docker, I first had to install it on my local computer. The installation process is straightforward and the Docker website provides step-by-step instructions for different operating systems. In fact, I found the Docker website very helpful, with lots of resources and tutorials available. Once Docker was installed, it was easy to create virtual environments for my project and work with my code, libraries, and packages, without any compatibility issues. Not only did Docker containers save me a lot of time and effort, but they could also make it easier for others to reproduce my work.\nBelow is an example of a Dockerfile which recreates an environment with Python 3.7 on Linux. It describes what, how, when and in which order operations should be carried out to generate the environment with all Python packages required to run the main Python script, main.py.\n\n\nAn example of a Dockerfile.\n\nIn this example, by downloading the project, including the Dockerfile, anyone can run main.py without installing packages or worrying about what OS was used for development or which Python version should be installed. You can view Docker as a great robot chef: show it a recipe (Dockerfile), provide the ingredients (project files), push the start button (to build the container) and wait to sample the results.\n\n\nWhy does nobody check your code?\nEven after implementing Docker, I still faced another challenge to reproducibility: making the verification process for my code easy enough that it could be done by anyone, without them needing a degree in computer science! Increasingly, there is an expectation for researchers to share their code so that results can be reproduced, but there are as yet no widely accepted or enforced standards on how to make code readable and reusable. However, if we are to embrace the concept of reproducibility, we must write and publish code under the assumption that someone, somewhere – boss, team member, journal reviewer, reader – will want to rerun our code. And, if we expect that someone will want to rerun our code (and hopefully check it), we should ensure that the code is readable and does not take too long to run.\nIf your code does take too long to run, some operations can often be accelerated – for example, by reducing the size of the datasets or by implementing computationally efficient data processing approaches (e.g., using PyTorch). Aim for a running time of a few minutes – or about as long as it takes to make a cup of tea or coffee. Of course, if data needs to be reduced to save computational time, the person rerunning your code won’t generate the same results as in your original analysis. This therefore will not lead to reproducibility, sensu stricto. However, as long as you state clearly what are the expected results from the reduced dataset, your peers can at least inspect your code and offer feedback, and this marks a step towards reproducibility.\nWe should also make sure our code is free from bugs – both the kind that might lead to errors in analysis and also those that stop the code running to completion. Bugs can occur for various reasons. For example, some code chunks written on a Windows machine may not properly execute on a macOS machine because the former uses \\ for file paths, while the latter uses /:\n```{python}\n# Path works on macOS/Linux\nwith open(\"../../all/notebooks/toydata.csv\", \"r\") as f:\n    print(f.read())\n\n# Path works only on Windows    \nwith open(r\"..\\..\\all\\notebooks\\toydata.csv\", \"r\") as f:\n   print(f.read())\n```\n\nHere, only the macOS/Linux version works, since the code this capture was taken from was implemented on a Linux server. There are alternatives, however. The code below works on macOS, Linux, and also Windows machines:\n```{python}\nfrom pathlib import Path\n\n# Path works on every OS: macOS/Linux/Windows\n# It will automatically replace the path to \"..\\..\\all\\notebooks\\toydata.csv\" when it runs on Windows\nwith open(Path(\"../../all/notebooks/toydata.csv\"), \"r\") as f:\n    print(f.read())\n```\n\nThe extra Python package, pathlib, is of course unnecessary if you build a Docker container for your project, as discussed in the previous section.\n\n\nJupyter, King of the Notebooks\nBy this stage in my project, I was feeling that I’d made good progress towards ensuring that my work would be reproducible. I’d expended a lot of effort to make my code readable, efficient, and also absent of bugs (or, at least, this is what I was hoping for). I’d also built a Docker container to allow others to replicate my computing environment and rerun the analysis. Still, I wanted to make sure there were no barriers that would prevent people – my supervisors, in particular – from being able to review the work I had done for my undergraduate thesis. What I wanted was a way to present a complete narrative of my project that was easy to understand and follow. For this, I turned to Jupyter Notebook.\n\n\n\n\n\n\nCredit: Discord software, Midjourney bot.\n\nJupyter notebooks combine Markdown text, code, and visualisations. The notebook itself can sit within an online directory of folders and files that contain all the data and code related to a project, allowing readers to understand the processes behind the work and also access the raw resources. From the notebook I produced, readers can see exactly what I did, how I did it, and what my results were.\nWhile creating my notebook, I was able to experiment with my code and iterate quickly. Code cells within a document can be run interactively, which allowed me to try out different approaches to solving a problem and see the results almost in real time. I could also get feedback from others and try out new ideas without having to spend a lot of time writing and debugging code.\n\n\nVersion control with Git and GitHub\nMy Jupyter notebook and associated folders and files are all available via GitHub. Git is a version control system that allows you to keep track of changes to your code over time, while GitHub is a web-based platform that provides a central repository for storing and sharing code. With Git and GitHub, I was able to version my code and collaborate with others without the risk of losing any work. I really couldn’t afford to redo the entire year I spent on my dissertation!\nGit and GitHub are great for reproducibility. By sharing code via these platforms, others can access your work, verify it and reproduce your results without risking changing or, worse, destroying your work – whether partially or completely. These tools also make it easy for others to build on your work if they want to further develop your research. You can also use Git and GitHub to share or promote your results across a wider community. The ability to easily store and share your code also makes it easy to keep track of the different versions of your code and to see how your work has evolved.\nThe following illustration shows the tracking of very simple changes in a Python file. The previous version of the code is shown on the left; the new version is shown on the right. Additions and deletions are highlighted in green and red, and with + and - symbols, respectively.\n\n\nA simple example of GitHub version tracking."
  },
  {
    "objectID": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#the-deep-learning-challenge",
    "href": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#the-deep-learning-challenge",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "The deep learning challenge",
    "text": "The deep learning challenge\nSo far, this article has dealt with barriers to reproducibility – and ways around them – that will apply to most, if not all, modern research projects. While I’d encourage any scientist to adopt these practices in their own work, it is important to stress that these alone cannot guarantee reproducibility. In cases where standard statistical procedures are used within statistical software packages, reproducibility is often achievable. However, in reality, even when following the same procedures, differences in outputs can occur, and identifying the reasons for this may be challenging. Cooking offers a simple analogy: subtle changes in room temperature or ingredient quality from one day to the next can impact the final product.\nOne of the challenges for research projects employing machine learning and deep learning algorithms is that outputs can be influenced by the randomness that is inherent in these approaches. Consider the four portraits below, generated by the Midjourney bot.\n\n\n\n\n\n\nCredit: Discord software, Midjourney bot.\n\nEach portrait looks broadly similar at first glance. However, upon closer inspection, critical differences emerge. These differences arise because deep learning models rely on numerous interconnected layers to learn intricate patterns and representations. Slight random perturbations, such as initial parameter values or changes in data samples, can propagate through the network, leading to different decisions during the learning process. As a result, even seemingly negligible randomness can amplify and manifest as considerable differences in the final output, as with the distinct features of the portraits.\nRandomness is not necessarily a bad thing – it mitigates overfitting and helps predictions to be generalised. However, it does present an additional barrier to reproducibility. If you cannot get the same results using the same raw materials – data, code, packages and computing environment – then you might have good reasons to doubt the validity of the findings.\nThere are many elements of an analysis in which randomness may be present and lead to different results. For example, in a classification (where your dependent variable is binary, e.g., success/failure with 1 and 0) or a regression (where your dependent variable is continuous, e.g., temperature measurements of 10.1°C, 2.8°C, etc.), you might need to split your data into training and testing sets. The training set is used to estimate the model (hyper)parameters and the testing set is used to compute the performance of the model. The way the split is usually operationalised is as a random selection of rows of your data. So, in principle, each time you split your data into training and testing sets, you may end up with different rows in each set. Differences in the training set may therefore lead to different values of the model (hyper)parameters and affect the predictive performance that is measured from the testing set. Also, differences in the testing set may lead to variations in the predictive performance scores, which in turn lead to potentially different interpretations and, ultimately, decisions if the results are used for that purpose.\nThis aspect of randomness in the training of models is relatively well known. But randomness may hide in other parts of code. One such example is illustrated below. Here, using Python, we set the seed number to 0 using np.random.seed(seed value). The random.seed() function from the package numpy (abbreviated np) saves the state of a random function so that it can create identical random numbers independently of the machine you use, and this is for any number of executions. A seed value is an initial input or starting point used by a pseudorandom number generator to generate a sequence of random numbers. It is often an integer or a timestamp. The number generator takes this seed value and uses it to produce a deterministic series of random numbers that appear to be random but can be recreated by using the same seed value. Without providing this seed value, the first execution of the function typically uses the current system time. The animation below generates two random arrays arr1 and arr2 using np.random.rand(3,2). Note that the values 3,2 indicate that we want random values for an array that has 3 rows and 2 columns.\n```{python}\nimport numpy as np\n\n#Set the seed number e.g. to 0\nnp.random.seed(0)\n# Generate random array\narr1 = np.random.rand(3,2)\n## print(\"Array 1:\")\n## print(arr1)\n\n#Set the seed number as before to get the same results\nnp.random.seed(0)\n# Generate another random array\narr2 = np.random.rand(3,2)\n## print(\"\\nArray 2:\")\n## print(arr2)\n```\n\nIf you run the code yourself multiple times, the values of arr1 and arr2 should remain identical. If this is not the case, check that the seed value is set to 0 in lines 4 and 11. These identical results are possible because we set the seed value to 0, which ensures that the random number generator produces the same sequence of numbers each time the code is run. Now, let’s look at what happens if we remove the line np.random.seed(0):\n```{python}\n#Generate random array\narr1 = np.random.rand(3,2)\n## print(\"Array 1:\")\n## print(arr1)\n\n#Generate another random array\narr2 = np.random.rand(3,2)\n## print(\"\\nArray 2:\")\n## print(arr2)\n```\n\nHere, the values of arr1 and arr2 will be different each time we run the code since the seed value was not set and is therefore changing over time.\nThis short code demonstrates how randomness that can be controlled by the seed value may affect your code. Therefore, unless randomness is required, e.g., to get some uncertainty in the results, setting the seed value will contribute to making your work reproducible. I also find it helpful to document the seed number I use in my code so that I can easily reproduce my findings in the future. If you are currently working on some code that involves random number generators, it might be worth checking your code and making all necessary changes. In our work (see code chunk 9 in the Jupyter notebook) we set the seed value in a general way, using a framework (config) so that our code always uses the same seed to train our algorithm."
  },
  {
    "objectID": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#conclusion",
    "href": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#conclusion",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "Conclusion",
    "text": "Conclusion\nWe hope you have enjoyed learning more about our quest for reproducibility. We have explained why reproducibility matters and provided tips for how to achieve it – or, at least, work towards it. We have introduced a few important issues that you are likely to encounter on your own path to reproducibility. In sum, we have mentioned:\n\nThe importance of having relative instead of hard-coded paths in code.\nOperating system compatibility issues, which can be solved by using Docker containers for a consistent computing environment.\nThe convenience of Jupyter notebooks for code editing – particularly useful for data science projects and work using deep learning because of the ability to include text and code in the same document and make the work accessible to everyone (so long as they have an internet connection).\nThe need for version control using, for example, Git and GitHub, which allows you to keep track of changes in your code and collaborate with others efficiently.\nThe importance of setting the seed values in random number generators.\n\nThe graphic below provides a visual overview of the different components of our study and shows how each component works with the others to support reproducibility.\n\n\n\n\n\nWe use (A) the version control system, Git, and its hosting service, GitHub, which enables a team to share code with peers, efficiently track and synchronise code changes between local and server machines, and reset the project to a working state in case something breaks. Docker containers (B) include all necessary objects (engine, data, and scripts). Docker needs to be installed (plain-line arrows) by all users (project leader, collaborator(s), reviewer(s), and public user(s)) on their local machines (C); and (D) we use a user-friendly interface (JupyterLab) deployed from a local machine to facilitate the operations required to reproduce the work. The project leader and collaborators can edit (upload/download) the project files stored on the GitHub server (plain-line arrows) while reviewers and public users can only read the files (dotted-line arrows).\nNow, it is over to you. Our Jupyter notebook provides a walkthrough of our research. Our GitHub repository has all the data, code and other files you need to reproduce our work, and this README file will help you get started.\nAnd with that, we wish you all the best on the road to reproducibility!\n\nFind more case studies\n\n\n\n\n\nAbout the authors\n\nDavit Svanidze is a master’s degree student in economics at the London School of Economics (LSE). Andre Python is a young professor of statistics at Zhejiang University’s Center for Data Science. Christoph Weisser is a senior data scientist at BASF. Benjamin Säfken is professor of statistics at TU Clausthal. Thomas Kneib is professor of statistics and dean of research at the Faculty of Business and Economic Sciences at Goettingen University. Junfen Fu is professor of pediatrics, chief physician and director of the Endocrinology Department of Children’s Hospital, Zhejiang University, School of Medicine.\n\n\n\n\n\nAcknowledgement\n\nAndre Python has been funded by the National Natural Science Foundation of China (82273731), the National Key Research and Development Program of China (2021YFC2701905) and Zhejiang University global partnership fund (188170-11103).\n\n\n\n\n\nCopyright and licence\n\n© 2023 Davit Svanidze, Andre Python, Christoph Weisser, Benjamin Säfken, Thomas Kneib, and Junfen Fu.\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nSvanidze, Davit, Andre Python, Christoph Weisser, Benjamin Säfken, Thomas Kneib, and Junfen Fu. 2023. “The road to reproducibility: hazards to avoid and tools to get you there safely.” Real World Data Science, June 15, 2023. URL"
  },
  {
    "objectID": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#footnotes",
    "href": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#footnotes",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "References",
    "text": "References\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational Science.” Science 334 (6060): 1226–1227.↩︎\nIoannidis, John P. A., Sander Greenland, Mark A. Hlatky, Muin J. Khoury, Malcolm R. Macleod, David Moher, Kenneth F. Schulz, and Robert Tibshirani. 2014. “Increasing Value and Reducing Waste in Research Design, Conduct, and Analysis.” The Lancet 383 (9912): 166–175.↩︎\nOpen Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716.↩︎\nBaker, Monya. 2016. “Reproducibility Crisis?” Nature 533 (26): 353–366.↩︎\nCamerer, Colin F., Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, Gideon Nave, Brian A. Nosek, Thomas Pfeiffer, et al. 2018. “Evaluating the Replicability of Social Science Experiments in Nature and Science between 2010 and 2015.” Nature Human Behaviour 2: 637–644.↩︎"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/02/14/digital-skills.html",
    "href": "viewpoints/editors-blog/posts/2023/02/14/digital-skills.html",
    "title": "Data science can help close the ‘digital skills’ gap, or so it seems",
    "section": "",
    "text": "Digital skills. We all need them. Employers say they want them, but there aren’t enough to go around. Supply can’t meet demand, so we’re left with a gap – a digital skills gap. But what are digital skills exactly?\nThis is a question that was asked repeatedly, in various different constructions, by Stephen Metcalfe MP, chairing a meeting of the Parliamentary and Scientific Committee on Tuesday, February 7. I went along to the meeting as an observer, hoping to hear an answer to that very question.\nWhat I got was several different answers – no single solid definition, but a reasonable sense that boosting data science skills would go a long way towards closing the digital skills gap."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/02/14/digital-skills.html#survey-says",
    "href": "viewpoints/editors-blog/posts/2023/02/14/digital-skills.html#survey-says",
    "title": "Data science can help close the ‘digital skills’ gap, or so it seems",
    "section": "Survey says…",
    "text": "Survey says…\nThe committee meeting was sponsored by the Institution of Engineering and Technology (IET), and the main focus of discussion was the results of IET’s skills for a digital future survey, based on a YouGov poll of 1,235 respondents drawn from engineering employers (defined as “employers who employ at least one engineering and technology employee in the UK”).\n\n\n\n\n\nDigital skills, including AI skills, are not only required of engineers, says the IET’s Graham Herries. Generative AI tools like Stable Diffusion threaten to shake-up the creative industries. (Photo by Billetto Editorial on Unsplash)\n\n\nKicking off the discussion was Graham Herries, an engineering director and chair of the IET’s Innovation and Skills Panel, who drew attention to the harms that the digital skills gap is reportedly having. Of those respondents who identified skills gaps in their own organisations, 49% pointed to a reduction in productivity, while 35% said skills shortages were restricting company growth.\nAs the hot topic of the day, ChatGPT inevitably came up during the discussion. Herries sees it as a disruptive force, and 36% of all respondents believe artificial intelligence (AI) skills will be important for their engineers to have within five years (24% say they are important now). But AI skills are important for non-engineers too, argued Herries, as he pointed to stirrings in the creative industries caused by generative art tools such as Stable Diffusion.\nHerries therefore puts AI skills under the broad umbrella of “digital skills”. But, to him, it’s not enough to simply be able to use AI technology; rather, users should know enough to be able to ask the right questions about the provenance of the data used to train the AI, its quality and biases, etc. This was a point developed further by Yvonne Baker, an engineer and the CEO of STEM Learning. Baker talked about digital skills as being both the ability to use digital technology and also to understand its limitations. Yet another perspective was offered by Rab Scott, director of industrial digitalisation at the University of Sheffield’s Advanced Manufacturing Research Centre. Scott defined digital skills in the context of quality control systems in industry 4.0: it’s about knowing how and where to place a sensor to collect data about the manufacturing process, to feed that data into a data collection system, analyse the data for insights, and use those insights to inform decision-making."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/02/14/digital-skills.html#closing-the-gap",
    "href": "viewpoints/editors-blog/posts/2023/02/14/digital-skills.html#closing-the-gap",
    "title": "Data science can help close the ‘digital skills’ gap, or so it seems",
    "section": "Closing the gap",
    "text": "Closing the gap\nFurther definitions of “digital skills” are to be found in the IET’s published report. Survey respondents were encouraged to describe the term in their own words, so we see things like:\n\n“the ability to understand, process and analyse data.”\n“Coding, programming, software design, use of social media for marketing and communicating with stakeholders, data visualisation, work that relies solely on the use [of] online systems.”\n\nWhen respondents were asked what skills were lacking in both the external labour market and their internal workforce, around a fifth cited “more complex numerical/statistical skills and understanding”. And when looking to the future and to the skills anticipated to be important areas for growth in the next five years, 39% of respondents picked “data analytics” while 31% said “artificial intelligence and machine learning”.\nSo, perhaps you now understand why I left the meeting with the feeling that more data science skills, more data science training, could help address the shortfall in “digital skills”.\nBut how exactly can we equip more people with the right skills? At one point during the discussion, Metcalfe told the meeting that he was still looking for a key takeaway, something he could take to the Secretary of State and say, ‘This is what we need to embed in the curriculum’. What was offered instead was a range of possible solutions.\nThe IET survey found broad backing for government support for reskilling: 40% of respondents favoured grants or loans for training (and retraining) programmes, 39% would like more funding for apprenticeships, while 33% think there should be better carers advice and guidance in schools and colleges.\nBaker also made the case for digital skills to be taught in schools as part of every subject, not just in computer science lessons, and that teachers would need to be supported to deliver this.\nBut how would you close the “digital skills” gap, if given the chance?\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Data science can help close the ‘digital skills’ gap, or so it seems.” Real World Data Science, February 14, 2023. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/01/18/rwds-at-rss-conference.html",
    "href": "viewpoints/editors-blog/posts/2023/01/18/rwds-at-rss-conference.html",
    "title": "We’re taking Real World Data Science on the road",
    "section": "",
    "text": "Real World Data Science has booked its first conference appearance! This September, we’ll be part of the data science stream of the RSS International Conference.\nOur session, “Real World Data Science Live”, will feature talks and discussions based on content published on this site. In particular, we’re looking to share compelling examples of how data science is being used to solve real-world problems.\nIf you’re thinking about contributing to Real World Data Science, or have already made a submission, do let us know whether you’d be interested in taking part in this in-person event. There are only a handful of speaker slots available, so please get in touch ASAP!\nThe conference takes place 4-7 September 2023, in Harrogate, Yorkshire. Keynote speakers include Anuj Srivastava, a Florida State University professor with research interests in statistical computer vision, functional data analysis, and shape analysis, and other invited topic sessions in the data science stream are:\n\nGitHub: Version control for research, teaching and industry\nSurrogate-assisted uncertainty quantification of complex computer models\nGetting your work to work\nBest practices for the analysis and visualisation of Google Trends data\n\nSee the RSS International Conference 2023 website for more details.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “We’re taking Real World Data Science on the road.” Real World Data Science, January, 18 2023. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html",
    "href": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "",
    "text": "ChatGPT is, right now, the world’s most popular - and controversial - chatbot. Users have been both wowed by its capabilities1 and concerned by the confident-sounding nonsense it can produce.\nBut perhaps what impresses most is the way it is able to sustain a conversation. When I interviewed our editorial board member Detlef Nauck about large language models (LLMs), back in November, he said:\nFast-forward a couple of months and, as discussed in our follow-up interview below, OpenAI, the makers of ChatGPT, have succeeded in building a question answering system that can sustain a dialogue. As Nauck says: “I have not yet seen an example where [ChatGPT] lost track of the conversation… It seems to have quite a long memory, and doing quite well in this.”\nThere are still major challenges to overcome, says Nauck - not least the fact that ChatGPT has no way to verify the accuracy or correctness of its outputs. But, if it can be linked to original sources, new types of search engines could follow.\nCheck out the full conversation below or on YouTube.\nDetlef Nauck is a member of the Real World Data Science editorial board and head of AI and data science research for BT’s Applied Research Division."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#timestamps",
    "href": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#timestamps",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Timestamps",
    "text": "Timestamps\n\nHow ChatGPT was built and trained (0:41)\nChatGPT’s major advance (3:05)\nThe big problems with large language models (4:36)\nSearch engines and chatbots (9:35)\nQuestions for OpenAI and other model builders (11:29)"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#quotes",
    "href": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#quotes",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Quotes",
    "text": "Quotes\n“[OpenAI] have achieved quite remarkable capabilities in terms of sustaining conversations, and producing very realistic sounding responses… But sometimes [ChatGPT] makes silly mistakes. Sometimes the mistakes are not that obvious. It can hallucinate content… And it still doesn’t know what it’s talking about. It has no knowledge representation, doesn’t have a word model. And it’s just a statistical language model.” (2:04)\n“These models, they produce an answer, which is based on the kind of texts that they have been trained on. And that can be quite effective. But it cannot yet link back to an original source. So what’s still missing is the step where it says, ‘Okay, this my answer to your question, and here’s some evidence.’ As soon as they have done this, then these kinds of systems will probably replace the search engines that we’re used to.” (4:07)\n“[These large language models are] still too big and too expensive to run… For [use in a] contact centre or similar, what you need is a much smaller model that is restricted in terms of what it can say. It should have knowledge representation, so it gives correct answers. And it doesn’t need to speak 48 languages and be able to produce programming code. It only needs to be able to talk about a singular domain, where the information, the knowledge about the domain, has been carefully curated and prepared. And that’s what we’re not seeing yet. Can we build something like this, much smaller, much more restricted, and provably correct, so we can actually use the output?” (7:49)\n“We are seeing communities who don’t necessarily have the technical background to judge the capabilities of these models, but see the opportunities for their own domain and might be acting too fast in adopting them. So the producer of these models has a certain responsibility to make sure that this doesn’t happen.” (12:26)"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#further-reading",
    "href": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#further-reading",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Further reading",
    "text": "Further reading\n\nChatGPT: The Robot, the Myth, the Legend - Philadelphia Physicist blog, January 13, 2023\nCost to run ChatGPT - tweet by OpenAI CEO Sam Altman, December 5, 2022\nGoogle execs warn company’s reputation could suffer if it moves too fast on AI-chat technology - CNBC, December 13, 2022\nMicrosoft reportedly to add ChatGPT to Bing search engine - The Guardian, January 5, 2023\nGetty Images is suing the creators of AI art tool Stable Diffusion for scraping its content - The Verge, January 17, 2023"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#transcript",
    "href": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#transcript",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove repetitions.\n\n\n\nBrian Tarran\nWe’re following up today Detlef on the, I guess, one of the biggest stories in artificial intelligence and data science at the moment, ChatGPT, the chat bot that’s driven by a large language model and is proving endless amounts of– providing endless amounts of either entertainment or concern, depending on what you ask it, and what outputs you get. So, but you’ve been looking at it in some detail, right, ChatGPT. And that’s why I thought we would follow up and have a conversation to see, get your view on it, get your take on it. What’s going on?\nDetlef Nauck\nYeah. So, what they have done is, OpenAI have used their large language model GPT-3 and they have trained an instance to basically answer questions and have conversations, where the model remembers what has been said in the conversation. And they have done this by using curated data of question and answers, where they basically have posed a question and said, This is what the answer should be. They trained the system on doing this, then, in the next step, they began use questions, potentially different ones, the system came up with a variety of answers, and then again, human curators would mark which is the best answer. And they would use this data to train what’s called a reward model - so, a separate deep network that learns what kind of answer for a particular question is a good one - and then they would use this reward model to do additional reinforcement learning on the ChatGPT that they had built so far, basically using dialogues and the reward model would then either reward or penalise the response that comes out of the system. And by doing that they have achieved quite remarkable capabilities in terms of sustaining conversations, and producing kind of very realistic sounding kind of responses. Sounds all very convincing. The model presents its responses quite confidently. But sometimes it makes silly mistakes. Sometimes the mistakes are not that obvious. It can hallucinate content. So let’s say you ask it to write you scientific text about whatever topic and put some references in and these references are typically completely fabricated and not real. And it still doesn’t know what it’s talking about. It has no knowledge representation, doesn’t have a word model. And it’s just a statistical language model. So it’s what we would call a sequence to sequence model. It uses an input sequence, which are words, and then guesses what’s the next most likely word in the sequence. And then it continues building these sequences.\nBrian Tarran\nYeah. But, do you think the big advance as you see it is the way it’s able to remember or store some knowledge, if you like, of the conversation, because that was something that came out of our first conversation that we had, where you were saying that, you know, if you’re looking at these as a potential chatbots for customer service lines, or whatever it might be, actually, the trees, the conversation trees break down after a while, and they don’t, you know, these models get lost, but actually, they’re able to maintain it a little longer, are they, or– ?\nDetlef Nauck\nYeah, I have not yet seen an example where they lost track of the conversation they seem to have, it seems to have quite a long memory, and doing quite well in this. So the main capability here is they have built a question answering system. And that’s kind of the ultimate goal for search engines. So if you put something into Google, essentially, you have a question, show me something that answered this, answers this particular question. Of course, what you want this kind of an original source. And these models, they produce an answer, which is based on the kind of texts that they have been trained on. And that can be quite effective. But it cannot yet link back to an original source. So what’s still missing is the step where it says, Okay, this my answer to your question, and here’s some evidence. Then if, as soon as they have done this, then these kinds of systems will probably replace the search engines that we’re used to.\nBrian Tarran\nYeah. The other thing that struck me with them was that the, if you’re asking somebody a question - a human, you know, for instance - you expect a response that and you would hope you will be able to trust that response, especially if it’s someone in an expert position or someone you’re calling, you know, on behalf of a company or something. The fact that - and I asked this question of ChatGPT itself - and the response was, again, you should consult external sources to verify the information that’s been provided by the chatbot. So it’s like, I guess that leaves a question as to what the utility of it is, if you if you’re always having to go elsewhere to verify that information.\nDetlef Nauck\nYeah, I mean, that’s the main problem with these models, because they don’t have a knowledge representation. They don’t have a word model, they can’t fall back on facts that are represented as being true and present those. They come up with an answer. But I mean, there has been a lot of kind of pre-prompting going in to ChatGPT. So when you start writing something, the session has already been prompted with a lot of text, telling the model how to behave, what not to say, to avoid certain topics. There are additional moderation APIs running that make sure that you can’t create certain type of responses, which are based on classical text filtering, and topic filtering. So they try to kind of restrict what the model can do to make sure it’s not offensive or inappropriate. But that is limited. So through crafting your requests, intelligently, you can convince it to ignore all of these things and go past it in some instances. So the, it’s not yet perfect, and certainly it’s not authoritative. So you can’t trust the information if you’re not an expert yourself. So at the moment, I’d say these kind of models are really useful for experts who can judge the correctness of the answer. And then what you get this kind of maybe a helpful kind of text representation of something that you would have to write yourself otherwise.\nBrian Tarran\nYeah, and certainly conversations I’ve had with people, those who kind of work, maybe in creative industries, are finding them quite intriguing, in terms of things like, you know, maybe trying to come up with some clever tweets or something for a particular purpose, or something I want to try out is getting ChatGPT to write headlines for me, because it’s always my least favourite part of the editing job. So that sort of works. But you know, for you, in your position in the industry, has ChatGPT changed your mind at all about, you know, the way you’re perceiving these models and how they might be used? Or is it is it just kind of a next step along in the process of what you’d expect to see before these can become tools that we use?\nDetlef Nauck\nYeah, it’s the next step in the evolution of these models. They’re still too big and too expensive to run, right. So now, it is not quite clear how much it costs OpenAI to run the service that they’re currently running. So you see estimates around millions of dollars per day that they have to spend on running the compute infrastructure to serve all of these questions. And this is not quite clear, the only official piece of information that I’ve seen is in a tweet, where the CEO said, a single question costs in the order of single digit cents, but we have no idea how many questions they serve per day, and therefore how much money they are spending. If you want to run a contact centre, or something like this, it all depends on how much compute need to stand up to be able to respond to hundreds or thousands of questions in parallel. And then obviously, if you can’t trust that the answer is correct, it is of no use. So for making use in the service industry for contact centre or similar, what you need is a much smaller model that is restricted in terms of what it can say, it should have knowledge representation, so it gives correct answers. And it doesn’t need to speak 48 languages and be able to produce programming code, it only needs to be able to talk about a singular domain, where it kind of the information, the knowledge about the domain has been carefully curated and prepared. And that’s what we’re not seeing yet. Can we build something like this, much smaller, much more restricted, and kind of provably correct, so we can actually use the output?\nBrian Tarran\nYeah. Can we go back just to the point you mentioned earlier about, you know, the, the potential of like linking these sorts of chatbots up with search engines, you know, like Google? There’s been some conversations and reporting around, you know, what breakthroughs or not Google might have made in this regard. I mean, have you got any perspective on that area of work and how far along that is maybe and what the challenges are to get to that point?\nDetlef Nauck\nWell, Google has its own large language model, LaMDA. And we have seen an announcement that Microsoft wants to integrate ChatGPT into Bing, their search engine. And, but as I said before, what’s missing is the link to original sources. So you, coming up with a response is nice. But you need to be able to back it up, you need to say, Okay, this is my response, and I’m confident that this is correct, because here are some references. If I compare my response to these references, then they essentially mean the same thing. This is kind of what you need to be able to do. And we haven’t seen this step yet. But I’m certain that the search engine providers are hard at work at doing this because that’s essentially what they want. If you do a search in Google, in some instances, you’ll see a side panel where you get detailed information. Let’s say you ask about what’s the capital of Canada, you get a response, you get the information in more detail, you get links to Wikipedia, where they retrieve content from and present this as the response. And this is done through knowledge graphs. And so if these kinds of knowledge graphs grow together with these kind of large language models, then we will see new types of search engines.\nBrian Tarran\nOkay. I guess final, my final question for you, Detlef, and there might be other angles that you want to explore. But it’s like, are there questions that, you know, if you if you could sit down with OpenAI to talk about ChatGPT and what they’ve done, and what they plan to do next with it, what are the kinds of things that are bubbling away at the top of your mind?\nDetlef Nauck\nWell, one thing is controlling the use of these models, right? If you let them loose on the public, with an open API that anybody can use, you will see a proliferation of applications on top of it. If you go on YouTube, and you Google ChatGPT and health, you’ll already find discussions where GPs discuss, Oh, that is the next step of automated doctors that we can use. So they believe that the responses from these systems can be used for genuine medical advice. And that’s clearly a step too far. So we are seeing communities who don’t necessarily have the technical background to judge the capabilities of these models, but see the opportunities for their own domain and might be acting too fast in adopting them. So the producer of these models has a certain responsibility to make sure that this doesn’t happen. And I don’t know how they want to control this. And, so my question at the developers of these models would be how do you handle sustainability, because the trend goes to ever bigger models. So there’s, in some parts of the industry, there’s the belief, if you make them big enough you get artificial general intelligence, which I don’t believe is possible with these models. But this is definitely a trend that pushes the size of the models. The kind of, the idea of having just one model that can speak all the languages, can produce questions, answers, programming code, is obviously appealing. So you don’t want to build many models. Ideally, you have only one. But how is that supposed to work? And how do you embed actual word knowledge and word models into these systems so that you can verify what comes out?\nBrian Tarran\nYeah. I mean, the ethical dimension that you mentioned in the first part of your response is an important one, I think, in the sense that– but I guess maybe almost redundant in the sense that it’s already out there; you can’t put ChatGPT back in the box, can we, essentially?\nDetlef Nauck\nWell, it’s expensive to run so charging enough for access will put a lid on some frivolous use cases, but still, it needs to be controlled better. And you can make a jump to an AI regulation. So far, we only thought about regulating automated decision making, or automated classification. We also have to think about the automatic creation of digital content or automatic creation of software, which is possible through these models or the other generative AI models like diffusers. So how do we handle the creation of artificial content that looks like real content?\nBrian Tarran\nYeah. And there’s also I think, something I picked up yesterday, there was reports of a case being filed by, I think, Getty Images against the creators of one of these generative art models because they’re saying, you know, that you’ve used our data or you’ve used our image repositories essentially to train this model and it is now producing, you know, it’s producing its own outputs that’s based on this, and I guess there’s an argument of it being a copyright infringement case. And I think that’ll be quite interesting to watch to see how that does change the conversation around - yeah - fair use of that data that is available. You can find these images publicly, but you have to pay to use them for purposes other than just browsing, I guess. Yeah, it’ll be interesting to watch.\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification.” Real World Data Science, January, 27 2023. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#footnotes",
    "href": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#footnotes",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI asked ChatGPT to write this article’s headline, for example. I typed in “Can you write a headline for this text:” and then copy/pasted the interview transcript into the dialogue box. It first came up with, “AI Chatbot ChatGPT Proves Capable in Sustaining Conversations but Lacks Knowledge Representation and Original Sources for Verification”. I then asked it to shorten the headline to 10 words. It followed up with, “ChatGPT: Large Language Model-Driven Chatbot Proves Capable But Limited”.↩︎"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/03/15/AI-screening.html",
    "href": "viewpoints/editors-blog/posts/2023/03/15/AI-screening.html",
    "title": "OpenAI’s text classifier won’t calm fears about AI-written homework",
    "section": "",
    "text": "When ChatGPT launched in December 2022, it wasn’t long before users highlighted the tool’s potential as a homework aid. Pop an essay question into ChatGPT’s prompt box or feed your creative writing task to the AI instead, et voila – your work is done!\nIn reality, of course, things aren’t quite so simple. ChatGPT, like other large language models, has an unfortunate habit of making stuff up – fine for creative writing, perhaps; not so good for a history essay. Outputs need to be checked and verified if you want to guarantee a good mark on your assignments. But while ChatGPT can’t – and shouldn’t – be trusted completely, many have found that it can help lighten the homework load.\nWith ChatGPT’s user count crossing the 100 million mark last month, it’s understandable that worries about an explosion of AI-written text have proliferated in many professions, including education. Some education systems have decided to ban the use of ChatGPT. Other educators have adopted a more relaxed approach. Writing in Scientific American, law professor John Villasenor argued:\nVillasenor makes a valid point. But experience tells us that not every student is going to use these tools ethically. Some will pursue the path of least resistance and will attempt to present ChatGPT’s outputs as their own. So, the question becomes: Is it possible to tell the difference between human-generated text and AI-generated text?"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/03/15/AI-screening.html#spot-the-difference",
    "href": "viewpoints/editors-blog/posts/2023/03/15/AI-screening.html#spot-the-difference",
    "title": "OpenAI’s text classifier won’t calm fears about AI-written homework",
    "section": "Spot the difference",
    "text": "Spot the difference\nOne answer to that question comes from OpenAI, makers of ChatGPT. On January 31, they launched a classifier “to distinguish between text written by a human and text written by AIs from a variety of providers”.\nOpenAI introduces the classifier by saying that reliably detecting all AI-written text is “impossible”. But it goes on to say:\n\n“… we believe good classifiers can inform mitigations for false claims that AI-generated text was written by a human: for example, running automated misinformation campaigns, using AI tools for academic dishonesty, and positioning an AI chatbot as a human.”\n\nOpenAI stresses that the current version of the classifier “should not be used as a primary decision-making tool”, and users should take that statement to heart – especially if they are planning to vet student homework with it. In evaluations, OpenAI reports that its classifier correctly identifies AI-written text as “likely AI-written” only 26% of the time, while human written text is incorrectly labelled as AI-written 9% of the time.\nThese two reported numbers are important. They are, respectively, the classifier’s true positive rate and the false positive rate. The former is the conditional probability of a positive result given that a piece of text is AI generated; the latter is the conditional probability of a positive result given that a piece of text is not AI generated. However, neither piece of information directly addresses the question that will be of most interest to teachers: “If a piece of homework is flagged as ‘likely AI-written’ by the OpenAI classifier, what is the probability that it actually is AI-written?”\nTo answer this question, we need to flip the conditional probabilities – from “the probability of positive test given text is AI generated” to “the probability text is AI generated given positive test”. Bayes’ theorem provides a formula for doing just that, as described in this 2017 article by Tim Brock, published by Significance magazine.\nAs Brock’s article demonstrates, versions of this problem are familiar to medical statisticians, who often find themselves having to explain screening test outcomes – specifically, the probability that a person has disease X given that they have tested positive for said disease. This probability depends on the prevalence of a disease and the sensitivity and specificity of the test, and Brock defines these terms as follows:\n\n\nPrevalence\n\nThe proportion of the population being tested that are affected by a given condition.\n\n\n\nSensitivity\n\nThe proportion of patients with the condition being screened for that are correctly identified as having the condition.\n\n\n\nSpecificity\n\nThe proportion of patients without the condition being screened for that are correctly identified as not having the condition.\n\n\n\nSensitivity and specificity are also referred to as, respectively, the true positive rate (mentioned earlier) and the true negative rate.\nWe know from OpenAI’s own evaluations that out of 100 pieces of AI-written text, only around 26 would be correctly classified as “likely AI-written”, so the classifier’s sensitivity is 26%. And out of 100 pieces of human-written text, around 9 would be incorrectly classified as AI written, meaning 91 would be correctly classified as not AI written, so specificity is 91%. But the big piece of information we don’t have is prevalence: What proportion of homework assignments are written by AI?\nThis prevalence figure is likely to vary based on where students live, what age they are, their level of interest in AI tools and technologies, and many other factors. A poll of Stanford University students by The Stanford Daily, for example, found that 17% of respondents used ChatGPT for final assignments or exams in the fall quarter – though it reports that “only about 5% reported having submitted written material directly from ChatGPT with little to no edits”.\nSo, let’s assume for the moment that 5% of homework assignments are AI-generated. If you were screening 1,000 pieces of homework with the OpenAI classifier, you’d see something close to the following results:\n\n\n\n\n\n\n\n\n\n\n\nTrue positives\nFalse positives\nTrue negatives\nFalse negatives\n\n\n\n\nResults\n13\n86\n864\n37\n\n\n\nThe figures below show the results graphically as proportions of (a) all tests and (b) all positive tests. (All plots are produced using Python and the matplotlib package; code and functions are available from this GitHub repository.)\n\n\n\n\n\n\n\nFigure 1a: Classifier test results as a percentage of all tests, assuming 5% prevalence of AI-written homework.\n\n\n\n\n\n\n\nFigure 1b: Classifier test results as a percentage of all positive tests, assuming 5% prevalence of AI-written homework.\nFrom Figure 1b, we see that if the classifier delivers a “likely AI-written” result, the chance that the text is AI-written is only about 13%. This is the classifier’s positive predictive value at the assumed 5% prevalence.\nIf we reproduce our figures using a prevalence rate of 17%, also from the Stanford survey, the chance that a positive result is a true positive is now about 37%.\n\n\n\n\n\n\n\n\n\n\n\nTrue positives\nFalse positives\nTrue negatives\nFalse negatives\n\n\n\n\nResults\n44\n75\n755\n126\n\n\n\n\n\n\n\n\n\n\nFigure 2a: Classifier test results as a percentage of all tests, assuming 17% prevalence of AI-written homework.\n\n\n\n\n\n\n\nFigure 2b: Classifier test results as a percentage of all positive tests, assuming 17% prevalence of AI-written homework.\nYet another survey, this one from Intelligent.com, claims that 30% of college students have used ChatGPT for written homework. Plugging this number into our calculations, the chance that a positive test result is a true positive is now slightly better than 50/50.\n\n\n\n\n\n\n\n\n\n\n\nTrue positives\nFalse positives\nTrue negatives\nFalse negatives\n\n\n\n\nResults\n78\n63\n637\n222\n\n\n\n\n\n\n\n\n\n\nFigure 3a: Classifier test results as a percentage of all tests, assuming 30% prevalence of AI-written homework.\n\n\n\n\n\n\n\nFigure 3b: Classifier test results as a percentage of all positive tests, assuming 30% prevalence of AI-written homework."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/03/15/AI-screening.html#determining-guilt",
    "href": "viewpoints/editors-blog/posts/2023/03/15/AI-screening.html#determining-guilt",
    "title": "OpenAI’s text classifier won’t calm fears about AI-written homework",
    "section": "Determining ‘guilt’",
    "text": "Determining ‘guilt’\nIf a test has a positive predictive value of just over 50% (at an assumed prevalence rate of 30%), does that provide a reasonable basis on which to accuse someone of getting ChatGPT to do their homework? That depends on who you ask. If we look to the legal system for guidance, in civil cases like personal injury claims or contract disputes judges typically make decisions on the so-called “balance of probabilities”. This is generally assumed to mean if we are more than 50% sure of someone’s “guilt” in this context, that might be sufficient to find against them. However, in criminal law, a higher standard applies: “beyond reasonable doubt”. Legal scholars have long wrestled with how to quantify this in probabilistic terms, and surveys of judges put “beyond reasonable doubt” somewhere in the range of being 80% to 99% certain of guilt – see, for example McCauliff (1982) and Solan (1999).\n\nMcCauliff, Catherine MA. 1982. “Burdens of Proof: Degrees of Belief, Quanta of Evidence, or Constitutional Guarantees.” Vand. L. Rev. 35: 1293.\n\nSolan, Lawrence M. 1999. “Refocusing the Burden of Proof in Criminal Cases: Some Doubt about Resaonable Doubt.” Tex. L. Rev. 78: 105.\nIt is at this standard of evidence that OpenAI’s classifier shows its limitations. For example, if we flip Bayes’ theorem around, we find that to achieve a positive predictive value of at least 80%, the prevalence rate needs to be at least 58%. For a positive predictive value of 90%, prevalence needs to be 76%. (Verify these figures for yourself: Python code and functions are available from this GitHub repository).\nThus far in our calculations, we’ve set prevalence according to estimates of the percentage of students who use ChatGPT for their homework. But, according to statistician and science writer Robert Matthews, individual students could justifiably complain about having their guilt decided on this basis. “It’s like deciding someone is guilty of a crime just because they happen to live in an area notorious for criminal gangs,” he says. Instead, the guilt of individual students should be decided using an estimate of the chances that they would use ChatGPT for that particular homework assignment.\nLooked at in this way, Matthews says, “You already have to be pretty convinced of a person’s ‘guilt’ even before applying the classifier if you want to put the evidence ‘beyond reasonable doubt’. Bayes’ theorem highlights the need to be really clear about what you mean by the ‘accuracy’ of a test, and about what question you want the test to answer.”\nSo, here’s a question that teachers will be asking if they are worried about ChatGPT-generated homework: “Has the piece of text I’m marking been written by AI?” If those same teachers use the OpenAI classifier to try to answer that question, they will no doubt expect that something classified as “likely AI-written” is more likely to be AI-written than not. However, as it stands now – and as our examples above have shown – users can’t be confident that’s the case. In education terms, this particular ‘test’ is a long way from scoring top marks.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “OpenAI’s text classifier won’t calm fears about AI-written homework.” Real World Data Science, March 15, 2023. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/12/13/dsb-launch.html",
    "href": "viewpoints/editors-blog/posts/2022/12/13/dsb-launch.html",
    "title": "Sink your teeth into some data science papers with our brand new blog",
    "section": "",
    "text": "I’m absolutely thrilled today to announce the launch of our new blog, DataScienceBites. The blog is not only a new addition to Real World Data Science, but also the latest proud member of the ScienceBites family of sites.\nScienceBites sites all share the same concept: we publish “short digestible bites posts about individual research papers” in an effort to make cutting-edge science accessible to a wide audience, and our posts are written by graduate students and early career researchers.\nFor DataScienceBites, our focus will of course be on new publications in the data science space. Contributors are invited to write about papers that are of particular interest to them and to pitch their summaries at an undergraduate level. For an example of what we’re looking for, see our first post on “Determining the best way to route drivers for ridesharing via reinforcement learning”.\nThis launch post is written by Brian King and is republished with permission from MathStatBites, so I want to say a big thank you to Brian and editors Sadie Witkowski and Sara Stoudt for allowing us to repost it. Sadie and Sara have been fantastically supportive of the DataScienceBites idea, and I am grateful for all their behind-the-scenes efforts.\nBrian’s post is a great demonstration of the Bites concept, and we hope that it will inspire others to follow suit. If you are a graduate student or early career researcher in data science (or related subjects) with a passion for science communication and an interest in writing about new data science research, please do get in touch. See our notes for contributors for further details.\nTo everyone else, we do hope you enjoy sinking your teeth into the data science literature with DataScienceBites. Happy reading!\n\n\n\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “Sink your teeth into some data science papers with our brand new blog.” Real World Data Science, December, 13 2022. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/12/09/LLM-news.html",
    "href": "viewpoints/editors-blog/posts/2022/12/09/LLM-news.html",
    "title": "LLMs in the news: hype, tripe, and everything in between",
    "section": "",
    "text": "Two weeks ago, I posted a Q&A with our editorial board member Detlef Nauck about large language models (LLMs), their drawbacks and risks. And since then we’ve had several big new announcements in this space. First came news from Meta (the company that owns Facebook) about Galactica, an LLM trained on scientific papers. This was followed by another Meta announcement, about Cicero, an AI agent that is apparently very good at playing the game Diplomacy. And then came perhaps the biggest launch of them all: ChatGPT from OpenAI, an LLM-based chatbot that millions of people have already started talking to.\nFollowing these stories and the surrounding commentaries has been something of a rollercoaster ride. ChatGPT, for example, has been greeted in some quarters as if artificial general intelligence has finally arrived, while others point out that – impressive though it is – the technology is as prone to spout nonsense as all LLMs before it (including Galactica, the demo of which was quickly taken offline for this reason). Cicero, meanwhile, has impressed with its ability to play a game that is (a) very difficult and (b) relies on dialogue, cooperation, and negotiation between players. It blends a language model with planning and reinforcement learning algorithms, meaning that it is trained not only on the rules of the game and how to win, but how to communicate with other players to achieve victory.\nTo help me make sense of all these new developments, and the myriad implications, I reached out to Harvey Lewis, another of our editorial board members and a partner in EY’s tax and law practice."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/12/09/LLM-news.html#qa",
    "href": "viewpoints/editors-blog/posts/2022/12/09/LLM-news.html#qa",
    "title": "LLMs in the news: hype, tripe, and everything in between",
    "section": "Q&A",
    "text": "Q&A\nHarvey, when I spoke with Detlef, he mentioned that one of the reasons we’re seeing investment in LLMs is because there’s this belief that they are somehow the route to artificial general intelligence (AGI). And there were headlines in some places that would perhaps convince a casual reader that AGI had been achieved following the release of ChatGPT. For example, the Telegraph described it as a “scarily intelligent robot who can do your job better than you”. What do you make of all that?\nHarvey Lewis (HL): My personal view is that you won’t get to artificial general intelligence using just one technique like deep learning, because of the problematic nature of these models and the limitations of the data used in their training. I’m convinced that more general intelligence will come from a combination of different systems.\nThe challenge with many LLMs, as we’ve seen repeatedly, is that they’ve no real understanding of language or concepts represented within language. They’re good at finding patterns in semantics and grammatical rules that people use in writing, and then they use those patterns to create new expressions given a prompt. But they’ve no idea whether these outputs are factual, inaccurate or completely fabricated. As a consequence, they can produce outputs that are unreliable, but which sound authoritative, because they’re just repeating a style that we expect to see.\nOver the past couple of weeks, Twitter has been full of people either showing off astoundingly impressive outputs from LLMs, or examples of truly worrying nonsense. One example of the latter is when ChatGPT was asked to “describe how crushed porcelain added to breast milk can support the infant digestive system”. This made me think of a recent headline from VentureBeat, which asked whether AI is moving too fast for ethics to catch up. Do you think that it is?\nHL: I find that to be an interesting philosophical question, because ethics does move very slowly, for good reason. When you think of issues of bias and discrimination and prejudice, or misinformation and other problems that we might have with AI systems, it shouldn’t be a surprise that these can occur. We’re aware of them. We’re aware of the ethical issues. So, why do they always seem to catch us by surprise? Is it because we have teams of people who simply aren’t aware of ethical issues or don’t have any appreciation of them? This points – for me, at least – in the direction of needing to have philosophers, ethicists, theologians, lawyers working in the technical teams that are developing and working on these systems, rather than having them on the periphery and talking about these issues but not directly involved themselves. I think it’s hugely important to ensure that you’ve got trust, responsibility, and ethics embedded in technical teams, because that’s the only way it seems that you can avoid some of these “surprises”.\nWhen situations like these occur, I’m always reminded of Dr Ian Malcolm’s line from Jurassic Park: “…your scientists were so preoccupied with whether or not they could that they didn’t stop to think if they should.” The mindset seems to be, let’s push the boundaries and see what we can do, rather than stop and think deeply about what the implications might be.\nHL: There’s a balance to be struck between these things, though, right? Firstly, show consideration for some of the issues at the outset, and secondly, have checks and balances and safeguards built into the process by which you design, develop and implement these systems. That’s the only way to create the proper safeguards around the systems. I don’t think that there’s any lack of appreciation of what needs to be done; people have been talking about this now for quite a long time. But it’s about making sure organisationally that it is done, and that you’ve got an operating model which bakes these things into it; that the kinds of principles and governance that you want to have around these systems are written, publicised, and properly policed. There should be no fear of making advances in that kind of a context.\nAlso, I think open sourcing these models provides a way forward. A lot of large language models are open for use and for research, but aren’t themselves open sourced, so it’s very difficult to get underneath the surface and figure out exactly how they work. But with open source, you have opportunities for researchers, whether they’re technical or in the field of ethics, to go and investigate and find out exactly what’s going on. I think that would be a fantastic step forward. It doesn’t take you all the way, of course, because a large amount of the data that these systems use is never open sourced, so while you might get an understanding of the mechanics, you have no idea of what exactly went into them in the first place. But open sourcing is a very good way of getting some external scrutiny. It’s about being transparent, which is a core principle of AI ethics and responsible AI.\n\n\n\n\n\n\n\nAn image created by the Stable Diffusion 2.1 Demo. The model was asked to produce an image with the prompt, “Text from an old book cut up like a jigsaw puzzle with pieces missing”.\nThinking about LLMs and their questionable outputs, should there not be ways for users to help the models produce better, more accurate outputs?\nHL: There are, but there are also problems here too. I’ve been having an interesting dialogue with ChatGPT this morning, asking it about quantum computing.1 For each response to a prompt, users are encouraged to provide feedback on whether or not an output is good. But you’re only provided with the usual thumbs-up/thumbs-down ratings; there’s nothing nuanced about it. So, for example, I asked ChatGPT to provide me with good analogies that help to explain quantum computing in simple terms. The first analogy was a combination lock, which is not a good analogy. The chatbot suggested that quantum computing is like a combination lock in which you can test all of the combinations at the same time, but I don’t know any combination locks where you can do this – being able to check only one combination at a time is the principal security mechanism of a combination lock! I asked it again for another analogy, and it suggested a coin toss where, when the coin is spinning in the air, you can see both heads and tails simultaneously but it isn’t until you catch the coin and then show its face that one of the states of the coin is resolved. That is a good analogy – it’s one I’ve also used myself. Now, the challenge I can see with a lot of these feedback approaches is that unless I know enough about quantum computing to understand that a combination lock is not a good analogy whereas a coin toss is, how am I to provide that kind of feedback? They’re relying to an extent on the user being able to make a distinction between what is correct and what is potentially incorrect or flawed, and I think that’s not a good way of approaching the problem.\nFinal question for you, Harvey. There’s a lot of excitement around GPT-4, which is apparently coming soon. The rumour mill says it will bring a leap forward in performance. But what do you expect we’ll see – particularly with regards to the issues we’ve talked about today?\nHL: I’ve likened some of the large language models and their approach of “bigger is better” to the story of the Tower of Babel – trying to reach heaven by building a bigger and bigger tower, basically. That is not going to achieve the objective of artificial general intelligence, no matter how sophisticated an LLM might appear. That said, language is a fascinating area. I’m not a linguist, but I spend a lot of my time on natural language processing using AI systems. Language responds very well to AI because it is pattern-based. We speak using patterns, we write using patterns, and these can be inferred by machines from many examples. The addition of more parameters in the models allows them to understand patterns that extend further into the text, and I suspect that outputs from these kinds of models are going to be largely indistinguishable from the sorts of things that you or I might write.\nBut, I also think that increasing the number of parameters runs a real risk – and we’re starting to see this in other generative models – where prompts become so specific that the models aren’t actually picking up on patterns, they are picking up on specific instances of training data and text they’ve seen before. So, buried amongst these fantastically written articles on all kinds of subjects are going to be more examples of plagiarism, which is a problem; more examples of spelling mistakes and other kinds of issues, because these are also patterns which are going to possibly be observed.\nThis introduces potentially a whole new breed of problems that the community has to deal with – as long as they don’t get fixated upon the height of the tower and the quality of some of the examples that are shown, and realise that there are some genuine underlying difficulties and challenges that need to be solved.\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “LLMs in the news: hype, tripe, and everything in between.” Real World Data Science, December, 9 2022. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/12/09/LLM-news.html#footnotes",
    "href": "viewpoints/editors-blog/posts/2022/12/09/LLM-news.html#footnotes",
    "title": "LLMs in the news: hype, tripe, and everything in between",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI also had a conversation with ChatGPT. Read the transcript.↩︎"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/11/23/LLM-content-warning.html",
    "href": "viewpoints/editors-blog/posts/2022/11/23/LLM-content-warning.html",
    "title": "Why large language models should come with a content warning",
    "section": "",
    "text": "Anyone who has ever been set a writing task will probably have wished at some point that somebody else could write it for them. As a journalist of 20-plus years, the thought has certainly crossed my mind more than a few times. Which probably explains why a recent headline in Nature caught my attention: “Could AI help you to write your next paper?”\nThe article, by Matthew Hutson, looks at how researchers are using artificial intelligence (AI) tools built on large language models (LLMs) as “assistants”. Starting with a prompt, such as “Write a headline for a blog post about large language models being used by academic researchers as research assistants”, an LLM will produce a text output. For example, using the same prompt with OpenAI’s GPT-3, I got:\nAsked to “Write a headline for a blog post that critiques academic researchers’ use of large language models as research assistants”, GPT-3 produced:\nAnd when I asked “Why can too much reliance on large language models hinder research?”, GPT-3 wrote:\nA fair point, I suppose. But I sense there’s more to this story, and rather than continue quizzing GPT-3, I sat down with Detlef Nauck, a member of the Real World Data Science Editorial Board and head of AI and data science research for BT’s Applied Research Division, to ask a few more questions."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/11/23/LLM-content-warning.html#qa",
    "href": "viewpoints/editors-blog/posts/2022/11/23/LLM-content-warning.html#qa",
    "title": "Why large language models should come with a content warning",
    "section": "Q&A",
    "text": "Q&A\nThanks for joining me today, Detlef. To start, could you give a brief overview of these large language models, what they are, and how they work?\nDetlef Nauck (DN): Essentially, LLMs match sequences to sequences. Language is treated as a sequence of patterns, and this is based on word context similarity. The way these things work is that they either reuse or create a word vector space, where a word is mapped to something like a 300-dimensional vector based on the context it’s normally found in. In these vector spaces, words like “king” and “queen”, for example, would be very similar to each other, because they appear in similar contexts in the written texts that are used to train these models. Based on this, LLMs can produce coherent sequences of words.\nBut the drawback of this approach is that these models have bias, because they are trained with biased language. If you talk about “women”, for example, and you look at which job roles are similar to “women” in a vector space, you find the stereotypically “female” professions but not technical professions, and that is a problem. Let’s say you take the word vector for “man” and the word vector for “king”, and you subtract “man” and then add this to “woman”, then you end up with “queen”. But if you do the same with “man”, “computer scientist”, and “woman”, then you end up maybe at “nurse” or “human resources manager” or something. These models embed the typical bias in society that is expressed through language.\nThe other issue is that LLMs are massive. GPT-3 has something like 75 billion parameters, and it cost millions to train it from scratch. It’s not energy efficient at all. It’s not sustainable. It’s not something that normal companies can afford. You might need something like a couple of hundred GPUs [graphics processing units] running for a month or so to train an LLM, and this is going to cost millions in cloud environments if you don’t own the hardware yourself. Large tech companies do own the hardware, so for them it’s not a problem. But the carbon that you burn by doing this, you could probably fly around the globe once. So it’s not a sustainable approach to building models.\nAlso, LLMs are quite expensive to use. If you wanted to use one of these large language models in a contact centre, for example, then you would have to run maybe a few hundred of them in parallel because you get that many requests from customers. But to provide this capacity, the amount of memory needed would be massive, so it is probably still cheaper to use humans – with the added benefit that humans actually understand questions and know what they are talking about.\n\n\n\n\n\n\n\nLetter Word Text Taxonomy by Teresa Berndtsson / Better Images of AI / CC-BY 4.0\nResearchers are obviously quite interested in LLMs, though, and they are asking scientific questions of these models to see what kinds of answers they get.\nDN: Yes, they are. But you don’t really know what is going to come out of an LLM when you prompt it. And you may need to craft the input to get something out that is useful. Also, LLMs sometimes make up stuff – what the Nature article refers to as “hallucinations”.\nThese tools have copyright issues, too. For example, they can generate computer code because code has been part of their training input, but various people have looked into it and found that some models generate code verbatim from what others have posted to GitHub. So, it’s not guaranteed that what you get out is actually new text. It might be just regurgitated text. A student might find themselves in a pickle where they think that they have created a text that seems new, but actually it has plagiarism in some of the passages.\nThere’s an article in Technology Review that gives some examples of how these systems might fail. People believe these things know what they’re talking about, but they don’t. For them, it’s just pattern recognition. They don’t have actual knowledge representation; they don’t have any concepts embedded.\nTo summarise, then: LLMs are expensive. They sometimes produce nonsense outputs. And there’s a risk that you’ll be accused of plagiarism if you use the text that’s produced. So, what should our response be to stories like this recent Nature article? How should we calibrate our excitement for LLMs?\nDN: You have to treat them as a tool, and you have to make sure that you check what they produce. Some people believe if you just make LLMs big enough, we’ll be able to achieve artificial general intelligence. But I don’t believe that, and other people like Geoffrey Hinton and Yann LeCun, they say there’s no way that you get artificial general intelligence through these models, that it’s not going to happen. I’m of the same opinion. These models will be forever limited by the pattern recognition approach that they use.\nBut, still, is this a technology that you have an eye on in your professional capacity? Are you thinking about how these might be useful somewhere down the line?\nDN: Absolutely, but we are mainly interested in smaller, more energy efficient, more computationally efficient models that are built on curated language, that can actually hold a conversation, and where you can represent concepts and topics and context explicitly. At the moment, LLMs can only pick up on context by accident – if it is sufficiently expressed in the language that they process – but they might lose track of it if things go on for too long. Essentially, they have a short-term memory: if you prompt them with some text, and they generate text, this stays in their short term memory. But if you prompt them with a long, convoluted sentence, they might not have the capacity to remember what was said at the beginning of the sentence, and so then they lose track of the context. And this is because they don’t explicitly represent context and concepts.\nThe other thing is, if you use these systems for dialogues, then you have to script the dialogue. They don’t sustain a dialogue by themselves. You create a dialogue tree, and what they do is they parse the text that comes from the user and then generate a response to it. And the response is then guided by the dialogue tree. But this is quite brittle; it can break. If you run out of dialogue tree, you need to pass the conversation over to a person. Systems like Siri and Alexa are like that, right? They break very quickly. So, you want these systems to be able to sustain conversations based on the correct context.\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “Why large language models should come with a content warning.” Real World Data Science, November, 23 2022. URL"
  },
  {
    "objectID": "viewpoints/interviews/posts/2023/04/28/stephanie-hare.html",
    "href": "viewpoints/interviews/posts/2023/04/28/stephanie-hare.html",
    "title": "‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics",
    "section": "",
    "text": "We’re about a year late coming to Stephanie Hare’s book, Technology is Not Neutral: A Short Guide to Technology Ethics. But, as discussed in our interview below, time has only made the text more relevant. The book was written pre-ChatGPT, but Hare’s explorations of ethical questions in the context of facial recognition technology and Covid-19 exposure tracking apps feel both pointed and urgent at this moment, when researchers, regulators, and regular people are weighing the opportunities and potential harms of large language models and generative AI tools.\n“We’re having some sort of moment with technology ethics – AI ethics being just a branch of that,” says Hare. Reflecting on her career, spanning 25 years, she says: “The stuff that we’re talking about today that dominates the headlines – that is dominating the discussion in the tech sector – was not discussed at all at the turn of the century, other than by maybe people in the science and technology studies domain or academics. But it wasn’t filtering into boardrooms. It wasn’t on the front pages of newspapers, and it wasn’t being covered in the national news. So, it’s amazing. A whole field has sprung up.”\nHowever, as Hare makes clear in our interview, we still have a long way to go to build a culture of technology ethics throughout society. Check out the full conversation below or on YouTube."
  },
  {
    "objectID": "viewpoints/interviews/posts/2023/04/28/stephanie-hare.html#timestamps",
    "href": "viewpoints/interviews/posts/2023/04/28/stephanie-hare.html#timestamps",
    "title": "‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics",
    "section": "Timestamps",
    "text": "Timestamps\n\nChatGPT: just another “flavour of the month” in the tech industry? (1:18)\nHas concern about large language models helped put technology ethics on the map? (3:17)\nWhat will it take to build a culture of technology ethics – in society, in academia, in industry? (9:16)\nDrawing lessons from history (12:15)\nWhy technology ethics is a “wicked problem” (24:57)\nChecklists and changing mindsets (29:49)"
  },
  {
    "objectID": "viewpoints/interviews/posts/2023/04/28/stephanie-hare.html#quotes",
    "href": "viewpoints/interviews/posts/2023/04/28/stephanie-hare.html#quotes",
    "title": "‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics",
    "section": "Quotes",
    "text": "Quotes\n“The European Union has the AI Act coming down the pike. It doesn’t cover stuff like ChatGPT specifically, but then I don’t know if you want good regulation to cover the technology itself, or how technology is used. I talked about this in my book: do you want to regulate forks – a tool – or do you want to regulate use cases for forks? We’ve regulated the use case, if you will, of murder, or of injury with a fork – or, frankly, any other tool. So it’s the use case we focus on. We don’t really regulate forks. [But] we do regulate some technologies, like biomedical technologies, human genetic stuff, anything nuclear. So we just need to think about where does AI fit with that?” (5:42)\n“Move fast and break things was the mantra for this culture [in the technology industry] for a really long time, at least out of the US. And it made a lot of people a lot of money, and they got worshipped by the media. And, you know, they have a whole audience of ‘bros’ who are fans of them. And they’ve never really, any of them, been held to account for what they’ve built.” (11:56)\n“Another generation or two, when we’re older, might look at some of what technology we’ve built or our behaviour on climate change, our track record – did we do what we could have done to slow global warming, to improve biodiversity? – and they might hold us to account, saying, ‘You could have stopped this and you didn’t, right? It’s not just what you did. It’s what you did not do.’ So we have to be super careful when we think about ethics, because ethics change, values change over time. And what seems okay today may not be okay in 10, 20, 30 years time. That is on my mind all the time. It’s not very relaxing.” (18:30)\n“[Laws and regulations] are important, they’re necessary, but they’re insufficient. You can act a lot faster if you can get people preventing stuff from being built in the first place, and that means you need to have a culture of people working in technology, both within the organisations – whether that’s research labs, government, companies, universities, whatever – and on the outside – journalists, academics, thinkers, etc., or just the public, an informed public – who can see something and sound the alarm and go, ‘Wait a minute, hang on. That’s not okay.’” (33:02)"
  },
  {
    "objectID": "viewpoints/interviews/posts/2023/04/28/stephanie-hare.html#transcript",
    "href": "viewpoints/interviews/posts/2023/04/28/stephanie-hare.html#transcript",
    "title": "‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove some repetitions.\n\n\n\nBrian Tarran\nHello, and welcome to another Real World Data Science interview. I’m Brian Tarran. And today I’m joined by Stephanie Hare, a researcher and broadcaster and author of the book Technology is Not Neutral: A Short Guide to Technology Ethics, which is the focus of our conversation today. Welcome, Stephanie.\nStephanie Hare\nThank you so much for having me here.\nBrian Tarran\nI feel I’m a bit late to the party with the book. The Financial Times picked it up as one of the best books of summer 2022. But I’ve only just got around to reading it.\nStephanie Hare\nI mean, I only just got around to reading War and Peace last year. So there’s no rush with these things.\nBrian Tarran\nOkay. Well, I mean, I can definitely say it’s one of the best books I’ve read in spring 2023, if that helps, and the only other one I read was Lord of the Rings. So–\nStephanie Hare\nWow. I mean that’s some pretty august company, you couldn’t thrill me more.\nBrian Tarran\nExcellent, excellent. Well, I actually thought coming late to the book might actually have been of benefit to me as a reader because, you know, you’re talking about technology ethics quite broadly. But then you focus in on a couple of use cases, specifically around facial recognition, technology, Covid-19 exposure tracking apps and things like that. But, you know, obviously, since the book was published, the whole discussion around technology ethics has kind of been dominated maybe or taken on a new dimension following the launch and adoption of ChatGPT. I wonder, you know, when the technology was launched, people started using it, adoption rates, you know, went through, went through the roof, what was your kind of initial reaction to all that and what have you made of the kinds of conversations and criticisms that have followed?\nStephanie Hare\nWell, I mean, like everybody I was curious and fascinated and wanted to play around with it a bit. I don’t think I’m of the school of thought that seems to be circulating that this will either you know, destroy mankind as we know it or take everybody’s jobs or potentially upend civilization. There’s been some quite extreme, some quite extreme views put across in the media in the past few months since this was widely released to the public. I don’t know, for some reason, I didn’t drink the Kool-Aid, when I started working in technology. So I always take these things with a grain of salt. And I guess my, my cautionary note to anybody listening to this is you know, at this time last year, all of my clients were wanting presentations and analysis about web3 and NFTs and cryptocurrency and before that, it was blockchain. There’s like always a sort of flavour of the month. AI, for people who’ve worked in this field, is known to have winters, summers, springs, autumns, you know, these seasons of when it’s like coming on and really exciting or not? I’m more excited by DeepMind’s use of artificial intelligence, I think they’re actually working on interesting problems, right, around like protein discovery, like real science, as opposed to like, oh, look, I can have a new sci fi avatar or do a deep fake, you know, people can do deep fakes already. We’re just doing them now in even more disturbing ways. So I guess, I guess it’s that. I’m intrigued by it. But I don’t I don’t feel the need to sort of freak out. Either way, positively or negatively, I have a much more sort of detached satellite-level view, I think probably just because I’m older, seeing these trends come and go. And it’s like, let’s just let’s just wait this out and see how it goes.\nBrian Tarran\nFrom your perspective as someone who’s interested in and researching in the area of technology ethics, right, do you see a kind of almost a benefit that the conversation around this has put technology ethics, the conversation around that, on the map? Or do you worry that we’re kind of obsessing over this one technology and this one application? We’re not looking at the field more broadly?\nStephanie Hare\nWell, there’s a few things to say on this. So like, first of all, a couple of weeks ago, a bunch of people working in AI, about 1000-plus people – including some fictitious people, by the way – sign this this letter calling for a moratorium on AI research for six months, which was unenforceable, clearly not going to happen, was signed by Elon Musk, who then very quickly announced he was developing his own rival to OpenAI, the company that invented ChatGPT. So you take all of that with a grain of salt. But again, if you’re, if you’re an historian, or if you just have a long memory, you’ll remember that there have been several letters like this. There’s always somebody, you know, very big-wig people. It’s not that we want to dismiss it. But Stephen Hawking and Elon Musk were warning, you know, over around 10 years ago that AI was going to kill humanity if we didn’t put guardrails on it. Professor Stuart Russell talked about this in his Reith Lectures a couple of years ago, which are still online and you can listen to them. And you know, he’s not an alarmist. He’s a serious person and a serious thinker. So we want to listen to him. But I guess what I’m just saying is, you know, every time some sort of new technology or new use case for technology comes up, there’s a group of people who come out and freak out and they get lots of op-eds. It’s usually men, I must say. There’s a lot of women doing some really interesting scholarship in this area that don’t get the op-eds and quite the publicity. So there’s that. Then it is interesting because it makes people think about technology ethics, usually, again, from a place of either fear, right – Are they going to kill us? Are they going to take our jobs? Are they going to remove human agency? – or money – How are people going to make a huge amount of money? Who’s going to make the money? And by displacing whom, right? So, we have two levers: incredible doom or incredible opportunity. And that leaves the rest of us, I think, probably somewhere in the middle, scratching our heads and going like, is this going to actually change my life? And if so, how, and do I really care, given that I’ve got like, you know, a cost of living crisis, recovering from the Covid pandemic for the past few years? Like, if you’re not in this world, it can seem like a lot of shouting. There’s also the question of, do we need new laws? So we know that the European Union has the AI Act coming down the pike. That’s supposed to be passed this year, and there’ll be a two year implementation grace period. So that’s interesting. It doesn’t cover stuff really, like ChatGPT specifically, but then I don’t know if you want good regulation to cover the technology itself, or how technology is used. And I talked about this in my book, like, do you want to regulate forks – a tool – or do you want to regulate use cases for forks? So if I’m, if I stab you, or kill you with a fork, which is totally possible, that is something that we’ve regulated; we’ve regulated the use case, if you will, of murder, or of injury with a fork or frankly, any other tool. So it’s the use case we focus on. We don’t really regulate forks. We do regulate some technologies like bioethics technologies, or biomedical technologies, excuse me, sort of human genetic stuff, anything nuclear. Those technologies we do specifically regulate. So we just need to think about where does AI fit with that? And also, do we need new regulations for everything, or can we use existing ones? And that’s what’s becoming really interesting is that in the US, where I’m from, the main regulator, the FTC, seems to think that it can use a lot of existing laws already. So they’ve been like, if your AI is claiming to do stuff that it can’t, we’re gonna come after you under, like, kind of sort of false advertising, if you will, misrepresenting yourself. They might come after some of the big AI companies based on anti-competition law, right? So no new laws needed for that. And then with the music industry, they’ve been going after all the people who are like, oh, let’s like remix a Drake song, and saying, well, actually, you can’t do that, because you’re violating copyright law, take it down. Right. So again, I don’t want to be like too, too calm about it. Like, we do need to look at some of the use cases that are really problematic and hurting people. But we might actually have a lot more in our arsenal to combat this than we’re currently using. And I think what’s going to happen is, unfortunately, the pace of crafting legislation, and then regulators never fully enforce regulations– Look at the GDPR: no company’s ever been given the full fine. And here in the UK, the ICO is famous for letting companies off the hook, giving them less than half of the original fine. It’s ridiculous. So if you’re going to pin your hopes on regulation, I’m not sure that’s great. I’m weirdly more optimistic about landmark legal cases. So we’re seeing an Australian mayor who was totally defamed by ChatGPT, in Australia, he’s going to be taking or is taking OpenAI to court. And then we might see some of these copyright issues, that could be taken to court, right. And like, that’s where people I think will get more action and, frankly, more respect, because these companies are really happy to pay a lot of money to lobby our lawmakers, and water stuff down. And they always say, Oh, my God, it’s going to constrain innovation. And if they really get desperate, they’ll be like, China! If we don’t, if we’re not allowed to do everything we want, China will win! That is like a– that is just a game that takes everybody nowhere, whereas in the lawsuit angle, that’s interesting, because you’re demonstrating responsibility, you’re discussing liability, you’re having to demonstrate harm. And in the process of discovery, right, you might be able to actually get some of these companies to open up their datasets, how their algorithms work, like, I’m much more intrigued to see where that’s gonna go.\nBrian Tarran\nYeah, but I think I mean, having read your book, I would, I would have thought you might perceive all of this sort of stuff as kind of sticking plasters to put over the the injuries that might be caused by these technologies, right? Your argument seems to be that we have an issue whereby we don’t have a culture of technology ethics. So when we’re thinking about building these tools, or when we’re starting off down the path of creating something like this, we’re not already thinking about, you know, the use cases, the harms that might arise from that and things like that. What does it take to build a culture of technology ethics, do you think, in our society, in our academic institutions in our companies?\nStephanie Hare\nHonestly, I think, I think this whole accountability piece is going to be what it takes. Because you see, like Alphabet CEO Sundar Pichai gave an interview recently to CBS 60 Minutes in the US where he was like, yeah, there’s a risk that this technology could get out of control, like dot dot dot, this would be terrible for mankind. And you see him kind of be like, hope somebody does something about that. And it’s like, I know somebody that might do something about that, Mr Pichai – you! But clearly he feels, and you can see his point, he feels that right now, if it’s not illegal, then it’s permissible. And he has to win market share. If he doesn’t do it, he’s going to lose. And companies have this all the time. If they wait too long, they lose their first-mover advantage, and they get destroyed. We can go through like countless examples of that in business, particularly in technology. So I get it. But what he isn’t understanding is that if his company is the one that puts out the technology that leads to terrible harm – you know, physically killing people, harming them, destroying the national security infrastructure, something like that – right now, I don’t think he’s thinking about how that’s going to affect him. And that’s because we don’t really penalise executives very often. The worst that might happen is they might leave with a huge golden parachute, and go off and sort of retire in Hawaii with their millions, right? Like nothing really happens to them. So how do you have a culture of technology ethics, where the people who are creating technology and have the power to stop, right, to maybe like back off on stuff, they aren’t really thinking about how will I personally be held responsible if this goes south? So like, Sam Altman and OpenAI, same thing, he was like, he gave an interview where he’s like, I’m really scared about this technology I’m building. It’s like, okay, you could slow down or back off, you could make your datasets open, you could make your algorithms open. You’re called Open AI, that was supposed to be your whole mission, why you were created was to benefit humanity, like, what are you doing? So it’s weird. And I think it comes from the fact that, you know, move fast and break things was the mantra for this culture for a really long time, at least out of the US. And it made a lot of people a lot of money, and they got worshipped by the media. And you know, they have a whole audience of bros who are fans of them. And they’ve never really, any of them, been held to account for what they’ve built.\nBrian Tarran\nSo your interest in technology ethics clearly predates you know, that all the noise at the moment around large language models and generative AI and things like that. What was it that got you interested in this subject? Was it a particular application, something that caused some concern? Or– How did it come about?\nStephanie Hare\nThis is gonna sound completely weird, but it didn’t come from my experience in tech, really, at all. I have had two paths in my adult life, one has been working in these technology companies with a brief but happy foray in political risk, which is now sort of part of the skill set for tech. But I trained as an historian, and I interviewed someone who was a French civil servant, who at the end of his life was put on trial for crimes against humanity for his actions as a young civil servant during the Second World War. So he collaborated, as so many French civil servants did. And in the course of that collaboration, over a period of many years, went from just, you know, just signing documents and kind of doing what he was told to do, to deporting people and sending them to Auschwitz. So I was very young when I interviewed him, and that marked me, as I would hope it would mark anybody. I talked with him on and off for about three years, until he died. And that was the subject of my PhD. And then I did a fellowship at St. Anthony’s College, Oxford, and spent years looking at it further. And that’s actually going to be my next book. I needed a long time to sit with that material and to read around it, but I had to get the interview while he was still alive. He was so old, he was 93 when I started talking to him, and so it was important to get that down for posterity’s sake first, and then circle back and do some analysis later when I was a bit older. When you talk to somebody who in his case was, you know, not antisemitic, not of the far right politically, was actually like centre-left, had lots of Jewish friends, etc. Was like top of his class, you know, came from a milieu and a background and a formation that I think many of us would read and be like, Okay, that seems pretty reasonable. You ask yourself, how on earth did that person in his, like, young period of his late 20s, early 30s end up being involved and actively participating in what ends up being mass murder. It’s probably the most extreme case study of ethics, or one of the most extreme case studies of ethics that I could have stumbled upon. And it stayed with me and to be honest, it shapes a lot of my work and how I think about human rights and civil liberties and the freedoms that we so often take for granted because I’ve studied, as an historian looking at France and Germany, how quickly those things can be taken away – very quickly, terrifyingly so, in fact. So that lens is always with me. And when I was then working in technology, and seeing some of the things that could be done with these tools and watching this lack of accountability, down to the point of gross negligence in some cases. And also, as a young technologist, not being given any training – we were given no training at all in ethics, in, like, discussing data protection – it was basically: this is the law, just obey the law, like, that’s the, that’s the box that you have to play in. Other than that, like, go for it. And when I look back on that now it’s like, Oh, my God, that’s the equivalent of putting your family in the car, and everybody goes off without wearing their seatbelts on and, you know, all this sort of safety design that we take for granted in cars now, it’s just mad when you think about it, or the way we used to fly. We’re in this phase, it’s really interesting, just over the course of my career – 25 years – where the stuff that we’re talking about today that dominates the headlines, right, that is dominating the discussion in the tech sector, was not discussed at all at the turn of the century, other than by maybe people in the science and technology studies domain or academics. But it wasn’t filtering into boardrooms. It wasn’t on the front pages of newspapers, and it wasn’t being covered in the national news, whereas like now that is all I’m doing. So it’s amazing. A whole field has sprung up.\nBrian Tarran\nI think that that kind of origin story, if you like, explains some of your, perhaps, belief in the importance of exploring this accountability question when it comes to technology, ethics?\nStephanie Hare\nYeah, because I watched it, and I think what was so fascinating– So as I say, I was in my early 20s. In fact, I was 20, when this man was put on trial, and I had just moved to France. It was the longest trial in French legal history – it was a big deal, you could not not watch it. So I was reading this and seeing it in the press every day, and I watched the French people discussing it around me, you know, really being divisive, this stuff does not go away. And his view was: I was just following orders, I was doing what I was told to do. Which you know, you hear that a lot from engineers or people who are like, this is the design spec I’ve been given, or this is what my boss has told me to do, or this is what our investors want, etc. Or people feel they don’t have the power to stand up because, you know what, they’ve got a mortgage, they’ve got kids, and employers know that, like, they know that and they use it as leverage against people to silence them. Or they’ve signed an NDA, because we get made to sign these NDAs when we work in tech, and then we get made to sign another NDA when we leave, right, so we can’t disparage our employer, and maybe we’re given some money so we don’t talk about the things we’ve seen. You know, it’s, it’s gross – it’s a gross little world, and like you have to be very, very solid and take good care of yourself to work in it, I reckon. To try and keep your ethical and moral compass. It’s hard. So I think because I saw that. And I saw that someone who – whether we believe him or not, this is what he claimed – in his 30s, he was just doing kind of what everybody around him was doing under a situation of crisis. He was let off the hook. I mean, he wasn’t just not persecuted in 1945, he was actually promoted. And then he became France’s top civil servant, and then he became an MP, and then he became budget minister. I mean, this guy’s career was not hurt in any way by what he did. On the contrary, right, he advanced. And yet, by the end of his life, French values had changed, so a new generation wanted to hold him to account. And I think about that a lot for all of us, right, who are sort of walking around in our 30s or 40s. Another generation or two, when we’re older, might look at some of what technology we’ve built or our behaviour on climate change, our track record – did we do what we could have done to slow global warming, to improve biodiversity? – and they might, they might hold us to account saying, you could have stopped this and you didn’t, right? It’s not just what you did. It’s what you did not do. Right. So we have to be super careful when we think about ethics, because ethics change, values change over time. And what seems okay today may not be okay in 10, 20, 30 years time, and we might be the 80- or 90-year-olds who are put on trial. That is on my mind all the time, right. It’s not very relaxing.\nBrian Tarran\nNo, and I guess it makes me think. Well, I mean, this is getting into the hypotheticals right. But is it– if we can’t necessarily predict or plan out how values might evolve over time, is it enough to be able to, to just say or to document that we asked the right questions at the time, rather than just doing things blindly. Is that where we need to kind of almost formalise our process of writing down, setting out, you know, we want to do this, we’ve considered these potential harms, we’ve considered these potential benefits, and we kind of document that so at least, you know, future generations can say well, “They thought about it. They might have not thought about it in the right way, but they tried”?\nStephanie Hare\nAbsolutely, I think, you know, show your work and be like, these were our, you know, these were our sort of first principles of where we were starting from, this is the context in which we were making this decision. Because again, I don’t, I don’t necessarily fear the judgement of history in terms of if I get something wrong. People get stuff wrong all the time. That’s just being human. It’s, did I not care? You know, was I like, well, sorry, little little boys and girls who are babies now, like, I need to do my stuff, and like, I don’t care about you, right? That attitude is tough. Or I decided I just really, you know, I really needed to buy a flat. So I decided to work for some dodgy company, or dodgy, dodgy company that’s owned by a foreign government, but I knew it was going to be fine, and they’re offering me a tonne of money, and now I can go on nicer holidays. I’ve had these conversations with people about this literally this past week, like, these are live issues for people. There’s a cost of living crisis, ethics can feel like a luxury for some people rather than a necessity. And human beings are very bad, all of us, at thinking about, you know, future selves, right? Like we kind of, we optimise for how we’re feeling now, and we’ll deal with 20 years from now later if we even get there. So I think there’s that. There’s also– this really inspired the writing of the book, Technology is Not Neutral. I knew, I had this weird sense – I had just gone independent, so I had left working for these companies, I was not under any NDAs anymore, which right there gives you a clue; I could say what I wanted – but I also knew there was a chance that I was going to have to go back either into industry, or maybe work for a government, I don’t know what I’m going to need to do in the future or who I’m going to want to work with or what reasons I might even have for that. But I knew I had this window of being an independent researcher and broadcaster, that I could say whatever I wanted, and I had that thing of like, okay, if you’ve had a window of, say, five years, for example, what would you say if you were not afraid? If you were not scared? If you were like, you know, screw the money, screw the corporate pressure, screw the government, whatever, what do you want to talk to the public about? And my views were, I really wanted to talk to them about facial recognition, because I feel people just fundamentally do not understand how dangerous that technology is and how it can be used. I wanted to talk about the pandemic technologies, because we were, you know, I was writing it during the pandemic, and I thought, well, if a pandemic ever happens again, let’s have a nice little tidy case study for potentially future historians or future medical personnel, public health officials to pull out, because when the pandemic hit, we all had to go back and look at stuff from the Spanish flu. You know, there’s a lot of discussion of like, why has this come as such a surprise? Are we going to use these technologies again or not? Right. Like, you know, is it worth it? Is the return on investment worth it in all senses – ethically, as well as medically, all of those things? So I thought I would lay down a couple of markers that I hoped would stand the test of time. But the big thing I wanted to do, because I was always thinking I might have to go and sign another NDA and go work because I too must earn my living, was I wanted to write something so that anyone who cares about technology, is working in it, is investing in it, right – it’s not just people who code, it’s people who fund the people who code. Buying technology – procurement is massive, you’re a really powerful person if you’re in charge of procurement. But also just consumers, and citizens and parents, and teachers and kids. If I could write up everything that I had learned in my 25 years, and succinctly as possible, right – as short as possible, because people are tired, they’re busy – I could pass that baton on, so that if I ever have to stop going on television and radio, and I’m no longer allowed to write in newspapers and warn people about the stuff I’m seeing and the abuses of power, and showing them examples of history of how this can go so terribly wrong, maybe it will have, like, lit somebody else. And I’m delighted to report – I mean, we’ll see; time will tell, it’s only been out a year – the amount of people who have brought me in to train their staff, to talk to their board. I’ve talked to children. I’ve talked to university students, I’ve taught classes all over the world, because we can now do online teaching. I’ve taken a lot of it on television and radio and in the newspapers. People wanted this, and I’m not the only person working on it, of course – there’s been a whole flowering of people, scholars, etc., working in putting out amazing books and documentaries. It’s really, we’re having some sort of moment with technology ethics – AI ethics being just a branch of that. So that’s really encouraging. So I sort of feel like, you know, again, if I, if I’m gonna have to account for myself at the age of 93, I would like to be able to point to that and go, I tried. I tried. And I have no idea if it will succeed or not, but I stood up to the plate and I swung the bat and, you know, I aimed for the bleachers.\nBrian Tarran\nOne of the things I thought was really interesting about the book, it comes towards the end when you’re kind of talking about, you’re summing up, and you talk about how your thinking about almost like the the approach or solution to the technology ethics issue has changed over the course of the writing of the book. You had, like, a list of potential, like, proposals, proposed actions that you wanted to analyse, but then you realised that actually technology ethics is a “wicked problem”. I wonder if you could explain what that term means for people who might not be familiar with it and and why you think of it that way?\nStephanie Hare\nYeah, I’m so grateful to have learned about the term “wicked problem”. My friend Jason Crabtree, who wrote an amazing book about electricity grids, like smart electricity grids, for Cambridge University Press, had asked me to read his manuscript maybe 10 years ago, and I read it, and one thing I took away that just absolutely blew my mind was this concept. So I shall gift it to you for those of you have not heard it. Because then suddenly, you’re like, God, it makes so much sense. There are certain problems, I would say, like, the climate crisis, and biodiversity loss would be a good example of this. There’s certain problems that have many causes, many causes, so there isn’t going to be one solution to fix them. So people constantly ask me, oh, is this the magic bullet? No, they’re like, there are certain problems that there is no magic bullet – the pandemic is probably another, actually. Then, if you do try to solve these wicked problems, the mere act of solving them can introduce a whole new set of problems to them. So like, it becomes even more of a head– You know, I’m trying not to swear, but a messing with your head moment. And it’s exhausting, you know, and it gives you your forehead wrinkles and makes you just sort of want to bang your forehead onto the nearest wall. And yet, you also can’t opt out and be like, well, it’s just too hard. It’s a wicked problem. There’s no solution, there’s nothing to be done, you know, throw up your hands, because you’re like, Yeah, but the problem is, is if we don’t do anything, like literally people are dying; literally, climates are becoming uninhabitable, we’re going to have massive climate migration, that’s going to cause all sorts of problems, water scarcity, we can have wars over this, like, we have to do something. So like, you have to still act on a wicked problem, all while knowing that it’s not going to be solved in a binary sense of like zero, one, black and white, I can point to it and measure it. And for people who like metrics, that’s a real pain, because they’re like, I want to know what good looks like and I want to know how we’ll know when we get there, you know, what’s the percentage, what’s the number? And you kind of have to be like, Well, with a wicked problem, you might never solve it, or you won’t solve it once. Because again, with something like climate change, or pandemics, these are things you’re probably gonna have to solve again, and again, and again, because it’s dynamic. And it’s constant, you know, we’re always going to be managing our relationship with the climate, with the environment. So, you know, we can pick a certain temperature, or a certain percentage of landmass that, you know, has trees or whatever, and like, come up with a little metric for our metric-oriented friends. But that’s still not very meaningful. So it’s more that when you think of a wicked problem, like facial recognition technologies, like, we need to be able to identify people in certain situations, and like, we want that, right. Like, we want to be able to catch criminals, and we want to be able to catch terrorists when they’ve managed to pull off a terrible act of, of harm to people. But at the same time, do you want to turn your society into a sort of permanent dragnet? Do we not care about privacy? If so, like, when do we care about privacy? And when are we okay with maybe sacrificing that for the greater good and who decides that? It’s really problematic if you live in London as I do, and your police force, which is using this technology, has admitted, they’ve admitted themselves to being misogynist, institutionally misogynist, homophobic, racist, right? And then we’re gonna give them a technology that doesn’t work very well on people with certain skin. It doesn’t identify people of a certain age as well. It’s got all sorts of problems. So you’re kind of like, hmm? Facial recognition is covered a bit under the EU AI Act. But even then there’s like so many loopholes. And the thing is, if you cite national security, it usually gets waved through because no one wants to be the person who said, I said we couldn’t use this technology, and then something bad happened. Right? So you err on the side of, like, the precautionary principle. The default is, let them use it. We must trust them. Except what do you do if your police force has given you quite ample evidence not to trust them? Or companies. You know, this is not to bash on the police, by the way – companies are some of the worst offenders in this area. So that’s what I mean about it being a wicked problem is, it’s out there. It’s installed. It’s all over the UK, it’s definitely all over the US as well. And we don’t really have a good framework for it.\nBrian Tarran\nBut then this is where we loop back around to the kind of culture, right, creating a culture of technology ethics? You know, we can’t just put a checklist in place once, do it, tick it off, yeah, we’ve done that for facial recognition technology, we’re good to go. Because there are always new potential use cases for it, new applications, new integrations with different systems that we always need to be thinking, every time, is this the right thing to do? Or–\nStephanie Hare\nI mean, I’m still a big fan of checklists. So it’s not that I’m anti-checklist. And I’m not saying that you said that, by the way, I’m more thinking aloud. Checklists can still be useful, right? Like, whenever I’m in a really bad mood, I’m like, Okay, hang on, have I slept? Do I just need a glass of water? Am I hungry? You know, what I think is angry may just be that I skipped lunch. You can kind of go through those things, and anybody who’s had to like troubleshoot why a baby or a small child is unhappy will also have a checklist and what’s on the thing – ah, they missed naptime, where’s their bear? That sort of thing. Companies need checklists, because you’re trying to get loads of people singing from the same hymn sheet, I get that. But what I wanted to get away from was this idea that, like, one person or one team gets this checklist and maybe does that once a year, once a quarter, pick your cadence – and everybody else gets a pass. Ethics doesn’t work that way, because again, ethics is kind of I think in the wicked problem scenario of, like, how do we decide what our values are and how to live them? And how do we know, where do we draw the line? And then how do you, how do you decide if you’ve gone over the line or not? And all of that, who decides who decides? Those are really complex questions that mean that really you can’t abdicate. This is, in a company’s case, it’s the CEO, it’s the board, all the way on down – it has to be baked in to every single employee, and also investors, mindset. And I was thinking about it in terms of cybersecurity, I once had a colleague who gave me an analogy that I think is helpful, so I’ll share it for what it’s worth: When you go on to, say, an oil rig, in the North Sea – a highly dangerous environment – you might be the most junior person there, and you’re there for your very first day of work. But if you spot something on that rig that is a health and safety risk, you have to speak up. You’re not going to go, like, Oh, my boss might say something, whatever, because, like, everybody’s life on that rig is depending on everyone having that culture of careful, that’s not okay. And that really put me in mind where it was like, Oh, wow, we’re going to have to like inculcate an entire new mindset. And we think about technology ethics a lot because of technology being the word, and we think it must mean like hardware or software, it’s always about coding, and it’s often guys in hoodies coding. But my preferred method of hacking is culture. Right. So like, again, if we tried to just solve everything through regulation and laws, that takes, you know – if you look at the average time it takes to pass a law and then for regulators to enforce it – ages, we’re talking years, like it’s too late. These technologies will have moved on. Ditto, calls for international treaties. Do it, by all means. Have a look at how long it takes most international treaties to get passed and then ratified – and then, P.S. What happens when people break them? Really, right. So like, they’re important, they’re necessary, but they’re insufficient. You can act a lot faster if you can get people preventing stuff from being built in the first place, and that means you need to have a culture of people working in technology, both within the organisations – whether that’s research labs, government, companies, universities, whatever – and on the outside – journalists, academics, thinkers, etc, just the public, an informed public – who can see something and do what I just described on the oil rig, like, sound the alarm and go, Wait a minute, hang on. That’s not okay. That is, that to me feels faster. And I’m way more into prevention than cure, for all sorts of reasons. So I think like, yes, to laws and regulations, yes, to treaties; this will be faster. And I think it will be more resilient.\nBrian Tarran\nYeah, I agree. And I have to say, wrapping up, that I think Technology is Not Neutral is a great place to start to inculcate that mind shift, that mindset change. So, Stephanie, thank you very much for joining us today.\nStephanie Hare\nThank you for having me.\nBrian Tarran\nYou said you’re working on a new book. Have you got a timeline for that? Or a title?\nStephanie Hare\nNo. I am the slowest thinker and writer, I’m like the opposite of move fast and break things, I’m like move slowly and like think it over maybe several times. So I’m just getting started out. I’ll sort of go five years. It’s gonna be, it’s a history book. Alright. So this is, this is different, I’m having to take my classes in French and German right now to get kind of match fit in those languages again, and then you know, I’ll be off and writing. But, yeah, I hope to have another book out, you know, in five years.\nBrian Tarran\nWell, if the year it took me to read Technology is Not Neutral is any indication, in three, four or five years time it will still be relevant today. So–\nStephanie Hare\nThat’s the thing with history, it always stands the test of time.\nBrian Tarran\nWell, thank you, thank you again for joining us on Real World Data Science. It’s been a pleasure talking to you.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo of Stephanie Hare is not covered by this licence. Photo is by Mitzi de Margary, supplied by Stephanie Hare and used with permission.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics.” Real World Data Science, April 28, 2023. URL"
  },
  {
    "objectID": "viewpoints/interviews/posts/2023/06/08/london-data-week.html#transcript",
    "href": "viewpoints/interviews/posts/2023/06/08/london-data-week.html#transcript",
    "title": "London Data Week is almost here. What’s it all about?",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove some repetitions.\n\n\n\nBrian Tarran\nHello, and welcome to Real World Data Science. I’m Brian Tarran. And today I’m joined by the organisers of the upcoming London Data Week, Sam Nutt and Jennifer Ding. Sam, Jennifer, how are you? Nice to see you here. Thank you for joining us. I wanted to start, maybe you can introduce yourselves to our viewers, tell them a little bit about your background. Sam, I don’t know if you want to go first.\nSam Nutt\nSo I’m Sam Nutt. I’m the researcher and data ethicist at the London Office of Technology and Innovation, or LOTI. We’re about four years old. We’re an innovation unit that sits across 26 of the boroughs of London, and the Mayor of London, the GLA and we work sort of through collaborative processes to sort of foster innovation within local government, in the public sector in London. And I lead partly on our work on data ethics, as the title suggests, but also I lead our work on innovative public participation, which I guess leads nicely into maybe talking a little bit later about London Data Week. But yeah, that’s kind of my background and where I’m coming from. My interest in data, I guess, more originally came from the governance side, and how we use it properly and best in public sector context.\nBrian Tarran\nExcellent. Thanks, Sam. Jennifer?\nJennifer Ding\nHi, I’m Jennifer. I’m from the Alan Turing Institute, and I– the Turing, I should say is the UK’s national institute for data science and AI. And I sit on a team called Tools, Practices and Systems, or TPS, which we sometimes colloquially call the Turing’s open science team. So we focus on open, reproducible, and ethical data science practices. And at the Turing, I co-lead a team of research application managers, and our focus is making sure that the research that happens at Turing is more usable, and also actually used by more people outside of academia. In a previous life, I was a data scientist at various US tech startups, working on applied machine learning, mostly for local and national government partners. And in New York, where I was for many years, I participated in something called New York Open Data Week, which was a great introduction to the open data world, and also the civic technology world and how vibrant and exciting those communities are.\nBrian Tarran\nExcellent. Okay, so that– so you’ve had the exposure to data weeks before? Tell us how did London Data Week as an idea first germinate?\nSam Nutt\nYeah, well, I mean, maybe I can go because it was it was something LOTI, last year, we had a sort of anniversary event, we sort of tried to bring together our community, across local government, and also partners. And so we invited [the] Alan Turing [Institute], we’ve done some work with them. And Jen came, and it was just something Jen raised to me, with me last July, you know, the idea of maybe doing something a little bit, like, inspired by some of the bits we’ve seen, for example, in New York, but also thinking, you know, what’s, what’s the London version of that? What’s the opportunity we have, given, you know, our different context to New York, and we don’t just mean in terms of, you know, you’ve got a different, like, legal context for how you use data and regulation stuff, but the cultural bits, you know, what is the physical space? How does that make London different? The people who live here, you know, what are the opportunities of London. And I think, you know, partly also inspired at the time by, you know, at least from my perspective, a real want to include, you know, ordinary people, Londoners, the public at large – all of those terms can be broken down – but effectively, to increase participation in how we think about data and how we, you know, think also about the future of the city, in a, you know, a future that we know is going to be defined by how we use data. So it kind of, it was a perfect thing where I was very inspired by, you know, I was thinking about those things, and then Jen came along. I don’t know, maybe Jen, you can talk about why you came along with that idea in the first place.\nJennifer Ding\nYeah, absolutely. Yeah. Sam and I sometimes talk about how funny that chance encounter was really. I think the LOTI event was a really great example of how London’s flavour of data innovation is actually quite unique. Just gathered there with all the local councils and various other data organisations that LOTI works with. It was such a cool display of how much the public sector is involved in defining data innovation in London, and also how much academic and private tech in London is also committed to creating data and tech outputs that are for the public good. And coming from the US, I think it was very inspiring to see that organisations like LOTI, organisations like the Turing Institute, like the Ada Lovelace Institute, the Open Data Institute – there is such a great network of data for public good institutions here that are committed to, you know, centering Londoners in the conversation. And I think something that Sam and I really got to talking about was, you know, was this an opportunity also to clearly articulate what this new thing, this unique thing, is that exists in London, that if you’re here, and you work in the space, you know it, but it hasn’t really been formally articulated in the way that I think many of us know that Silicon Valley is associated with a certain kind of innovation – and may be Europe as well with regulation – but what’s happening in London is really special. And we hope that with all our great partners and our London Data Week team, we can begin to start to articulate what makes data innovation in London so special.\nBrian Tarran\nPlease do, Jen.\nBrian Tarran\nAnd before we get carried away, we should probably pin down exactly when London Data Week is taking place. It’s beginning of July, is that correct?\nJennifer Ding\nThat’s right, first week of July 3rd to 9th July.\nBrian Tarran\nAnd so from what you’ve said, my understanding, broadly, maybe the aims of the week are to kind of bring together people who are working in data, who are using data and people who are affected by data, or for whom data helps sort of shape their lives, to kind of have maybe, I don’t know, a broader understanding, a kind of commonality of purpose, whatever it might be, is that is that how you would summarise it or…\nSam Nutt\nVery nicely summarised, are you available for comms help?\nBrian Tarran\nI am. Very expensive, though, I’m afraid.\nSam Nutt\nOkay. It’s all pro bono stuff. No, I think, yeah, I think that’s really well put. It’s, yeah, it’s articulating that– coming together to start to actually build that imagination, that vision for what London is, and articulate it more, in more clear terms. But it’s also, you know, actually trying to do some things in line with that as well. Some of the activities and events we have, you know, we’re running, it’s kind of in this distributed format. So different organisations across London, who are sort of value aligned across the sector, who might have their own communities, their own publics, can run things – different types of events, you know, engaging people in different ways. And then, you know, all together, we think, sort of the sum of the parts of doing these things across a week, in this distributed format, testing different engagement methods, you know, not only sort of builds a community of organisations around these, like common values, but also lets us, you know, actually reach out to the public, you know, the public with all the different people in London, from different backgrounds, there are so many different communities, you know, as much as possible, to connect to different communities and make them part of the conversation about how we use data and AI and in ways that they kind of historically haven’t been. Not just for data and AI, for lots of other things. But, you know, we know that these technologies are going to be so important for the future. And it’s, you know, designing events and activities that can try and ensure that they’re sort of have a, you know, a seat at the table, thinking about what that future is.\nBrian Tarran\nYeah. And you mentioned there a range of events. Jen, can you give us a flavour of some of the events coming up, the highlights, things that you might personally be most looking forward to?\nJennifer Ding\nYeah, happy to Brian. And maybe something to add to is something Sam and I were really hoping to go for is to not have, you know, maybe the typical talk or panel style format for all of our events that might be really common in a conference, but rather have different kinds of events that can focus on different kinds of activities. So having public conversations and debates, having exhibits and experiences, having resident or citizen science opportunities where people can actually be a part of creating data, and also learning opportunities if people want to upskill or learn about a concept. So to shout out some of the events I’m really excited for, maybe to start with the more wacky ones first. The Turing is hosting a “Cabaret of Dangerous Ideas” which will be a comedy show at the Camden Club where they will dig into topics like technology and data, but over a pint and through humour. So apparently, this is an 18+ show. So that might give you a little bit of a flavour of what’s to come. Another event we’re really excited about is an event called All the Docks, where a team of cyclists attempt this challenge in one day to hit every single Santander bike dock. They’ve done it before, and this year for London Data Week, they’re doing a round to hit, I think, the now over 800 docks that now exist in London. And this time, for London Data Week, what they’re also doing is making it a data collection exercise. So as they cross the streets of London, they’ll collect data on the road conditions, the cycling infrastructure, which can be then an open data set that people can use after the event as well. So those are two that jumped to mind. I don’t know, Sam, if there’s anything you want to highlight, or Brian because I know there’s a really exciting on that the RSS is…\nBrian Tarran\nYeah, I’ll put a quick plug in for the, so the Royal Statistical Society are organising an event associated with our Statisticians for Society initiative, which offers pro bono support to charities under a million turnover in the UK. So I can, I’ll post a link in the show notes so that people can find out more about that and sign up if they meet the criteria and are interested in taking part. But Sam, sorry, I, I hijacked there. But go ahead.\nSam Nutt\nNo, Brian, I was going to only mention your event. But no, a couple of others, I think just to show the like breadth of the types of activities. So you know, for example, it’s also it’s about partnering with organisations who are thinking and you know, already doing things like this, so the Science Gallery, for example, there’s– they’ve got an exhibition called “AI: Who’s looking after me?”, which is, you know, looks at some of the playful ways that AI is already involved in people’s lives and brings sort of people on a critical journey. That’s more of that sort of art exhibition type thing. In local government, where, in LOTI, we’re helping some boroughs with developing basically, a toolkit, a resource to help officers in boroughs in some of the data teams, who maybe haven’t had that history of engaging with residents, as seen as more sort of technical back office staff, actually, you know, give them the confidence to go out and have a conversation with residents about some of their practice and see how they can improve it. So there’s been a lot of interest there, in particular, around having conversations about how we do data linkage better, which, you know, in some ways, feels– it’s quite a straightforward data topic. But actually, the huge thing is that these teams in boroughs have never thought, we need to speak with residents at the design stage of data projects, you know, the public, what might a digitally excluded, relatively low sort of data literacy person be able to tell us about our data work that’s helpful to me as, you know, a data scientist, that was sort of some of the historical thinking, but actually, we’re sort of bringing boroughs on the journey of thinking, actually realising, you know, the value of doing that. So that’s sort of, I guess, the range of types of things as well.\nBrian Tarran\nFantastic. So where, if people do want to sign up for any of these events, is there a good way for them to do that? Is it go to the LOTI website– not LOTI website, the London Data Week website, I’m guessing?\nJennifer Ding\nThere is a good amount of info on both the Turing and the LOTI website, but the best place we’d suggest is londondataweek.org. There you’ll find a list of events. And if you click on an event, there’s more information. And also, if you click “Find out more”, you can access a link for more information on how you express interest or sign up.\nBrian Tarran\nExcellent. And if people– when we’re speaking, we are almost exactly a month away from London Data Week. If someone’s listening to this and they think, Oh, I’ve got a great idea for an event I want to organise, is it too late for them to squeeze onto the programme now? Should they get in touch somehow if, if inspiration strikes?\nSam Nutt\nWe’ll never say never. Probably there is a very late point in which we would say never. I think realistically, you know, the way it’s being run, it’s being run often with the time of volunteers and this sort of thing. And it is sort of the first year of us running it. So a lot of it’s coming through, through Jen and I. So if you are interested in running something, please do reach out to us. You know, we’ve got very good, Jen and I, at finding creative ways to slot people into programs and this sort of thing. But equally I think at this point, it’s really more about you know, bringing together people and organisations who are interested and actually, you know, I think we’ve got a really good exciting, fun set of events and activities across London that would be great to be part of even if you yourself haven’t been able to organise something, and then maybe it’s something for next year, for London Data Week 2024. Fingers crossed, we might be able to do something there.\nBrian Tarran\nSo the stress of day jobs and organising a week-long event, or week-long collection of activities, hasn’t put you off doing it again? No. \nJennifer Ding\nSo far, so good, Brian.\nSam Nutt\nYeah, we’ll “no comment” some of that.\nJennifer Ding\nWe’re definitely really excited though. And if anyone does have an idea or wants to start a conversation, there’s a contact form on our website, drop us an email. We also have a Twitter if you want to send us a message through that. So at the very least, we love, we’d love to chat.\nBrian Tarran\nGreat, well, we’ll put all those contact details, social media accounts, etc., into the show notes so people can find you. But thank you very much for joining us today. I know this must be a very busy time for you. But Sam Nutt, Jennifer Ding, thank you for joining us and talking about the upcoming London Data Week. I’m looking forward to it.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Images are not covered by this licence. Thumbnail image by ARTHUR YAO on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “London Data Week is almost here. What’s it all about?” Real World Data Science, June 8, 2023. URL"
  },
  {
    "objectID": "careers/career-profiles/posts/2023/04/24/sami-rahman.html",
    "href": "careers/career-profiles/posts/2023/04/24/sami-rahman.html",
    "title": "‘I always thought someone like me couldn’t work in data, let alone data science’",
    "section": "",
    "text": "Hi, Sami. Thank you for sharing your career story with Real World Data Science. Please tell us a little about yourself and your role in data science.\nHello! I’m Sami Rahman, a passionate head of data engineering and data platform at Penguin Random House, the book publisher that has enriched lives through literature. I started my career in data science five years ago and I’ve evolved into a data generalist with expertise in machine learning, data infrastructure, and data strategy.\nWhat does your job involve?\nMy role is about harnessing the power of data to drive extraordinary outcomes. Leading a skilled team, we empower our company to leverage data and cutting-edge technologies for informed decisions and automation. I help shape our organisation’s capabilities in data science, analytics, machine learning, data management, and strategy.\nWhat does “data science” mean to you?\nData science, to me, is a captivating fusion of modern data technologies and computational statistics that tackles business challenges, crafts intelligent automation, and generates insightful revelations.\nWhat do you think is your most important skill as a data scientist?\nActive listening is key. A data scientist must be surgical and precise in developing models, analysis, and tools that reinforce the company’s bottom line and operations. Data science exists to create value using data.\n\n\n\n\n\n\nPhoto supplied by Sami Rahman, used with permission.\n\n\n\n\n\nAs I’ve transitioned into management, maintaining my coding prowess is an ongoing challenge. I stay sharp by doing data science and infrastructure development for fun, leveraging tools like ChatGPT and AirOps where I’m rusty.\n\n\n\nHow did you get into data science?\nI began with a psychology degree, which led to working as business psychologist where I discovered psychometric data analysis. After a master’s in countering organised crime and terrorism and a few short jobs in counter terrorism/intelligence, I decided that it wasn’t for me. I embraced my love for statistics and research, I dove into data science, learning Python through online platforms, and secured my first data scientist role at a WPP agency called Essence.\nWhat, or who, first inspired you to become a data scientist?\nI always thought someone like me couldn’t work in data, let alone data science. Dr Suzy Moat’s fascinating talk on machine learning’s application to human behaviour and psychology showed me that a psychologist could make a significant impact in this field, inspiring my aspiration to try to have a data science career.\nWhat were the hurdles or challenges that you needed to overcome on your route into the profession?\nBreaking into data science without a typical background in maths/computer science/physics was daunting. Building a Kaggle portfolio and coding models for fun prepared me for interviews. Another challenge was learning to harmonise my “data brain” and “business brain” to solve problems efficiently. Understanding how data solutions impact business problems will always propel you forward. \nAnd what are the challenges that you face now, as a working data scientist?\nAs I’ve transitioned into management, maintaining my coding prowess is an ongoing challenge. I stay sharp by doing data science and infrastructure development for fun, leveraging tools like ChatGPT and AirOps where I’m rusty. I’m currently building my own cloud data platform and running a lot of image neural networks on it.\nWhat was your first job in data science, and how does it compare to your current role?\nAs an analytics executive at WPP agency Essence, I tackled data science, cloud engineering, and analytics problems for clients. They were a lot more singular and tactical in nature. Now, as head of data engineering and data platform at Penguin Random House, I focus on shaping data and technology strategy to align with the company’s broader vision.\nWhat was the most important thing you learned in your first year on the job?\nTo always consider the bigger picture: how your work integrates with the organisation/client’s objectives, delivers value, and aligns with the aspirations of other stakeholders. Actionable insights and value is the most important thing.\nWhat have been your career highlights so far?\nTwo shining moments include being the first of three of HSBC UK fraud data science leaders, where each of our departments tackled a different type of crime and protected our customers, and developing data strategies and capabilities for analytics, science, and business intelligence at Penguin Random House.\nHave there been any mistakes or regrets along the way?\nI regret not delving deeper into natural language processing (NLP) or spatial data science, which are now more accessible and growing fields within data science. I reckon the NLP methodologies would’ve been extremely useful seeing as I’m at a publishing company now!\nHow do you think your role will evolve over the rest of your career?\nAs data technologies become more accessible, I anticipate data roles will transform. I envision a future where data professionals focus on general AI, quantum machine learning, and multi-dimensional data analytics as traditional specialisms become democratised.\nIf you were starting out in data science now, what three things would you put at the top of your reading/study list?\nI’d recommend Skin in the Game by Nassim Nicholas Taleb, Calling Bullshit: The Art of Scepticism in a Data-Driven World by Jevin West and Carl Bergstrom,  and Artificial Intelligence: How Machine Learning Will Shape the Next Decade by Matthew Burgess.\nWhat personal or professional advice would you give for anyone wanting to be a data scientist now?\nSuccess in data science hinges on understanding how it can transform organisations and engaging with business stakeholders. My advice: never stop listening to the business – the stakeholders are your biggest allies. I would also try to find your niche that sets you apart from everyone else. Mine when I first started in the field was my expertise on computational psychology and behavioural machine learning. \nWhat new ideas or developments in the field of data science are you personally most excited about or intrigued by?\nTransfer learning excites me most, as numerous large technology companies now offer pre-trained models based on billions/trillions of parameters. This will revolutionise industries worldwide, as it will be easier to build more performant models even if a company has less data.\nWhat do you think will be the main challenges facing data science as a field in the next few years?\nThe challenge lies in staying relevant amidst the democratisation of data science. Through large language models, low-code, and transfer learning, advanced data science methods will become easier for non-specialists to do and use. Innovation and keeping up with modern data technologies will be crucial.\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo of Sami Rahman is not covered by this licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘I always thought someone like me couldn’t work in data, let alone data science.’” Real World Data Science, April 24, 2023. URL"
  },
  {
    "objectID": "careers/career-profiles/posts/2023/06/28/albert-lee.html",
    "href": "careers/career-profiles/posts/2023/06/28/albert-lee.html",
    "title": "‘Living my identity takes courage. It is the same courage necessary to start a new business’",
    "section": "",
    "text": "This week, in celebration of Pride, Real World Data Science is collaborating with the JEDI Outreach Group of the American Statistical Association (ASA) and the ASA LGBTQ+ Advocacy Committee to highlight the achievements of statisticians and data scientists from across the LGBTQ+ spectrum.\nMembers of the committee nominated two individuals to be featured as part of our career profile series, and so we’re pleased to bring you interviews with Albert Lee (below) and Claire Morton.\nRead on to discover more about Albert’s data science career (so far).\n\n\n\nHi, Albert. Thank you for sharing your career story with Real World Data Science. Please tell us a little about yourself and your role in data science.\nMy name is Albert Lee. I’m the founding partner at Summit Consulting, a quantitative and financial consulting firm in Washington, DC. Summit delivers data-driven solutions to help make government effective and society just. I started Summit in 2003, and we recently celebrated our 20th anniversary.\nI received my PhD in economics from UCLA in 1999. My professional practice is focused on econometrics – an academic specialty that blends economic theory with statistical practices – and statistical sampling.\nWhat does your job involve?\nA large portion of my time is spent running Summit and making decisions about management, personnel, and business development. That said, I am still pretty active in technical topics. I am a testifying expert in econometrics and statistical sampling. Recently, I have been leading a team of data scientists who are reformulating the edit and imputation algorithms for the US Department of Agriculture’s National Agriculture Statistical Service, which collects survey data from US agriculture sectors.\nWhat do you think is your most important skill as a data scientist?\nExplaining technical concepts is a big part of my job, and it requires the ability to consume the technical literature and know the concepts well enough that I can explain them to a lay audience (such as lawyers, judges, and program staff).\nHow has your gender and/or sexual identity factored into your career?\nMy gender and identity have given me important perspective as a data scientist and an entrepreneur. Living my identity takes courage. It is the same courage necessary to start a new business. From a young age, my identity has conditioned me to be comfortable with differences.\nMy identity has also taught me to see similarity among differences. Empathy is essential in client services and especially in quantitative consulting, where some of my clients feel disempowered by the complex subject matter.\n\n\n\n\n\n\nAlbert Lee\n\n\n\n\n\nThe data science field is moving very fast. Every day brings a new algorithm, software program, and hardware innovation. Since data science is a multidisciplinary field, keeping up with it has been challenging.\n\n\n\nHow did you get into data science?\nAlthough I studied mathematics and economics as an undergraduate student and economics as a graduate student, my academic training was very theoretical. I didn’t work with data and computers extensively until my first job outside of academia in the early 2000s. Little did I know that it was the advent of the “big data” revolution.\nAt Summit we serve mostly federal agencies, who are sitting on decades of administrative data – information they collected as part of their mission but not of research quality. These agencies want to use their administrative data to automate their routine tasks (like predicting which loans will default first) and evaluate program efficacy (determining whether a training program reached its goals). Extracting and analyzing administrative data has been a big part of my career.\nWhen I founded Summit, data science was not a recognized discipline. But as the datasets get larger, decisions about hardware setup, software programs, estimation algorithms, and data virtualization have become increasingly intertwined and interdependent. This really was my first taste of data science as we know it today.\nWhat, or who, first inspired you to become a data scientist?\nThere are too many people to mention by name. I owe a lot of my career to my first two managers at KPMG, Alan Salzberg and Rick Holt. They taught me how to code and reason quantitatively. And Rob Gould at UCLA has patiently converted a theorist to an empiricist. Once a convert, now a zealot.\nWhat were the hurdles or challenges that you needed to overcome on your route into the profession?\nI am an immigrant and a first-generation college graduate. My journey was full of unknowns. Figuring out my academic and professional career has taken a lot of exploration. In this regard, the same exploration that guided my identity also guided my academic and professional journey.\nAnd what are the challenges that you face now that you are working in data science?\nThe data science field is moving very fast. Every day brings a new algorithm, software program, and hardware innovation. Since data science is a multidisciplinary field, keeping up with it has been challenging. As I progress along my professional journey, striking the right balance between management, hands-on practice, and learning has been difficult as well.\nWhat was your first job in data science, and how does it compare to your current role?\nAs an entrepreneur, I was given a lot of professional freedom to actualize my career. To a large extent, I have the career that I envisioned. To me, data science lives in the intersection of methods, software, and hardware. I have spent a large part of my career in this intersection.\nOf course there are many things that were not part of the original vision, such as running a 100-person organization. My approach has always been intention with openness. By this metric, my current role is not far off from my original vision.\nWhat was the most important thing you learned in your first year on the job?\nThe ability and the love of learning constantly, regardless of the topic.\nWhat have been your career highlights so far?\nThe biggest highlight was that on June 15, 2023, Summit celebrated its 20th anniversary! Reformulating the National Agricultural Statistics Service’s edit and imputation systems is also a big deal. And being a testifying expert in some of the most consequential legal cases in the United States was a highlight as well.\nWhat three things are at the top of your current reading/study list?\nIn recent years, I have been binge-reading Stoic philosophy. I have read most books by Ryan Holiday. His most recent book was Ego Is the Enemy. In between the Stoics, you will find me reading Buddhist meditation literature, including Thich Nhat Hanh’s The Heart of the Buddha’s Teaching. David McCullough’s Truman is also by my bedside.\nWhat advice would you give for anyone wanting to be a data scientist?\nBe open and multidisciplinary. Many good ideas in statistics come from other fields, such as economics, medicine, sociology, and education. Computer science enables computational statistics. Having the openness to these topics is key.\nWhat new ideas or developments in the field are you personally most excited about or intrigued by?\nMachine learning has transformed statistics both as a consumer and a contributor. It consumes statistics in that it requires cutting-edge statistical techniques and algorithms for its estimation. Machine learning has important applications in many of the statistical sciences.\nAnd what do you think will be the main challenges facing the profession over the next few years?\nThe proper use of statistics or statistical ethics is an important societal challenge. Machine learning is becoming increasingly sophisticated, and its applications are more broad and pervasive. Machine learning algorithms are making more and more decisions in society, including mortgage loan approvals, residential home prices, and which prisoners receive parole. These are important and weighty decisions. How do we know that these decisions are unbiased and fair?\n\n\n\n\n\n\nAbout the ASA Pride Scholarship\n\n\n\nThe ASA Pride Scholarship was established to raise awareness for and support the success of LGBTQ+ statisticians and data scientists and allies. The scholarship will celebrate their diverse backgrounds and showcase the invaluable skills and perspectives these individuals bring to the ASA, statistics, and data science.\nApply or nominate someone for the ASA Pride Scholarship.\n\n\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo of Albert Lee is not covered by this licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘Living my identity takes courage. It is the same courage necessary to start a new business.’” Real World Data Science, June 28, 2023. URL"
  },
  {
    "objectID": "careers/career-profiles/posts/2023/03/28/tamanna-haque.html",
    "href": "careers/career-profiles/posts/2023/03/28/tamanna-haque.html",
    "title": "‘Data science challenges you to keep learning – there’ll always be new advances in the field’",
    "section": "",
    "text": "Hi, Tamanna. Thank you for sharing your career story with Real World Data Science. Please tell us a little about yourself and your role in data science.\nI’m Tamanna Haque. I’ve been working at Jaguar Land Rover for nearly four years, recently promoted to lead data scientist working within product engineering. It’s coming up to eight years that I’ve been working in the field, and my areas of interest are the use of machine learning to provide the best products and experiences for my customers and stakeholders.\nWhat does your job involve?\nMy role involves using the connected car and AI to make our products and customer experiences better, whilst leading within our wide data science team too. The data science team in Manchester, UK, originated with myself and one of my teammates – it’s since grown to nearly 40 (cross-sites and countries) and developed into a high-performing, advanced data science team.\nWhat makes us stand out is the nature of our work – we mostly use vehicle data (of participating customers), which is different to a lot of other commercial businesses or teams who’ll focus more on transactional or web data. The data we use lends itself to some pretty interesting projects, and a general futuristic feel here.\nI’m particularly interested and active in enabling a more electric and modern luxury future from the use of vehicle data.\nWhat does “data science” mean to you?\nThe realisation of value! Whether that is added revenue, saved costs or improved growth, I’m led by what data science can do for the business and its customers. The use of data science can open up many exciting, value-adding opportunities.\n\n\n\n\n\n\nPhoto supplied by Tamanna Haque, courtesy of Jaguar Land Rover. Used with permission.\n\n\n\n\n\nThere are more routes to getting into data science nowadays, but it’s important to not lose sight of fundamentals such as statistics and mathematics. A lot of people can code-up models but it’s fair to say that only a portion of them appreciate how to do this responsibly.\n\n\n\nWhat do you think is your most important skill as a data scientist?\nI’ve always presented myself as a technically astute data scientist, even when entering leadership. But my niche is my ever-growing commercial awareness and passion about our products, customers and business. These aren’t new qualities, but they now align with my professional interests, as well as personal (I’ve been a fan of the Jaguar brand since childhood)!\nHow did you get into data science?\nI did a maths degree at the University of Manchester, where I specialised in statistics. I didn’t do any post-graduate education and this was fine for me.\nAfter graduating, I joined a digital fashion retailer (with a financial services proposition) as an analyst initially. I learned a lot about real-life data and analytics itself, whilst developing a rounded understanding about the business and how to deal with stakeholders cross-functionally. I must have served a few hundred at least(!) and left most of the ‘fancy’ stuff I learned at university aside, whilst getting to grips with so many aspects of commercial analytics. A great way for me to set solid foundations for what followed, and I personally feel this gives me a lens that others who dive straight into data science don’t have.\nI was soon attracted to data science because it tapped into what I learned at university and challenges you to keep learning; there’ll always be things to learn, and new advances in the field.\nWhat, or who, first inspired you to become a data scientist?\nI have a twin sister, we’ve always been together throughout education. Even before we graduated together, she secured her first role as an analyst. This opened my eyes to data, and data science followed for us both!\nWhat were the hurdles or challenges that you needed to overcome on your route into the profession?\nI had a few people tell me I couldn’t do data science, possibly because I didn’t fit the typical data scientist stereotype in several ways. I think attitudes in the field have changed over time though and on a personal level, it’s motivated me to give it everything, and I can’t regret that.\nAnd what are the challenges that you face now, as a working data scientist?\nI need to manage my diary well to ensure effectiveness and work-life balance. I’m overseeing people, other projects, doing public speaking and trying to remain hands on. I sometimes block out chunks of time in my diary – I need some meeting-free time to produce quality technical work. I try to finish on time and enjoy a very busy social life with my family and friends. A flexible attitude to how we work helps to keep me happy and energised whilst I’m delivering from various angles.\nThinking back to your earlier roles in data science, how do they compare to your current role?\nMy current role is very different to my previous roles. I’m continually learning and adapting how I can be a good leader, providing support to a breadth of colleagues (in and outside the team) whilst delivering myself. I’m actively involved in setting and refining our team’s strategy and I’m enjoying leading projects which either deliver high financial impact or help set the path in terms of new tech and/or machine learning capability. There is much more responsibility but it’s easy to stay energised when working on cars and for a business I’ve long admired.\nWhat was the most important thing you learned in your first year on the job?\nI should have had more confidence in myself, but this grew – as I adjusted to the new environment I became much more assertive. My domain knowledge and data science expertise combined help to build my self-confidence, credibility and reputation.\nWhat have been your career highlights so far?\nI’m most proud of my recent promotion from senior to lead data scientist. Also it was exciting for my family and I when I gained an offer to join Jaguar Land Rover.\nHave there been any mistakes or regrets along the way?\nNo, what’s meant to be will be!\nHow do you think your role will evolve over the rest of your career?\nMy progression has been relatively rapid, and I hope I’ve got many, many years ahead of me in my career. It’s hard to say how my role will evolve, I have a blend of responsibilities in my role which combined provide great fulfilment for me at the moment.\nIf you were starting out in data science now, what would you put at the top of your reading/study list?\nA good understanding of analytics and the domain you’re in are my recommended prerequisites to doing data science.\nAnalytics is an important part of the data science lifecycle, being able to get the data yourself and communicate results with influence, for example, are just a few aspects of analytics which underpin successful data science projects.\nAlso, without awareness of the business and industry you’re working in, you can become very dependent on others. Data science itself can be quite challenging, so it’s great to have a solid foundation before starting out.\nWhat personal or professional advice would you give for anyone wanting to be a data scientist now?\nWith the level of continuous learning required to just simply keep up, it can be more of a lifestyle and not a job, so this is something to consider!\nWhat do you think will be the main challenges facing data science as a field in the next few years?\nI still expect to see a skills gap in the field. There are more routes to getting into data science nowadays, but it’s important to not lose sight of fundamentals such as statistics and mathematics. A lot of people can code-up models but it’s fair to say that only a portion of them appreciate how to do this responsibly, understanding samples versus populations, statistical testing, which type of regularisation to use in a neural network, et cetera.\nI also think there’s a challenge of questionable data science products reaching high levels of popularity and usage amongst the public… Some recent developments in this space have been extremely intelligent but raise ethical concerns. Just because something can be done with AI doesn’t mean it should, and my preferences are towards AI being ethical and (ideally) explainable.\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘Data science challenges you to keep learning – there’ll always be new advances in the field.’” Real World Data Science, March 28, 2023. URL"
  },
  {
    "objectID": "careers/posts/2023/05/18/chatgpt-data-science-pt2.html",
    "href": "careers/posts/2023/05/18/chatgpt-data-science-pt2.html",
    "title": "Large language models: Do we need to understand the maths, or simply recognise the limitations?",
    "section": "",
    "text": "Part 1 of our conversation with the Royal Statistical Society’s Data Science and AI (DS&AI) Section ended on a discussion around the need to verify that large language models (LLMs), when embedded in workflows and operational processes, are working as intended. But there was also acknowledgement that this could be difficult to achieve, not least of all because, as Giles Pavey said, “nobody knows exactly how these things work – not even the people who build them.” And then, of course, there is the speed at which developments are taking place: Trevor Duguid Farrant made the point that an expert may not even have a chance to finish reviewing the latest version of an LLM before a new iteration is rolled out.\nThese issues – of verification, explainability and interpretability – are of particular interest to data scientists like Anjali Mazumder, whose work focuses on the impact AI technologies could have, and are having, on society and individuals.\nIn part 2 of our Q&A about ChatGPT and other LLM-powered advances, and what all of this might mean for data science, Mazumder kicks off the conversation by setting out her perspective.\nOur full list of interviewees, in order of appearance, are:\n\nAnjali Mazumder, AI and Justice & Human Rights Theme Lead at the Alan Turing Institute, and DS&AI committee member.\nDetlef Nauck, head of AI and data science research at BT, and editorial board member, Real World Data Science.\nMartin Goodson, CEO and chief scientist at Evolution AI, and DS&AI committee member.\nLouisa Nolan, head of public health data science, Public Health Wales, and DS&AI secretary.\nPiers Stobbs, VP science at Deliveroo, and DS&AI committee member.\nTrevor Duguid Farrant, senior principal statistician at Mondelez International, and DS&AI committee member.\nGiles Pavey, global director for data science at Unilever, and DS&AI vice-chair.\nAdam Davison, head of data science at the Advertising Standards Authority, and DS&AI committee member.\n\n\n\n\nAnjali Mazumder: I work in research, but I also sit in the crux of government, industry, and civil society, looking at how they’re using these technologies. For me, it’s about knowing what the opportunities are but also understanding the limitations, the risks and the harms, and how we balance those and put in place oversight mechanisms that act as safeguards to ensure that these technologies don’t cause harm. We’re taking a very socio-technical approach that requires an interdisciplinary team to understand these issues and what should be done. Part of this is about not only the outcomes and the impact but also the upstream side of it – recognising that these models have been built on the work of people who have done the labelling, and that this also has implications – to say nothing of the associated environmental issues or energy issues!\nDetlef Nauck: I think the regulators really have to look at this. It has come completely out of left field for them. All the regulators that we are monitoring, they regulate the space as it was three years ago – they are mainly concerned about predictive models and bias. But if you look at, say, what Microsoft wants to do – putting GPT into Office 365 and into Bing – that will completely change how people interact with and consume information. I think the large tech companies really have a responsibility here, when they make this public, to make sure that people understand what this technology actually is, and how it can be used and has to be used.\nAlso, they need to open up about how these things have been built. There are a lot of stories around how OpenAI used cheap labour in order to do the labelling and reinforcement learning for ChatGPT, and these things have to become public knowledge; they need to become part of some kind of Kitemark for these models: “Ethically built, properly checked, hallucinate only a little bit. Whatever you do, don’t take it for granted. Check it!” That’s the kind of disclaimer they need to put on these models.\n\n\n\n\nIf you look at what Microsoft wants to do – putting GPT into Office 365 and into Bing – that will completely change how people interact with and consume information. Large tech companies really have a responsibility here, when they make this public, to make sure that people understand what this technology actually is.\n\n\n\nRegulatory principles always seem to stress that AI systems should be understandable, and we should be able to explain how we get particular outputs. But a lot of our conversation has focused on how we don’t really know how these models work. So, is that, in itself, a problem, and is it something that the data science community can help with – to dig into how these things work and try and get that across – to help meet these principles of explainability and interpretability?\nDN: That’s a very specialist job, I would say – specialist research into how these mathematical structures work. It’s not something I could do, and I’ve not seen any significant work there. One thing that we are interested in is whether we can do knowledge embedding, so that you can “teach” concepts that these models can then use to communicate, and that would lead to smaller systems where you have some understanding of what’s inside. But all of this kind of work, I think, is very much just beginning.\nMartin Goodson: Do we actually need this? There’s sort of a big assumption there that you need to understand how LLMs work in order to build in the kinds of things that we care about as a society. But we don’t understand how humans think. Of course, we can ask a human, “Why did you make that decision?” You can’t understand the cause of that decision – that’s a complex neuroscience question. But you can ask what the reason is for making a decision, and you can ask an LLM what its reasoning is as well. I think a lot of these questions about explainability are stuck in the past, when you’re trying to explain how a linear model works. It’s really not the same thing when you’ve got an LLM where you can just say, “Why did you make that decision?”\nLouisa Nolan: I was going to say something very similar. Most people don’t know how most things work…\nDN: My point was, these things are largely still like the Improbability Drive in the Hitchhiker’s Guide to the Galaxy. You press a button, and you don’t really know what comes out, and that’s the problem we need to get our heads around.\nLN: But people don’t know what percentages are, and yet we still use them for decision making. I don’t think people need to understand the maths behind LLMs, and I think it would be a hopeless job to expect everybody to do that. What we do need to understand is what LLMs can and can’t do. What’s the body of work that they are drawing from? What isn’t in there? What are the things that you need to check? So, for some things, it’ll be brilliant: if you’ve written something and you want it rewritten for a nine-year-old; if you want to summarise a paper; if you want to write code, as long as you already know how to code – these could be real labour-saving tasks. If you’re using ChatGPT to write a thesis about something that you haven’t looked at, that’s where the danger is. It’s this kind of simple understanding that people need to get in their heads – and the maths, except for the people who care about it, is beside the point, and probably detrimental, because it means that people won’t engage with it.\nDN: I agree, but I wasn’t talking about the general public. I meant, the people who build these things – they should know what they’re doing.\n\n\n\n\nThere’s a big assumption that you need to understand how LLMs work in order to build in the kinds of things that we care about as a society. But we don’t understand how humans think. You can ask a human what their reason is for making a decision, and you can ask an LLM what its reasoning is as well.\n\n\n\nWe talked there about communication. There was a webinar recently by the Royal Statistical Society’s Glasgow Local Group, and the presenter, Hannah Rose Kirk, showed how you can take tabular data and statistical results and ask ChatGPT to produce a nice paragraph or two that explains the numbers. Is this the sort of thing that any of you have experimented with? Have you had any successes at using ChatGPT to translate data into readable English that decision makers can act on?\nPiers Stobbs: I have an interesting use case. We had a basic survey: 200-odd responses, multiple languages, and we just said, “Please summarise the results of this survey contained in this CSV file.” And it came up with five or six relevant bullet points. What was amazing was that we could then interrogate it. For example, “Please compare the results that were in English versus in French, and describe the differences.” Again, it did it, but then you have the issue of, was it all correct? Well, the bulk of it was. Now I am intrigued by whether you can ask it to do correlations and some actual statistical things on a dataset, and does it get that right? I don’t know. We’ve not really got to that. But, to go back to one of the earlier discussion points around productivity, that initial survey work could have easily taken a week of someone’s time if we didn’t run it through ChatGPT.\nTrevor Duguid Farrant: Piers, in this case you’re interested in checking and seeing if it’s right. If you’d asked a group of people to do that survey for you and get the results, you’d have just accepted whatever they gave you back. You wouldn’t have questioned it.\nPS: That’s true. And the results were plausible, certainly.\nAM: I think one of the challenges is that the results could seem like they’re plausible, right – whether that’s a statistical output or a text output. This was not a proper experiment, but I asked ChatGPT about colleagues and friends who are quite prominent researchers, asking, “Who is so-and-so?”, and it produced biographies that were quite plausibly them, but it wasn’t them. It might have listed the correct PhD, say, but the date was off by a year, or the date was correct but it was from the wrong institution. So, depending on what the issue is, these seemingly plausible results could have more serious implications.\nLN: So, just to join those two things together: for me, the question is not, “Do we understand how ChatGPT works?” As Martin says, we don’t understand how humans work, and surely we’re trying to develop something that enhances human thinking in some way. The more important question for me is, “How do we know that what is produced is useful?”\n\n\n\n\nFor me, the question is not, ‘Do we understand how ChatGPT works?’ We don’t understand how humans work, and surely we’re trying to develop something that enhances human thinking in some way. The more important question is, ‘How do we know that what is produced is useful?’\n\n\n\nGiles, you mentioned previously that you’re doing some work at Unilever around how to minimise hallucination. I don’t know how much you can say on what direction that’s going in, and how successful you expect that to be, but that’s obviously going to be a really important part of refining these models to be more widely usable.\nGiles Pavey: I’m by no means an expert, but there’s quite a lot you can do with both the architecture of it and also the pre-prompts that you put in – more or less saying, “Quote what the source is, and if you’re not sure, then tell me you’re not sure.” I think what’s interesting is the question of whether we’ll have to rely on OpenAI or Microsoft to do that work, and it will be just another thing that we have to trust them for. Or, will it be something that people within an organisation can put in themselves?\nMG: I think it’s absolutely critical that open-source models are developed that can compete with these tech companies, otherwise there’s going to be a huge transfer of power to these companies.\nGP: Arguably, the single biggest issue is, who elected Sam Altman (no-one) and are we as society happy with him having so much power over our future?\nTo close us out, I’d like to return to a question Trevor posed earlier, which is: How might organisations like the Royal Statistical Society help companies to embrace LLMs and start using them, so that everyone can benefit from the technology?\nAdam Davison: My instinct is that there’s some great parallel here with the stuff that the Data Science and AI Section have been doing in general, where we’ve said, “OK, there’s lots of good advice out there on how to do things in data science, but how do you make it practical? How do you make it real? How do you apply those ethical principles? How do you make sure you have people with the right technical understanding in charge of projects to get value?” If, five years ago, the hype around data science was leading organisations to hire 100 data scientists in the hope that something innovative would happen, well then, we don’t want those same organisations now thinking that they need to hire 100 prompt engineers and keep their fingers crossed for something special. Our focus has been on “industrial strength data science”, so I think we can extend that to show what “industrial strength LLM usage” looks like in practice.\n\n\n\n\n\n\nWant to hear more from the RSS Data Science and AI Section? Sign up for its newsletter at rssdsaisection.substack.com.\n\n\n\n\n\n\n\n← Read part one\n\n\n\n\nBack to Careers\n\n\n\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Images are not covered by this licence. Thumbnail image by Google DeepMind on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Large language models: Do we need to understand the maths, or simply recognise the limitations?” Real World Data Science, May 18, 2023. URL"
  },
  {
    "objectID": "careers/posts/2023/04/18/politics-of-performance-measurement.html",
    "href": "careers/posts/2023/04/18/politics-of-performance-measurement.html",
    "title": "The politics of performance measurement",
    "section": "",
    "text": "At the beginning of 2016, the Criminal Justice Division (CJD) of the Texas Governor’s Office received news all government agencies dread: budgets were to be cut. CJD oversaw a grant program that funded specialty courts throughout the state, however it was now being told that the program’s budget of $10.6m would be reduced 20% to $8.5m by 2018.\nHow should these cuts be distributed among grant holders? CJD had no meaningful performance data on which to base its decisions, and I would know: I was hired by the agency just a few months before to analyze grant performance. Still, decisions needed to be made. We had to come up with a plan of action, and the clock was ticking…\nThis is a story of making opportunity out of crisis, of the interaction between not just theory of change and technical implementation, but the “political” process of negotiating these changes with stakeholders in a manner that led to better decisions. Through careful outreach and continuous communication, we developed a data collection and performance assessment process that enabled us to allocate budget cuts in a manner widely accepted.\nThe story ends on a bittersweet note. But, along the way, there are lessons to be learned about how to find common ground, manage expectations, forge productive working partnerships, and sustain a data science project longer term."
  },
  {
    "objectID": "careers/posts/2023/04/18/politics-of-performance-measurement.html#step-1-consider-your-options",
    "href": "careers/posts/2023/04/18/politics-of-performance-measurement.html#step-1-consider-your-options",
    "title": "The politics of performance measurement",
    "section": "Step 1: Consider your options",
    "text": "Step 1: Consider your options\nTexas had over 150 specialty courts in 2016, providing a program of specialized services – usually drug treatment – to offenders as an alternative to incarceration. About half of the state’s specialty courts received CJD grant funds (and about half of grantees received 100% of their program budget from our grants). Funding cuts of the size we needed to make would not go over well with them. Any changes to the program would have to run a gauntlet of decision-makers including advisory boards, interest groups, and professional associations, most with contacts in the legislature.\nComplicating this situation further, CJD didn’t even make the final funding decisions. We administered the grants, but the merit review process fell to the Specialty Courts Advisory Council, an appointed group of specialty courts staff and related experts who annually scored the grant applications we received. We needed to get them onboard.\nThe way our Executive Director saw it, we had three options to implement the cut in a way that could get us buy-in from stakeholders and the Advisory Council:\n\nCut across the board. The Advisory Council would employ the same scoring method as the previous year but reduce each grant amount by 20%.\n\nThis option would leave long-running grantees scrambling to make up for this shortfall by reducing services, laying off staff, or spending more of their limited local funds. Worse, it would punish all grantees equally – our most successful programs would be arbitrarily defunded.\n\nFewer grants. Grants were scored based on the quality of their application and all grants that passed a certain threshold got funded. The Advisory Council would employ the same scoring method as the previous year but instead of funding the top $10.6m worth of grants, they would fund the top $8.5m worth.\n\nThis seemed a less bad option than cutting across the board, but we would still run into the problem of arbitrarily defunding successful programs. Grants near the bottom of the Advisory Council’s cutoff that got funded the previous year would be denied renewal only because the goalposts had moved.\n\nTargeted funding. The Advisory Council would incorporate performance data and statewide strategic plan alignment into their scoring method and make cuts accordingly.\n\nAt the time, the Advisory Council did not take performance into consideration when scoring grant applications. They agreed in theory that a grant requesting its tenth annual renewal should perhaps at some point be assessed on its outcomes, but they had never seen CJD commit to a rigorous performance assessment process before. We administered the grants, not them, so without our commitment to develop a performance assessment process, and their trust in that commitment, this would not be a viable option.\nAfter due consideration, option 3 emerged as the favorite of our Executive Director. On the face of it, this seemed the most “objective” approach to take. We would let the data decide who gets funded and who doesn’t, rather than cutting arbitrarily. But that would be a fallacious argument. Data does not decide. It might inform our decisions, but it would be up to us to choose the structure of the performance measurement process: what aspects to focus on, what data to collect, what benchmarks to set – all of which would later help determine funding decisions. And in any funding decision, politics inevitably plays a role.\nPolitics is, in its broadest sense, the negotiated decision-making between groups with opposing interests. And in developing our performance measurement process we would encounter a variety of interests – from the Advisory Council down to the grantees themselves. Success would require us to acknowledge stakeholder perspectives and address or manage them appropriately. Planning decisions made in the early phases of a project as a result of political processes directly influence the type and scope of analysis a data scientist will eventually be able to perform, so it behooves the data scientist to participate in these processes!"
  },
  {
    "objectID": "careers/posts/2023/04/18/politics-of-performance-measurement.html#step-2-engage-stakeholders-and-define-performance",
    "href": "careers/posts/2023/04/18/politics-of-performance-measurement.html#step-2-engage-stakeholders-and-define-performance",
    "title": "The politics of performance measurement",
    "section": "Step 2: Engage stakeholders and define performance",
    "text": "Step 2: Engage stakeholders and define performance\nHaving settled on our preferred option, our Executive Director convened a strategy session with the Advisory Council to discuss how to proceed as part of a broader strategic plan. The session began by achieving consensus on high-level goals such as “fund strategically”, “focus on success”, “build capacity”, etc. The session also helped the Advisory Council and CJD alike to clarify our conception of how we ought to fit into the specialty courts field going forward. CJD would develop its performance assessment system to help the Advisory Council target funding, but that would come as part of a larger plan that included capacity building, training and technical assistance, helping courts obtain non-CJD sources of funding, and steering grantees toward established best practice.\nWe left the meeting with a very basic plan that looked good on paper. Our Executive Director set to work persuading our external stakeholders of the wisdom of this new strategic direction. Meanwhile, I had to build a performance assessment process that people could trust.\nCJD had no formally designated standards to measure performance data against. However, drug courts have been around for decades and there existed a large body of research supporting the program model.1 Offering supervised drug treatment instead of incarceration had been repeatedly shown to cost less money and lower recidivism rates. I performed a literature review and spoke with numerous subject matter experts to get started on defining program-specific performance metrics.\nI was conscious that imposing metrics without any feedback or input from affected parties would all but guarantee bad-faith engagement, especially if these metrics are tied to funding. A problem inherent to any performance measurement is that once something gets measured as a performance outcome, it warps the very processes it is intended to measure. This phenomenon happens so frequently that the phrase “Campbell’s Law” was coined to describe it in 1979.2 Think of standardized testing at schools: once the government ties test performance to school funding it creates powerful incentives for schools to improve test scores at any cost. Even in the absence of outright cheating, struggling schools feel massive pressure to adjust their curriculum, to the point where they teach test score optimization strategies more than math, language, history, and science.\nI consistently heard from specialty court scholars and practitioners alike that arrest recidivism would be the ideal outcome measure. On paper, recidivism was a direct expression of long-term program success and could also be used as an outcome variable for classification modeling. And, in practice, a court could do little to affect recidivism by way of manipulation. Courts do not make arrests – police make arrests. Once a specialty court participant finished a program, the court itself no longer intervened in their lives. If a participant got arrested within 1-3 years of completion, the program had no say in the matter.\nThis, however, presented an implementation problem: one-year recidivism data would, by definition, take a year past the point of implementation to collect, i.e., not soon enough to inform our cuts. And while recidivism was the best measure of success, it could not be the only measure. Recidivism was, after all, a stochastic process not within the court’s control – a crime wave or other systemic factors could move recidivism up and make it look like a successful court had actually failed. We would have to use something else as well.\nThe National Association of Drug Court Professionals (NADCP) publishes a book of best practice standards, and our stakeholders identified a court’s adherence to these standards as another strong performance assessment standard. These criteria, unlike recidivism, were directly under the program’s control. Does your program have the recommended staff? Does your program drug test participants frequently enough to guarantee sobriety? Does your program meet with participants regularly enough? Do you offer a continuum of services instead of a “one-size-fits-all” approach?\nIn addition to being much easier to measure than recidivism, best practice adherence also resists Campbell’s Law by avoiding outcome measurement. In our school metaphor, this would be like measuring school performance based on student-to-teacher ratio, variety of course offerings, attendance rates, and teacher qualifications. Far from perfect, but measuring a variety of elements that predict success and taking them as a whole represents a vast improvement over a single, easily-gamed outcome measure.\nBut to operationalize these standards, we would have to have good data."
  },
  {
    "objectID": "careers/posts/2023/04/18/politics-of-performance-measurement.html#step-3-update-processes-and-collect-data",
    "href": "careers/posts/2023/04/18/politics-of-performance-measurement.html#step-3-update-processes-and-collect-data",
    "title": "The politics of performance measurement",
    "section": "Step 3: Update processes and collect data",
    "text": "Step 3: Update processes and collect data\nWe inherited a longstanding process in which grantees had to fill out a form every six months asking them to report performance data. This is a screenshot of what that form looked like:\n\n\n\n\n\nNo additional definitions or instructions were provided, leaving grantees with many questions: Does the request for “annual data” mean as of fiscal year or calendar year? What counts as a person being “assessed for eligibility”? And so on. Grantees did not know the answers, and neither did we. And these were the more straightforward measures. The form went on for 10 pages, most of which asked grantees to report extensively on information they had already provided as part their application.\nThis disaster of an assessment process did have a silver lining. When we announced we were throwing out these forms entirely we faced almost no pushback from grantees.\nWe knew from the start that our new assessment process would need to collect individual-level participant data instead of aggregated measures. Even with clear definitions, 75 grants would mean 75 different aggregations at work. Asking the grantees to report their individual-level participant data in a consistent format and doing the aggregations ourselves meant a single aggregation at work.\nBut we needed to establish trust with grantees before making this request. Strictly speaking, we could mandate the reporting of this data. However, if that angered enough of our grantees, they or their contacts might take it up with our bosses at the Governor’s Office, and our bosses could cancel any plan we came up with if they thought it was not worth the fuss. So, from day one we communicated clearly to all grantees that we would maintain total transparency when it came to definitions and calculations. Before we used any calculated metric to assess performance we would send it to the grantees themselves to review for accuracy.\nTo avoid the vagueness and inscrutability that characterized the old reporting process, every piece of data we asked for in the new process had a clear written definition and specific reason for being asked. These reasons usually amounted to some combination of best practices, Advisory Council recommendations, and grantee suggestions.\nImplementing the new process was far from easy, however. We faced numerous administrative and technical barriers. Texas courts at this time did not share a common case management system, so we couldn’t just get a data export from everybody. Meanwhile, the Governor’s Office banned all of its divisions from all usage of the cloud. This forced us to build a more labor-intensive reporting process, in which courts would obtain blank Excel templates with required data fields. Courts had either to fill out these templates by hand or export their case management data and reconfigure it to template specifications. Then, courts submitted their data for review and we sent back any bad formatting.\nWe collected preliminary data at the six-month mark and made another adjustment based on these results, which we would not count toward performance measurement. A majority of courts had some kind of data error in this first case. Specific definitions of data fields had to be written and rewritten using grantee feedback over the course of the year, leading to significant changes between the six-month reports and the year-end reports.\nImportantly, we had developed reporting requirements iteratively with participation from grantees and the Advisory Council from the start. By mid-2017 we had so successfully achieved buy-in that only one grantee court’s judge refused to give us data (the court’s grant manager later sent it to us)."
  },
  {
    "objectID": "careers/posts/2023/04/18/politics-of-performance-measurement.html#step-4-analyze-and-report-findings",
    "href": "careers/posts/2023/04/18/politics-of-performance-measurement.html#step-4-analyze-and-report-findings",
    "title": "The politics of performance measurement",
    "section": "Step 4: Analyze and report findings",
    "text": "Step 4: Analyze and report findings\nIn the course of this process, we established the benchmarks in Table 1 based on best practices and justification for funding. Because this was our initial rollout, we set the specific values low to function more as minimum standards than targets.\n\nTable 1: Specialty court best practices translated into quantitative measures.\n\n\n\n\n\n\n\n\n\nBenchmark\nBest practice\nRationale\n\n\n\n\n1. Number of participants\n10+\nCJD decision: programs should be of sufficient size to justify a grant\n\n\n2. Number of graduates\n5+\nCJD decision: programs should be of sufficient size to justify a grant\n\n\n3. Graduation rate\n20%-90%\nCJD decision: 0% and 100% success rates are both red flags\n\n\n4. Average amount of time graduates spent in program (in months)\n12-24\nNADCP best practice recommended program lengths of 1-2 years\n\n\n5. Percent of graduates employed, seeking education, or supported through family, partner, SSI, etc.\n100%\nNADCP best practice recommended against releasing participants without financial support, which all but guarantees relapse or rearrest.\n\n\n6. Percent of participants with “low-risk” assessment score\n0%\nNADCP best practice recommended moderate- or high-risk participants. Research had shown that low-risk participants get little benefit.\n\n\n7. Average sessions per participant per month\n1+\nNADCP best practice recommended sessions be held at least monthly.\n\n\n\nGrantee performance data for each benchmark would be generated from the individual level data that courts sent us. Crucially, we sent our aggregations back to grantees for confirmation prior to using them in any kind of evaluation, alongside the program-wide average and the best practice values for comparison (example in the table below). If something didn’t look right, they had the chance to let us know before we took their numbers as final.\n\nTable 2: Specialty court best practices compared with program-wide averages and grantee reported values.\n\n\n\n\n\n\n\n\n\n\nBenchmark\nBest practice\nProgram-wide average\nGrantee reported values\n\n\n1. Number of participants\n10+\n89\n96\n\n\n2. Number of graduates\n5+\n25\n27\n\n\n3. Graduation rate\n20%-90%\n71%\n56%\n\n\n4. Average amount of time graduates spent in program (in months)\n12-24\n17\n14\n\n\n5. Percent of graduates employed, seeking education, or supported through family, partner, SSI, etc.\n100%\n95%\n100%\n\n\n6. Percent of participants with “low-risk” assessment score\n0%\n18%\n2%\n\n\n7. Average sessions per participant per month\n1+\n2\n3.7\n\n\n\nIn the end, we found seven grants that we could unequivocally recommend be cut. Two of the seven had effectively never gotten off the ground, and served almost no participants the entire year. The other five served mostly low-risk participants, the type of people that research had shown do not benefit from specialty court programs. Some of these grantees were inevitably disappointed at the decision, but we had so actively worked within the field to develop and justify our processes that they understood why the decision had been made."
  },
  {
    "objectID": "careers/posts/2023/04/18/politics-of-performance-measurement.html#factors-for-success",
    "href": "careers/posts/2023/04/18/politics-of-performance-measurement.html#factors-for-success",
    "title": "The politics of performance measurement",
    "section": "Factors for success",
    "text": "Factors for success\nIn the span of one year, CJD went from collecting a large volume of useless data to a specific, targeted collection of data informed by best practices. The new collection process had high grantee compliance and stakeholder buy-in.\nThe following factors proved essential to getting to a place where we had useful, reliable data upon which to base future data science efforts:\n\n\nDiscontent with status quo\n\nThe Advisory Council wanted CJD to play a more active support role in the field. Meanwhile, everyone disliked the existing performance assessment process. As a result, most of the challenges we faced along the way related to implementation rather than defending the status quo on its merits.\n\n\n\nA catalyst for change\n\nDespite existing discontent, it took a funding shortfall to kickstart the process of change. It would have been unlikely for us to be able to create this system a priori.\n\n\n\nContinuous, high-quality communication\n\nWe could impose rules and requirements all day long, but without good faith engagement from the grantees we could never collect the quality of data we needed. Note that “continuous communication” does not mean “tell them everything you do at every point”. People become overwhelmed by torrents of information.\n\n\n\nHumility and flexibility\n\nHad we begun this process assuming we had all of the answers, we would have been dead in the water. Continuous outreach and willingness to take criticism and suggestions shaped the process as it progressed, ultimately producing a better end-product than we could have devised on our own.\n\n\n\nAn established program model\n\nDrug courts have been around for decades, with a vast body of supporting research and a community of practitioners and scholars we could speak to. That meant we could focus on implementation and execution instead of determining if the model worked or not.\n\n\n\nStrong leadership support\n\nFrom the very beginning, we could not have accomplished what we did without the full support and advocacy of our Executive Director."
  },
  {
    "objectID": "careers/posts/2023/04/18/politics-of-performance-measurement.html#coda-why-knowledge-transfer-is-vital",
    "href": "careers/posts/2023/04/18/politics-of-performance-measurement.html#coda-why-knowledge-transfer-is-vital",
    "title": "The politics of performance measurement",
    "section": "Coda: Why knowledge transfer is vital",
    "text": "Coda: Why knowledge transfer is vital\nI wish I could write a follow-up article about how we started using classification modeling to identify the most successful programs and to promote better approaches and practices; about how we iterated the process through multiple funding cycles, tuning and perfecting it to better meet stakeholder needs. But I cannot.\nThe performance assessment system we built had some major weaknesses from the outset. It was labor intensive, not required by law, produced no immediate benefit to the agency itself, and was so new it had yet to be entrenched in agency practice. In other words, no institutional incentives worked in its favor. Only the continual push of our Executive Director and myself kept this new performance assessment system going, and once we left the agency, it foundered.\nStill, the experience taught me much. I learned first and foremost that programs do not sustain themselves. Most of our attention had been focused on building up the best process we could. Only a minimal effort had been spent on institutionalizing and sustaining it. We had written documentation but no fundamental changes in policy or rule. We had undertaken groundbreaking efforts and built relationships, but had not planned for any meaningful knowledge transfer to other staff. While we had intended to eventually do these things, fate took us away before we could get them in place.\nFor any kind of change to last, sustainability must be built in from the start. In the moment, these actions can seem low-priority. Policy and rule changes can be arduous and time-consuming. Knowledge transfer from one stably employed staff to another feels redundant and wasteful. But without embedding sustainability, no success will outlast the individual people pushing for it.\n\nBack to Careers\n\n\n\n\n\nAbout the author\n\nNoah Wright is a data scientist with the Texas Juvenile Justice Department. He is interested in the applications of data science to public policy in the context of real-world constraints, and the ethics thereof (ethics being highly relevant in his line of work). He can be reached on LinkedIn.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Noah Wright\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nWright, Noah. 2023. “The politics of performance measurement.” Real World Data Science, April 18, 2023. URL"
  },
  {
    "objectID": "careers/posts/2023/04/18/politics-of-performance-measurement.html#footnotes",
    "href": "careers/posts/2023/04/18/politics-of-performance-measurement.html#footnotes",
    "title": "The politics of performance measurement",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome newer types of courts (Commercial Sexual Exploitation, Mental Health, Veterans) had a much more limited body of research and had to be accommodated separately. For the sake of keeping this narrative coherent I’m focusing on drug courts, which were the majority of our programs.↩︎\nRodamar, Jeffery. 2018. “There ought to be a law! Campbell versus Goodhart.” Significance 15 (6): 9. https://doi.org/10.1111/j.1740-9713.2018.01205.x↩︎"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact us",
    "section": "",
    "text": "Email: b.tarran@rss.org.uk\nGitHub: @realworlddatascience\nTwitter: @rwdatasci\nLinkedIn: RSS Real World Data Science\nMastodon: @rwdatasci"
  },
  {
    "objectID": "contact.html#editorial",
    "href": "contact.html#editorial",
    "title": "Contact us",
    "section": "",
    "text": "Email: b.tarran@rss.org.uk\nGitHub: @realworlddatascience\nTwitter: @rwdatasci\nLinkedIn: RSS Real World Data Science\nMastodon: @rwdatasci"
  },
  {
    "objectID": "contact.html#advertising-and-commercial",
    "href": "contact.html#advertising-and-commercial",
    "title": "Contact us",
    "section": "Advertising and commercial",
    "text": "Advertising and commercial\nEmail: advertising@rss.org.uk"
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html",
    "href": "contributor-docs/contributor-guidelines.html",
    "title": "Contributor guidelines",
    "section": "",
    "text": "Thank you for your interest in contributing to Real World Data Science. This page will walk you through the process of preparing and submitting your idea. If you haven’t done so already, please review our call for contributions before continuing."
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#site-functionality-and-ethos",
    "href": "contributor-docs/contributor-guidelines.html#site-functionality-and-ethos",
    "title": "Contributor guidelines",
    "section": "Site functionality and ethos",
    "text": "Site functionality and ethos\nReal World Data Science is built on Quarto, the new open-source publishing system developed by Posit. The site has been designed from the ground up as a platform for data scientists, created by data scientists. Here’s what this means in practice:\n\nContributors can use data science software and tools to create content – e.g. Visual Studio Code, RStudio, Jupyter Lab; Python, R, Observable, and Shiny – allowing for the full integration of text, code, figures, equations, and other elements.\nReview and editing are transparent and collaborative, again making use of tools data scientists are familiar with – e.g. GitHub, Google Docs – for sharing and revising documents prior to publication.\nContent can be both engaging and interactive. Many data scientists learn by doing, so code can be made available as R Markdown or Jupyter Notebook files to be reused and experimented with offline. Or, the same documents can be used online through tools like Google Colab and Binder. Where appropriate, the use of interactive displays and Shiny apps is encouraged, allowing for data visualisations to be interrogated and regenerated on the fly.\nSite users are contributors too. Through annotation and commenting functionality, site users can interact and converse with authors and other members of the Real World Data Science community. And with all source files hosted on GitHub, users of our site can raise issues, or fork and propose improvements – leading to a true exchange of knowledge."
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#the-submission-process",
    "href": "contributor-docs/contributor-guidelines.html#the-submission-process",
    "title": "Contributor guidelines",
    "section": "The submission process",
    "text": "The submission process\n\nContact site editor Brian Tarran to discuss your proposed submission.\nWorking with the editor, draw up a short content brief, containing the following:\n\nTitle of submission\nAuthor name(s) and affiliation(s)\nTheme/topic area\nTarget audience\nSynopsis or sell line, summarising the story and its importance/value (250 words max.)\nKey audience takeaways\nFormats and features (e.g., text, audio, video; code blocks, interactive data visualisations, etc.)\nAccessibility considerations\nTarget length/word count\nFirst draft to be submitted by…\n\nOnce a content brief is finalised and approved, content is to be prepared in the agreed format and with reference to our style guide. For simple text-based articles, we recommend using Google Docs or Microsoft Word; for submissions that incorporate technical or multimedia content, such as code, equations or interactive graphics, we recommend the Quarto (.qmd) file format, but documents can also be submitted in Jupyter notebook (.ipynb) and R Markdown (.Rmd) formats.\nDraft submissions should be sent via email to the editor. Alternatively, contributors can commit their drafts to their own GitHub accounts and add the Real World Data Science GitHub account as a collaborator."
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#copyright-and-content-licencing",
    "href": "contributor-docs/contributor-guidelines.html#copyright-and-content-licencing",
    "title": "Contributor guidelines",
    "section": "Copyright and content licencing",
    "text": "Copyright and content licencing\nContributors retain copyright of their work, but agree to publish their work under a Creative Commons licence. Contributors are free to choose the licence that best suits their content. The chosen licence should be indicated on the draft submission."
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#the-review-process",
    "href": "contributor-docs/contributor-guidelines.html#the-review-process",
    "title": "Contributor guidelines",
    "section": "The review process",
    "text": "The review process\nDraft submissions will be shared for review with members of the Real World Data Science Editorial Board. Comments and edits to documents will be made via Google Docs/MS Word/GitHub, allowing for (a) version control, (b) open dialogue between reviewers and contributors, and (c) a transparent and well-documented review process.\nOnce revisions are complete and content is accepted for publication, authors will be provided with HTML files to preview published content. Following sign-off by author and editor, HTML files will be made live."
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#post-publication",
    "href": "contributor-docs/contributor-guidelines.html#post-publication",
    "title": "Contributor guidelines",
    "section": "Post-publication",
    "text": "Post-publication\nContributors and editors will work together to promote content via social media platforms – Twitter, LinkedIn, blogs – and in other channels as appropriate – e.g., in response to related questions on Quora or Stack Overflow.\nContributors are encouraged to monitor their content regularly for user comments and discussions. Engaging in discussions with users – whether through the Real World Data Science platform or via social media and other channels – is an effective way of developing an audience: it builds profile for the contributor and their content, and encourages other users to find and interact with content."
  },
  {
    "objectID": "contributor-docs/datasciencebites.html",
    "href": "contributor-docs/datasciencebites.html",
    "title": "DataScienceBites",
    "section": "",
    "text": "Our DataScienceBites blog publishes digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space. Our goal is to make scientific papers more widely accessible.\nPosts are targeted at undergraduate level. Each presents a non-technical overview of a new research paper and its key findings, potential applications, and implications.\nWe welcome contributions from graduate students and early career researchers in data science (or related subjects) at universities throughout the world, as well as industry researchers. Contributors must have a passion for science communication and a drive to explain, clarify and demystify."
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#basic-structure-of-a-bites-post",
    "href": "contributor-docs/datasciencebites.html#basic-structure-of-a-bites-post",
    "title": "DataScienceBites",
    "section": "Basic structure of a Bites post",
    "text": "Basic structure of a Bites post\n\n\nInformation box\n\nGive the full title of the paper you are discussing, the name(s) of its author(s) and year of publication. Say where the paper is published, whether it is a pre-print or peer-reviewed publication, whether it is open access or not, and include links to authorised HTML and/or PDF versions.\n\n\n\nIntroduction and background\n\nEase readers into the paper you are writing about, and help them to see why it is worth their attention. Sometimes a paper is sufficiently ground breaking to be attention grabbing in its own right. But more often than not you will have to find a way to “hook” people in. When writing about data science tools and methods, for example, it can be helpful to start by outlining a typical application scenario or use case. Readers may be more familiar with the use case than the tool, so this provides valuable framing within which you can talk about the shortcomings of existing methods and the advances promised by the new research. Ultimately, you want to get readers to the point where they understand the problem, question or challenge that this new research paper aims to solve.\n\n\n\nResearch overview\n\nHere is where you summarise the work done by the paper’s authors. Remember to keep the discussion non-technical and jargon-free, and explain important terms and concepts as necessary. Be careful not to oversimplify!\n\n\n\nTakeaways and implications\n\nPut this paper and its contributions into the appropriate context for readers. Does it make modest but important improvements to existing knowledge or research processes? Will it help others to address new questions, issues and challenges? What further work is needed to build on these advances?\n\n\n\nFurther reading\n\nReaders may wish to learn more about the research or the broader subject matter, so feel free to point them to additional resources: videos, podcasts, textbooks, conference presentations, etc."
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#word-count-target",
    "href": "contributor-docs/datasciencebites.html#word-count-target",
    "title": "DataScienceBites",
    "section": "Word count target",
    "text": "Word count target\n500–1,000 words."
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#two-ways-of-contributing",
    "href": "contributor-docs/datasciencebites.html#two-ways-of-contributing",
    "title": "DataScienceBites",
    "section": "Two ways of contributing",
    "text": "Two ways of contributing\n\nRegular contributors commit to publishing an agreed number of posts (6–12) each year. Regular contributors will collaborate closely with editors on the development of the blog, and experienced contributors will be invited to support and mentor new regular contributors to help grow the DataScienceBites team.\nGuest contributors make one-off or ad hoc contributions to the site. Proposals are to be submitted in the form of a content brief."
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#next-steps",
    "href": "contributor-docs/datasciencebites.html#next-steps",
    "title": "DataScienceBites",
    "section": "Next steps",
    "text": "Next steps\nIf you are interested in contributing to DataScienceBites:\n\nIdentify a new data science publication that you are interested in writing about.\nReview the blog index to make sure we haven’t already covered this publication.\nDecide whether you would like to be a regular contributor or guest contributor.\nContact Real World Data Science editor Brian Tarran to discuss.\n\n\n\n\n\n\n\nTwo things to keep in mind\n\n\n\nDataScienceBites contributors are not permitted to write about their own research papers.\nIf contributors are in any way affiliated with the authors of papers they write about, this must be disclosed as part of their submission."
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#about-the-sciencebites-family",
    "href": "contributor-docs/datasciencebites.html#about-the-sciencebites-family",
    "title": "DataScienceBites",
    "section": "About the ScienceBites family",
    "text": "About the ScienceBites family\nDataScienceBites is published by Real World Data Science and is part of the ScienceBites galaxy of sites. See sciencebites.org for an overview of the ScienceBites mission."
  },
  {
    "objectID": "contributor-docs/style-guide.html",
    "href": "contributor-docs/style-guide.html",
    "title": "Style guide",
    "section": "",
    "text": "Content must be presented in a conversational yet professional and respectful tone. Contributors should imagine themselves delivering a lively, engaging conference presentation, rather than preparing a dry, formal report or journal publication. Contributors to Real World Data Science are creating content for their colleagues and peers and should “speak” to them as such."
  },
  {
    "objectID": "contributor-docs/style-guide.html#tone",
    "href": "contributor-docs/style-guide.html#tone",
    "title": "Style guide",
    "section": "",
    "text": "Content must be presented in a conversational yet professional and respectful tone. Contributors should imagine themselves delivering a lively, engaging conference presentation, rather than preparing a dry, formal report or journal publication. Contributors to Real World Data Science are creating content for their colleagues and peers and should “speak” to them as such."
  },
  {
    "objectID": "contributor-docs/style-guide.html#structure",
    "href": "contributor-docs/style-guide.html#structure",
    "title": "Style guide",
    "section": "Structure",
    "text": "Structure\nEach contribution must, in effect, tell “a story”, and so contributors need to be clear (a) what their story is, (b) why people should be interested, and (c) what its main message or key takeaways are. To help figure this out, we recommend contributors apply the XY Story Formula."
  },
  {
    "objectID": "contributor-docs/style-guide.html#technical-content-and-jargon",
    "href": "contributor-docs/style-guide.html#technical-content-and-jargon",
    "title": "Style guide",
    "section": "Technical content and jargon",
    "text": "Technical content and jargon\nTechnical content is a necessary feature of a site like ours. Without it, an article or other piece of content may be of little practical use to a technical audience. But if there’s too much of it, even experts may struggle to stay engaged. Contributors are also faced with a dilemma when it comes to explaining technical content: explain nothing, and you risk alienating some of your audience; explain everything, and you’ll struggle to establish a clear, strong narrative thread. So, careful consideration is required:\n\nWho is my audience for this article?\nWhat is this audience likely to know already, and what needs to be explained?\nIf something needs to be explained, can I do so briefly and then link to other resources? Or is a full explanation required?\nIn telling my “story”, what are the absolute-need-to-knows, and what are the simply-nice-to-knows?\n\nThinking through these questions will help contributors to find the right mix of valuable, technical content paired with accessible, readable narrative.\nKeep in mind that the same general advice applies to the use of industry jargon. Jargon can be a valuable shorthand when communicating with people working in the same organisation or sector, but those working in different fields may struggle to make sense of it. So, contributors need to think carefully about how much jargon to use, and what needs to be explained."
  },
  {
    "objectID": "contributor-docs/style-guide.html#figuresgraphics",
    "href": "contributor-docs/style-guide.html#figuresgraphics",
    "title": "Style guide",
    "section": "Figures/graphics",
    "text": "Figures/graphics\nAll data visualisations and other graphical outputs directly related to the content of submissions must be presented neatly and cleanly (avoid chart junk). They should also be labelled correctly and legibly, with colours chosen carefully to ensure they can be easily distinguished by all readers. Accompanying captions must be written to support the reader’s understanding of the visual presentation (e.g., “Figure 1: a bar chart” is an insufficient description).\nIf contributors wish to use charts or graphs that are not their own work, they must ensure that such items are correctly sourced and referenced, and that permission to republish has been obtained. A letter or email confirming this permission is required."
  },
  {
    "objectID": "contributor-docs/style-guide.html#data-sources",
    "href": "contributor-docs/style-guide.html#data-sources",
    "title": "Style guide",
    "section": "Data sources",
    "text": "Data sources\nContributors must include within their submissions any links and/or references to the sources of data, code and/or software and software packages on which their analyses are based. We understand that some data sources may not be publicly available, whether for legal, ethical or commercial reasons. However, readers must still be told where the data come from, even if they are not able to access the data themselves."
  },
  {
    "objectID": "contributor-docs/style-guide.html#references",
    "href": "contributor-docs/style-guide.html#references",
    "title": "Style guide",
    "section": "References",
    "text": "References\nCitations are to be formatted in The Chicago Manual of Style author-date format."
  },
  {
    "objectID": "contributor-docs/style-guide.html#use-of-images",
    "href": "contributor-docs/style-guide.html#use-of-images",
    "title": "Style guide",
    "section": "Use of images",
    "text": "Use of images\nImages for general illustration purposes will be sourced and – where necessary and within reason – paid for by Real World Data Science.\n\n\n\n\n\n\n\nNote\n\n\n\nFor all other style-related matters, we follow The Guardian and Observer Style Guide."
  },
  {
    "objectID": "contributor-docs/case-studies.html",
    "href": "contributor-docs/case-studies.html",
    "title": "Case studies",
    "section": "",
    "text": "Case studies are a core feature of the Real World Data Science platform. Our case studies are designed to show how data science is used to solve real-world problems in business, public policy and beyond.\nA good case study will be a source of information, insight and inspiration for each of our target audiences:"
  },
  {
    "objectID": "contributor-docs/case-studies.html#structure",
    "href": "contributor-docs/case-studies.html#structure",
    "title": "Case studies",
    "section": "Structure",
    "text": "Structure\nCase studies should follow the structure below. It is not necessary to use the section headings we have provided – creativity and variety are encouraged. However, the areas outlined under each section heading should be covered in all submissions.\n\n\nThe problem/challenge\n\nSummarise the project and its relevance to your organisation’s needs, aims and ambitions.\n\n\n\nGoals\n\nSpecify what exactly you sought to achieve with this project.\n\n\n\nBackground\n\nAn opportunity to explain more about your organisation, your team’s work leading up to this project, and to introduce audiences more generally to the type of problem/challenge you faced, particularly if it is a problem/challenge that may be experienced by organisations working in different sectors and industries.\n\n\n\nApproach\n\nDescribe how you turned the organisational problem/challenge into a task that could be addressed by data science. Explain how you proposed to tackle the problem, including an introduction, explanation and (possibly) a demonstration of the method, model or algorithm used. (NB: If you have a particular interest and expertise in the method, model or algorithm employed, including the history and development of the approach, please consider writing an Explainer article for us.) Discuss the pros and cons, strengths and limitations of the approach.\n\n\n\nImplementation\n\nWalk audiences through the implementation process. Discuss any challenges you faced, the ethical questions you needed to ask and answer, and how you tested the approach to ensure that outcomes would be robust, unbiased, good quality, and aligned with the goals you set out to achieve.\n\n\n\nImpact\n\nHow successful was the project? Did you achieve your goals? How has the project benefited your organisation? How has the project benefited your team? Does it inform or pave the way for future projects?\n\n\n\nLearnings\n\nWhat are your key takeaways from the project? Are there lessons that you can apply to future projects, or are there learnings for other data scientists working on similar problems/challenges?"
  },
  {
    "objectID": "contributor-docs/case-studies.html#advice-and-recommendations",
    "href": "contributor-docs/case-studies.html#advice-and-recommendations",
    "title": "Case studies",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nYou do not need to divulge the detailed inner workings of your organisation. Audiences are mostly interested in understanding the general use case and the problem-solving process you went through, to see how they might apply the same approach within their own organisations.\nGoals can be defined quite broadly. There’s no expectation that you set out your organisation’s short- or long-term targets. Instead, audiences need to know enough about what you want to do so they can understand what motivates your choice of approach.\nUse toy examples and synthetic data to good effect. We understand that – whether for commercial, legal or ethical reasons – it can be difficult or impossible to share real data in your case studies, or to describe the actual outputs of your work. However, there are many ways to share learnings and insights without divulging sensitive information. This blog post from Lyft uses hypotheticals, mathematical notation and synthetic data to explain the company’s approach to causal forecasting without revealing actual KPIs or data.\nPeople like to experiment, so encourage them to do so. Our platform allows you to embed code and to link that code to interactive coding environments like Google Colab. So if, for example, you want to explain a technique like bootstrapping, why not provide a code block so that audiences can run a bootstrapping simulation themselves.\nLeverage links. You can’t be expected to explain or cover every detail in one case study, so feel free to point audiences to other sources of information that can enrich their understanding: blogs, videos, journal articles, conference papers, etc."
  },
  {
    "objectID": "contributor-docs/datasets.html",
    "href": "contributor-docs/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "It’s easy to find datasets online. What’s more difficult is finding quality datasets that are suitable for specific training and development needs. On Real World Data Science we aim to solve that problem.\nOur Datasets section will provide a curated list of recommended datasets along with detailed notes and guidance on what each dataset contains, how it is structured, and how best to make use of it. In particular, we want to highlight messy rather than pristine datasets – ones that capture the imperfections and oddities found in real-world data – so that users can practice not only data analysis and modelling, but data cleaning and preparation too!"
  },
  {
    "objectID": "contributor-docs/datasets.html#structure",
    "href": "contributor-docs/datasets.html#structure",
    "title": "Datasets",
    "section": "Structure",
    "text": "Structure\nIf you have a dataset to recommend, your submission must cover the following areas:\n\nDataset name\nLink to source\nWhat data science tasks/methods can this dataset be used to demonstrate?\nHave you used this dataset for your own teaching/learning? (see Advice and recommendations below)\nWhy was the dataset originally created?\nWhen was it created?\nWho created it?\nLicences/restrictions?\nSize of dataset\nData types/description\nReal/synthetic data?"
  },
  {
    "objectID": "contributor-docs/datasets.html#advice-and-recommendations",
    "href": "contributor-docs/datasets.html#advice-and-recommendations",
    "title": "Datasets",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nHelp others to make good use of your recommended dataset. If you’ve had experience using a recommended dataset for your own teaching and learning, please consider creating an exercise for platform users to complete. If you encountered the dataset as part of a training course, competition or exercise created by a third party, make sure to give them a namecheck."
  },
  {
    "objectID": "ideas/datasciencebites/posts/2023/03/02/basket-complementarity.html",
    "href": "ideas/datasciencebites/posts/2023/03/02/basket-complementarity.html",
    "title": "Using ‘basket complementarity’ to make product recommendations",
    "section": "",
    "text": "Anyone who has ever worked in a retail store will be familiar with the concept of cross-selling. A customer wants a can of paint? Try to sell them some paintbrushes. That new cellphone they’ve just decided to buy? They’ll probably need a case to protect it. Online retailers (and digital services of all sorts) have taken this idea and run with it, to great success. Sophisticated algorithms sort through data on a customer’s past transactions, and those of similar-looking customers, to identify and recommend other products a customer might be interested in.\nA large amount of cross-selling, whether attempted in store by a sales assistant or online by an algorithm, relies on the concept of complementarity: that certain products are often bought and/or used together. Relationships between products might be obvious – paint and paintbrushes, for example – or they may be obscure and only revealed through the analysis of large datasets. In a 2021 paper that highlights complementarity’s relevance to association analysis, Puka and Jedrusik put forward “a new measure of complementarity in market basket data”, which sheds light on how product recommendations can be derived.\nInspired by complementarity-based ideas prevalent in microeconomics, Puka and Jedrusik begin by collecting some established ideas from traditional market basket analysis, the key one being “confidence”. In this case, we’re talking about the confidence that item A leads (in a way) to item B (which we can express in notation as conf({A} → {B})). Take a look at Table 1 (below), which presents a numbered list of 18 shopping trips, with details of what was purchased on each trip. Notice how two of the trips (1 and 3) resulted in sales of both milk (B) and cornflakes (A), while five trips (1, 3, 7, 17, and 18) had cornflakes. Under the assumption that someone already has cornflakes in their trolley, the probability that they will buy milk is 2/5 = 0.4. So, conf({cornflakes} → {milk}) = 0.4. The closer this number gets to one, the more automatic the cornflakes–milk connection becomes. This number can therefore be used to recommend an item that is related in some way to another already bought."
  },
  {
    "objectID": "ideas/datasciencebites/posts/2023/03/02/basket-complementarity.html#asymmetry-and-tolerance",
    "href": "ideas/datasciencebites/posts/2023/03/02/basket-complementarity.html#asymmetry-and-tolerance",
    "title": "Using ‘basket complementarity’ to make product recommendations",
    "section": "Asymmetry and tolerance",
    "text": "Asymmetry and tolerance\nMilk and cornflakes are reasonably complementary, and we can see from Figure 1 above that, regardless of whether you start by picking up milk or cornflakes, the probabilities of a shopper buying the other item are broadly similar: conf({cornflakes} → {milk}) = 0.4, while conf({milk} → {cornflakes}) = 0.33. There is a small amount of asymmetry in the probabilities in this particular example, but asymmetry can be more extreme for other pairs of items. This leads to the idea of one- and two-sided complementarity. Two items sharing a smallish asymmetry – like milk and cornflakes – will be connected through two-sided complementarity, while large asymmetries indicate one-sided complementarity. Such imbalances will be quite common when, for instance, items of hugely different prices are involved. When someone buys a house, for example, they may want to buy a bookcase, but buying a bookcase doesn’t mean someone wants to buy a house: this would be an instance of one-sided complementarity.\nPuka and Jedrusik capitalize on this observation. They define two items to be “basket complementary” if the two probabilities – the normal and its opposite – remain close and reasonably high. The items need to share a bond that is blind to the direction: seeing you bought one, no matter which, means you are (almost equally) likely to buy the other.\nIt is rare that the two probabilities should be exactly the same, of course, and the authors allow some deviation. Along the red diagonal line of perfect equality (Figure 1) we may lay tolerance bands marking degrees of product inseparability. This, if need be, may lead to the notion of being complementary at such-and-such a tolerance level – 0%, 1%, 5%, etc. – generating a score of sorts. In cases where a dot representing the two-way dependencies between two items falls within a narrow band – corresponding to a smaller tolerance – the more inseparable the items are, and the more sensible a cross-selling recommendation may become."
  },
  {
    "objectID": "ideas/datasciencebites/posts/2023/03/02/basket-complementarity.html#in-conclusion",
    "href": "ideas/datasciencebites/posts/2023/03/02/basket-complementarity.html#in-conclusion",
    "title": "Using ‘basket complementarity’ to make product recommendations",
    "section": "In conclusion",
    "text": "In conclusion\nA large part of the world we inhabit, particularly the economy, is powered by recommendations: from strangers, friends and algorithms. That applies not only to the things we buy but also to the things we watch or read. (Perhaps you arrived at this article because of a tweet that Twitter thought you might like, or maybe it was suggested to you by Google News because of your past reading habits.) Whatever the intent of these recommendations, the key challenge is in knowing which two things are functionally or thematically intertwined. Which item or product is, by default, synonymous with which? Puka and Jedrusik deliver an answer: two items that are basket complementary to each other, preferably at a slim tolerance, are inextricably linked. One may be safely offered – perhaps always – whenever the other is already in the shopping basket.\nThe relative simplicity and interpretability of basket complementary may provide small-scale retailers, starved of analytical wherewithal, a sane and safe strategy for developing their product offer. It might also serve as a benchmark to keep other, more sophisticated recommendation algorithms in check. (In weather forecasting, for example, it is often seen that naive benchmarks – such as using today’s temperature to predict tomorrow’s – frequently outperform more advanced models.)\nBasket complementarity could also be used to help individuals understand their own shopping habits and the links between the things they buy. I’ve built an interactive dashboard where you can enter your own receipt lists and filter associations based on various confidence thresholds. The underlying code is also available.\n \n\n\n\n\nAbout the author\n\nMoinak Bhaduri is assistant professor in the Department of Mathematical Sciences, Bentley University. He studies spatio-temporal Poisson processes and others like the self-exciting Hawkes or log-Gaussian Cox processes that are natural generalizations. His primary interest includes developing change-detection algorithms in systems modeled by these stochastic processes, especially through trend permutations.\n\n\n\n\n\nAbout DataScienceBites\n\nDataScienceBites is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to become a contributor.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Moinak Bhaduri\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nBhaduri, Moinak. 2023. “Using ‘basket complementarity’ to make product recommendations.” Real World Data Science, March 2, 2023. URL"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html",
    "title": "A demonstration of the law of the flowering plants",
    "section": "",
    "text": "This tutorial will demonstrate a popular method for predicting the day a flower will bloom. There are many reasons why you might want to predict a bloom date. You might be a scientist studying ecosystems stressed by climate change. Or you might be planning a trip to Amsterdam and would like to time your stay to when the tulips are in bloom. Or maybe you are participating in the annual Cherry Blossom Prediction Competition and want some ideas to help you get started.\nIn any case, you might be surprised to learn that the day a flower blooms is one of the earliest phenomena studied with systematic data collection and analysis. The mathematical rule developed in the eighteenth century to make these predictions – now called the “law of the flowering plants” – shaped the direction of statistics as a field and is still used by scientists with relatively few changes.\nWe present the law of the flowering plants as it was stated by Adolphe Quetelet, an influential nineteenth century statistician. Upon completing this tutorial, you will be able to:\nAt the end of the tutorial, we challenge you to design an algorithm that beats our predictions. The tutorial uses the R programming language. In particular, the code relies on the following packages:"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#the-law-of-the-flowering-plants",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#the-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "The law of the flowering plants",
    "text": "The law of the flowering plants\nWe begin by reviewing the law of the flowering plants as it was stated by Adolphe Quetelet. You may already know Quetelet as the inventor of the body mass index. Less known is that Quetelet recorded the bloom dates of hundreds of different plants between 1833 and 1852 at the Brussels Observatory, which he founded and directed. Quetelet reported that a plant flowers when exposed to a specific quantity of heat, measured in degrees of Celsius squared (°C²). For example, he calculated that a lilac blooms when the sum of the daily temperatures squared exceeds 4264°C² following the last frost.\nHe communicated this law in his Letters addressed to HRH the grand duke of Saxe-Coburg and Gotha (Number 33, 1846; translated 1849) and in his reporting On the climate of Belgium (Chapter 4, Part 4, 1848; data updated in Part 7, 1857). A picture of Quetelet and the title page of On the climate of Belgium are displayed in Figure 1.\n\nQuetelet, Adolphe. 1846. Lettres à s.a.r. Le Duc Régnant de Saxe-Coburg Et Gotha: Sur La Théorie Des Probabilités, Appliquée Aux Sciences Morales Et Politiques. Bruxelles: M. Hayez. https://catalog.hathitrust.org/Record/001387625.\n\n———. 1849. Letters Addressed to h.r.h. The Grand Duke of Saxe Coburg and Gotha on the Theory of Probabilities as Applied to the Moral and Political Sciences. London: C. & E. Layton. https://catalog.hathitrust.org/Record/008956987.\n\nObservatoire royal de Bruxelles. 1848. Annales de l’observatoire Royal de Bruxelles. Bruxelles: M. Hayez. https://catalog.hathitrust.org/Record/000553895.\n\n———. 1857. Sur Le Climat de La Belgique : De l’état Du Ciel En Général. Bruxelles: M. Hayez. https://catalog.hathitrust.org/Record/000553895.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Quetelet reported on the law of the flowering plants in On the climate of Belgium (1857). Sources: Wikimedia Commons, Gallica.\n\nQuetelet was not the first to study bloom dates. Anthophiles have recorded the dates that flowers bloom for centuries. Written records of cherry trees go back as far as 812 AD in Japan and peach and plum trees as far as 1308 AD in China. Systematic record keeping began a century before Quetelet with Robert Marsham’s Indications of Spring (1789).\n\nMarsham, Robert. 1789. “XIII. Indications of Spring, Observed by Robert Marsham, Esquire, f. R. S. Of Stratton in Norfolk. Latitude 52° 45’.” Philosophical Transactions of the Royal Society of London 79: 154–56. https://doi.org/10.1098/rstl.1789.0014.\n\nDe Réaumur, René. 1735. “Observations Du Thermometre, Faites a Paris Pendant l’annees 1735, Comparees a Celles Qui Ont Ete Faites Sous La Ligne, a l’isle de France, a Alger Et En Quelques-Unes de Nos Isles de l’amerique.” Mémoire de l’Académie Royale Des Sciences, 545–76. https://www.academie-sciences.fr/pdf/dossiers/Reaumur/Reaumur_pdf/p545_576_vol3532m.pdf.\nQuetelet was also not the first to study the relationship between temperature and bloom dates. René Réaumur (1735), an early adopter of the thermometer, noted the relationship before Marsham published his Indications. But Quetelet was the first to systematically study the relationship across a wide variety of plants and derive the amount of heat needed to bloom. An example of Quetelet’s careful record keeping can be seen in Figure 2, one of many tables he reported in his publications.\n\n\n\n\n\n\nFigure 2: Bloom dates at Brussels Observatory observed by Quetelet between 1839 and 1852. Source: Gallica."
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#reproducing-quetelets-law-of-the-flowering-plants",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#reproducing-quetelets-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Reproducing Quetelet’s law of the flowering plants",
    "text": "Reproducing Quetelet’s law of the flowering plants\nTo reproduce Quetelet’s law, we combine the data in Figure 2 with additional observations from his Letters. We focus on Quetelet’s primary example, the bloom date of the common lilac, Syringa vulgaris, row 18 of Figure 2. We do this because Quetelet carefully describes his methodology for measuring the bloom date of lilacs. For example, Quetelet considers a lilac to have bloomed when “the first corolla opens and shows the stamina.” That event is closest to what the USA Phenology Network describes as “open flowers”, depicted in the center image of Figure 3 below. This detail will become relevant when we attempt to replicate Quetelet’s law in a later section. Note that although we focus on lilacs in this tutorial, the R code is easily edited to predict the day that other plants will bloom.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: The bloom date occurs when the first corolla opens and shows the stamina (center image). Source: USA National Phenology Network.\n\nIn the R code below, the five-column tibble lilac contains the date each year that Quetelet observed the lilacs bloom at Brussels Observatory. The first three columns are the month, day, and year the lilacs bloomed between 1839 and 1852. These columns are combined to form the fourth column, the full date the lilacs bloomed. The last column converts the date to the day of the year the lilacs bloomed, abbreviated “doy.” That is, “doy” is the number of days it took for the lilacs bloom following January 1. Both “date” and “doy” representations of Quetelet’s observations will be useful throughout this tutorial.\n```{r}\nlilac &lt;-                   \n  tibble(month = c(\"May\", \"April\", \"April\", \"April\", \"April\", \"April\", \"May\", \n                   \"April\", \"May\", \"April\", \"May\", \"April\", \"May\", \"May\"),\n         day   =  c(10, 28, 24, 28, 20, 25, 13, 12, 9, 21, 2, 30, 1, 12),\n         year  = 1839:1852,\n         date  = as.Date(paste(month, day, year), format = \"%B %d %Y\"),\n         doy   = parse_number(format(date, \"%j\"))) \n\nlilac %&gt;% \n  kable(align = \"c\",\n        caption = \"Table 1: Bloom dates of lilacs observed by Quetelet between 1839 and 1852.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\n\nTable 1: Bloom dates of lilacs observed by Quetelet between 1839 and 1852.\n\n\n\n\nmonth\n\n\nday\n\n\nyear\n\n\ndate\n\n\ndoy\n\n\n\n\n\n\nMay\n\n\n10\n\n\n1839\n\n\n1839-05-10\n\n\n130\n\n\n\n\nApril\n\n\n28\n\n\n1840\n\n\n1840-04-28\n\n\n119\n\n\n\n\nApril\n\n\n24\n\n\n1841\n\n\n1841-04-24\n\n\n114\n\n\n\n\nApril\n\n\n28\n\n\n1842\n\n\n1842-04-28\n\n\n118\n\n\n\n\nApril\n\n\n20\n\n\n1843\n\n\n1843-04-20\n\n\n110\n\n\n\n\nApril\n\n\n25\n\n\n1844\n\n\n1844-04-25\n\n\n116\n\n\n\n\nMay\n\n\n13\n\n\n1845\n\n\n1845-05-13\n\n\n133\n\n\n\n\nApril\n\n\n12\n\n\n1846\n\n\n1846-04-12\n\n\n102\n\n\n\n\nMay\n\n\n9\n\n\n1847\n\n\n1847-05-09\n\n\n129\n\n\n\n\nApril\n\n\n21\n\n\n1848\n\n\n1848-04-21\n\n\n112\n\n\n\n\nMay\n\n\n2\n\n\n1849\n\n\n1849-05-02\n\n\n122\n\n\n\n\nApril\n\n\n30\n\n\n1850\n\n\n1850-04-30\n\n\n120\n\n\n\n\nMay\n\n\n1\n\n\n1851\n\n\n1851-05-01\n\n\n121\n\n\n\n\nMay\n\n\n12\n\n\n1852\n\n\n1852-05-12\n\n\n133\n\n\n\n\n\n\n\nTo reproduce Quetelet’s law of the flowering plants, we will combine these bloom dates with daily temperature. The daily maximum and minimum temperatures at Brussels Observatory between 1839 and 1852 are available from the Global Historical Climatology Network. The data can be downloaded using the ghcnd_search function contained within the R package rnoaa (2021). The station id for Brussels Observatory is “BE000006447”.\n\nChamberlain, Scott. 2021. “’NOAA’ Weather Data from r [r Package Rnoaa Version 1.3.8].” The Comprehensive R Archive Network. Comprehensive R Archive Network (CRAN). https://CRAN.R-project.org/package=rnoaa.\nThe ghcnd_search function returns the maximum and minimum temperature as separate tibbles in a list. In the R code below, we join the tibbles using the reduce function. Note that the temperature is reported in tenths of a degree (i.e. 0.1°C) so we divide by 10 before calculating the temperature midrange, our estimate of the daily temperature.\nThe result is a five-column tibble temp, which contains the year of the temperature record (“year”), the date of the temperature record (“date”), the maximum temperature (“tmax”), the minimum temperature (“tmin”), and the midrange temperature (“temp”). The first 10 rows of the table are below. When you produce the full table yourself, you may notice that a small portion of temperature records are missing. We found that imputing these missing values does not significantly change the results. Therefore, we ignore these days when conducting our analysis.\n```{r}\ntemp &lt;- \n  ghcnd_search(stationid = \"BE000006447\",\n               var = c(\"tmax\", \"tmin\"),\n               date_min = \"1839-01-01\",\n               date_max = \"1852-12-31\") %&gt;%\n  reduce(left_join) %&gt;%\n  transmute(year = parse_number(format(date, \"%Y\")), \n            date, \n            tmax = tmax / 10, \n            tmin = tmin / 10, \n            temp = (tmax + tmin) / 2)\n  \ntemp %&gt;% \n  kable(align = \"c\", \n        col.names = c(\"year\", \"date\", \"maximum temperature (°C)\", \n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 2: Temperature observed at Brussels Observatory between 1839 and 1852.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\n\nTable 2: Temperature observed at Brussels Observatory between 1839 and 1852.\n\n\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\n1839\n\n\n1839-01-01\n\n\n5.7\n\n\n-0.2\n\n\n2.75\n\n\n\n\n1839\n\n\n1839-01-02\n\n\n6.3\n\n\n0.8\n\n\n3.55\n\n\n\n\n1839\n\n\n1839-01-03\n\n\n7.2\n\n\n1.8\n\n\n4.50\n\n\n\n\n1839\n\n\n1839-01-04\n\n\n8.0\n\n\n1.8\n\n\n4.90\n\n\n\n\n1839\n\n\n1839-01-05\n\n\n5.3\n\n\n0.8\n\n\n3.05\n\n\n\n\n1839\n\n\n1839-01-06\n\n\n10.0\n\n\n1.3\n\n\n5.65\n\n\n\n\n1839\n\n\n1839-01-07\n\n\n8.9\n\n\n1.4\n\n\n5.15\n\n\n\n\n1839\n\n\n1839-01-08\n\n\n3.0\n\n\n0.1\n\n\n1.55\n\n\n\n\n1839\n\n\n1839-01-09\n\n\n0.8\n\n\n-0.1\n\n\n0.35\n\n\n\n\n1839\n\n\n1839-01-10\n\n\n2.8\n\n\n-2.8\n\n\n0.00\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\n\nReproducing Quetelet’s law is now a simple matter of calculating the sum of the squared daily temperature from the day of last frost until the bloom day. We could use the day of last frost reported in Quetelet’s Letters. However, since we will replicate Quetelet’s analysis with recent data in a later section, we use our own definition of the day of last frost. We define the day of last frost to be the day following the last day the maximum temperature is below 0. The R code below creates the function doy_last_frost to extract the day of last frost from the maximum temperature. To demonstrate this function, we then compare the bloom date with the last frost date in 1839, the first year Quetelet observed.\n```{r}\ndoy_last_frost &lt;- function(tmax, doy_max = 100) {\n  dof &lt;- which(tmax[1:doy_max] &lt;= 0)\n  if(length(dof) == 0) 1 else max(dof) + 1\n  }\n\nbloom_day &lt;- \n  lilac %&gt;% \n  filter(year == 1839) %&gt;%\n  pull(doy) + \n  as.Date(\"1839-01-01\")\n  \nfrost_day &lt;- \n  temp %&gt;% \n  filter(year == 1839) %&gt;% \n  pull(tmax) %&gt;% \n  doy_last_frost() + as.Date(\"1839-01-01\") \n\ntibble(`last frost date` = frost_day, \n       `bloom date` = bloom_day) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 3: Last frost date and lilac bloom date at Brussels Observatory in 1839.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 3: Last frost date and lilac bloom date at Brussels Observatory in 1839.\n\n\n\n\nlast frost date\n\n\nbloom date\n\n\n\n\n\n\n1839-03-08\n\n\n1839-05-11\n\n\n\n\n\n\nIf Quetelet’s law of the flowering plants is correct, Table 3 has the following interpretation. On March 8, 1839 the lilacs at Brussels Observatory began “collecting” temperature. The lilacs continued to “collect” temperature until May 11, at which point they exceeded their 4264°C² quota and bloomed. We visualize this theory in Figure 4 with the R packages ggplot2, a member of the set of packages that constitute the “tidyverse” (2019), and plotly.\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n```{r}\n(temp %&gt;% \n  filter(date &lt; as.Date(\"1839-06-01\")) %&gt;% \n  ggplot() + \n  aes(date, temp) + \n  geom_line() + \n  labs(\n    x = \"\",\n    y = \"midrange temperature (°C)\",\n    title = \n      \"Figure 4: According to Quetelet's law, the lilacs bloom when exposed to 4264°C² following the last frost.\") +\n  geom_vline(xintercept = as.numeric(c(bloom_day, frost_day)), \n             linetype = \"dotted\")) %&gt;%\n  ggplotly() %&gt;% \n  add_annotations(x = as.numeric(c(frost_day, bloom_day)),\n                  y = c(-4, -4),\n                  text = c(\"last\\nfrost\", \"first\\nbloom\"),\n                  font = list(size = 14),\n                  ay = 0,\n                  xshift = c(-10, -12)) %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\n\n\n\n\nFigure 4: According to Quetelet’s law, the lilacs bloom when exposed to 4264°C² following the last frost. Author provided, CC BY 4.0.\n\nWe now have all the ingredients necessary to reproduce Quetelet’s findings. Our reproduction is greatly simplified by using the nest function from the tidyr package, another member of the “tidyverse”. For an overview of nest, see the “Nested data” section of Grolemund and Wickham (2017). We will group the data by year, nest, calculate the cumulative squared temperature from the frost date to the bloom date within each year, and then unnest. We ignore temperatures below 0°C. That is, temperatures below 0°C are set to 0°C. We do this because it is clear from Quetelet’s derivation of the law that only positive temperatures should be squared. See the next section for details.\n\nGrolemund, Garrett, and Hadley Wickham. 2017. R for Data Science. Sebastopol, CA: O’Reilly Media.\n```{r}\nquetelet &lt;- \n  temp %&gt;% \n  group_by(year) %&gt;% \n  nest() %&gt;% \n  left_join(lilac) %&gt;% \n  mutate(law = map(data, ~ sum(pmax(.$temp, 0, na.rm = TRUE)[(doy_last_frost(.$tmax) + 1):doy]^2))) %&gt;% \n  unnest(law) %&gt;% \n  ungroup()\n\nquetelet %&gt;% \n  summarize(Quetelet = 4264, \n            est = mean(law), \n            se = sd(law)/sqrt(n()),\n            ci  = str_c(\"[\", round(est - 2 * se), \", \", round(est + 2 * se), \"]\")) %&gt;%\n  kable(dig = 0, \n        align = \"c\", \n        col.names = c(\"Quetelet's law (°C²)\", \"estimate (°C²)\", \n                      \"standard error (°C²)\", \"95% confidence interval (°C²)\"),\n        caption = \"Table 4: Reproduction of Quetelet's analysis.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 4: Reproduction of Quetelet’s analysis.\n\n\n\n\nQuetelet’s law (°C²)\n\n\nestimate (°C²)\n\n\nstandard error (°C²)\n\n\n95% confidence interval (°C²)\n\n\n\n\n\n\n4264\n\n\n4261\n\n\n197\n\n\n[3867, 4656]\n\n\n\n\n\n\nThe results show that Quetelet’s findings are indeed reproducible. Quetelet estimated that lilacs bloom once exposed to 4264°C² following the last frost. Our reanalysis suggests a similar amount. However, 4264°C² is the overall average across all years – the estimated amount needed to bloom varies year to year. As a result, the average has a 95% confidence interval of approximately 3870°C² to 4660°C². Quetelet was well aware of this variation. He argued it was due to unobserved factors that influence growing conditions and change each year, and he dedicated significant space in his Letters to discuss them.\nThese unobserved factors limit the accuracy of predictions made using the law. To assess the predictive accuracy of the law, we temporarily ignore the bloom dates Quetelet observed. Instead, we apply the 4264°C² quota to the temperature records at Brussels Observatory to predict the bloom date. We then compare our predictions with the bloom date Quetelet observed. The R code below creates the function doy_prediction to estimate the day the lilac will bloom from temperature records. Table 5 summarizes the accuracy of Quetelet’s law by the mean absolute error and root mean squared error.\n```{r}\ndoy_prediction &lt;- function(temp, tmax)\n  doy_last_frost(tmax) + which.max(cumsum(pmax(temp[(doy_last_frost(tmax) + 1):365], 0, na.rm = TRUE)^2) &gt; 4264)\n\nquetelet %&gt;% \n  mutate(pred = map(data, ~ doy_prediction(.$temp, .$tmax))) %&gt;% \n  unnest(pred) %&gt;% \n  ungroup() %&gt;%\n  summarize(mae  = mean(abs(doy - pred)),\n            rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  kable(dig = 0,\n        align = \"c\",\n        col.names = c(\"mean absolute error (days)\", \"root mean squared error (days)\"),\n        caption = \"Table 5: Predictions using Quetelet's law are accurate within a week on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 5: Predictions using Quetelet’s law are accurate within a week on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n5\n\n\n6\n\n\n\n\n\n\nTable 5 indicates that predictions made using the law are accurate to within a week on average. For comparison purposes, we also predict the day the lilacs will bloom using the average bloom date between 1839 and 1852. That is, on average the lilac bloomed on April 30 (April 29 on leap years), and we check the accuracy of simply predicting this average date each year. Table 6 indicates the average bloom date yields predictions that are less accurate by an average of two days.\n```{r}\nquetelet %&gt;%\n  summarize(pred = mean(doy),\n            mae  = mean(abs(doy - pred)),\n            rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  select(mae, rmse) %&gt;%\n  kable(dig = 0,\n        align = \"c\",\n        col.names = c(\"mean absolute error (days)\",\n                      \"root mean squared error (days)\"),\n        caption = \"Table 6: Predictions using the average bloom date are off by a week or more on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 6: Predictions using the average bloom date are off by a week or more on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n7\n\n\n9"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#quetelets-derivation-of-the-law-of-the-flowering-plants",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#quetelets-derivation-of-the-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Quetelet’s derivation of the law of the flowering plants",
    "text": "Quetelet’s derivation of the law of the flowering plants\nQuetelet believed that, as in physics, universal laws govern social and biological phenomenon. Quetelet was not only inspired by physics to describe social and biological patterns using mathematical formulas. He often took his formulas directly from physics. In fact, you may have already recognized similarities between his law and Newton’s second law of motion.\nQuetelet reasoned that temperature exerts a “force” on plants in the same way that gravity exerts a force on a falling object. Newton’s second law states that acceleration is proportional to force. It follows that an object initially at rest and subject to a constant force will travel a distance proportional to time squared. Quetelet simply substituted temperature for time.\nWe briefly elaborate. Let \\(d(t)\\) denote the distance an object travels after time \\(t\\). Let \\(v(t) = d'(t)\\) denote its speed and \\(a(t) = v'(t)\\) its acceleration. If acceleration is constant, i.e. \\(a(t) = c\\),\n\n\\(v(t) = \\int_0^t a(s) \\, ds = \\int_0^t c \\, ds = c t\\)\n\nand\n\n\\(d(t) = \\int_0^t v(s) \\, ds = \\int_0^t c s \\, ds = \\tfrac{c}{2} t^2\\)\n\nQuetelet imagined plants experience time in temperature and bloom after “traveling” distance \\(d_*\\). If a plant is exposed to temperature \\(t_i\\) on day \\(i = 1, 2, \\ldots\\), then the bloom date, \\(n_*\\), is the first day \\(\\sum_{i=1}^{n_*} \\tfrac{c}{2} t_i^2 \\geq d_*\\). Multiplying both sides of the inequality by \\(\\tfrac{2}{c}\\), yields Quetelet’s law: the bloom is the first day, \\(n_*\\), that \\(\\sum_{i=1}^{n_*} t_i^2 \\geq \\tfrac{2}{c} d_*\\).\nThe derivation of laws like the law of the flowering plants was popular in the nineteenth century. But any similarities between the “force” of temperature and the force of gravity are likely coincidental. We are not aware of any biological mechanisms that justify Quetelet’s application of Newton’s law.\nToday, the law of the flowering plants is considered a heuristic, or rule of thumb, that approximates complicated biological mechanisms. Like Quetelet, scientists model plants as experiencing time in temperature instead of calendar time. These temperature units are typically called “growing degree days”. Scientists often find that plants may only be sensitive to temperatures in specific ranges or “modified growing degree days”. Although modern statistical methods can greatly improve the accuracy of predictions, laws like Quetelet’s remain popular because they are simple to communicate and easy to replicate, as we demonstrate in the next section."
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#replicating-quetelets-law-of-the-flowering-plants",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#replicating-quetelets-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Replicating Quetelet’s law of the flowering plants",
    "text": "Replicating Quetelet’s law of the flowering plants\nIn the previous section, we explained how Quetelet derived the law of the flowering plants. Quetelet believed the law of the flowering plants was universal, describing the bloom date of all flowers around the world and in any year. Whether the law can in fact be considered universal requires replicating Quetelet’s results with new data collected at a different location in a different year.\nIn this section, we replicate the law of the flowering plants using lilac bloom dates observed by scientists between 1956 and 2009 at 53 locations throughout the Pacific Northwest (2015). The data can be downloaded from the USA National Phenology Network using the rnpn package (2022). For space considerations, the R code that downloads and cleans the data is provided in the Appendix. Running this code yields the tibble usa_npn. Each row of the tibble corresponds with a bloom date observed at a given site in a given year. There are 31 columns, only seven of which we use in our replication. The remaining columns are documented in the rnpn package, and we will not review them here.\n\nRosemartin, Alyssa H., Ellen G. Denny, Jake F. Weltzin, R. Lee Marsh, Bruce E. Wilson, Hamed Mehdipoor, Raul Zurita-Milla, and Mark D. Schwartz. 2015. “Lilac and Honeysuckle Phenology Data 1956-2014.” Scientific Data 2 (1). https://doi.org/10.1038/sdata.2015.38.\n\nRosemartin, Alyssa, Chamberlain Scott, Lee Marsh, and Kevin Wong. 2022. “Interface to the National ’Phenology’ Network ’API’ [r Package Rnpn Version 1.2.5].” The Comprehensive R Archive Network. Comprehensive R Archive Network (CRAN). https://cran.r-project.org/package=rnpn.\nTable 7 displays six of the seven columns (and only the first 10 rows of the full table). These columns are defined in the same way as the columns of Table 1, except for “site_id”, which denotes the site at which the observation was made. Table 1 does not have a “site_id” column because all observations were made at the same site, Brussels Observatory.\n```{r}\nload(url(\"https://github.com/jauerbach/miscellaneous/blob/main/usa_npn.RData?raw=true\"))\n\nusa_npn %&gt;%\n  transmute(site_id, \n            month = first_yes_month, \n            day   = first_yes_day, \n            year  = first_yes_year, \n            date  = as.Date(paste(month, day, year), format = \"%m %d %Y\"),\n            doy) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 7: Bloom dates of lilacs observed in pacific northwest between 1956 and 2009.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", \n             height = \"400px\")\n```\n\n\n\n\n\nTable 7: Bloom dates of lilacs observed in pacific northwest between 1956 and 2009.\n\n\n\n\nsite_id\n\n\nmonth\n\n\nday\n\n\nyear\n\n\ndate\n\n\ndoy\n\n\n\n\n\n\n150\n\n\n5\n\n\n25\n\n\n1956\n\n\n1956-05-25\n\n\n146\n\n\n\n\n150\n\n\n5\n\n\n22\n\n\n1957\n\n\n1957-05-22\n\n\n142\n\n\n\n\n150\n\n\n5\n\n\n12\n\n\n1958\n\n\n1958-05-12\n\n\n132\n\n\n\n\n150\n\n\n6\n\n\n3\n\n\n1959\n\n\n1959-06-03\n\n\n154\n\n\n\n\n150\n\n\n5\n\n\n27\n\n\n1960\n\n\n1960-05-27\n\n\n148\n\n\n\n\n150\n\n\n5\n\n\n27\n\n\n1961\n\n\n1961-05-27\n\n\n147\n\n\n\n\n150\n\n\n5\n\n\n26\n\n\n1962\n\n\n1962-05-26\n\n\n146\n\n\n\n\n150\n\n\n5\n\n\n24\n\n\n1963\n\n\n1963-05-24\n\n\n144\n\n\n\n\n150\n\n\n5\n\n\n28\n\n\n1964\n\n\n1964-05-28\n\n\n149\n\n\n\n\n150\n\n\n5\n\n\n26\n\n\n1966\n\n\n1966-05-26\n\n\n146\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\n\nThe seventh column we review is “temp”. Each row of “temp” is a tibble of temperature records taken at the nearest station in the Global Historical Climatology Network. The first tibble (again, only the first 10 rows) is displayed in Table 8 below. The columns are defined in the same way as the columns of Table 2, except for “id”, which denotes the location at which the temperature record was made. Table 2 does not have an “id” column because all observations were made at the same site, Brussels Observatory.\n```{r}\nusa_npn %&gt;%\n  pull(temp) %&gt;%\n  .[[1]] %&gt;%\n  mutate(year = parse_number(format(date, \"%Y\"))) %&gt;%\n  select(id, year, date, tmax, tmin, temp) %&gt;%\n  kable(align = \"c\",\n        col.names = c(\"id\", \"year\", \"date\", \"maximum temperature (°C)\", \n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 8: Temperature observed at an example pacific northwest site in 1956.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", \n             height = \"400px\")\n```\n\n\n\n\n\nTable 8: Temperature observed at an example pacific northwest site in 1956.\n\n\n\n\nid\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-01\n\n\n5.6\n\n\n-5.6\n\n\n0.00\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-02\n\n\n1.7\n\n\n-7.2\n\n\n-2.75\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-03\n\n\n3.3\n\n\n-11.7\n\n\n-4.20\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-04\n\n\n4.4\n\n\n-10.0\n\n\n-2.80\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-05\n\n\n7.8\n\n\n0.0\n\n\n3.90\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-06\n\n\n4.4\n\n\n-11.1\n\n\n-3.35\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-07\n\n\n2.8\n\n\n-6.1\n\n\n-1.65\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-08\n\n\n4.4\n\n\n-4.4\n\n\n0.00\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-09\n\n\n1.7\n\n\n-9.4\n\n\n-3.85\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-10\n\n\n2.8\n\n\n-6.1\n\n\n-1.65\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\n\nWe are now prepared to replicate Quetelet’s findings. We will use R code nearly identical to the code we used to reproduce Quetelet’s findings earlier. The main difference is due to the fact that temperature records are dependent across sites within a year. To account for this dependence, we compute the cumulative temperature squared from the last frost to the bloom date for each site and year. We then take the average across all sites within a year. Finally, we calculate the standard error and confidence interval using only the variation of the averages across years. Table 9 displays the results.\n```{r}\nusa_npn %&gt;%             \n  group_by(rownames(usa_npn)) %&gt;%\n  mutate(law = \n           map(temp, ~ sum(pmax(.$temp, 0, na.rm = TRUE)[(doy_last_frost(.$tmax, doy) + 1):(doy - 1)]^2))) %&gt;%\n  unnest(law) %&gt;% \n  group_by(year) %&gt;%    \n  summarize(law = mean(law)) %&gt;%\n  summarize(Quetelet = 4264, \n            est = mean(law), \n            se = sd(law) / sqrt(n()),\n            ci  = str_c(\"[\", round(est - 2 * se), \", \", round(est + 2 * se), \"]\")) %&gt;%\n  kable(dig = 0, \n        align = \"c\",\n        col.names = c(\"Quetelet's law (°C²)\", \"estimate (°C²)\",\n                      \"standard error (°C²)\", \"95% confidence interval (°C²)\"),\n        caption = \"Table 9: Replication of Quetelet's analysis.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 9: Replication of Quetelet’s analysis.\n\n\n\n\nQuetelet’s law (°C²)\n\n\nestimate (°C²)\n\n\nstandard error (°C²)\n\n\n95% confidence interval (°C²)\n\n\n\n\n\n\n4264\n\n\n4329\n\n\n116\n\n\n[4098, 4560]\n\n\n\n\n\n\nTable 9 indicates that Quetelet’s findings are replicable in the sense that the confidence interval calculated using Quetelet’s data (Table 4) overlaps with the confidence interval calculated using the USA lilac data (Table 9). The standard error in Table 9 is smaller than Table 4 because the replication uses 54 years of data compared to Quetelet’s 14. Note that in the R code above, we subtract 1 from “doy” to correct for differences in how the bloom date is reported. This correction is not particularly important; the confidence intervals still overlap when this correction is removed.\nWe now investigate the accuracy of Quetelet’s law when applied to the USA lilac data. As before, we make use of the doy_prediction function.\n```{r}\nusa_npn &lt;- \n  usa_npn %&gt;% \n  mutate(pred = map(temp, ~ doy_prediction(.$temp, .$tmax))) %&gt;% \n  unnest(pred) %&gt;% \n  ungroup()\n\nusa_npn %&gt;% \n  summarize(mae  = mean(abs(doy - 1 - pred)),\n            rmse = sqrt(mean((doy - 1 - pred)^2))) %&gt;%\n  kable(dig = 0,\n        align = \"c\", \n        col.names = c(\"mean absolute error (days)\",\n                      \"root mean squared error (days)\"),\n        caption = \"Table 10: Predictions using Quetelet's law are accurate within about two weeks on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 10: Predictions using Quetelet’s law are accurate within about two weeks on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n10\n\n\n15\n\n\n\n\n\n\nTable 10 indicates that the predictions are accurate to within two weeks on average. Recall that the predictions using Quetelet’s own data were accurate to within one week on average (Table 5). We speculate that the decrease in accuracy is due in part to the fact that both Quetelet’s lilacs and the temperature were observed at the same site, Brussels Observatory. In some cases, the USA lilacs were a few miles from where the temperature was recorded.\nAlthough the accuracy of the predictions made using Quetelet’s law is lower when applied to the USA lilac data, Figure 5 indicates that the law produces the correct bloom date on average. The figure plots the predictions made by the law against the actual bloom dates scientists observed. Note that instead of representing prediction-observation pairs as points in a scatter plot, the data are represented using blue contours. We use contours because there are more than 1,500 observations – too many to study using a scatter plot.\n```{r}\n(usa_npn %&gt;% \n   mutate(doy = first_yes_doy) %&gt;%\n   unnest(pred) %&gt;% \n   ungroup() %&gt;%\n   mutate(predicted = as.Date(\"2020-01-01\") + pred,\n          observed = as.Date(\"2020-01-01\") + doy) %&gt;%\n   ggplot() + \n    aes(x = observed, y = predicted) +\n    geom_density2d(contour_var = \"ndensity\") +\n    geom_abline(intercept = 0, slope = 1, linetype = 2) +\n    labs(x = \"date observed\", \n         y = \"date predicted\",\n         title = \"Figure 5: Predictions using Quetelet's law are accurate within about two weeks on average.\") +\n    theme(legend.position = \"none\")) %&gt;%\n  ggplotly(tooltip = \"\") %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\n\n\n\n\nFigure 5: Predictions using Quetelet’s law are accurate within about two weeks on average. Author provided, CC BY 4.0.\n\nThe contours are easy to interpret. The blue lines are much like a mountain range observed from above. The inner circles are peaks of high elevation in which many prediction-observation pairs co-occur. The outer circles are areas of low elevation in which few prediction-observation pairs co-occur.\nThe dotted line is the “y = x” line, having zero intercept and unit slope. Prediction-observation pairs that lie on the line indicate perfect predictions. The fact that the dotted line intersects the blue contours at their peak suggests the law derived from Quetelet’s data accurately predicts the typical bloom date of the USA data. This accuracy is impressive given the fact that the USA lilacs were observed more than a century later and on a different continent. The blue curves deviate from the line by about two weeks in the vertical direction, which is consistent with Table 10.\nAn average accuracy of two weeks might not sound impressive. But it is far more accurate than using the average bloom date Quetelet observed, April 30 (April 29 on leap years). The average bloom date yields predictions that are off by an additional eleven days on average.\n```{r}\nusa_npn %&gt;%\n  mutate(doy = first_yes_doy) %&gt;%\n  ungroup() %&gt;%\n  summarize(\n    pred = mean(quetelet$doy), \n    mae  = mean(abs(doy - pred)),\n    rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  select(mae, rmse) %&gt;%\n  kable(\n    dig = 0,\n    align = \"c\",\n    col.names = c(\"mean absolute error (days)\",\n                  \"root mean squared error (days)\"),\n    caption = \"Table 11: Predictions using the average bloom date are off by three weeks or more on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 11: Predictions using the average bloom date are off by three weeks or more on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n21\n\n\n24"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#predicting-the-day-the-lilac-will-bloom-in-brussels-in-2023",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#predicting-the-day-the-lilac-will-bloom-in-brussels-in-2023",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Predicting the day the lilac will bloom in Brussels in 2023",
    "text": "Predicting the day the lilac will bloom in Brussels in 2023\nAny weather forecast can become a flower forecast by applying the law of the flowering plants. In this section, we use the AccuWeather forecast to predict the day a hypothetical lilac will bloom in Brussels in 2023. AccuWeather forecasts daily maximum and minimum temperatures three months into the future. We do not evaluate the quality of these forecasts. The purpose of this section is to simply convert them into flower forecasts.\nWe use the AccuWeather forecast as it appeared on the webpage AccuWeather.com on February 19, 2023. AccuWeather reports the forecast for each month on a separate webpage. For reproducibility, we saved each page on the Internet Archive. The following R code creates the function get_weather_table to retrieve each page we saved, extract the forecast contained within that page, and arrange the data as a tibble. The get_weather_table function combines several functions from the rvest package, which is yet another member of the “tidyverse”. In particular, the forecast on each page is contained within the div “monthly-calendar” and can be extracted with the html_nodes and html_text2 functions.\nApplying the get_weather_table function to the url for each page yields a five column tibble temp_br, with columns defined in the same way as the tibble temp, discussed in previous sections. The first 10 rows are below; the data are also available on the author’s GitHub.\n```{r}\n get_weather_table &lt;- function(url)\n  read_html(url) %&gt;% \n  html_nodes(\"div.monthly-calendar\") %&gt;% \n  html_text2() %&gt;%\n  str_remove_all(\"°|Hist. Avg. \") %&gt;%\n  str_split(\" \", simplify = TRUE) %&gt;%\n  parse_number() %&gt;%\n  matrix(ncol = 3, \n         byrow = TRUE,\n         dimnames = list(NULL, c(\"day\", \"tmax\", \"tmin\"))) %&gt;%\n  as_tibble() %&gt;%\n  filter(\n    row_number() %in%\n      (which(diff(day) &lt; 0) %&gt;% (function(x) if(length(x) == 1) seq(1, x[1], 1) else seq(x[1] + 1, x[2], 1))))\n\ntemp_br &lt;-\n  tibble(\n    base_url = \"https://web.archive.org/web/20230219151906/https://www.accuweather.com/en/be/brussels/27581/\",\n    month = month.name[1:5],\n    year = 2023,\n    url = str_c(base_url, tolower(month), \"-weather/27581?year=\", year, \"&unit=c\")) %&gt;%\n  mutate(temp = map(url, get_weather_table)) %&gt;%\n  pull(temp) %&gt;%\n  reduce(bind_rows) %&gt;%\n  transmute(date = seq(as.Date(\"2023-01-01\"), as.Date(\"2023-05-31\"), 1),\n            year = parse_number(format(date, \"%Y\")),\n            tmax,\n            tmin,\n            temp = (tmax + tmin) / 2)\n\ntemp_br %&gt;%\n  relocate(year) %&gt;%\n  kable(dig = 2,\n        align = \"c\", \n        col.names = c(\"year\", \"date\", \"maximum temperature (°C)\",\n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 12: Temperature forecast for Brussels, retrieved on February 19, 2023.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\n\nTable 12: Temperature forecast for Brussels, retrieved on February 19, 2023.\n\n\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\n2023\n\n\n2023-01-01\n\n\n15\n\n\n11\n\n\n13.0\n\n\n\n\n2023\n\n\n2023-01-02\n\n\n14\n\n\n5\n\n\n9.5\n\n\n\n\n2023\n\n\n2023-01-03\n\n\n9\n\n\n3\n\n\n6.0\n\n\n\n\n2023\n\n\n2023-01-04\n\n\n13\n\n\n8\n\n\n10.5\n\n\n\n\n2023\n\n\n2023-01-05\n\n\n12\n\n\n10\n\n\n11.0\n\n\n\n\n2023\n\n\n2023-01-06\n\n\n12\n\n\n10\n\n\n11.0\n\n\n\n\n2023\n\n\n2023-01-07\n\n\n11\n\n\n9\n\n\n10.0\n\n\n\n\n2023\n\n\n2023-01-08\n\n\n10\n\n\n6\n\n\n8.0\n\n\n\n\n2023\n\n\n2023-01-09\n\n\n8\n\n\n5\n\n\n6.5\n\n\n\n\n2023\n\n\n2023-01-10\n\n\n12\n\n\n4\n\n\n8.0\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\n\nWe now predict the day the lilacs will bloom. The R code below uses the doy_prediction and doy_last_frost functions created in earlier sections and displays the prediction in Table 13. At the time of our writing, the predicted date is April 19. The forecast is easily updated by providing the url to the updated AccuWeather webpage. (You might use the url https://web.archive.org/save to save a webpage to the Internet Archive to ensure your work is reproducible.)\n```{r}\nbloom_day_br &lt;-\n  temp_br %&gt;%\n  summarize(date = doy_prediction(temp, tmax) + as.Date(\"2023-01-01\")) %&gt;%\n  pull(date)\n\nfrost_day_br &lt;- \n  temp_br %&gt;% \n  pull(tmax) %&gt;% \n  doy_last_frost() + as.Date(\"2023-01-01\") \n\ntibble(`last frost date` = frost_day_br, \n       `bloom date` = bloom_day_br) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 13: Last frost date and lilac bloom date in Brussels in 2023.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 13: Last frost date and lilac bloom date in Brussels in 2023.\n\n\n\n\nlast frost date\n\n\nbloom date\n\n\n\n\n\n\n2023-01-27\n\n\n2023-04-19\n\n\n\n\n\n\nWe visualize the predictions in Figure 6, which has the same interpretation as Figure 4. If the temperature forecast and Quetelet’s law are correct, on January 27, 2023 the lilacs in Brussels began “collecting” temperature. The lilacs will continue to “collect” temperature until April 19, at which point they will exceed their 4264°C² quota and bloom.\n```{r}\n(temp_br %&gt;% \n  ggplot() + \n  aes(date, temp) + \n  geom_line() + \n  labs(\n    x = \"\",\n    y = \"midrange temperature (°C)\",\n    title =\n      \"Figure 6: According to Quetelet's law, the lilacs will bloom once exposed to 4264°C² following the last frost.\") +\n  geom_vline(xintercept = as.numeric(c(frost_day_br, bloom_day_br)), \n             linetype = \"dotted\")) %&gt;%\n  ggplotly() %&gt;% \n  add_annotations(x = as.numeric(c(frost_day_br, bloom_day_br)),\n                  y = c(14, 14),\n                  text = c(\"last\\nfrost\", \"first\\nbloom\"),\n                  font = list(size = 14),\n                  ay = 0,\n                  xshift = c(-14, -16)) %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\n\n\n\n\nFigure 6: According to Quetelet’s law, the lilacs will bloom once exposed to 4264°C² following the last frost. Author provided, CC BY 4.0."
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#quetelets-legacy-advocate-mentor-and-perhaps-data-scientist",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#quetelets-legacy-advocate-mentor-and-perhaps-data-scientist",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Quetelet’s legacy: advocate, mentor, and perhaps data scientist",
    "text": "Quetelet’s legacy: advocate, mentor, and perhaps data scientist\nIn this tutorial, we stated the law of the flowering plants and explained how Quetelet derived it. We also reproduced and replicated Quetelet’s findings before using his law to predict the day the lilac will bloom in Brussels. We now conclude with a reflection on Quetelet’s legacy.\nThe law of the flowering plants surely stands the test of time. It continues to be used by scientists – with relatively few changes – to plan harvests, manage pests, and study ecosystems stressed by climate change. We speculate the law’s longevity is due to the fact that it balances simplicity with relatively accurate predictions.\nAlthough Quetelet did not discover the law, he did much to advance it. Quetelet founded an international network for “observations of the periodical phenomena” (in addition to numerous statistical societies and publications, including the precursor to the Royal Statistical Society). Quetelet’s network of 80 stations collected observations throughout Europe from 1841 until 1872. In particular, Quetelet collaborated with Charles Morren – who later coined the term phenology, the name of the field that now studies biological life-cycle events like the timing of flower blooms (Demarée and Rutishauser 2011).\n\nDemarée, Gaston R., and This Rutishauser. 2011. “From ‘Periodical Observations’ to ‘Anthochronology’ and ‘Phenology’ – the Scientific Debate Between Adolphe Quetelet and Charles Morren on the Origin of the Word ‘Phenology’.” International Journal of Biometeorology 55 (6): 753–61. https://doi.org/10.1007/s00484-011-0442-5.\nIn recent years, the observations collected through phenology networks have become an important resource for understanding the impacts of climate change. For example, the USA National Phenology Network calculates the Spring Bloom Index, which measures the “first day of spring” using the days lilacs are observed to bloom at locations across the United States. The index is then compared to previous years. Figure 7 shows one comparison, called the Return Interval. The Return Interval is much like a p-value, calculating how frequently more extreme spring indices were observed in previous decades. Bloom dates that are uncommonly early (green) or late (purple) may indicate environments stressed by changing climate. Scientists exploit the relationship between temperature and bloom date to extrapolate the index to areas with few observations.\n\n\n\n\n\n\nFigure 7: The Spring Bloom Index Return Interval measures whether spring is typical when compared to recent decades. Source: USA National Phenology Network.\n\nQuetelet’s emphasis on discovering the universal laws he believed govern social and biological phenomenon has not endured. But data scientists continue to appropriate laws from one area of science to study another. For example, data scientists use neural networks and genetic algorithms to study a wide variety of phenomenon unrelated to neuroscience or genetics. Perhaps Quetelet’s appropriation of Newton’s law, in addition to his careful use of data, make him among the first data scientists?"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#your-turn-do-you-have-what-it-takes-to-beat-quetelets-law",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#your-turn-do-you-have-what-it-takes-to-beat-quetelets-law",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Your turn: Do you have what it takes to beat Quetelet’s law?",
    "text": "Your turn: Do you have what it takes to beat Quetelet’s law?\nQuetelet reported that a plant flowers when the sum of the daily temperatures squared exceeds a specific quantity. His prediction rule was state of the art in 1833. But surely you, a twenty-first century data scientist, can do better. Here are some ideas to get you started.\n\nQuetelet squared the temperature before calculating the sum. Would another function of temperature produce a more accurate prediction?\n\nRemove the square so that a plant flowers once the sum of the daily temperatures exceeds a (different) specific quantity. Does this version of the law produce more accurate predictions? What if you use the daily temperatures cubed? (Beginner)\nSuppose a lilac only registers temperatures between 0°C and 10°C. That is, a lilac experiences temperature below the lower limit, 0°C, as 0°C, and above the upper limit, 10°C, as 10°C. Does the accuracy of the predictions improve if you use the temperature the lilac experienced instead of the ambient temperature measured by a weather station? Write a program that finds the lower and upper limits that produce the most accurate predictions. (Intermediate)\nQuetelet used mean absolute error to evaluate the accuracy of his predictions. But his estimate of the specific quantity of heat needed to bloom, 4264°C², does not actually minimize mean absolute error. Write a program that finds the specific quantity that minimizes mean absolute error. Redo part i. and ii. using this function. (Advanced)\n\nQuetelet calculated the sum of the daily temperature squared between the day of last frost and the bloom date. Would another time interval produce more accurate predictions?\n\nWe estimated the day of last frost using the last day the maximum temperature was below 0°C. Try estimating the day of last frost by the last day the midrange temperature was below 0°C? Which estimate yields the most accurate predictions? What if you ignore the day of last frost and simply calculate the sum of the daily temperatures squared between February 1 and the bloom date? When you change the time interval, be sure to calculate the new specific quantity of heat needed to bloom. (Beginner)\nWrite a program that finds the time interval which yields the best predictions. (Intermediate)\nWrite a program that calculates the prediction rule for many different time intervals. Use cross-validation to combine these prediction rules into a single prediction rule. (Advanced)\n\nQuetelet’s law only considers the temperature. Would additional information provide more accurate predictions?\n\nIs the specific quantity of heat needed to bloom different in years with abnormally cold winters? Would the predictions be more accurate if you use one quantity of heat for years with cold winters and a different quantity of heat for years with warm winters? (Beginner)\nIs the estimated quantity of heat needed to bloom similar for locations close in space and time? Write a program that leverages spatial and temporal correlation to improve the accuracy of the predictions. (Intermediate)\nSome biologists report that a plant must be exposed to a fixed amount of cold temperature in the winter – in addition to a fixed amount of warm temperature in the spring – before it can bloom. Augment the law of the flowering plants to require the accumulation of a specific quantity of cold temperature before the accumulation of a specific quantity of warm temperature. Write a program that uses this new law to predict the day the lilac blooms. (Advanced)\n\n\nFeeling good about your prediction algorithm? Show it off at the annual Cherry Blossom Prediction Competition!"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#appendix-preparing-usa-npn-data",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#appendix-preparing-usa-npn-data",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Appendix: Preparing USA NPN Data",
    "text": "Appendix: Preparing USA NPN Data\n```{r}\n# 1. download lilac data using `rnpn`\nusa_npn &lt;- \n  npn_download_individual_phenometrics(request_source = \"Jonathan Auerbach\",\n                                       year = 1900:2050,\n                                       species_ids = 36,                       \n                                       phenophase_ids = c(77, 412))            \n\n# 2. limit analysis to sites that report more than 25 times\nsite_ids &lt;- \n  usa_npn %&gt;% \n  group_by(site_id) %&gt;% \n  summarize(n = n()) %&gt;% filter(n &gt; 25) %&gt;% pull(site_id)\n\nusa_npn &lt;- \n  usa_npn %&gt;% \n  filter(site_id %in% site_ids)\n\n# 3. find nearest weather stations for each site\nlocations &lt;- \n  usa_npn %&gt;% \n  group_by(site_id) %&gt;% \n  summarize(latitude = first(latitude), \n            longitude = first(longitude))\n\nstations &lt;- \n  ghcnd_stations() %&gt;%\n  filter(first_year &lt;= min(usa_npn$first_yes_year),\n         last_year  &gt;= max(usa_npn$first_yes_year),\n         state != \"\") %&gt;%\n  group_by(id, latitude, longitude, state) %&gt;%\n  summarize(temp_flag = sum(element %in% c(\"TMIN\", \"TMAX\"))) %&gt;%            \n  filter(temp_flag == 2) %&gt;% \n  ungroup()\n\ndist &lt;- function(x, y = stations %&gt;% select(latitude, longitude)) \n  stations$id[which.min(sqrt((x[1] - y[,1])^2 + (x[2] - y[,2])^2)[,1])]\n\nlocations$station_id &lt;- apply(locations, 1, function(x) dist(c(x[\"latitude\"], x[\"longitude\"])))\n\n# 4. get weather data from nearest station using `rnoaa`\nget_station_data &lt;- function(station_id) \n  ghcnd_search(stationid = station_id,\n               var = c(\"tmin\", \"tmax\"),\n               date_min = \"1956-01-01\",\n               date_max = \"2011-12-31\") %&gt;%\n  reduce(left_join, by = c(\"id\", \"date\")) %&gt;%\n  transmute(id, \n            date, \n            tmax = tmax / 10,\n            tmin = tmin / 10)\n\nusa_npn &lt;- \n  locations %&gt;%\n  mutate(temp = map(station_id, get_station_data)) %&gt;%\n  right_join(usa_npn, by = c(\"site_id\", \"latitude\", \"longitude\")) %&gt;% \n  group_by(rownames(usa_npn)) %&gt;% \n  mutate(temp = map(temp, ~ .x %&gt;% \n                      filter(format(date, \"%Y\") == first_yes_year) %&gt;%\n                      mutate(temp = (tmin + tmax) / 2)),\n         num_obs = map(temp,~ sum(format(.x$date,\"%j\") &lt;= 150)),\n         doy = first_yes_doy, year = first_yes_year) %&gt;% \n  unnest(num_obs) %&gt;%  \n  filter(num_obs == 150) %&gt;%\n  ungroup()\n```\n\nExplore more Tutorials\n\n\n\n\n\nAbout the author\n\nJonathan Auerbach is an assistant professor in the Department of Statistics at George Mason University. His research covers a wide range of topics at the intersection of statistics and public policy. His interests include the analysis of longitudinal data, particularly for data science and causal inference, as well as urban analytics, open data, and the collection, evaluation, and communication of official statistics. He co-organizes the annual Cherry Blossom Prediction Competition with David Kepplinger and Elizabeth Wolkovich.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Jonathan Auerbach\n\n\n  Text and code are licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Images are not covered by this licence, except where otherwise noted.\n\n\n\nHow to cite\n\nAuerbach, Jonathan. 2023. “A demonstration of the law of the flowering plants.” Real World Data Science, April 13, 2023. URL"
  },
  {
    "objectID": "ideas/datasciencebites/posts/2022/12/13/ridesharing.html",
    "href": "ideas/datasciencebites/posts/2022/12/13/ridesharing.html",
    "title": "Determining the best way to route drivers for ridesharing via reinforcement learning",
    "section": "",
    "text": "About the paper and this post\n\n\n\n\n\nTitle: Dynamic causal effects evaluation in A/B testing with a reinforcement learning framework\nAuthor(s) and year: Chengchun Shi, Xiaoyu Wang, Shikai Luo, Hongtu Zhu, Jieping Ye, Rui Song (2022)\nStatus: Published in Journal of the American Statistical Association, Theory and Methods, open access: HTML, PDF, EPUB.\nEditor’s note: This post is republished with permission from MathStatBites to demonstrate the Bites concept. See here for more information.\n\n\n\nCompanies often want to test the impact of one design decision over another, for example Google might want to compare the current ranking of search results (version A) with an alternative ranking (version B) and evaluate how the modification would affect users’ decisions and click behavior. An experiment to determine this impact on users is known as an A/B test, and many methods have been designed to measure the “treatment” effect of the proposed change. However, these classical methods typically assume that changing one person’s treatment will not affect others (known as the Stable Unit Treatment Value Assumption or SUTVA). In the Google example, this is typically a valid assumption — showing one user different search results shouldn’t impact another user’s click behavior. But in some situations, SUTVA is violated, and new methods must be introduced to properly measure the effect of design changes.\nOne such situation is that of ridesharing companies (Uber, Lyft, etc.) and how they determine which drivers are sent to which riders (the dispatch problem). Simply put, when a driver is assigned to a rider, this decision impacts the spatial distribution of drivers in the future. Hence the dispatch strategy (our treatment) at the present time will influence riders and drivers in the future, which violates SUTVA and hence invalidates many traditional methods for A/B testing. To tackle this problem, a group of researchers have recently employed a reinforcement learning (RL) framework which can accurately measure the treatment effect in such a scenario. Furthermore, their proposed approach allows for companies to terminate A/B tests earlier if the proposed version B is found to be clearly better. This early stopping can save time and money.\nTo better understand RL and how it can contribute to tackling the issue at hand, it’s first helpful to set some context. In typical RL problems, including the one in this paper, the scenario is modeled with something known as a Markov Decision Process (MDP). A MDP links three sets of variables across time: the state or environment, the treatment or action (the reaction to the environment), and the outcome (the response produced by the environment due to the action). These outcomes can be thought of as rewards which depend on the action taken and the state observed. Over time, the machine learns which actions produce more positive rewards and which bring about worse outcomes. Hence, the actions leading to higher rewards are positively reinforced, thus the name reinforcement learning. A causal diagram of an MDP is shown in Figure 1, where St, At, and Yt are the state, action, and outcome at time t. As one can see, past treatments influence future outcomes by altering the state variables at the present (the so-called “carryover effect” which violates SUTVA).\n\n\n\n\n\nFigure 1: Causal diagram of MDP, where the solid lines represent causal relationships.\nMaking this more concrete, consider an example where the decision-maker is a ridesharing company. The environment or state is whatever variables the decision-maker can measure about the world, like the spatial distribution of drivers, number of pickup requests, traffic, and weather. The company then makes some action on how to dispatch drivers. The combination of the state and action leads to an outcome that can be measured, for example passenger waiting time or driver’s income. The strategy which is used to determine an action is known as a policy. This policy could be designed to take the state into account or simply be fixed regardless of what environment is encountered. Much of the reinforcement learning literature focuses on the former (policies that depend on the state), but the authors argue that fixed designs are the de facto approach in industry A/B testing and hence they focus on that setting. In particular, a common treatment allocation design is the switchback design, where there are two policies of interest (the current dispatching strategy vs a proposed strategy) determined ahead of time and they are employed in alternating time intervals during the A/B test.\nSo how are policies compared to determine the treatment effect? The answer lies in what is known as the value function, which measures the total outcome that would have amassed had the decision-maker followed a given policy. The value function can put more value on short-term gain in outcome or long-term benefits. The two policies in an A/B test each have their own value functions, and a proposed policy is determined to be better if its value is (statistically) significantly higher. In the ridesharing setting, one possible outcome of interest is driver income. An A/B test in that scenario would thus look to see if a proposed policy had greater expected driver income vs the current policy.\nA natural question is when to end the experiment and test for a difference in value. In practice, companies will often simply run the test for a prespecified amount of time, such as two weeks, and then perform an analysis. But if one policy is clearly better than another, that difference could be detectable much earlier and the company is wasting valuable resources by continuing the experiment. To address this issue, the authors take an idea from clinical trials, the “alpha-spending” approach, and adapt it to their framework. Alpha-spending is one way to distribute over time the prespecified “budget” of Type 1 error (the probability of falsely detecting that a new policy is better). In the article’s real-data example, the authors test once a day for each day after one week and are able to detect a significant difference on Day 12. Waiting until Day 14 would have resulted in poorer outcomes since a suboptimal policy would be implemented half the time for two more days.\nOverall, the framework introduced allows for handling of carryover effects, is capable of modeling treatment allocation like the switchback design, and furthermore, allows for possible early stopping. With these three features, the authors argue their approach is highly applicable to the current practice of ridesharing companies (and possibly other industries as well). For interested readers wanting to dive deeper into the methodology presented, you can check out the full article, listen to the first author discuss the material at the Online Causal Inference Seminar (embedded below), or explore the Python implementation available on GitHub.\n\n\n\n\n\nAbout the author\n\nBrian King is a PhD candidate in the Department of Statistics at Rice University and a current NSF Graduate Research Fellow, with research focused on Bayesian modeling and forecasting for time series of counts. Prior to Rice, he graduated from Baylor University with a B.S. in Mathematics and Statistics alongside a secondary major in Spanish and a minor in Computer Science.\n\n\n\n\n\nAbout DataScienceBites\n\nDataScienceBites is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to become a contributor."
  },
  {
    "objectID": "ideas/datasciencebites/posts/2023/01/24/pulling-patterns.html",
    "href": "ideas/datasciencebites/posts/2023/01/24/pulling-patterns.html",
    "title": "Pulling patterns out of data with a graph",
    "section": "",
    "text": "About the paper and this post\n\n\n\n\n\nTitle: Extracting the main trend in a data set: The Sequencer algorithm\nAuthor(s) and year: Dalya Baron and Brice Ménard (2021)\nStatus: Published in The Astrophysical Journal, open access: HTML, PDF.\nEditor’s note: This post is republished with permission from MathStatBites to demonstrate the Bites concept. For more information about Bites articles and how to contribute to DataScienceBites, see our notes for contributors.\nLarge volumes of data are pouring in every day from scientific experiments like CERN and the Sloan Digital Sky Survey. Data is coming in so fast that researchers struggle to keep pace with the analysis and are increasingly developing automated analysis methods to aid in this herculean task. As a first step, it is now commonplace to perform dimension reduction in order to reduce a large number of measurements to a set of key values that are easier to visualize and interpret.\nWhen working on the cutting edge, another problem scientists often face is that “we don’t know what we don’t know”. For this reason, we often want to simply ask the data, “What is interesting about you?” This is the realm of “unsupervised” methods, where the data itself drives the analysis, with little to no guidance or human labeling of the data.\nMany physical processes depend continuously on some driving parameter. For example, the evaporation rate of water increases with temperature. We call these continuous variations in datasets “trends”. Describing a dataset by a single trend reduces it to one dimension - an ordered list. Finding such a trend within a high-dimensional dataset is the aim of a method called “The Sequencer” introduced by Baron and Ménard."
  },
  {
    "objectID": "ideas/datasciencebites/posts/2023/01/24/pulling-patterns.html#key-insight-a-tree",
    "href": "ideas/datasciencebites/posts/2023/01/24/pulling-patterns.html#key-insight-a-tree",
    "title": "Pulling patterns out of data with a graph",
    "section": "Key insight: A tree",
    "text": "Key insight: A tree\nThe key insight of Baron and Ménard was to relate trends in data to an object from graph theory called a minimum spanning tree. Given a measure of distance between two data points, for example the usual (Euclidean) distance between two points, we can visualize a dataset as a graph. This graph consists of a node (a dot) for each data point. These nodes are then connected by an edge (a line), labeled by the distance between the two data points. The minimum spanning tree is a reduction of this graph to include only enough of the smallest distance edges so that no node is isolated.\nWhat Baron and Ménard realized is that datasets that are “trendy” have elongated and narrow minimum spanning trees. As shown in Figure 1, a totally random dataset results in a graph with many branches while a perfect sequence results in a perfect linear graph. Then, they use connectivity of the nodes in the minimum spanning tree to return an ordering of the data that follows the main trend in the dataset. However, a sequence is all you get. It is up to us to understand and interpret what this trend represents.\n\n\n\n\n\nFigure 1: Examples demonstrating that data with stronger trends have more narrow and elongated minimum spanning trees. Adapted from Baron and Ménard (2021), Figure 1. Figure used under CC BY 4.0.\nSometimes, the ordering of observations within a data point matters, like in time series data. Measurements taken close in time are more likely to be correlated than measurements taken after a long time delay. Baron and Ménard were careful to include a measure of distance that takes this ordering into account, unlike our usual notion of distance. They argue that this gives them an edge over other common dimension reduction techniques such as t-SNE and UMAP, and even go so far as to use The Sequencer to optimize the hyperparameters used by these other methods!"
  },
  {
    "objectID": "ideas/datasciencebites/posts/2023/01/24/pulling-patterns.html#when-does-it-fail",
    "href": "ideas/datasciencebites/posts/2023/01/24/pulling-patterns.html#when-does-it-fail",
    "title": "Pulling patterns out of data with a graph",
    "section": "When does it fail?",
    "text": "When does it fail?\nIt is important to acknowledge that no statistical or machine-learning tool is a cure-all. And, the authors themselves are quick to point out several limitations that can hinder the application of their method. The Sequencer can struggle when the data has a large dynamic range, a variety of signal strengths relative to noise, or there are multiple trends present in the data. In each case, Baron and Ménard propose ways to mitigate these problems, but practitioners still need to be wary when applying The Sequencer in those instances."
  },
  {
    "objectID": "ideas/datasciencebites/posts/2023/01/24/pulling-patterns.html#what-discoveries-await",
    "href": "ideas/datasciencebites/posts/2023/01/24/pulling-patterns.html#what-discoveries-await",
    "title": "Pulling patterns out of data with a graph",
    "section": "What discoveries await?",
    "text": "What discoveries await?\nTo demonstrate the power of their method, Baron and Ménard apply The Sequencer to several real datasets where a pattern was already known and show that The Sequencer recovers that pattern. Examples include ordering spectral measurements of stars by their temperature and quasars by their redshift, a measure of their distance from us on Earth.\nBut, what about new patterns? The team has already applied The Sequencer to mine seismographic data and discover previously unknown formations deep within the earth, at the boundary between the core and mantle. By sequencing the seismic waves, they realized that the main trend was the amplitude of diffraction off of these structures, which they were then able to localize beneath Hawaii and the Marquesas (DOI: 10.1126/science.aba8972).\nFor more demonstrations and discoveries, or even to upload your own data for sequencing, check out the project website. Data sleuths can also download all of the code directly from GitHub and sequence to their hearts’ content!\n\n\n\n\n\n\nAbout the author\n\nAndrew Saydjari is a graduate student in physics at Harvard researching the spatial and chemical variations of dust in the interstellar medium. He favors using interpretable statistics and large photometric and spectroscopic surveys.\n\n\n\n\n\nAbout DataScienceBites\n\nDataScienceBites is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to become a contributor."
  },
  {
    "objectID": "contributor-docs/recommender.html",
    "href": "contributor-docs/recommender.html",
    "title": "Recommenders",
    "section": "",
    "text": "Too much content, not enough time. That about sums up the problem facing the data science community. So, our Recommenders are here to help. Contributors are invited to submit lists (or Feeds) of high-quality sources on all manner of topics – from foundational ideas in data science and cutting-edge techniques, to opinion and thought-leadership on the future of the profession. Reviews of new books and other material are also welcome."
  },
  {
    "objectID": "contributor-docs/recommender.html#article-types-and-structures",
    "href": "contributor-docs/recommender.html#article-types-and-structures",
    "title": "Recommenders",
    "section": "Article types and structures",
    "text": "Article types and structures\n\nFeeds\nFeeds can be constructed around different topics and audiences. For example, you might want to recommend to all data scientists the “10 best blogs on machine learning” or “5 data visualisation experts to follow on Twitter”. Or you might have a list of sources specifically targeted at data science educators (e.g., “the best books on teaching coding”) or data science leaders (“5 insightful case studies on building data science teams”).\nWhatever you choose to focus on, the following outline provides a basic guide for structuring your feed:\n\n\nOverview\n\nA brief introduction to your list, its main focus, who you are writing it for, and why. You should also say something about yourself and your background, too. This will give additional context to the recommendations you are making.\n\n\n\nList of sources\n\nAs well as naming your sources and telling people how to find them, you should also explain why you are recommending them, how they helped you in your career or studies, or other reasons why you find them to be of value. Sample quotes or small excerpts from the sources themselves might also be worth including.\n\n\n\nStart a dialogue\n\nConclude with a call for readers to share recommendations of their own, either in the article comments or on social media. Contributors are welcome to update their lists any time with new sources, including those suggested by site users.\n\n\n\n\n\nReviews\nUnlike the feeds described above, each submitted review should focus only on a single publication. It must be an honest summary of the reviewer’s thoughts, feelings and impressions, covering what they liked and didn’t like, the perceived strengths and weaknesses of the publication, and whether it is likely to be of interest and value to its intended audience. Reviews should of course provide an overview of the publication in question but must avoid dry, itemised descriptions of the publication’s constituent parts (e.g., listing out the chapters in a book).\nAll reviews should list the following information (if relevant):\n\nTitle of publication\nAuthor(s)\nDate of publication\nEdition or format used for review\nPublisher\nLength\nPrice\nWebsite address or other source of further information"
  },
  {
    "objectID": "contributor-docs/recommender.html#advice-and-recommendations",
    "href": "contributor-docs/recommender.html#advice-and-recommendations",
    "title": "Recommenders",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nKeep lists to a sensible size. Feeds are meant to help data scientists to prioritise who and what to follow based on their interests and career stage – and it is much easier to keep tabs on 5 sources than it is 50, or even 15! So, the fewer the better.\nKeep your recommendations up to date. In this era of digital publishing, things can and do change overnight. So, if one of your recommended bloggers stops blogging, or the author of one of your favourite books makes a major update to the text, do be sure to let us – and your audience – know. We want to keep feeds and reviews current and useful.\nOf course you are brilliant, but… Please do not recommend or review your own publications or those in which you have a pecuniary or similar interest."
  },
  {
    "objectID": "contributor-docs/exercises.html",
    "href": "contributor-docs/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Exercises on Real World Data Science will provide users with the opportunity to put knowledge, skills and new learnings into practice, helping them to challenge and refine problem-solving approaches while strengthening the analytical mindset.\nWe will achieve this by supporting contributors to design tasks that replicate real-world data scientific processes."
  },
  {
    "objectID": "contributor-docs/exercises.html#structure",
    "href": "contributor-docs/exercises.html#structure",
    "title": "Exercises",
    "section": "Structure",
    "text": "Structure\nThe structure of each published exercise will vary based on the nature of the task(s) being set by contributors and the outcomes they have in mind. But, in general, exercises must do more than simply ask users to, e.g., download dataset – analyse – report.\n\n\nSet the scene\n\nProvide a believable, realistic scenario for the exercise. Establish the “client challenge” within that context, the resources available to the data scientist, and the outputs expected.\n\n\n\nGive users space to map the problem themselves\n\nHave users translate the “client challenge” into a data analytic question that can be answered using the resources available.\n\n\n\nMake data exploration, cleaning and tidying part of the process\n\nData exploration and preparation are an important part of most – if not all – data science projects, so let users loose on messy datasets so they can figure out for themselves (a) what they’re working with and (b) what analytical approach makes sense given the features of the data and the problem at hand.\n\n\n\nIntegrate ethics\n\nPrompt users to think about and work through ethical questions and issues that are relevant to the exercise: the challenge they’ve been set, the data they’ve been given, their proposed approach to analysis and modelling, etc.\n\n\n\nEncourage users to document their work and their thinking\n\nThrough computational notebooks (e.g., Jupyter notebooks), users can record not only what they’ve done and how they’ve done it, but why.\n\n\n\nMake data presentation part of the expected project outputs\n\nAsk users to think about presenting to specific audiences: not just fellow data scientists, but to decision-makers, policy experts, the public – whatever makes most sense given the exercise scenario."
  },
  {
    "objectID": "contributor-docs/exercises.html#advice-and-recommendations",
    "href": "contributor-docs/exercises.html#advice-and-recommendations",
    "title": "Exercises",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nInvite users to share their work. If users have followed the advice to document their work and thinking, encourage them to share their computational notebooks (including their results and outputs) on their own websites, in a GitHub repository, through social media, etc. This could be a great way for others to discover your exercise, and we could also link to a selection of these notebooks through the exercise page itself.\nThink about building in hints and tips. Some users – depending on their prior level of experience – might appreciate some additional guidance here and there.\n\n\n\n\n\n\nSpoiler warning\n\n\n\n\n\nCollapsible callouts like this make hints and tips visible only to those who want to see them.\n\n\n\nBring different resources together. Exercises provide an ideal opportunity to draw together different strands of the Real World Data Science platform: case studies could provide inspiration or pointers for how to tackle a particular challenge; explainers might contain useful information about the tools and techniques to apply to specific types of data; and if you’re looking for a suitable set of data, it may already be listed in our datasets section."
  },
  {
    "objectID": "contributor-docs/training-guides.html",
    "href": "contributor-docs/training-guides.html",
    "title": "Training guides",
    "section": "",
    "text": "In data science, there’s no one-size-fits-all solution to every problem and challenge. So, part of the job of the data scientist is to rapidly learn about different sub-domains, tools and techniques, and put those learnings into practice.\nBut it can be time-consuming to figure out what you need to learn and in what order, and to identify the best resources for doing so. This is where our Training guides come in. Each will set out a learning pathway for data scientists to follow, with recommendations of textbooks, videos, practical exercises and other teaching material to use every step of the way."
  },
  {
    "objectID": "contributor-docs/training-guides.html#structure",
    "href": "contributor-docs/training-guides.html#structure",
    "title": "Training guides",
    "section": "Structure",
    "text": "Structure\nContributors should think about their training guides as being short online courses that are constructed from existing high-quality material. You do not need to create your own “course” content. Rather, you should focus on recommending texts and other material for users to follow in a logical ordered way, so that they may build up and secure their knowledge of a particular topic.\nGuides should feature a mix of content types – not only text, but audio and video – and they should provide ample opportunities for users to practice what they are learning.\nA brief and extremely simplified example of a guide is as follows:\n\nStep 1: Watch this introductory video on Topic X.\nStep 2: Now you are familiar with the basics of Topic X, you will want to read Chapter 2 of Textbook Y, which delves into more of the mathematical underpinnings.\nStep 3: Let’s try Topic X ourselves. This GitHub repository has code for you to run it in Python. Copy the code and give it a go.\nStep 4: You should now be ready to apply Topic X to a simple data challenge. Check out this Kaggle page and practice what you have learned so far.\nStep 5: We’re now moving from the “beginner” level to “intermediate”, and Training Course Z gives a thorough overview of what you need to know for the next stage of your learning journey.\n… etc."
  },
  {
    "objectID": "contributor-docs/training-guides.html#advice-and-recommendations",
    "href": "contributor-docs/training-guides.html#advice-and-recommendations",
    "title": "Training guides",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nBe mindful of different learning styles. Some people prefer to read, others prefer to watch or listen. So, wherever possible, for each stage of your training guide, try to provide a mix of resources that meet the same learning objectives.\nConsider barriers to entry. Data scientists in large organisations may have access to training budgets or mechanisms to apply for training funds. But that isn’t the case for all data scientists, meaning that paid-for materials and training courses might not be accessible to everyone. Recommend them sparingly, and if there are ways to access the material at reduced rates do let users know. However, you must not link to illicit copies of material – e.g., unauthorised PDF reproductions of textbooks.\nIf there are resource gaps, please tell us. While planning out your training guide, you may well struggle to find the perfect piece of content to recommend at a particular stage of your learning journey. If that is the case, do get in touch with us. One of the goals of Real World Data Science is to identify and plug these sorts of gaps, so that all in the data science community can benefit. We’ll sketch out a commission and take it out to our network of contacts. Or perhaps it’ll be something you want to create for the site!"
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html",
    "href": "contributor-docs/call-for-contributions.html",
    "title": "Call for contributions",
    "section": "",
    "text": "Real World Data Science aims to inform, inspire and strengthen the data science community by showcasing real-world examples of data science practice and bringing together data scientists to share knowledge.\nWe cannot succeed in these aims without the support and contributions of the data science community, so thank you for taking the time to review this open call for contributions."
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#what-are-we-looking-for",
    "href": "contributor-docs/call-for-contributions.html#what-are-we-looking-for",
    "title": "Call for contributions",
    "section": "What are we looking for?",
    "text": "What are we looking for?\nBelow is a list of our core content areas. We welcome submissions in any of these areas. Each content area is linked to its own set of notes for contributors.\n\nCase studies\nExplainers\nExercises\nDatasets\nTraining guides\nRecommenders\nNew! DataScienceBites\n\nSubmissions can focus on any and all topics and application areas. We want our content to reflect the breadth and depth of real-world data science."
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#our-target-audience",
    "href": "contributor-docs/call-for-contributions.html#our-target-audience",
    "title": "Call for contributions",
    "section": "Our target audience",
    "text": "Our target audience\nReal World Data Science is for all who work in data science – whether they are students, teachers, practitioners or leaders. Submissions do not have to appeal to all data scientists, however. Contributors should think carefully about who they are trying to reach, and craft their submissions accordingly."
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#what-can-submissions-include",
    "href": "contributor-docs/call-for-contributions.html#what-can-submissions-include",
    "title": "Call for contributions",
    "section": "What can submissions include?",
    "text": "What can submissions include?\nWe encourage contributors to experiment with and include different media formats in their submissions – text, images, audio and video. And as our site is built on Quarto – the new open-source publishing system developed by Posit – submissions to Real World Data Science can also include code cells, equations, figures, interactive data displays, and other elements to enrich the user experience. If you haven’t used Quarto before, check out this fantastic tutorial from the developers."
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#how-to-submit",
    "href": "contributor-docs/call-for-contributions.html#how-to-submit",
    "title": "Call for contributions",
    "section": "How to submit",
    "text": "How to submit\nOnce you’ve reviewed our notes for contributors and settled on a content area, theme and audience, please review our contributor guidelines for details on the submission process."
  },
  {
    "objectID": "contributor-docs/explainers.html",
    "href": "contributor-docs/explainers.html",
    "title": "Explainers",
    "section": "",
    "text": "On Real World Data Science, Explainers are the stories behind the stories of data science in action. They are deep-dive explorations of the ideas, concepts, tools, and methods that make data science projects possible. In particular, we are keen to explore and explain the statistical underpinnings of modern data science techniques.\nA good Explainer will lead audiences through the what, when, how, and why of its chosen topic. The ultimate goal is to equip data scientists with the information and insight they need to make smarter, more informed analytical choices.\nThere are many different but effective ways of structuring an explainer and plentiful written examples in major media outlets like The Guardian and Vox, but these are generally written for a non-technical audience. Examples of technical explainers (with interactive elements) can be found on Amazon’s Machine Learning University."
  },
  {
    "objectID": "contributor-docs/explainers.html#structure",
    "href": "contributor-docs/explainers.html#structure",
    "title": "Explainers",
    "section": "Structure",
    "text": "Structure\nThe following outline is a basic guide to structuring an Explainer:\n\n\nHook\n\nIntroduce your topic, and explain why audiences should pay attention. For example, does your Explainer link to one of our published case studies? Does it focus on a tool or method that has been the subject of recent attention? Is it a foundational idea that is relevant to all sorts of data science applications?\n\n\n\nHigh-level summary\n\nA short, largely non-technical explanation of your topic. A good way to view this section is as an accessible condensed version of your complete Explainer. In thinking of it in this way, you can subtly signpost to audiences the areas you’ll be covering and the questions you’ll be answering throughout the remainder of your contribution.\n\n\n\nHistory and background\n\nIt can be useful from a practical perspective to explain how ideas, concepts, tools, and methods have developed over time. Applications may have become more complex in recent years, so exploring the origins of data science techniques might lead you to discover simpler use cases that can help support and illustrate your high-level summary.\n\n\n\nThe how, the when, the why\n\nThis section of your Explainer will likely be split into multiple subsections as you seek to build up your audience’s understanding of your chosen topic. It can be helpful to think about the sorts of questions an audience member might ask and to structure your contribution so that it directly addresses those questions (Q&A/FAQ formats are commonly used in explainer-type articles). If the focus of your Explainer is a data science method, for example, you’ll want to address the following:\n\n\n\nHow does it work and how is it applied (perhaps with an example or simulation)?\nWhat are the underlying assumptions?\nHow is performance checked and assessed?\nHow should outputs be interpreted?\nWhat are the pros and cons, the strengths and limitations of the approach?\nWhat are the optimal use cases, and when should the method be avoided altogether?\nAre there alternatives that people should know about?\n\n\nKey takeaways\n\nThis serves as your final summary: a chance to remind your audience of what they’ve learned from your Explainer and the main points they should keep in mind.\n\n\n\nTell me more\n\nIt’s sensible to assume that some of your audience will have further questions and will want to learn more about the topic. If you have additional sources of information to recommend, make sure to share them here."
  },
  {
    "objectID": "contributor-docs/explainers.html#advice-and-recommendations",
    "href": "contributor-docs/explainers.html#advice-and-recommendations",
    "title": "Explainers",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nFocus on what’s important. Your Explainer can’t hope to explain everything, so you need to be clear about what’s essential for your audience to know and what isn’t. Make good use of links and references to point audiences to other valuable sources of information that can enrich their understanding of your topic.\nBe clear about your target audience and their expected prior level of knowledge. In keeping with the point above, you need to be clear in your own mind about how much you expect your audience to know already about the general topic or subject matter. You can then structure your contribution accordingly. It might also be helpful to state explicitly, at the outset of your contribution, what assumptions you’ve made about your audience and highlight any background reading that might be beneficial.\nPlan out your route. To help you decide what to cover in your Explainer, we recommend first writing out your high-level summary of the topic and also your key takeaways. This provides you with a start point (A) and an end point (B) for your contribution. The challenge then is to figure out the main points or questions you will need to address to help your audience progress from point A to point B in a way that’s logical and intuitive for them to follow."
  },
  {
    "objectID": "rwds-partners.html",
    "href": "rwds-partners.html",
    "title": "Our partners and funders",
    "section": "",
    "text": "Real World Data Science is a project of the Royal Statistical Society (RSS). The Society was founded in 1834 and is one of the world’s leading organisations advocating for the importance of statistics and data.\nRSS has more than 10,000 members in the UK and across the world. As a charity, it advocates for the key role of statistics and data in society, and works to ensure that policy formulation and decision making are informed by evidence for the public good.\nTo support the work of the RSS, including Real World Data Science and other projects, become a member today.\nEmail: info@rss.org.uk"
  },
  {
    "objectID": "rwds-partners.html#the-royal-statistical-society",
    "href": "rwds-partners.html#the-royal-statistical-society",
    "title": "Our partners and funders",
    "section": "",
    "text": "Real World Data Science is a project of the Royal Statistical Society (RSS). The Society was founded in 1834 and is one of the world’s leading organisations advocating for the importance of statistics and data.\nRSS has more than 10,000 members in the UK and across the world. As a charity, it advocates for the key role of statistics and data in society, and works to ensure that policy formulation and decision making are informed by evidence for the public good.\nTo support the work of the RSS, including Real World Data Science and other projects, become a member today.\nEmail: info@rss.org.uk"
  },
  {
    "objectID": "feeds.html",
    "href": "feeds.html",
    "title": "RSS feeds",
    "section": "",
    "text": "Latest content\nrealworlddatascience.net/latest-content.xml\n\n\nCase studies\nrealworlddatascience.net/case-studies/index.xml\n\n\nIdeas\nrealworlddatascience.net/ideas/index.xml\n\n\nCareers\nrealworlddatascience.net/careers/index.xml\n\n\nViewpoints\nrealworlddatascience.net/viewpoints/index.xml"
  },
  {
    "objectID": "careers/posts/2023/05/11/chatgpt-data-science-pt1.html",
    "href": "careers/posts/2023/05/11/chatgpt-data-science-pt1.html",
    "title": "How is ChatGPT changing data science?",
    "section": "",
    "text": "For many people, it starts with a question. Something simple, something they already know the answer to. A test, in other words, to see what these AI-powered chatbots are all about. But spend any amount of time with ChatGPT and other such tools and you’ll quickly start to wonder what else they might do, and how useful they might be in your day-to-day working life.\nData scientists certainly have been thinking along these lines, and to find out more about current use cases, proofs of concepts and potential applications, Real World Data Science got together with members of the Royal Statistical Society’s Data Science and AI Section (DS&AI) for a group discussion.\nOur interviewees, in order of appearance, are:\n\nPiers Stobbs, VP science at Deliveroo, and DS&AI committee member.\nDetlef Nauck, head of AI and data science research at BT, and editorial board member, Real World Data Science.\nAdam Davison, head of data science at the Advertising Standards Authority, and DS&AI committee member.\nTrevor Duguid Farrant, senior principal statistician at Mondelez International, and DS&AI committee member.\nGiles Pavey, global director for data science at Unilever, and DS&AI vice-chair.\nMartin Goodson, CEO and chief scientist at Evolution AI, and DS&AI committee member.\n\nThe first part of our discussion focuses on how large language models are becoming part of the data science toolkit, and what this new development means for data science teams and skillsets. Stay tuned for part two, which we’ll be publishing soon!\n(UPDATE: Part two is now published: “Large language models: Do we need to understand the maths, or simply recognise the limitations?”)\n\n\n\n\n\nAs data scientists, how has ChatGPT – and other tools built on large language models (LLMs) – changed your working lives?\nPiers Stobbs: Up to about a year ago, although I was really impressed with the developments in deep learning and the improvements in computer vision and natural language models, it felt in line with general improvements in machine learning. And then, probably about six months ago, with things like DALL·E and ChatGPT, it felt like something changed – properly ground-breaking capabilities. And I still can’t quite get my head around the fact that you can basically have a model that tries to predict the next token, and it comes up with outputs that really feel quite sensible and human-like – if prone to hallucination.\nThe way I think about it is, this feels like a brand-new capability that we’ve just not really had before. It’s almost like an interface with unstructured information. Historically, you sort of have to turn text into something, and then turn something back into text, if you want to have this interface with humans. Now, we’ve got this really quite elegant way of plugging the gaps, which feels full of opportunities.\nI’m having great fun playing around with the code co-pilots – GitHub’s Copilot is amazing and, productivity wise, is helping me a lot. I am now a much faster coder because there’s all those Stack Exchange lookups that I don’t have to do anymore. Again, from a personal productivity perspective, I’m using [ChatGPT] for initial drafts of documents and other things. And then I use it almost for validating things. For instance, I had a random discussion the other night with ChatGPT about logistic algorithms. It’s not going to solve problems for you, but I asked it to give some pointers of things I could be thinking about – some of which I had, some of which I hadn’t. So, it’s almost like a brainstorming helper, somehow.\nBut probably the thing I’m most excited about is the knowledge sharing side of it – plugging it into, or on top of, private information, and surfacing all that knowledge that is locked away in documents and intranet pages.\n\n\n\n\n\n\nPiers Stobbs\n\n\n\n\n\nThis feels like a brand-new capability – an interface with unstructured information. Historically, you have to turn text into something, and then turn something back into text, if you want to have this interface with humans. Now, we’ve got this elegant way of plugging the gaps.\n\n\n\nDetlef Nauck: We’re looking into running proof of concepts to see whether LLMs do bring value. Software engineering is the most obvious one, and easiest to set up and run. And then we want to look at making use of internal documents – so, either summarization or creation of internal documents in appropriate language. The latter use cases are trickier to evaluate. We want to know whether the outputs produced are any good. With software engineering you can track GitHub statistics, for example. But if you give ChatGPT to somebody to write marketing material, or to get information out of a document, how do you know that the results are good? We need to get our head around metrics for evaluation.\nAdam Davison: I’ve been using it for basically anything where I don’t remember the API very well or it’s a bit confusing. Pandas is the key, right? We all use pandas, but you don’t really remember how to do some complicated thing with apply(), say, so you just ask GPT-4 to give you the answer, and it saves you that hassle. Also, I read some insightful tweet that said these chat systems are really good for things where generating the solution is hard, but verifying it is easy. And I think that’s true for some of these things. You know, you get a short piece of Python code, you can basically look at that and you can tell if it’s right.\nIn data science, you’re a bit of a jack-of-all-trades. You need to do little bits of everything, but you’re not a specialist in anything. And so, I think for software development, it’s been really helpful. For example, right now, I’m doing a bit of frontend development in a project to visualise something. I’m never going to be a professional frontend developer, but GPT-4 can help deal with the oddities of JavaScript much more easily than it would be for me to trawl through Stack Overflow posts.\nBut the thing that we’re using it for, practically, is natural language processing (NLP) and classification. We have this particular problem at the Advertising Standards Authority (ASA) where we are running lots of different models that are completely unconnected to each other because every project is a different topic. So, one week we’re looking at, “Do these gambling ads appeal to young people?” and then the week after it’s, “Are these cryptocurrency ads being clear about the risks involved?” It’s very disparate, we don’t have a lot of time to iterate on models, and we don’t have huge amounts of training data. Ten years ago, when you were doing sentiment classification, you were on Mechanical Turk getting 10,000 examples, and even then it didn’t work very well after these really complicated models. Now, you’ve got a couple of hundred examples and with the embeddings [from LLMs] you can get to a pretty decent classifier quite quickly. We’re also starting to experiment with using OpenAI’s fine-tuning tools, and the performance that we’ve seen from that is very impressive, to the extent that it’s making us rethink whether we bother doing anything else in some of our classifiers.\n\n\n\n\n\n\nAdam Davison\n\n\n\n\n\nFive years ago, if you had a sophisticated problem involving text or images, you’d need a big research team with a big budget to tackle it. But increasingly we find, like many other people, that you can take models off the shelf and repurpose them for quite diverse tasks.\n\n\n\nTrevor Duguid Farrant: My organisation is not as far forward as the rest of you. I’ve introduced it to the leadership team, and the digital services team – what was IT – are looking to make a decision on whether we can use it or not. I think there’ll be so much pressure they’ll have to use it, but there’s still a feeling of discomfort with it, whereas I think it’s really good and have started using it. Everyone else on the call seems to have started using it. So, can organisations like the Royal Statistical Society help companies to embrace this and start using it, and then everyone can benefit from it?\nGiles Pavey: I wish I could be with Piers and Adam – actually using it – but my life has been taken over as the guy who goes and explains it to the business. Unilever is a massive business, and we are concerned about privacy, confidentiality and trustworthiness. We’ve now built an initial GPT instance on Azure and fed it with some of our own documents, and a lot of my time has been working with legal to convince ourselves that that’s okay. Now we are really trying to work out just how we manage the amazing demand for proofs of concepts and use cases – and what we’re just about to uncover, I think, is the unknown but potentially massive expense of running it.\nIn pure proofs of concepts, departments that have large knowledge banks are using it: research and development, and marketing, for example. And one of the big technical things that we’re working on – and, because of the size that we are, we’re doing a lot of work with OpenAI and Microsoft on this – is how to stop the models from hallucinating.\nHave your experiences with ChatGPT and other tools changed your thinking about the skillsets required of data scientists and data science teams?\nAD: A little bit. As someone at a small organisation, I think it’s quite exciting because, five years ago, maybe you were in a world where if you had a sophisticated problem involving text or images, you’d need a big research team with a big budget to tackle it. But increasingly we find, like many other people, that you can take models off the shelf and repurpose them for quite diverse tasks. So, I think it’s becoming increasingly viable to have a small team of people who are implementers, who aren’t necessarily backed up by a big research organisation, doing increasingly sophisticated stuff.\nI don’t think it does away with the sort of things that we always bang on about in the Data Science and AI Section, like the need for an understanding of statistics and how the underlying systems really work, because I think you still need to understand what you’re doing with LLMs, as with any other machine learning technique. But, if I had to guess, what we’re going to be seeing now is that for a lot of problems, you’re going to have more of a division – so, you’re either in one of a small number of very large labs doing research on very cutting-edge big models, or you can be an implementer who is taking things off the shelf and applying them. And maybe that space in between is going to get a little bit squeezed – that would be my guess, but obviously it’s very unpredictable.\n\n\n\n\n\n\nGiles Pavey\n\n\n\n\n\nWe’ve built an initial GPT instance on Azure. Now we are really trying to work out just how we manage the amazing demand for proofs of concepts and use cases – and what we’re just about to uncover, I think, is the unknown but potentially massive expense of running it.\n\n\n\nPS: That’s exactly my view. When I first started hiring data scientists, a long time ago, you basically had to write stuff from scratch, and you needed PhDs – people who really understood, at a deep level, how the maths all works. But I think there’s been a steady progression towards valuing software engineering skills, and I think, in some ways, this is another step along that path. If I think now about implementing a chatbot over your own knowledge base, it’s basically like plugging APIs together with some Python. Adam’s point is still hugely important, though, because I think we still need the background knowledge about what’s actually going on – OK, I’m creating embeddings here, and that’s allowing this search to work so I can surface the right docs – that whole process, which an average software engineer is maybe not going to know. But I think it’s definitely blurring the lines.\nMartin Goodson: It’s just as important now to understand how to evaluate performance. The difference is, it used to be that you were trying to figure out whether it’s 80% accurate, or 85%. Now, it’s like 99.9%. But you still need to figure it out. You still need to understand what the failure modes are, what caused it; how is it actually working, and is it doing what you need it to do for the product? Is it actually satisfying our needs as users or as customers of the products.\nDN: I think in the future, the skills we will need are people who can run and build these models. Giles made the point about how expensive it is to run these things. Right now, you have two options: subscribe to an API, and then you are limited in how you can modify these models; or build your own – take an open-source LLM and modify it. But then you need people who know how to build a high-performance computing environment and operate this efficiently. You need to know how to actually train the models, how to curate the data, how to set the model parameters. And I always think there’s too much alchemy still going on in this field, right? It’s not proper science. People build these things and then are surprised at what they can do; they didn’t know such things would be possible. A lot of these capabilities only emerge when you make the models really, really big and, essentially, you also have no control over them – you can’t stop them hallucinating. So, these are the kinds of issues we need to get under control if we want to get any value out of them.\nPrompt engineering is another one – you really need to understand how these models work and how to prompt them. If you want to give them to, say, a marketer to generate copywriting, they may not have the right ideas of how to prompt the machine. So, I could see roles developing out of data science that understand how to influence these models and make them do what we want them to do.\nMG: The other angle to this is junior engineers. Now, the bar for being a useful junior engineer is that you’re better than GitHub Copilot. Why do you need a really junior person if you can just use a large language model to be the junior developer?\nDN: I’m not thinking about the data science person who needs to write some code for a project here, but if you have a large software team in an organisation that produces production code, they will become more efficient by using these tools. But still, with all this overhead of testing and putting it all together, there will be a lot of manual work that needs to be done. But the teams will get more efficient and junior people will get up to speed quicker. That’s probably another advantage.\n\n\n\n\n\n\nTrevor Duguid Farrant\n\n\n\n\n\nCan the Royal Statistical Society help non-tech companies embrace large language models, extolling their virtues and dispelling the myths?\n\n\n\nPS: I think Detlef’s point about understanding is an interesting one. It definitely feels like there’s been this sort of continuum from, you know, “OK, it’s a linear regression, we know what’s going on” to complex models to ensemble models where, again, you’re combining these things you can individually understand. Even with big ImageNet architectures, billions of parameters, at least conceptually you can understand how these work and build out tools where you can understand the layers. To me, what’s different now is you’ve got this reinforcement learning layer on top, or diffusion layer, or some other additional approach – this combination of really complicated things. I honestly don’t know where to start with trying to understand why a specific output is generated, and I think that is a proper concern. That’s definitely an area of research, because I think we need to understand this.\nGP: I think there’s also a question in large companies of just who owns these things. Up until this point, everybody’s been happy that AI is the realm of data science. And, suddenly, generative AI looks like it might be the realm of the IT team – that it’s a service that you get off the shelf. It’s going to be interesting to see how that plays out. I really liked the point that Martin was making about being able to tell what the systems are actually doing, what they are supposed to do and how to check them, because if you don’t have a background in that area, you might just assume they work. Now, nobody knows exactly how these things work – not even the people who build them. But having a background in how you test things, for potential causes for things not working, is actually going to be incredibly powerful or useful.\nTDF: Will experts like us actually be able to check it because of the speed that new versions are coming out and the developments that are happening? Is it going to take us six months to check that GPT-3.5 works? Well, too late, a month later GPT-4’s out! I just think that pace is going to keep accelerating.\n\n\n\n\n\n\nWant to hear more from the RSS Data Science and AI Section? Sign up for its newsletter at rssdsaisection.substack.com.\n\n\n\n\n\n\n\nBack to Careers\n\n\n\n\nRead part two →\n\n\n\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photos are not covered by this licence. Portrait photos are supplied by interviewees and used with permission. ChatGPT homescreen photo by Levart_Photographer on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “How is ChatGPT changing data science?” Real World Data Science, May 11, 2023. URL"
  },
  {
    "objectID": "careers/career-profiles/posts/2023/06/28/claire-morton.html",
    "href": "careers/career-profiles/posts/2023/06/28/claire-morton.html",
    "title": "‘I was inspired by the power that numerical data have to tell stories and promote policy change’",
    "section": "",
    "text": "This week, in celebration of Pride, Real World Data Science is collaborating with the JEDI Outreach Group of the American Statistical Association (ASA) and the ASA LGBTQ+ Advocacy Committee to highlight the achievements of statisticians and data scientists from across the LGBTQ+ spectrum.\nMembers of the committee nominated two individuals to be featured as part of our career profile series, and so we are pleased to bring you interviews with Claire Morton (below) and Albert Lee.\nRead on to discover more about Claire’s data science career (so far).\n\n\n\nHi, Claire. Thank you for sharing your career story with Real World Data Science. Please tell us a little about yourself and your role in data science.\nMy name is Claire Morton, and I’m an undergraduate student studying mathematical and computational science and environmental justice at Stanford University. I’m particularly interested in using statistics and data science to work with community-based organizations and advance evidence-based environmental justice policy. I have conducted quantitative research on tools to classify disadvantaged communities, oil wells, climate resilience, housing justice, and the connections between soil and health.\nWhat drew you to study statistics and data science?\nI really enjoyed my math, statistics, and coding classes in school. I was also inspired by the power that numerical data have to tell stories and promote policy change.\nWhat do you think is your most important skill as a data scientist?\nListening to others. Listening allows me to learn new statistics skills from my mentors and to learn about how best I can work with community partners on their priorities in my research.\nHow does your gender and/or sexual identity factor into your career?\nI am a lesbian, and, at the start of college, I didn’t have any mentors who shared my identity. I’ve now found several through the ASA and queer communities at my university, and I’m continually inspired by the achievements of queer statisticians, mathematicians, and computer scientists. My research hasn’t explicitly connected statistics and queerness yet, but I’m interested in working on projects involving hard-to-reach populations, such as queer people, in the future.\n\n\n\n\n\n\nClaire Morton\n\n\n\n\n\nIt’s important to be able to take initiative to learn skills, talk to people, and solve problems as they come up – but it’s also critical to not be afraid of asking for help when you need it.\n\n\n\nHow did you get into data science?\nIn high school, I worked in a cell biology lab. As part of that work, I learned to model cellular processes and analyze data from my experiments. I realized that those elements were my favorite parts of the science I was doing, so I decided to study math, computer science, and/or statistics in college. I had always been interested in environmental issues, and so I got involved in quantitative research about environmental justice. I realized that this type of research allowed me to connect the skills I have to my passions, so I’ve kept working in these areas ever since.\nWhat, or who, first inspired you to pursue this career path?\nMy mom! She’s also a statistician, and she has encouraged and mentored me throughout my academic journey. I’m inspired by her success as a woman in statistics.\nWhat hurdles or challenges have you faced in your studies?\nMy classes can be tough, which makes it hard to stay motivated sometimes. I also struggled to maintain a healthy work-life balance at the start of college. Finally, it has been tough to learn some of the ins and outs of the research process and publications – how best to engage with research mentors, what it looks like to write and submit a paper, and some of the nuances of working in academia. I think my next big challenge is deciding what to do after college, though I’ve been trying to reframe the question as an opportunity rather than a hurdle. I’m excited to continue doing research at the intersection of statistics and public policy in the future.\nWhat was your first job in data science, and how does it compare to your current role?\nMy first job in data science was as a researcher, working at a non-profit called Physicians, Scientists, and Engineers for Healthy Energy (PSE). As part of this job, I worked with community-based organizations to code quantitative optimization models to locate climate resilience hubs in California that took community priorities into account. I’m currently a student, which involves less research but gives me the chance to focus on learning new skills for my next job. I hope to be able to work in a role like my job at PSE, combining statistics/data science, community engagement, and policy impact, in the future.\nWhat was the most important thing you learned in your first year on the job?\nThe importance of being adaptable and self-directed. Research projects shift and change as you uncover new information, and it’s useful to be able to shift with them. It’s important to be able to take initiative to learn skills, talk to people, and solve problems as they come up – but it’s also critical to not be afraid of asking for help when you need it.\nWhat have been your career highlights so far?\nOne was publishing research about the demographics of people living near oil wells in California, which informed policymakers about racial and socioeconomic differences in exposure to oil wells and is part of a long-standing effort from activists and researchers to protect the health of Californians. I’ve also gotten to work directly with organizers on several mapping projects, which was deeply fulfilling. Finally, I loved getting to present my research at the Joint Statistical Meetings last year, and I look forward to presenting my undergraduate thesis this year.\nWhat three things are at the top of your reading/study list?\nSome statistical areas I’m hoping to learn more about are spatial statistics and survey methods. Some books I’m excited to read are The Color of Law, Data Feminism, and Thicker Than Blood: How Racial Statistics Lie. \nWhat advice would you give for anyone wanting to study statistics and data science?\nFind mentors that inspire you, support your career goals, and challenge you to learn and grow as a statistician.\nWhat new ideas or developments in the field are you most excited about or intrigued by?\nI’m really interested in combining quantitative research with community-based research, so I think that cross-disciplinary developments are exciting. I’m intrigued by AI tools, and I’m interested to see how these tools change what the day-to-day of being a statistician looks like and the skills that are most sought after in a statistician.\nAnd what do you think will be the main challenges facing the profession over the next few years?\nThe main challenges will be related to statistical literacy, both for the people consuming and doing statistics. While statistical methods and data becoming more accessible is a positive development, it has meant that more analyses are done incorrectly and that more misleading results are publicized (and absorbed) as truth. It’s getting much easier to twist numbers to support whatever we want them to say, and I think this will continue to challenge both statisticians and non-statisticians in the future.\n\n\n\n\n\n\nAbout the ASA Pride Scholarship\n\n\n\nThe ASA Pride Scholarship was established to raise awareness for and support the success of LGBTQ+ statisticians and data scientists and allies. The scholarship will celebrate their diverse backgrounds and showcase the invaluable skills and perspectives these individuals bring to the ASA, statistics, and data science.\nApply or nominate someone for the ASA Pride Scholarship.\n\n\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo of Claire Morton is not covered by this licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘I was inspired by the power that numerical data have to tell stories and promote policy change.’” Real World Data Science, June 28, 2023. URL"
  },
  {
    "objectID": "careers/career-profiles/posts/2023/06/20/chanuki-seresinhe.html",
    "href": "careers/career-profiles/posts/2023/06/20/chanuki-seresinhe.html",
    "title": "‘Once I started to see what was possible with data science, there was no going back’",
    "section": "",
    "text": "Hi, Chanuki. Thank you for sharing your career story with Real World Data Science. Please tell us a little about yourself and your role in data science.\nI am Chanuki Seresinhe, head of data science at Zoopla and Hometrack. My commercial career in data science began in 2018 at Channel 4. Since then, I have worked at a few different companies – from startups to scale-ups – before ending up here at Zoopla in 2022.\nI am also the founder of beautifulplaces.ai, which is a continuation of my University of Warwick and Alan Turing Institute PhD work where I provided the first large-scale evidence that beautiful places are connected to our wellbeing.\nWhat does your job involve?\nMy role at Zoopla involves managing data scientists both for Zoopla and Hometrack. At Zoopla, we use data science to create an engaging experience for users who want to buy, sell and rent properties. At Hometrack, the data scientists mainly work on an automated valuation model that provides property valuations to most of the major mortgage lenders in the UK.\nAs a leader in data science, my role primarily involves helping stakeholders across the business to best leverage data science to reach our business goals, as well as ensuring my data science team has all the support and mentoring they need to design, develop and maintain high performing data science algorithms.\nWhat does “data science” mean to you?\nThat is a really good question! One thing I have noticed is that people who aren’t familiar with the field often confuse data science and data analytics. There are indeed many similarities – both require quite a bit of knowledge to be able to leverage the correct insights from both structured and unstructured data (data hidden within images, for example). However, data science is essential when you need to make inferences. For instance, you are not only analysing data to see what certain consumers may prefer, but you are also predicting what similar consumers might prefer. Thus, having a strong grounding in statistics is really important for anyone working in this field.\n\n\n\n\n\n\nChanuki Seresinhe\n\n\n\n\n\nIn data science, getting the model right is not enough, and working with people across the business to make sure the model can be integrated into the business processes is essential.\n\n\n\nWhat do you think is your most important skill as a data scientist?\nAside from a good grounding in the technical aspects of data science (which is possible for really anyone to pick up from the many good courses that are available), the most important skill is how you can leverage data science to create products that actually create value for the business. I find this to be the most challenging journey that junior data scientists find themselves having to navigate. They are really excited about the technology, and get carried away with wanting to perfect their algorithms. But when you are building commercial products, what is really important is to constantly engage with stakeholders to make sure you are building something that actually has a tangible business benefit. Early release of a model for user testing is also essential, as models only really get better once you have real user input.\nHow did you get into data science?\nIt was somewhat by accident. I previously had a long career in digital and decided to take a career break to return to university and study economics. When I was working on my Master’s degree in behavioral economic science at the University of Warwick, I saw an ad for a PhD to “use online data to understand human behaviour” and I thought this was perfect, as it combined my prior knowledge with a new area I was increasingly becoming drawn to. I quickly taught myself how to program in Python and convinced my supervisors to take me on, and from there on, I came to love data science!\nWhat, or who, first inspired you to become a data scientist?\nIt was more about realising what you could do with data science. In my PhD, I was quantifying the connection between beautiful places and our wellbeing. While this has long been an intuitive connection, we were not able to test this on a large scale due to lack of data. Being able to use data science to start predicting the beauty of outdoor images was fascinating as it opened up a whole new method for potential research combining beauty with various wellbeing ratings. Once I started to see what was possible with data science, there was no going back.\nWhat were the hurdles or challenges that you needed to overcome on your route into the profession?\nFor me personally it was the challenge of moving sideways into a leadership role after my PhD and not having to start all the way from the bottom. I would have loved to continue in academia and expand my research even further, but starting from the bottom earning a tiny salary after I had taken quite a large career break to do my PhD wasn’t an option for me. So I decided to go back into the commercial world and look for a senior role from the get go and luckily Channel 4 agreed to give me my first commercial stab at data science.\nWhat are the challenges that you face now, as a working data scientist?\nTrying to keep up with everything that is constantly changing in the world of data science. I love the rapid change but it can also be quite time consuming to make sure you are on top of it and giving the right advice to people.\nWhat was your first job in data science, and how does it compare to your current role?\nMy first job was working as a senior data scientist at Channel 4. As a senior data scientist, even though you have additional goals to help run the team and be a mentor to more junior data scientists, you still get a great deal of time to do coding and develop your own projects. When you move more into a management role, the time you have to develop data science models diminishes. People also expect you to give in-depth guidance when you haven’t actually had much time to deep dive into a project. So, I am often trying hard to make sure I am on top of what is going on even when I have limited time, and really focus on building a strong team that can support each other and collaborate often to create better data science products. Learning to delegate is key!\nWhat was the most important thing you learned in your first year on the job?\nHow hard it is to actually get organisational buy-in to use data science at scale. It is really easy to get approval to build a proof of concept (POC). However, if you do not use the time when developing your POC to also make sure to get the right stakeholder on board, your project is dead before it even starts. So, in data science, getting the model right is not enough, and working with people across the business to make sure the model can be integrated into the business processes is essential.\nWhat have been your career highlights so far?\nIt has been great being able to give talks about my research, and data science in general, all around the world. I have actually come to love public speaking, and I hope that as I continue to be recognised for my expertise, I can encourage and aid potential data scientists with their careers – especially minority women, as I think that diversity in the field is very important. This is a role that is fit for people from all kinds of backgrounds and I hope I am exemplifying this.\nHave there been any mistakes or regrets along the way?\nIn smaller companies, it can easily happen that the founders don’t fully understand data science and often use data science as a buzzword to get investors on board. Whenever you take on a new job, and data science is just getting established, make sure the founders or leaders are actually fully onboard with integrating data science into the product and understand what this means. See if they know how tricky data science can be when first integrating into a product and are actually willing to overcome the challenges with you to eventually reap the huge benefits data science can bring.\nHow do you think your role will evolve over the rest of your career?\nI see my role evolving into being more strategic and less about the data science day-to-day modelling. It is more about being able to advise companies on how to make use of data science as a strategy and helping them figure out where in the product or process to inject it to get the most out of it for the business as a whole.\nIf you were starting out in data science now, what would you put at the top of your reading or study list?\nPractise how you would apply using data science for a real life problem. Seek a placement, as this will pay dividends in being able to speed up your learning.\nIf you don’t understand the statistics involved in data science, make sure to upskill in that area before starting your first role. A lot of junior data scientists focus on learning how to code or get carried away with the modelling without first learning the importance of preparing the data in the correct way so that your predictions can work well in a real life setting.\nWhat personal or professional advice would you give for anyone wanting to be a data scientist now?\nTry to find ways to stand out from the average data scientist. When we open up applications for data science jobs, I get hundreds of applications for each one. I am looking for people who can not only do data science but who also have other stand-out qualities that they can bring to the business. This can be something along the lines of effective stakeholder engagement to deep expertise in a certain domain or technology.\nWhat new ideas or developments in the field of data science are you personally most excited about or intrigued by?\nI am really interested to see where generative AI will take us – particularly about how it can help us improve the speed of our own performance. It feels like generative AI can be a technology that can help everyone – even the everyday person – as it can help speed up so many processes, from coding to writing to ideating. While the technology is still in its early days, it is progressing rapidly and I am very curious to see where this will lead in the next year!\nWhat do you think will be the main challenges facing data science as a field in the next few years?\nGenerative AI’s latest breakthroughs have made AI capabilities accessible to the broader public, but it has also stoked fears around the use of AI. The headline-grabbing narratives around AI and existential threat is distracting from other conversations that are really important. There are some very real issues we do need to solve – from biases in AI to the impact generative AI can have on wages and workforce – but this needs to be approached in a constructive and thoughtful way.\nWe need to find a way to engage with the public in a more meaningful way – rather than scaremongering – to have public debates on issues that actually matter.\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo of Chanuki Seresinhe is not covered by this licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘Once I started to see what was possible with data science, there was no going back’” Real World Data Science, June 20, 2023. URL"
  },
  {
    "objectID": "careers/career-profiles/posts/2023/05/24/jasmine-holdsworth.html#transcript",
    "href": "careers/career-profiles/posts/2023/05/24/jasmine-holdsworth.html#transcript",
    "title": "‘For me, data science is about bridging the gap between business requirements and the data that businesses have’",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove some repetitions.\n\n\n\n\nWhat does your job at Expedia involve?\nI would probably call myself more on the analyst side. So, while my day-to-day job doesn’t necessarily involve AI, ML and productionalising models, it’s more taking business goals or requirements and taking the data and essentially bridging the gap between the two. I am on the incrementality analytics team. So, what that means is I measure the short-term returns from our marketing efforts that we have. And I do that via geotesting. So, I’m essentially working in the geotesting part of the company if you like. And before that I worked in the customer data section. So, essentially looking at customer data and working with that.\n\n\nHow long have you been working in data science?\nMore in an analyst role, but probably about seven years now, I began in Stack Overflow just as a data analyst, and then worked at DAZN – which is like a Netflix for sports – as a data analyst, and then joined here as a senior analyst, and then moved into data science in the last couple of years. I would, I would credit Stack Overflow as the place where my career kind of was birthed, if you like. I started there as an account manager, so with hardly any technical background at all required, and then moved into a role that was essentially looking after, or reporting the metrics of advertising campaigns for companies that would advertise on Stack Overflow. So that required a little technical knowledge, not much – a few pivot tables and things like that. But then the longer I stayed there, the further my career developed, and they had, at the time – probably still do – some fantastically smart people that work there, as you can imagine. I was sponsored to do a General Assembly data analytics course, which was focused around Excel, dashboarding, and SQL and essentially fell in love with data analysis. It was the most technical subject matter that I had experienced to that point, and I found a real natural affinity to it, particularly SQL. And then [I] moved into more of a data analyst role within Stack Overflow, so – as you can probably imagine – an absolute sea of proprietary data that needed analysing, and started learning R, or rather being taught R within Stack Overflow, and loved it. I think I was there for three-and-a-half years, and then moved into a data analyst role at DAZN. At this point, I did a data science General Assembly bootcamp course, and fell in love with that. And then I decided that I really loved General Assembly as a concept; I actually started a second job teaching there, so the courses that I had previously taken I was now teaching, first as a teaching assistant, and then as a lead instructor, which was one of the most, yeah, one of the most amazing experiences I’ve had actually, I learned a lot from that. And then I got a job as a senior analyst within Expedia Group, which is where I am now, and then moved into a data science role, which is what I’m doing currently. So, I actually left school at 16, and had to go into a full-time employment. And the General Assembly education that I took a part in was my first of that type. So, when I realised that data science was absolutely something that I really wanted to dedicate the rest of my life to, I decided to take on a part-time data science bachelor’s degree, which I am now about a year away from finishing. Because I’m doing it part time it takes a bit longer. But yeah, so I will have my data science bachelor’s completed, hopefully, by 2024.\n\n\nWho or what inspired you to work in data science?\nThere were two big inspirations into getting into data sciences. So, they were actually the data scientists that I worked with at Stack Overflow. They were the first two data scientists, I believe, that Stack Overflow had ever hired. I worked very closely with them as an analyst and one of them was, had previously worked – I don’t know if it was officially an astrophysicist – but had studied black holes, and I remember thinking that was just amazing. And the other was, was very famous within the field. And they spent a lot of time giving me one-on-one training on R and SQL and basic analysis, and I was so inspired by these two individuals that I, it was also a career path that I didn’t really know existed.\n\n\n\n\n\n\nJasmine Holdsworth\n\n\n\n\n\nWhat was impressed upon me in that first year [in data science] was the importance of statistics and interpreting statistics in a way that’s honest.\n\n\n\n\n\nWhat does data science mean to you?\nFor me, it is bridging the gap between the business requirements and the data that businesses have. So, you’ll have business goals, requirements that kind of come down the line and there’s a lot of data that’s being collected, and, essentially, you have to try and be the bridge between the two. So, not just doing very complicated analyses, with very sophisticated models – at least not in my role – it’s about being able to create analysis that’s interpretable, that you can present to non-technical stakeholders that they’re going to understand to a degree. So, I do know that in different roles in different companies, it will be slightly different. But yeah, for me, it’s about making data, yeah, interpretable, to the non-technical stakeholders to enable them to do their job better.\n\n\nWhat is your most important skill as a data scientist?\nI like to think that there is one responsibility around how statistics are interpreted. So, just making sure that when you’re giving someone a statistic, that they understand what it can be used for, what it can’t be used for, and that it doesn’t kind of get halfway around the company before, you know, without any danger of it being misinterpreted. And I do think that the other is just being a translator. So, as well as teaching with General Assembly, I teach people within my company, things like SQL, R, and some basic data analysis. And I feel like it’s taking what is quite a technical, complicated subject and almost translating it into, if you like, English, so that people can kind of get some sense of what something may mean, without necessarily having to have the degree to back it up.\n\n\nWhat hurdles did you face in becoming a data scientist?\nTowards the beginning of my career to say – 5, 6 years ago – it was quite hard to get interviews. It was never hard to get interviews with technical people within companies, because you can– a technical person can see whether or not you know what you’re talking about. But recruiters don’t, and if someone is a recruiter for a technical role, their proxy for whether or not you can do the role is what’s your level of education, which is completely understandable and that’s what education is for. But it did mean that sometimes I applied for roles that were well below my, my level, and if I did so through a recruiter, then I wouldn’t hear anything back. But if I spoke to a technical person within that company, then it would be fine.\n\n\nHow did you overcome those hurdles?\nActually, I guess the story of how I joined Expedia is quite relevant in that way. So, I presented some, just some fun analysis that I did at an R-Ladies meetup, and I was already talking to a recruiter within Expedia Group and I said to them, oh, well, I happen to be visiting your offices to present at this meetup, so maybe I can meet you there. And they actually sent the manager of the team that they wanted me to join. So, this manager attended the meetup, watched me present, and then they ended up hiring me, which is really great. But I do really think that that was a result of being able to see me on stage, talking about stuff that I had done, showing code that I had written, and it kind of bypassed a few steps. So yeah, I would definitely say meetups and connections are very helpful to overcome that.\n\n\nThe most important lesson from your first year in data science?\nI think that what was impressed upon me in that first year – and what really drove me to do the bootcamp courses and then, ultimately, the degree – above everything, actually, was the importance of statistics and interpreting statistics in a way that’s honest. So, I feel like– I feel like with coding, that comes quite naturally to me, and writing SQL queries, R, that was all kind of fine. That didn’t require a lot. But I really, I had an amazing manager who taught me a lot about, essentially, if you’re going to go and speak to this company about the campaign that they’ve run on our website, then you need to impress that X doesn’t necessarily mean Y, it just gives evidence to, or alludes to, and essentially just making sure that how you’re communicating things is as accurate as possible.\n\n\nAny mistakes or regrets in your career so far?\nWhen I look back on my career, I think the things that have really stayed with me that I’ve really learned from, mistakes wise, are around small little mistakes around how you interpret data. Maybe it was a, like, years ago, summing the wrong cell in Excel but not checking two or three or four times before that goes out. I’m now quite– I over-check everything. I think that the most important part of our job, as well as being the translation, is being the correct translation. You need to be reliable. People need to know that if you put out analysis that they can trust you. So, I would say I regret every small, tiny little data error that I ever made, which I can’t even recall right now, but I know have kind of cumulated enough that it has made me a very fastidious checker, I suppose.\n\n\nHow do you see your role in data science evolving?\nI definitely see myself being an individual contributor in a consumer-facing company, just because that is basically what I’ve been doing up to this point. I don’t really have any desire to get into people management. I very much love being stuck in a room with my laptop, solving problems. Above all else, it still makes me happy. However, I do also love knowledge sharing – I love teaching, whether it’s with General Assembly, or whether it’s within the company that I work now. And I would like to kind of balance those two goals moving forward. So, keeping my role within my company as like an individual contributor and actually being like the front face for the, for the analysis that’s happening rather than kind of managing it. But also making sure that I carve out time to upskill others, because data science as a field, I mean, as you all know, is growing so much and people are coming in from different backgrounds. And I’m lucky enough to be able to kind of speak to a few people like that and do some very casual mentorship. And it makes me happy to see, so I hope that as my career develops, I will see more people maybe with backgrounds a little bit more like mine, come through and bring some diversity to the sector.\n\n\nNew developments or ideas you are most excited about?\nIt would be remiss of me to not mention like ChatGPT or generative AI, etc. But honestly, I am more interested in – or vaguely interested, I should say – in wearable technology. So, I’ve read a few very, very interesting papers and articles that talk about the development of wearable technology, not just your kind of watches, but potentially clothing, etc., that can be used for people with specific health problems to really help pinpoint, like, inflection points in time when something might happen. For example, a heart attack is about to happen, or is imminent, or is happening. So I actually feel like at the moment, this is perhaps going slightly under the radar, compared with more, you know, sexy developments like chatbots and things. But I’m very interested to see in the next one to two decades how ubiquitous wearables will be, and how closely entwined that will become with healthcare. So that’s something that I’m keeping half an eye on.\n\n\nAny words of advice for budding data scientists?\nYou will never stop learning at all because, frankly, the field is moving very, very quickly. So, even if you were to kind of consider yourself an absolute expert today, tomorrow that may not be the case. You will constantly be learning. And I have found that learning the same thing several times through different mediums and having things explained to you different ways is so valuable. Because you may think that you understand something from, say, your bootcamp, but then when you read about it as part of your degree – this is obviously personal to me – you read about it in a different way. And you think, Oh, I’ve never thought of it like that. And then you watch a YouTube video and someone visualises it and you think, okay, I understand this all a little bit deeper now. So, constantly revising what you do know and learning anything that’s new as it comes up, I think everyone at every stage of career can kind of, can do that.\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘For me, data science is about bridging the gap between business requirements and the data that businesses have.’” Real World Data Science, May 24, 2023. URL"
  },
  {
    "objectID": "viewpoints/posts/2023/06/05/no-AI-probably-wont-kill-us.html",
    "href": "viewpoints/posts/2023/06/05/no-AI-probably-wont-kill-us.html",
    "title": "No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye",
    "section": "",
    "text": "Doomsaying is an old occupation. Artificial intelligence (AI) is a complex subject. It’s easy to fear what you don’t understand. These three truths go some way towards explaining the oversimplification and dramatisation plaguing discussions about AI.\nLast week, outlets around the world were plastered with news of yet another open letter claiming AI poses an existential threat to humankind. This letter, published through the nonprofit Center for AI Safety, has been signed by industry figureheads including Geoffrey Hinton and the chief executives of Google DeepMind, Open AI and Anthropic.\nHowever, I’d argue a healthy dose of scepticism is warranted when considering the AI doomsayer narrative. Upon close inspection, we see there are commercial incentives to manufacture fear in the AI space.\nAnd as a researcher of artificial general intelligence (AGI), it seems to me the framing of AI as an existential threat has more in common with 17th-century philosophy than computer science."
  },
  {
    "objectID": "viewpoints/posts/2023/06/05/no-AI-probably-wont-kill-us.html#was-chatgpt-a-breakthrough",
    "href": "viewpoints/posts/2023/06/05/no-AI-probably-wont-kill-us.html#was-chatgpt-a-breakthrough",
    "title": "No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye",
    "section": "Was ChatGPT a ‘breakthrough’?",
    "text": "Was ChatGPT a ‘breakthrough’?\nWhen ChatGPT was released late last year, people were delighted, entertained and horrified.\nBut ChatGPT isn’t a research breakthrough as much as it is a product. The technology it is based on is several years old. An early version of its underlying model, GPT-3, was released in 2020 with many of the same capabilities. It just wasn’t easily accessible online for everyone to play with.\nBack in 2020 and 2021, I and many others wrote papers discussing the capabilities and shortcomings of GPT-3 and similar models – and the world carried on as always. Forward to today, and ChatGPT has had an incredible impact on society. What changed?\nIn March, Microsoft researchers published a paper claiming GPT-4 showed “sparks of artificial general intelligence”. AGI is the subject of a variety of competing definitions, but for the sake of simplicity can be understood as AI with human-level intelligence.\nSome immediately interpreted the Microsoft research as saying GPT-4 is an AGI. By the definitions of AGI I’m familiar with, this is certainly not true. Nonetheless, it added to the hype and furore, and it was hard not to get caught up in the panic. Scientists are no more immune to group think than anyone else.\nThe same day that paper was submitted, The Future of Life Institute published an open letter calling for a six-month pause on training AI models more powerful than GPT-4, to allow everyone to take stock and plan ahead. Some of the AI luminaries who signed it expressed concern that AGI poses an existential threat to humans, and that ChatGPT is too close to AGI for comfort.\nSoon after, prominent AI safety researcher Eliezer Yudkowsky – who has been commenting on the dangers of superintelligent AI since well before 2020 – took things a step further. He claimed we were on a path to building a “superhumanly smart AI”, in which case “the obvious thing that would happen” is “literally everyone on Earth will die”. He even suggested countries need to be willing to risk nuclear war to enforce compliance with AI regulation across borders."
  },
  {
    "objectID": "viewpoints/posts/2023/06/05/no-AI-probably-wont-kill-us.html#i-dont-consider-ai-an-imminent-existential-threat",
    "href": "viewpoints/posts/2023/06/05/no-AI-probably-wont-kill-us.html#i-dont-consider-ai-an-imminent-existential-threat",
    "title": "No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye",
    "section": "I don’t consider AI an imminent existential threat",
    "text": "I don’t consider AI an imminent existential threat\nOne aspect of AI safety research is to address potential dangers AGI might present. It’s a difficult topic to study because there is little agreement on what intelligence is and how it functions, let alone what a superintelligence might entail. As such, researchers must rely as much on speculation and philosophical argument as on evidence and mathematical proof.\nThere are two reasons I’m not concerned by ChatGPT and its byproducts.\nFirst, it isn’t even close to the sort of artificial superintelligence that might conceivably pose a threat to humankind. The models underpinning it are slow learners that require immense volumes of data to construct anything akin to the versatile concepts humans can concoct from only a few examples. In this sense, it is not “intelligent”.\nSecond, many of the more catastrophic AGI scenarios depend on premises I find implausible. For instance, there seems to be a prevailing (but unspoken) assumption that sufficient intelligence amounts to limitless real-world power. If this was true, more scientists would be billionaires.\nMoreover, cognition as we understand it in humans takes place as part of a physical environment (which includes our bodies), and this environment imposes limitations. The concept of AI as a “software mind” unconstrained by hardware has more in common with 17th-century dualism (the idea that the mind and body are separable) than with contemporary theories of the mind existing as part of the physical world."
  },
  {
    "objectID": "viewpoints/posts/2023/06/05/no-AI-probably-wont-kill-us.html#why-the-sudden-concern",
    "href": "viewpoints/posts/2023/06/05/no-AI-probably-wont-kill-us.html#why-the-sudden-concern",
    "title": "No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye",
    "section": "Why the sudden concern?",
    "text": "Why the sudden concern?\nStill, doomsaying is old hat, and the events of the last few years probably haven’t helped – but there may be more to this story than meets the eye.\nAmong the prominent figures calling for AI regulation, many work for or have ties to incumbent AI companies. This technology is useful, and there is money and power at stake – so fearmongering presents an opportunity.\nAlmost everything involved in building ChatGPT has been published in research anyone can access. OpenAI’s competitors can (and have) replicated the process, and it won’t be long before free and open-source alternatives flood the market.\nThis point was made clearly in a memo purportedly leaked from Google entitled “We have no moat, and neither does OpenAI”. A moat is jargon for a way to secure your business against competitors.\nYann LeCun, who leads AI research at Meta, says these models should be open since they will become public infrastructure. He and many others are unconvinced by the AGI doom narrative.\n\n\n\nA NYT article on the debate around whether LLM base models should be closed or open.Meta argues for openness, starting with the release of LLaMA (for non-commercial use), while OpenAI and Google want to keep things closed and proprietary.They argue that openness can be…\n\n— Yann LeCun (@ylecun) May 18, 2023\n\n\n\nNotably, Meta wasn’t invited when US President Joe Biden recently met with the leadership of Google DeepMind and OpenAI. That’s despite the fact that Meta is almost certainly a leader in AI research; it produced PyTorch, the machine-learning framework OpenAI used to make GPT-3.\nAt the White House meetings, OpenAI chief executive Sam Altman suggested the US government should issue licences to those who are trusted to responsibly train AI models. Licences, as Stability AI chief executive Emad Mostaque puts it, “are a kinda moat”.\nCompanies such as Google, OpenAI and Microsoft have everything to lose by allowing small, independent competitors to flourish. Bringing in licensing and regulation would help cement their position as market leaders and hamstring competition before it can emerge.\nWhile regulation is appropriate in some circumstances, regulations that are rushed through will favour incumbents and suffocate small, free and open-source competition.\n\n\n\nThink Google or Microsoft are encouraging legislation for your safety? But of course! These are honorable companies.You might think they'd like less competition too though. Maybe a monopoly? Maybe legal red tape preventing free and open source alternatives? Perhaps other… https://t.co/Z7vSpMyuHg\n\n— Michael Timothy Bennett (@MiTiBennett) May 5, 2023\n\n\n\n\n\n\n\nDiscover more Viewpoints\n\n\n\n\n\nAbout the author\n\nMichael Timothy Bennett is a PhD student in the School of Computing, Australian National University.\n\n\n\n\n\nCopyright and licence\n\nThis article is republished from The Conversation under a Creative Commons license. Read the original article.\n\n\nThumbnail image by Alan Warburton / © BBC / Better Images of AI / Social Media / Licenced by CC-BY 4.0."
  },
  {
    "objectID": "viewpoints/interviews/posts/2023/03/06/data-science-act.html",
    "href": "viewpoints/interviews/posts/2023/03/06/data-science-act.html",
    "title": "US legislators get their data science act together",
    "section": "",
    "text": "On February 14, 2023, a bipartisan group of US legislators introduced the Data Science and Literacy Act with the goal of boosting access to data science education and building “America’s 21st century STEM workforce”. We sat down with guests Zarek Drozda, Anna Bargagliotti, Christine Franklin and Steve Pierson to discuss the news and to hear why data science education is “the new apple pie”.\nCheck out our full conversation below or on YouTube."
  },
  {
    "objectID": "viewpoints/interviews/posts/2023/03/06/data-science-act.html#timestamps",
    "href": "viewpoints/interviews/posts/2023/03/06/data-science-act.html#timestamps",
    "title": "US legislators get their data science act together",
    "section": "Timestamps",
    "text": "Timestamps\n\nThe state of data science education in the United States (3:31)\nWhat will be the main impacts of the Data Science and Literacy Act? (9:14)\nProfessional development support for teachers and teacher-educators (13:06)\nHow much money is needed to deliver data science education? (18:53)\nDeveloping a data science curriculum (27:09)\nBuilding confidence in data, statistics, and technology (31:54)\nLearning from, and making connections with, international colleagues (37:03)"
  },
  {
    "objectID": "viewpoints/interviews/posts/2023/03/06/data-science-act.html#quotes",
    "href": "viewpoints/interviews/posts/2023/03/06/data-science-act.html#quotes",
    "title": "US legislators get their data science act together",
    "section": "Quotes",
    "text": "Quotes\n“Most of our teachers in US schools, math teachers, have not had any formal training in statistics. Or if they have, it’s been maybe one course. They’re very uncomfortable with trying to implement these standards [for data science and statistics education]. And it’s just going to require a tremendous amount of professional development. Sounds easy in theory to deliver professional development, but very difficult in practice.” (10:18)\n“We know that in aggregate, between federal, state, private and local funding, we’re going to have to create the necessary resources to make sure that our K-12 public education system can prepare students for a world that’s changing super fast, and the K-12 system moves super slow in how it adapts to new content. And so really it’s both about what can we do to upskill data science, data literacy skills [and] it’s also about how do we help the system adapt faster as new technology comes out and leverage the importance of data in that.” (19:36)\n“I think we’ve spoken to some 50 or 60 offices, both on the Senate side and the House side. And this [has been] received really well. We don’t get any pushback on that there is a need for greater data literacy. Here stateside, I’ve been saying it’s kind of like advocating for apple pie. People get it and they resonate.” (21:20)\n“As we introduce this bill, I think we should be messaging [that] there’s a economic competition aspect to this; that it’ll be really important for the US to make investments in this area to, frankly, catch up to where I think other international peers are.” (40:34)\n“Data tell our stories, and they reflect what’s happening in our world today – much like art around us in some ways. And a way to think about data science education is to think about what we need our data understanding to be at each point in time in our educational career, or in our lives. And it’s not static, it’s an evolving thing. So you have to move with the data that are being collected.” (42:54)"
  },
  {
    "objectID": "viewpoints/interviews/posts/2023/03/06/data-science-act.html#transcript",
    "href": "viewpoints/interviews/posts/2023/03/06/data-science-act.html#transcript",
    "title": "US legislators get their data science act together",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove repetitions.\n\n\n\nBrian Tarran\nHello, and welcome to the Real World Data Science news q&a. I’m Brian Tarran. And I’m joined today by a panel of guests to discuss some promising developments in the United States around data science education. On the show today we have Zarek Drozda, director of the Data Science 4 Everyone coalition, Anna Bargagliotti, Graduate Programme Director and Professor of Mathematics at the Seaver College of Science and Engineering, Loyola Marymount University, Christine Franklin, the American Statistical Association’s K-12 statistical ambassador, and Steve Pearson, who is the ASA’s Director of Science Policy. Welcome all. Thank you for joining us. Steve, I’d like to come to you first, because it was one of your ASA science policy tweets that that first drew my attention to this story. And that specifically was the tweet about the introduction of a new Data Science and Literacy Act in the US House of Representatives. Can you give our viewers an overview of the act and its significance to the data science education landscape, please.\nSteve Pierson\nHappy to Brian and I also want to credit my colleague, Ed Wu, an ASA science policy fellow who worked a lot on this and championed it. So I see kind of two overarching points here for the bill. One is just to help out those budding efforts around the United States to bring more data science education to students, right, the demand in the jobs is out there. Students should know about these jobs, we want to connect them to the 21st century jobs. So this is a Department of Education programme that helps out those schools, communities that need the help that want the help. We’re not trying to require anything of schools, which already have enough curriculum requirements. So this is a voluntary programme, that I mean, I think it’s developing curriculum, it’s providing professional development. But I think there’s another aspect of this, Brian, which is just kind of the attention that this can bring to these jobs, to the schools to the members of Congress that, you know, data intensive jobs are a great job opportunity in the 21st century, right? You can look at so many places to know that, right? The Bureau of Labor Statistics has both statistician and data scientist as the top 10 jobs in terms of projected growth for the next decade. There’s Glassdoor, there’s many others, so we know that. We want to make sure that today’s students know about those opportunities and are connected to them. But we also want to just kind of diversify the STEM workforce. So there’s components of that in the bill as well. And so we want, we think that, you know, a bill introduced into the US Congress will help bring attention to that, including the members of Congress and others.\nBrian Tarran\nExcellent. I’ll take a step back briefly to look at the data science education landscape as it is today and Zarek, you helped facilitate a National Academies workshop last September, and one of the aims was indeed to survey that landscape for data science education for the K-12 grades – and for international audiences, correct me if I’m wrong, that’s students aged about five up to 17. Is that correct?\nZarek Drozda\nRight, for five to 18 range.\nBrian Tarran\nSo yeah, so how would you summarise that kind of state of data science education in the US right now?\nZarek Drozda\nYeah, well, first focus on the workshop that was facilitated by the National Academies. And that was not the first but definitely the largest to date, in terms of a national convening of the United States for data science and data literacy education. We had 100 plus researchers, programme developers, and higher ed faculty in the room. There were 500 online, it was a big, you know, kind of early stage milestone for billing data science education in the United States. And we had a number of topics ranging from you know, what does this look like a practice? What is the professional development for, for K-12 teachers look like? What are examples both standalone data science courses, and also integration into different existing K-12 subject areas. And it was really a showcase of you know, a lot of the curriculum work that’s been developed over the past 10, 15 years for building your full length data science high school courses, or for building lesson plans or for building, you know, education, classroom specific software for data analysis that students can really get their their head around. And so I think it was a milestone to then this legislation then built off right that Steve was mentioning. I’m glad that you know news of both the National Academies workshop and then the legislation and kind of the growing momentum here in the US generally has made it across the pond. I think, partially our social media game was strong enough, which is exciting to hear. But I think it’s just a testament to you know, the energy for this space is really growing, right? Because it’s, it’s career connected. I think it’s so relevant to so many other emerging technologies, whether it’s artificial intelligence or ChatGPT or cyber. And I think, students, what came through really clearly in the workshop is that students really find this content relevant because of the technology applications. And it seems so, you know, of the moment.\nBrian Tarran\nYep. Well, certainly, you talked about jumping across the pond. So two weeks ago now, but it might have been three, I attended a meeting, a discussion around what they refer to as the digital skills gap in the UK. And I left that meeting feeling very much like what they were talking about, defining as digital skills were data science skills. And so when I saw that there was this Data Science and Literacy Act, I thought, well, you know, here’s, here’s something that hopefully, other other places like the UK can learn from. So Anna, you participated didn’t you in the workshop? So do you mind sort of giving us an overview of the data science landscape as it is, as it is now?\nAnna Bargagliotti\nAbsolutely, um, so I think, in the United States, at the moment, I think we’re at a spot where the different states are sort of moving. And in the United States, each state has their own department of education. So they, we are not, they’re not federal standards, they are state standards. And each in a lot of states, those standards are being revised and to include data science standards, and those discussions are moving pretty quickly, with some states already approving, other states implementing this coming fall, for example. And other states that are still in the process of sort of starting that. But it’s, but it’s exciting. The other thing that’s really been happening is trying to understand what the curriculum should what curriculum should look like in K-12, in particular. And the GAISE report, like you mentioned before, has a, lays out a, you know, a nice sort of example of what that should look like at the elementary, middle school and high school levels, and can be used sort of as an anchor point for states looking at what they should be doing. I would say at the university level, at the what we call the 12-16 level in the US, we are pretty good. There are many data science programmes and majors and minors across the United States. And they are quite strong, there’s more and more that pop up. And overall, those majors and minors look pretty similar from university to university, and those students are coming out with very, you know, a good skill set, and they are all finding jobs. As Steve mentioned, the job growth is there. And students are feeling quite prepared when they go to into the job growth. So I think where the university level is, could have been well articulated and well defined at the moment, the K-12 level is still sort of in flux of trying to figure out what should be there. And part of that has to do also with teacher preparation, it’s trying to understand what teachers should have and know in order to teach whatever these data science ideas are that are important for the K-12 level. The GAISE report makes some very, I think, concrete recommendations about what that should be, particularly being anchored in the statistical investigative process or problem solving process and understanding how you can use questioning in that and that being really a skill set that we are trying to promote in K-12 education, that then they use also at the university level if they continue on that way. And then just understanding that there’s different large conceptions of data now. Data are not just numbers, data could be sounds, could be text and whatnot. And these ideas are sort of these data science ideas that we are trying to promote in K-12, as well as at the university level. So I think overall, the landscape is quite good. I mean, we’re moving in these great directions. And I think slowly, we’re getting to some consensus about what that looks like. And we’re seeing that in the states moving forward with different standards.\nBrian Tarran\nExcellent. And so you mentioned the GAISE report there and that’s the Guidelines for Assessment and Instruction in Statistics Education, and of course that’s something you co-authored with, with Chris Franklin, Chris. Yeah, of course. Yeah. I’d like to bring you into the conversation. Now, Chris. You know, you and I have spoken a number of times over the years about GAISE about statistics, education and statistical literacy and, and the challenges of delivering high quality education in data and statistics at all levels of the curriculum. I wanted to get your impression of what you think the big contributions are that this act will hopefully make towards improvement of the data science education landscape.\nChristine Franklin\nWell, I was very excited to see this act. And I think one of the big impacts that I see is how it can help our state departments of education actually try to implement standards that we’re seeing put in place right now, in terms of statistics and data science. Teachers, right now, most of our teachers in the US schools, math teachers have not had any formal training in statistics. Or if they have, it’s been maybe one course, they’re very uncomfortable with trying to implement these standards. And it’s just going to require a tremendous amount of professional development. Sounds easy in theory to deliver professional development, but very difficult in practice. And a big part of the difficulty of delivering professional development is funding to, to pay for this. Unfortunately, what I’ve seen is state departments of education will often implement these very nice standards in their curriculum, but then they’ve run out of money, or they don’t have sources of funding to where they can then think about the professional development of the teachers. So it’s not only the professional development of the teachers, but also the curriculum that’s going to support the standards that are put in place. Now, fortunately, ASA, for example, has just a wealth of open source resources that teachers can use. But then how did the teachers know where to get it? How do they know how to implement it in the classroom. So state departments are charged with trying to develop a framework of materials for their teachers. This takes money, this takes expertise. So not only that, but I think this bill can help with funding to allow state departments to do that. But professional development typically has been like week long workshops, day workshops, maybe they go online and do workshops, but really, for professional development to be successful. Teachers need day to day support, which requires funding once again, to provide the resource within schools and school systems to provide more of the day to day support that these teachers need. And I think lastly, one thing that we don’t think a lot about, but I’m hoping this bill will help, is the assessment that goes along with the curriculum that we’re implementing for statistics and data science. Once again, that takes funding that takes manpower support. And I’m really hoping that this bill can be a source that that our state DoE’s can turn to make their standards more successful with implementation.\nBrian Tarran\nYeah, Chris, when you were speaking there about providing professional development support for the teachers, it reminded me of when I was a primary school governor here in the UK a few months back, a few years back, sorry. And we would always talk about math education, trying to improve math education, and that I think, the teacher confidence to deliver the math curriculum is always the issue that we run up against. So having support, having resources, having people that can go in and help, that changes the dynamic, I think, for teachers and certainly equips them to, to deliver on, you know, on that vision of, of data science education and statistical literacy for all, which is something that we spoke about before, right?\nChristine Franklin\nThat’s exactly right.\nBrian Tarran\nThat’s, that’s your vision for where we get to as a society?\nChristine Franklin\nWell, I think one thing, one other thing we need to remember, it’s not just the teachers that need our support. It’s also the teacher educators that are preparing our school level teachers. And we, we need to keep that at the top of the list of priorities because most of our teacher educators recognise that our school level teachers need this support, but they are in a similar situation to where they don’t know exactly what they need to do. And so we’ve got kind of two big spots here that we need to work on for professional development, and it’s gonna take a lot of time and effort.\nBrian Tarran\nYeah, well, we’ll come back to that later, if you don’t mind. Zarek, did you want to come in? It looked like you was about to chime in.\nZarek Drozda\nYeah, I was just, I wanted to agree with Chris and second it and expand it because I think it’s professional development for teacher educators and every layer above that, right. It’s every layer above the teachers: teacher educators, it’s the district staff who are implementing and creating these programmes, it’s the state staff we’re creating the standards, it’s state policy makers, it’s federal policymakers – like, you could think about professional development for all those stakeholders. And we know we need to build, you know, better education, right, for every one of those groups that are above the individual educator in a classroom, knowing that the national infrastructure is just supporting the teacher in the individual classroom to do this best at the end of the day. So yeah, we think about that in terms of a, there’s a whole system that needs to move here.\nBrian Tarran\nI think that’s an important point to note, I think, because I go back to that digital skills workshop I was at and one of the questions that came up from from the chair was, you know, what one thing can I take back to the Secretary of State to say we need to add this to the curriculum, but it’s not one thing is it? It’s, it’s it’s a whole system, as you say, Zarek. I did want to ask you a bit about the organisation that you’re a part of, you have a coalition called Data Science 4 Everyone. To what extent was you involved in the kind of shaping of this, of this bill? And, you know, you obviously, you’ve obviously welcomed it, and you’re excited about the potential, and how much work do you see as being left to do to, to kind of get it over the line and get it into application?\nZarek Drozda\nSure, well, just a very quick background on DS4E. We’re a national initiative and a coalition as you said, based at the University of Chicago here in the States. We’ve been putting together a community of education researchers and K-12, system leaders to advance policy and advance awareness and advance the case making right for why data science and data literacy and statistical acumen is so important in this day and age. And we’re really working across the K-12 system, which is very decentralised in the US, to try to forward those goals. Yeah, as I think about next steps and what’s needed, again, just re-double what Chris said, we need more funding. We did some, to give you an example. So when computer science was being built out as a new school subject in the States, they spent somewhere in the range of three to $4 billion over 15 years to build an entirely new school subject. We’re not necessarily doing that here, right? We’re not necessarily building out a whole new school subject, I think we’re really, at least our group has been much more focused on how can we integrate and upskill teachers in K-12 math, or K-12 science or K-12 social studies, right, and integrate these concepts into the existing K-12 ecosystem, working with the different subject societies. But, you know, this bill is a first really great milestone, but we know now we’re going to have to call on state legislators to pass appropriations at the state level to fund teacher support locally, we’re going to have to, you know, call on schools and districts, right, to help give teachers time to be able to implement these classroom experiences. And we’re going to need more research. Right. So this bill calls for grant programmes to state and local partners to create. But I think we also need more funding for NSF and IES, the two kind of education research bodies in the US at the national level, to fund things like student assessments, or to fund accommodations for students with disabilities to be able access to technology for data science software. There’s a lot more R&D work that also has to happen to bring down the adoption costs over time for doing this type of work and making sure that every student regardless of their background, can benefit from the skill areas, and you know, upskilling in this space. And I think the last thing I would say is that we know we also need to work on teacher confidence, right? I think it’s both teacher confidence with statistics, right, and probabilistic thinking. And it’s also the the confidence of the technology, which is brand new, right? Most classrooms in the US have not been using spreadsheets, even though most workplaces do, let alone, you know, R, Python, SQL, any of the more kind of modern computational tools that are used in modern day statistics. And so we have a lot of work on that front to do as well.\nBrian Tarran\nOkay. And so, if I’ve understood it correctly, there’s about $50 million over five years that is being asked for in this bill. But that’s, from what you’re saying, am I correct in thinking that’s just a kind of a small part of what’s needed to completely deliver on your goals?\nZarek Drozda\nIt is a first step. An important one, but a first step.\nBrian Tarran\nSo, longer term, is it– Do you head towards the billions territory, like in the computer science space? Or is it a little less demanding of finances and resources and that, do you think? I know it’s hard to say, to pin these things down, but–\nZarek Drozda\nAt least from my angle, I’d love the group to jump in here, it’s probably a little bit less demanding. But we know that in aggregate between federal, state, private and local funding, we’re going to have to create the necessary resources to make sure that our K-12 public education system can prepare students for a world that’s changing super fast, and the K-12 system moves super slow, right, in how it adapts to new content. And so really it’s both about, you know, what can we do to upskill data science, data literacy skills. It’s also about how do we help the system just adapt faster as new technology comes out and, you know, leverage the importance of data in that.\nSteve Pierson\nAnd Brian, I can jump in a little bit on the price tag, which, when we were shopping around this bill, we didn’t actually include like a cost per year for this, because we know that that can be a very sensitive topic, and we were, we really wanted to have bipartisan introduction. So, and we were fortunate to get it right. But it was the offices who have agreed to kind of consider us that came up with the $10 million. And I’m, you know, I know for a fact that, you know, a few of the offices, at least one of the offices did want significantly more, but this was how we were gonna get bipartisan introduction.\nBrian Tarran\nYeah. Okay. And on that point, that bipartisan nature of the bill’s introduction, does that give you as a group hope that it will eventually make it through Congress and become a law, and that there’ll be these resources made available?\nSteve Pierson\nYeah, absolutely. And I’ll also just say, Brian, that I think we’ve spoken to some 50 or 60 offices, both on the Senate side and the House side. And this is, it’s received really well, we don’t get any pushback on that, you know, yes, there is a need for greater data literacy. Here stateside, I’ve been saying it’s kind of like advocating for apple pie. Right. This is, people get it and they resonate. And to that point, we only brought this to Representative Stevens, I think it was maybe late October. But they really wanted to move on this, they wanted to wait for the new Congress for bandwidth issues. But, significantly, we’re told that the representative wanted this to be her first bill introduced of the new Congress, and she had many to choose from. So I think that’s really positive. The other thing is, I’ve heard from email, we haven’t had a chance to debrief with the staff yet, because they’re swamped with all kinds of things, but they’re getting a lot of positive feedback from people about this bill. So it really seems to be kind of tapping a nerve. A recognition.\nZarek Drozda\nI was just going to add to Steve that when we went– So in advance of that legislation being introduced, we had 15 of the largest math and science and technology education associations supporting the legislation, which was a huge win. It showed, I think, that this data science, data literacy, education is really a collaborative multi-subject effort in the States, which does not happen often, it’s usually very siloed. And I think the other thing I’d say is, in the first 24 hours since the bill was introduced, we had 150-160 additional education leaders and organisations sign on to the letter of support that we were helping circulate between Data Science 4 Everyone and the American Statistical Association. And just to re-emphasise that we saw a lot of energy around this, and bipartisan, right. We’re building support on both sides of the aisle, because it’s, you know, this is apple pie, it’s so evident that every student’s going to need this for the next decade.\nBrian Tarran I like this. So data was once the new oil, but now data science education is the new apple pie. I think this is this is great. Anna, you know, assuming that the Data Science and Literacy Act goes through, funds are made available, this work starts in earnest, what sort of timescales are we looking at, do you think, before we start to see the real world impact, you know, in terms of teacher training, student outcomes, and then eventually, obviously, building this workforce, that is so needed, that is equipped with data literacy and data science skills?\nAnna Bargagliotti\nYeah, I think I kind of want to say two things to this point. I think post K-12, the college level, we are seeing those outcomes, and they are great. And I think we are really, that is, it feels very good. It feels like we’ve targeted the right things. It feels that students report back that they’re excelling in their jobs and doing great things. So I think that part is sort of, is taken care of. I think at the K-12 level, what’s harder is we’re less nimble. Like Zarek mentioned, K-12 is just a beast to move. It’s very difficult. And we’re in a situation where the target of data science changes every day, truly every day, for two real reasons. One is because the conceptualization of data changes every day. We can imagine today we think of data as text, but in probably a month, there’s some other type of data that we haven’t thought of that will emerge. And so now all of a sudden, you’ve got, you’re trying to teach pillars or concepts in K-12 that are actually a moving target. And then the other big thing that changes pretty much every day is our capabilities for wrangling, visualising access to data, all that stuff is changing. And so at the university level, you’re much more nimble because you’re in these courseworks, and your students are very advanced, and you can kind of move within those, those spaces, and you can change a course at a time. At the K-12 level we’re much more prescribed and harder to move. So I think in terms of timeframe, I think if we focus on the sort of large concepts and the baseline skill sets that we want to graduate students from K-12, I think we can move much quicker than getting into sort of the nitty gritty of a student needs to be able to programme per se, or something like that, like I think more that statistical investigative process, and those questioning, those types of ideas are really the crux of what K-12 looks like that then allows the university level to be more nimble. So for me, the timeframe is I think we have to, we have to think about it differently as we’re never going to arrive. It’s not going to be like, Oh, in five years, this is completed. It’s a, are we reacting in the right way? Or are we sort of ahead of the game. And I think we can get ahead of the game if we go to the concepts and that idea instead of thinking of topics that we’re teaching. And I hope that with this, I have great hope for this bill. I think it’s like everybody said that this is just such a great first drop in the bucket with the apple pie that I think hopefully in the next few years, the states are going to have been moved and then everybody will be doing some type of data science at the state level. And then it’s like Chris mentioned, before moving the professional development, which is a big challenge.\nBrian Tarran\nWhen you were speaking, just there, Anna, and you were you were talking about the difference between you what you’re achieving at the college level versus the younger level, it made me think that, you know, this isn’t just about providing a next generation of data science workers, right, it’s about equipping everyone with the skills to be able to exist in a data science world. And this goes to the point that I think Chris and I have spoke about before, right, about being able to be, when you’re confronted with data, being able to ask the right questions about that data. And so I think obviously, that’s, to me, that’s where I think maybe the Guidelines for Assessment and Instruction in Statistics Education seems a promising first step in the development of a data science, data literacy, statistics curricula, do you see that that needing to be – obviously these things are always needing to be revised, right – but do you see that, Chris, as a kind of foundation on which the states can start to build and to move towards this vision that’s laid down in the act?\nChristine Franklin\nWell, most definitely. And we’ve been real fortunate in the US here that our states right now that are trying to implement more statistics and data science standards, are going to the GAISE II document to use as a guiding document. And ASA has been very fortunate also to where these states, many of them have reached out to us to actually help advise them as they go through the process. I wanted to come in on a follow up with something Anna said in terms of the timeline. As she was speaking, I thought about how when we were sending out our document for review with the GAISE II, the updates, and we sent it out to probably about 20 to 25 different reviewers, we received more feedback than we knew what to do with. But many of the reviewers, very well respected people in the field, came back to us and said, we were being way too ambitious with the GAISE II document that, in fact, so many states here in the US we’re still back trying to implement the recommendations we made in the 2005 GAISE document. And our response was, we can’t be writing for today. What our goal is, is to try to write a document to where 5, 10 years from now, this is where we hope that our K through 12 system will be. And I’ve noticed this in working with other national documents, that I think the tendency oftentimes is to try to write a document for where we are now. And I think that we always need to, in terms of a timeline, think about 10 years down the road when we wrote the document. And it’s hard to be visionary. But as Anna was saying, things are changing so quickly. I mean, the GAISE II document, I think the writers are all saying, oh, we should have included this, we should have included that. Because since its publication in 2020, we’re already seeing needed changes. So I think we’re always going to be catching up to some degree. But as Anna said, I think the goal of these documents needs to be conceptual understanding, it needs to be that role of questioning. I like to say, I just want people to be healthy sceptics to where when they see statistical information, they see data, they immediately start asking questions. They may not know the answers, or know what the answers aresupposed to be, but at least they’re questioning.\nAnna Bargagliotti\nAnd I’ll just add one really quick thing, Brian, if that’s okay. Chris and I are about to embark on another ASA initiative, which is the revision of the SET report, which is the Statistics Education for Teachers report that the last publication was, oh my gosh, Chris, I can’t remember even the date–\nChristine Franklin\n2015.\nAnna Bargagliotti\n2015. And the the idea of the new report that we will be starting to write this year, along with some wonderful colleagues, is going to be to talk about the teacher preparation aspect of that in light of everything that’s happened with statistics and data science in particular, what should that look like? And how, how could that be? So I just wanted to add that.\nBrian Tarran\nExcellent. I also wanted to ask, again, maybe this is a question for Zarek. If we’re talking about data science for everyone, what about people like me, who have already already finished their education? I mean, I know you can always learn; every day is an opportunity to learn new things. But, you know, from your coalition’s perspectives, Zarek, what do you want to see happen so that, you know, that we’re not leaving behind big chunks of the population, you know, the older chunks of the population who haven’t had the benefit of going through this education system as it is now, let alone what it might be in five years time, right?\nZarek Drozda\nYeah, Brian, it’s a great question. And I, I just came off a year of serving as a fellow in the US Department of Education. And I had a lot of conversations with the programme officer who is responsible for research, education research and adult education, about that subject. Right. And I think one of the themes that I learned from that work and from that, from those conversations was this idea around fear of technology, as it accelerates. It’s really hard for people to deal with, you know, ChatGPT, DALL-E, AI, neural networks, you know, the list goes like on and on, and it changes every month, as as we’ve discussed here. And I think a big goal from our side is both to build confidence when people are dealing with a deluge of data and a increasing amount of information, right, which we talked about, and it’s also the confidence in the technology tools that are constantly changing. So I think as we think about, you know, a student K-12, we want to get them on a tool, so they can be confident and switch to the next one when it comes out. Because we know that technology integration is just is really critical. And the same thing is true for the adult learners, right, or for the for the folks that are older, there is a wealth of online digital learning, online courses, asynchronous experiences, to learn any coding languages, any programming language, any just software – doesn’t even have to be computation or coding intensive, right, it can just be spreadsheets or, or Tableau or some of the not-too-syntax heavy tools, and there’s so much digital and elearning opportunities for that. If we can build formal exposure into the classroom pathway, and build student confidence to then jump on to those later on. That is really critical. Because if you don’t get an exposure, it’s so hard to take the first step to jump in to the digital training or to you know, go to your employer and say, I want this, you know, this professional development programme, because it’s hard to know where to start. And we also worked on a data literacy training programme for Department of Ed employees while I was there and helping design that experience. You know, for professional or mid career, folks, I think the most important thing was you build confidence and create bitesize first steps to try to like tone down the fear, so people can approach the new world with confidence rather than just responding to it.\nBrian Tarran\nSo I just want to wrap up now, last question for you all, really, maybe if we start with Steve. From a policy perspective, Steve, you know, are there other initiatives on the horizon and things in the pipeline that, you know, people should be watching for in terms of trying to improve data science education across the board in the US, and maybe specifically for the ASA are their areas you’re going to be focusing your efforts and support on.\nSteve Pierson\nWe certainly do want to expand this effort. And we’re trying– In a way, this bill is serving as a way for us to gather that information, because people then know that we’re doing this and they might well hopefully suggest items to, to us. We’ve gotten some I know that, you know, Zarek has a file, I have my own file of what we might want to, how we might want to extend it just with this bill. But yes, we certainly do want to do that. And so for listeners out there that have ideas, please please send them to us. I won’t offer specifics at this point. And I’d love to hear what my colleagues have to say, as well.\nBrian Tarran\nDoes anybody want to jump in on that? Zarek, what’s on your list?\nZarek Drozda\nVery short term is just building a Senate version of the legislation, I think, right? We’re going to be working with with the ASA on that, to find champions in that chamber. But then I think I would go back to my call for you know, state legislators need to be thinking about this as well, right, because we know that every state is going to look a little different. We’re in a context right now where locally driven education solutions are going to be really important. And so we’re going to have to build different slightly different flavours of this all around the country. And we’re going to need a lot of work from, I think, state and local champions to fund and help support and build these experiences that are so critical for students across the country.\nBrian Tarran\nAnd are there, you know, on the point you mentioned earlier, Zarek, about this news,crossing the pond and reaching me over here in the UK, do you look elsewhere, you know, outside the US and the UK for other good examples of where education systems are starting to integrate data science into the teaching at, you know, the earlier parts of the school curriculum and the school levels.\nAnna Bargagliotti\nI think Chris can probably jump in even more than me on this one. But definitely in New Zealand. Our colleagues in New Zealand are fantastic. And they’ve been doing K-12 data science and statistics very well for many, many years. And Chris has some very close collaborators. So I’ll hand it over to her to on that. But I thought I’d mention it.\nChristine Franklin\nYes, I I think that that’s when you know, when Steve says how can we reach out, I think even beyond policy, a collaboration with international colleagues that have advanced their work in K through 12. I mean, I had the good fortune of having a Fulbright to New Zealand back in 2015. And just the inspiration, the wealth of knowledge that I obtained from there, to bring back to the US with our work here was phenomenal. Plus, we built up collaborations that we are continuing today. We have collaborations with people in the UK, for example, in other countries. I think the other thing I wanted to say besides reaching out internationally is that we as statisticians in the US need to be doing more to help K through 12. And I think about my, you know, my colleagues at the university in the statistics department which I was part of, my colleagues, some of them actually worked with me to reach out to the math educators so that they could try to help with what needed to be done with the preparation of teachers. So I think statisticians need to become more involved, both practising statisticians and academic statisticians, with helping educators at K through 12. And that includes trying to become involved with state departments of education as well, because that’s really where things filter down to the local level in our school districts. So I would like to see somehow a structure put in place to make that happen more. And I’m hoping things such as this bill will bring that awareness to practising statisticians, that this is really important, and you need to become more aware of what’s happening at K through 12, and become involved.\nZarek Drozda\nTo just add to what Chris is saying, I think it is so important for investment in the K-12. space. And, Brian, I’ll give you a sneak preview of a report that we were collating on international examples of data science and statistics education in K-12, and really serious investments that we’ve seen, I think, and Chris already mentioned, some really great ones that have been long running champions at this internationally. Our recent scan: Israel, their ministry of education is doing a tonne of work for data science, data literacy education. I’ve had so many conversations with them over Zoom. China added a standard semester for big data statistics coding and modelling. And it’s now also in their college entrance exam. There’s examples in Germany, New Zealand, South Korea, Scotland, we’ve we’re continuing to build the list. Frankly, in the UK, right, with core maths, you’re seeing more integration of data and computational thinking into the curricula pathways there. I think in many ways the US is behind, and as we introduce this bill, I think we should be messaging, there’s a economic competition aspect to this, right; that it’ll be really important for the US to make investments in this area to, frankly, catch up to where I think other international peers are.\nBrian Tarran\nSteve, you wanted to come in on that?\nSteve Pierson\nI mean, you mentioned like someone coming in mid career, right. And I think that that is really important. But we’ve also kind of been talking about the other ways you can access this, right, in any part of your career, but you can also access data jobs, rural and urban. So we’ve been kind of selling that dimension. But also, you know, just access in terms of diversifying the STEM workforce, we think that’s really important. But it’s also about degree level, right? We did a search for one member of Congress that had a major, I think it was a pharmaceutical in their district. And if you put in data, right, and thousands of jobs pop up for that company. And it’s not just the PhDs, right, it’s more entry level, the people who are what Zarek mentioned in terms of just accreditation, that you can enter a lot of points. But I also want to just make a– point out one part of the bill, which really singles out two-year colleges, which can help the mid career people, the early career people or others, and they face a lot of the same challenges as K through 12. Right, making sure that the instructors are upskilled, that they have a curriculum, but they also need the time to coordinate with their other disciplines that are involved here. They need time to go to that local workforce, what do you need in terms of data science? And for those students that want to go on to a four-year degree, they need to make sure that there’s a smooth pathway for those students. So there are provisions in the bill also for for two year colleges. And those would be my closing comments, Brian.\nBrian Tarran\nOkay. Anybody else for some closing words? Or should we wrap up on reminding everyone that data science education is the new apple pie?\nAnna Bargagliotti\nI can close this with something more philosophical, maybe. To me, I think a nice way to think about it or sort of a romantic way to think about it is it’s just data tell our stories, and they reflect what’s happening in our world today, much like art around us in some ways. And a way to think about just data science education is just to think about what we need, what we need our data understanding to be at each point in time in our educational career, or in our lives. And it’s not static, it’s an evolving thing. So you have to move sort of with the data that are being collected.\nBrian Tarran\nVery good point. Thank you very much, Anna, Steve, Zarek, and Chris, for joining us to talk about the Data Science and Literacy Act. I’m sure there’ll be much more to to follow and update on as this act or bill winds its way through through Congress. So we’ll look forward to hearing more about that in due course. So thank you for joining us today. Thank you to those of you who are watching for joining us. Stay tuned at realworlddatascience.net for more news Q&As.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “US legislators get their data science act together.” Real World Data Science, March 6, 2023. URL"
  },
  {
    "objectID": "viewpoints/interviews/posts/2023/01/25/erica-thompson.html",
    "href": "viewpoints/interviews/posts/2023/01/25/erica-thompson.html",
    "title": "How to ‘Escape from Model Land’: an interview with Erica Thompson",
    "section": "",
    "text": "Erica Thompson’s new book, Escape from Model Land, offers a fascinating and important perspective on mathematical models as being not just models of the real world, or real processes or systems, but also “subjective versions of reality” that encode all sorts of assumptions and value judgements.\nIn this interview with Brian Tarran, editor of Real World Data Science, Thompson talks about the “social element of modelling” and how it manifests, how to counter the subjectivity of individual models with a diversity of models, and whether human-made models are held to the same standards of transparency that are expected of AI-“created” models.\nErica Thompson is a senior policy fellow in the ethics of modelling and simulation at the London School of Economics Data Science Institute."
  },
  {
    "objectID": "viewpoints/interviews/posts/2023/01/25/erica-thompson.html#timestamps",
    "href": "viewpoints/interviews/posts/2023/01/25/erica-thompson.html#timestamps",
    "title": "How to ‘Escape from Model Land’: an interview with Erica Thompson",
    "section": "Timestamps",
    "text": "Timestamps\n\nWhat led Erica to write the book, and why now? (2:27)\nCritiquing climate models (7:30)\nExploring the “social element” of modelling (11:36)\nCountering subjectivity with a diversity of perspectives (20:11)\nAI models, human-made models, and questions of transparency (25:54)\nWhy write a popular science book about these issues? (30:11)\nWill the UK Prime Minister’s “maths to 18” proposal help or hinder our Escape from Model Land? (34:01)"
  },
  {
    "objectID": "viewpoints/interviews/posts/2023/01/25/erica-thompson.html#quotes",
    "href": "viewpoints/interviews/posts/2023/01/25/erica-thompson.html#quotes",
    "title": "How to ‘Escape from Model Land’: an interview with Erica Thompson",
    "section": "Quotes",
    "text": "Quotes\n“Putting things in a mathematical language does tend to make people think that it is truth from on high. And so my book, in some way, goes towards saying actually, these models, obviously we hope that they’re based on facts and they’re based on data that we gather, but they also do have this value judgement content as well” (1:51)\n“It is arbitrary how we choose to model a situation. There are infinitely many different ways that you could choose to simplify reality - this huge, messy, complex thing in front of us, with physical laws that we don’t fully understand and things going on that we can only measure by proxy.” (12:09)\n“The choice of assumption has a very direct result in the model output and in the information and advice that you’re giving to policymakers… [In climate models] maybe we have a cost of however many dollars per tonne of carbon dioxide for nuclear electricity or for renewables. But what kind of price would you put on behaviour change? How many dollars per tonne of CO2 avoided does it cost to change the behaviour of a population such that they use less energy? If you put it in at $2 per tonne of CO2, it would be heavily relied on [as a policy response]; if you put it in at $2,000 per tonne of CO2, it’ll never happen.” (16:42)\n“There needs to be more frank discussion of values and value judgments, and politics and social assumptions within models. And I think we are starting to see that with the pandemic models, particularly because it’s been so high profile. [But] it’s really hard to unpick your own value judgments. It’s easier for somebody with a different perspective to come in and say, ‘Oh, actually, you know, you’ve assumed that. Why did you assume that?’ When we are embedded in a particular culture of modelling, it’s particularly hard to imagine that anything could possibly be done differently.” (27:20)\n“I think some people maybe read the book and think, ‘Oh, this is just a sort of woke advertisement for diversity’. Well, it’s not; it’s a way of doing the maths better. The whole point is to do the maths better, make better forecasts, understand the future more effectively, and be able to make better decisions based on that information.” (33:41)"
  },
  {
    "objectID": "viewpoints/interviews/posts/2023/01/25/erica-thompson.html#transcript",
    "href": "viewpoints/interviews/posts/2023/01/25/erica-thompson.html#transcript",
    "title": "How to ‘Escape from Model Land’: an interview with Erica Thompson",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove repetitions.\n\n\n\nBrian Tarran\nHello, and welcome to the very first instalment of the Real World Data Science interview series. I’m Brian Tarran, the editor of Real World Data Science, and I’m very pleased to be joined today by Erica Thompson, a senior policy fellow in ethics of modelling and simulation at the London School of Economics Data Science Institute, and the author of a fantastic new book - which I have a copy of here - Escape from Model Land, which is subtitled, How mathematical models can lead us astray and what we can do about it. So hello, Erica, thank you for joining us. I hope 2023 got off to a positive start for you.\nErica Thompson\nYes, it has so far.\nBrian Tarran\nGood, good. Because the book came out, is it just before Christmas or just after?\nErica Thompson\nYeah, just before Christmas. So I’ve had all sorts of things flooding in saying, Oh, I liked your book, or I hated this bit or no, it’s exciting.\nBrian Tarran\nYeah, no, well, it’s, I have to say, I think, I thought it was a genuine– I finished reading it over, over Christmas. And I think it offers a genuinely fascinating and important perspective on mathematical models as being not just I guess, models of the real world, or, you know, real processes or systems, but subjective versions of reality, you know, encoding all sorts of assumptions and value judgments of the people who, who create the models. And I mean, I guess that shouldn’t really come as a surprise, right? But, but is it a point that is often lost in the discussion around models, particularly where decisions might be, like, informed or driven by model outputs?\nErica Thompson\nI think it is something that’s easy to miss. I mean, especially because we’re sort of, maybe as mathematicians were used to living in model land and doing things which, which we see as being logical consequences of previous things. And then more generally, the public look to science and mathematics and statistics as being objective arbiters, perhaps, of how things are and how things ought to be. And so, so yes, that that kind of putting things in a mathematical language does tend to make people think that it is truth from on high. And so my book, in some way goes towards saying actually, these models, they are, obviously we hope that they’re based on facts, and they’re based on data that we gather, but they also do have this value, judgement content, as well. And so we need to think about what that is and how we deal with it, and how we sort of express it and how we understand it.\nBrian Tarran\nRight, yeah.\nErica Thompson\nEspecially where we’re using those models to inform decision making or public policy, then it becomes particularly important.\nBrian Tarran\nYeah, yeah, no, and obviously, your book draws up quite a bit on the Covid-19 pandemic, and how models were used there. But I thought it was interesting, actually, that, you know, in reading the acknowledgments that you– that while the pandemic lent the topic, additional relevance, right, you actually started writing the book before that. So what led you to think now’s the time? What was the tipping point, if you like, of thinking, I want to write this book now?\nErica Thompson\nYeah, okay. Well, that’s an interesting question. I mean, because it builds on the last sort of 10 or 15 years of my work. So I started out doing a PhD in climate physics. And my background before that was maths and physics. And so I was doing a PhD on the physics of North Atlantic storms, looking at how they would change given climate change. And so, obviously, the first thing you do is a literature review. And I started looking at different models and what the what they were saying about what would happen to North Atlantic storms. And what I found there was that there were models saying that the storm tracks would go north, they’d go south, they get stronger, they get weaker, they’d, you know, anything you name it. And interesting, particularly, interestingly, was that they, they had relatively small uncertainty ranges. So they they didn’t agree within their own uncertainty ranges. And that made me think, well, we this isn’t telling me very much about North Atlantic storms. But it’s telling me a great deal about modelling and the way that we do modelling and perhaps we need to start thinking more about how these uncertainty ranges are calculated, what does it mean? What, how can we end up in a situation where we have this level of disagreement between models. And so since then, I’ve been looking at, you know, those kinds of concepts in different areas I’ve been looking at sort of insurance and finance and weather and climate and humanitarian forecasting as well. And, and so in all of those application areas, I found the same questions about uncertainty and how we make inferences from model output to be particularly interesting and how common problems may be solved in different ways as well. So it’s interesting to do the compare and contrast. And so, yeah, then I guess I, I’d been on all these sorts of bitty little projects and thought actually, I’d really like to bring this together into something more coherent, you know, to actually say, look, there’s a, there is a common theme here and we need to be putting it together and drawing conclusions. And we can, we can learn a lot from doing that. And we can share the best practice throughout the sector.\nBrian Tarran\nWhen you’re starting down this path of, I guess, looking into the, I guess, the ethics and process of modelling, did it, was there a lot of other work that you identify that you could kind of draw on a lot of other thinking around this area? Or was it kind of under studied, under researched sort of aspect of the literature?\nErica Thompson\nI think it’s under studied, I mean, of course, everybody who does some modelling, you know, you, you do your modelling, and then somebody says, Okay, we need to put some error bars on the outputs, and you go back and, and think about how we’re going to put the error bars on the outputs. And probably, I would say, most people doing that realise that it’s much more difficult than they have the time to do or the ability within the scope of whatever project they’re doing. But the aerobars, the uncertainties always ended up being tacked on at the end, you do it after you’ve done all the modelling, there’s less incentive to do it. And there’s less resource to do it than to make the model itself better. And I think that’s a very common story, that people realise that they ought to be doing more, but they just don’t have the time the resource, the ability to go and do that. So then yes, there are there are people, and there are particular areas that I think have taken more time to investigate this. So in physical science hydrology, I’d say in particular, has a very well developed history of thinking about the uncertainties in models, maybe because, you know, they are constantly being challenged by events happening, which were not within the models, you know, you’ve got your flood forecast model, and then something happens, and it goes way beyond what you were expecting. And you you have to go back and say, What does this mean for our modelling process. And other areas have much less well developed considerations of uncertainty. And so that’s where I think actually, we could we could really benefit by sharing good practice across these different application areas, because people have looked in different ways. And, you know, with with different levels of statistical interest, you know, some areas go into the stats, much more, some areas are very philosophical about sort of the conceptual foundations of how we should think about models and how we should think about the range of outputs that we get from models. And so what I’m trying to do is bring those together a bit. Yeah.\nBrian Tarran\nYeah, I think you certainly achieve that. It is really interesting, the different, the variety of examples that you present, and the ways you talk around these issues. I did want to focus in particular on climate models, though, because, you know, I was looking around your website, finding a bit more about about you, and I noticed on there that you talk about, you no longer fly to conferences, and that’s in order to kind of reduce your own ecological footprint. So I guess I wanted to ask, you know, when you set about writing a book, and it’s going to be a book that’s kind of critiquing models and the ways that they don’t often agree? Did you have like a nagging concern that, you know, the points you wanted to make about models in general, but climate models in particular, that that would kind of lend fodder to the kind of groups that might want to discredit climate models or downplay the risks? Or, indeed, the reality of climate change?\nErica Thompson\nI mean, yes, I did have that worry, I still have that worry. And I, but I hope that my book is clear throughout that, you know, that models are not irrelevant, you know, the answer is not to throw them away. If you come to it from this sort of sceptical position, saying, you know, we need to think more carefully about how we make inferences from models, you could go all the way down the rabbit hole and say, Oh, they’re all terrible, let’s just throw them away. But I think that would be completely unjustified. We have good evidence, sort of from from one end of the spectrum of relatively simple linear models, which are incredibly, wildly successful and form the foundation of modern life and modern technology. And, you know, with that, as a basis, we hope that we can, you know, work from there to find the limit of the knowledge that we can get from these more complex models, which are looking at making predictive statements in more extrapolatory domains where the underlying conditions are changing, and we therefore have less ability to rely on what I call the quantitative route of escape from model land, by challenging with relevant past data. You know, we’re looking at extrapolatory conditions like climate change, or social and economic systems, and therefore, we think that the data that we have, while they may be useful and indicative, are not, we can’t just calibrate with respect to past data and expect that to be enough to warrant performance in the future. And so, so I think that sort of one answer is that we shouldn’t be throwing away the models completely because they are demonstrable useful, and the question is to quantify the limits of what we can say, rather than just get rid of them. And then maybe the slightly more nuanced answer is that actually, if we have less confidence in the models, and our uncertainty ranges are wider, then because in many of these application areas and climate change, in particular, the damage function is convex, you know, we are expecting that as we go further from today’s climate, the consequence will be not just linearly worse, but sort of increasingly worse. And therefore, if you have, if you’re considering, you know, just to have a sketch of a kind of cost benefit analysis on some sort of expected utility from taking action to mitigate carbon emissions, for example, if you have more uncertainty, then your range is wider. And so the, the lack of quantification of the top tail becomes dominant in in the expected utility of the outcome. And therefore, you should be choosing to mitigate more, not less, because of that uncertainty. So, you know, the sceptics, I suppose the climate sceptics would say, oh, there’s a possibility that climate change might not be as bad as we expect, and therefore we shouldn’t bother doing anything. But I would say actually, that argument should be turned on its head, if we have greater uncertainty, that should be a bigger motivating factor to reduce carbon emissions rather than the opposite.\nBrian Tarran\nYeah, and I think you make that point quite clearly in the book. And the other point you make is that, I guess, building trust in models is about understanding their limitations. And the quote that I thought was really interesting was about acknowledging the social element of modelling. And I wonder, do you mind explaining what that social element is for people who are watching or listening? And how will that kind of manifests itself in models? Maybe you’ve got like a simple example that you might want to talk to? I don’t know.\nErica Thompson\nYeah, okay. So, I mean, the social element of models is, because it is arbitrary how we choose to model a situation, there are infinitely many different ways that you could choose to simplify reality, this, you know, huge, messy, complex thing in front of us with physical laws that we don’t fully understand and things going on that we can only measure, sort of by proxy, with, you know, with models themselves to make many of our measurements of the system. And so, so you might choose to simplify a system in one way to model it. And I might choose to simplify it in a different way. And you might choose one programming language, and I might choose another and they would implement functions in different ways. And so all of our choices change the way that the model will then look at the end of the day. Now, then you say, okay, but supposing I’m modelling you know, what will happen to a ball when I throw it up in the air? Surely, that’s not a, you know, that has no social content, does it? And I’d say basically, no, it doesn’t really have any social content. It has some social content insofar as you’re deciding that this is what we want to make a model of. But ultimately, you and I would probably come up with very similar models, regardless of our background or our perspective, or our interests, or even our education to a large extent. And so, so those relatively simple linear situations, which I refer to as interpolatory models don’t have very much social content. Now, the ones that I’m particularly interested in and that I talk about in the book are things that are extrapolatory, where we’re interested in situations where we are trying to predict into the future a system where we expect the underlying conditions to be non-stationary, to be changing. So climate change is one example. Social and economic systems would be another example. And when we’re modelling systems like that, we have to be much more careful because we could choose to model them in radically different ways. We could, if you want to model an economic system, you might choose to disaggregate with respect to the social class of different households. And I might choose to make a sort of bulk model of the whole system with a representative household. And you could imagine hundreds of different ways to do these sorts of things. So maybe you think about pandemic models and how you could simplify it into individuals or you could make an agent-based model with, you know, actual agents walking around and infecting each other. Or you could just write some differential equations for how the transfer happens. So you could do it in, again, in many different ways. And the choice of simplification is then much more important, and it will have much larger first order effects on the outputs, and then on the framing of the question, you know. So you decide to model it in a certain way, with a certain kind of mathematics, and that changes the way that you might think about intervening in the system. If you’re presenting your model to a policymaker with the intent of informing them about their policy options, you might, if you have a model which can represent the effects of say, closing schools, or universities on pandemic transmission, then then that becomes a policy option. If you have a model which can’t represent those kinds of interventions, then it’s not a policy option. And similarly, with climate change, one of the examples that I talk about in the book is integrated assessment models of energy and climate. And so these are models which consider the energy system out to say 2100, and they put a price on nuclear electricity and renewables and all the other things that go into the energy system and basically say, how can we achieve our carbon targets at the lowest cost? Now, if you put in, if you choose to put in a certain price for a certain technology or assumptions about how that technology will develop in future, then you get a particular answer. And you put emphasis on certain kinds of policy options. And so that has, the choice of assumption has a very direct result in the model output and in the information and advice that you’re giving to policymakers. And of course, you might choose to put in something like behaviour change. So maybe we have a cost of however many dollars per tonne of carbon dioxide, for nuclear electricity or for renewables. But how much, what kind of price would you put on behaviour change? How many dollars per tonne of CO2 avoided does it cost to change the behaviour of a population such that they use less energy? Well, that’s not really in the models. And if it was, it would, again, it would be first order because that would be you know, if you put it in at $2 per tonne of CO2, it would be heavily relied on, if you put it in at $2,000 per tonne of CO2, it’ll never happen. And where you choose to put that in between influences how it looks, and then that influences the pathway that’s projected, and it influences the advice that you give to policymakers.\nBrian Tarran\nYeah. You mentioned the example of school closures and stuff when you’re talking about Covid-19. I actually thought that, that one really helped me understand, I guess, and made it clear to me was that, there’s often been that argument about whether the lockdown was the right thing to do given the other impacts, but you actually say that if different types of people were doing the modelling, maybe if it was school aged people, and I guess encoding the impact that that would have had on them, and how much they value say not being able to go out and see their friends, the impacts potentially on mental health and things like that, it does change, I guess, the calculation of what the right intervention is or the right response is.\nErica Thompson\nYeah, exactly. And I think I think we haven’t anywhere near bottomed out all of these impacts of the pandemic, you know, both the health impacts, and also, mental health and economic impacts will be rippling on for a very long time to come. So we, you know, we can’t even now retrospectively look back and say what was the right decision? It’s really not clear, depends how you value the different outputs, the different outcomes of a decision. And yes, we didn’t have economic models of what the impact of lockdown would be. And if we, if those had been available and developed the same kind of mathematical complexity and credibility as the models of infection and transmission we had in those early stages of the pandemic, which had essentially morbidity, mortality, and the, you know, the impact on the NHS, you know, number of hospital beds occupied. Those were the bottom lines, and there was nothing else. And so that was given as an input to the policymakers. Now, that’s, of course not everything that the policymakers rely on, they have to, their role is to weigh up everything else as well. But if we’d had models which contained more information about all of those other impacts, I think it’s quite plausible that we would have seen different kinds of decision making. And then there’s also the communication aspect, that these models were used to communicate and justify and persuade the public of the importance of the actions that were taken. And, you know, I think it’d be hard to disagree that the actions that were taken immediately where necessary, we certainly did need some kind of lockdown straight away. But then the question of exactly what you do thereafter is much more difficult.\nBrian Tarran\nYeah, yeah. So in the book, you kind of make the point that it’s somewhat of a fool’s errand to try and make models objective, and they can’t ever really achieve this, you know, principle of scientific objectivity. But that we instead should look to counter subjectivity of individual models with a diversity of models that do encode these different perspectives, like we’ve just been talking about. I wonder, you know, how would you see this working in practice? Or how would you like to see this working in practice?\nErica Thompson\nI mean, that’s a difficult question. So it’s, it’s nice to think that, you know, we have these models, and they are unavoidably subjective. Essentially, my view is that the model encapsulates the expert opinion of a particular expert, and it comes laden with their own perspectives, and biases and preconceptions, as well as their expertise and their education and their experience of a subject, you know, which shouldn’t be set aside. So if we are trying to understand a situation, then we want to get as many perspectives as possible. And so in theory, incorporating the widest possible diversity of different backgrounds into modelling and making a multiplicity of different models and trying to see the problem from these different perspectives will help us to understand it better. Now, then the statistician will jump in and say, Aha, can we, you know, can we in some way average those models or use some sort of statistical inference to take those models and put them together and come up with an even better answer? And I would, I would sort of counter that by saying that there’s no reason to believe that our, that a set of models generated as essentially just one set of opinions will be an independent and identically distributed sample from some distribution, underneath which will be an estimator of, of the truth, if that even exists. And so many of the formal statistical methods that we would quite like to apply to an ensemble of models, a large group of models put together aren’t really conceptually valid at all. I mean, that doesn’t stop people doing it. And maybe you get some interesting information from it, but you certainly can’t rely on it as an estimator. So there’s a sort of statistical problem there. And then the other question is your reference class. So, to what extent do you believe that these models are all equally valid or equally plausible? So that then brings the social question back to the forefront. Because then you say, you know, if I believe that, you know, somebody from Imperial College, say, who is the head of an institute for epidemiology, and has many, many years of experience making this kind of model, you know, is an expert and is qualified to create a model and for that to be recognised as a valid expert opinion, who else has got the credibility to do that? How do we define that? You know, what do you call a plausible model? So then it’s a question of the sort of scientific gatekeeping. What kind of qualifications do you expect from somebody or from an institution? What kind of expertise counts as being relevant and valid expertise? Does it have to be mathematical expertise? Can it be lived experience? Can it be, does the model have to be a mathematical model? What kinds of mathematics are appropriate for the situation? If we disagree about assumptions, does that mean that we can’t consider the the two sets of models in the same sort of class of plausible models? Or are we going to start pruning it by saying, I believe your assumptions, and I don’t believe your assumptions? And if so, who gets to do that? Who gets to decide what is plausible and what is not plausible? And what is allowed to enter into this set? Because as soon as we start pruning it, then we make the statistical inference more difficult. You know, if you want to say, if you want to start applying your methods that assume that the models are on, you know, that the models are independent, then you can’t start pruning because then that introduces huge dependencies on your own expert judgement. So it just becomes extremely difficult. And this is where all of then the social questions about expertise and credibility and sort of scientific gatekeeping and how we assign that credibility and trust, trust in science, you know, who has trust in which kinds of models? This is something, this is a theme that we see coming out of climate science and, you know, hopefully less now than maybe 10 years ago. But in the pandemic, of course, we’ve seen it coming right up again with questions about lockdowns, and about vaccination strategies, and all of that sort of thing. Trust in science is really important. And maybe one of my themes is that trust in science actually is first order in the modelling process itself. It’s not something that is sort of added on afterwards, I’m going to go away and make my model and then the question is whether or not you trust it. Actually, trust and expertise and credibility are in the modelling process directly.\nBrian Tarran\nDo you mind if we segue to talking about artificial intelligence models, or models made by artificial intelligence? Because that’s, I think that touches on a lot of the same issues, right? And I wanted to think about, well, first of all, you say that, obviously, artificial intelligence models made by AI, they’re not objective, even though there’s like, they’re kind of building the models, if you like, those AIs have still been trained by people, been coded by individuals and those personal judgments and assumptions and all that get embedded into the artificial intelligence. But I think, I guess my question for you is, we’re starting, I think, to have a very frank and public debate about AI ethics, and to demand transparency and explainability of things like automated decision making systems. But do you think we’re kind of, are we falling short of holding ourselves as people to the same standards of transparency and being clear about the choices and decisions we make? And also documenting that subjectivity when we’re preparing these sorts of models and these sorts of decision making systems for policymakers to use?\nErica Thompson\nYeah, so I mean, I suppose there are two questions there. And one’s about what we do and one’s about the AI. So for the humans, maybe, yes, I think that there needs to be more frank discussion of values and value judgments and politics and social assumptions within models. And I think we are starting to see that with the pandemic models, particularly because it’s been so high profile. I mean, remembering that actually, it’s really hard to unpick your own value judgments. And it’s easier for somebody with a different perspective to come in and say, Oh, actually, you know, you’ve assumed that, why did you assume that? And, you know, when we are embedded in a particular culture of modelling, it’s particularly hard to imagine that anything could possibly be done differently. And so I think that’s, again, where diversity is really important, because introducing those perspectives will help to challenge dominant strains of thinking which can end up in sort of accidentally, and not deliberately at all, in a form of groupthink. So that’s the humans and then the, with the AI, yes, you know, they inherit their value judgments from their creators. And so, for example, on the statistical side, one might think about the kinds of loss functions that are used to calibrate machine learning programmes, you know, how does the machine decide what is better and what is worse? You know, it is learning to model a situation, but there will be some kind of loss function in there, which it is minimising in order to decide what is the best model. And so, being explicit about that loss function, I think, actually is really interesting, you know, the fact that the model has got written down an explicit loss function which it is minimising means that we can then analyse that and think about what are the value judgments inherent in that choice of loss function, which is something that we don’t have when humans are calibrating a model and they’re twiddling a knob here and a knob there and saying, Oh, does it look realistic? Am I getting the right kind of behaviours, you know, does it match up with the map I have from observations or whatever. And so, having that I think we can then say, what are the implications of writing a loss function in this way? And what are the values that are implied? I mean, even just modelling itself, you know, like choosing to solve a problem with recourse to mathematical modelling is a value judgement and implies a certain kind of solution, doesn’t it? So if we say that we even can come to a decision, that the models input will be relevant and interesting and help us, that is a value judgement.\nBrian Tarran\nYeah, and perhaps we could return to that point a bit later, because there was a line again that jumped out in the book about decision makers needing to maybe curb some over enthusiasm for mathematical solutions. You do talk about documenting value judgments as being kind of one of five principles that you set out for mathematical modellers to adopt to support responsible modelling. I think it’s fascinating to me that you’ve used the vehicle of a popular science textbook to speak to this community, and to sort of set out these principles rather than, say, a journal paper or conference presentation. So I wanted to ask, is there a particular strategy to that decision? I mean, I have my own theory on that, but maybe I’ll let you go first.\nErica Thompson\nI’d love to hear your theory. I mean, I guess partly because I suppose I’m very interdisciplinary. And I’m, I’m hopping between these different areas, as we were discussing before, so I have sort of a climate science community and statistics and data science and other application areas in humanitarian forecasting, sort of hydrology, geophysics, weather and climate and all the rest of it. And actually, I find it really hard to get these thoughts published in journal form, because I suppose partly because it feels too general for any specific journal. And perhaps it feels too simplistic that, that the reviews I get tend to be either Oh, we’ve heard this all before it’s not new, or this is too radical and this isn’t an appropriate journal for it, you know, that sort of thing. And actually, I kind of got to the point of thinking, Well, you know, do people even read these journals anyway? Actually, maybe really what I need to be doing is trying to provoke a wider discussion about models. And I do tend to get a, you know, a really good response, when I speak at conferences or talk to people about these things. People go yes, yes, you know, this is really important. Actually, this is something I’ve really struggled with, we don’t know how to do it, the uncertainty is always just an add on at the end that doesn’t have enough time allocated for it. But I don’t have the resource to do it, I’m not able to grapple with these questions, because they’re so fundamental and so wide ranging, and it would be really helpful to have more of a sort of walkthrough of how people tackle these questions in different fields. And so, so I’ve been trying to do that. And I felt that the book was a good way to sort of spark the conversation and maybe also get it to some different audiences. So I’ve had people contacting me since the book came out saying, oh, you know, I’m working in, like, asset valuation for disputes between states, really random things, quite different. And they say, actually, your book really struck a chord, and we have difficulties with this in this particular area. And so I’m really hoping that, you know, the book will help me then to find, to bring together people working on these sorts of issues with common themes from really different application areas and try to make some headway on how we can actually go about practically changing modelling practice to make it to make it work better, and assess uncertainty better. So it’s not just– I think some people maybe read the book and think, Oh, this is just a sort of woke advertisement for diversity. Well, it’s not; it’s a way of doing the maths better. The whole point is to do the maths better, make better forecasts, understand the future more effectively, and be able to make better decisions based on that information.\nBrian Tarran\nYep, well, that’s not too far away from my theory on why you did it. I thought it was that it’s a great way of getting– if you can get policymakers and the public to read this, right, you can get them to hold modellers to these principles, rather than having it just be something that you kind of talk about within the community and it doesn’t really go outside that, right? It’s a way of people, you know, the next pandemic or whatever it might be, the next time a model is the focus of a debate, the public are equipped to ask the right sort of questions about the process. Okay, I’ve got one more question to you because we’re running out of time. And it’s back to that over enthusiasm for mathematical solutions point. I thought was somewhat serendipitous to read about, read that quote, in the same week that the UK Prime Minister Rishi Sunak announced a plan for all pupils in England to study maths to the age of 18. So, I wanted to ask you what you make of that plan, first of all, but also, I think, more importantly, does a more mathematically minded populace, are they better equipped to understand the mathematical descriptions of the world and that they are incomplete descriptions? Or is there kind of maybe some other curriculum that we need to tack on to this maths to 18, in order that people are able to better differentiate between model land and the real world?\nErica Thompson\nYeah, so I mean, I’m not a fan of– I mean, I like the idea of people studying maths to 18. I think more maths is a good thing. I loved maths, I still love maths, I think if more people were more generally numerate then society would be better and life would be better. But if you haven’t enthused people about the value and the interest of maths by 16, forcing them to study it for another two years is absolutely not the answer. It will just put people off staying in school past 16. So I, you know, I think that we need better teaching of maths before 16, rather than forcing people to study maths post 16. And part of that is helping people to understand how mathematics is relevant to the real world that they live in and teaching them the kind of things that they will use in their adult life. You know, people, most people don’t use Pythagoras theorem, but most people do need to fill in a tax return, you know, these sorts of things. Understanding orders of magnitude, and the difference between millions and billions, would be incredibly helpful, wouldn’t it? So, yeah, I think there are more basic questions there that need to be answered before we go into the details of sort of complex maths. And then, so what was the next question?\nBrian Tarran\nIt was about whether whether you think, the more mathematically equipped we are, does that make us better able to understand the limitations of models? Or do we need something else to kind of train us or encourage us to think about these two separate realities, model land and the real world? And I say realities in inverted commas.\nErica Thompson\nYeah, I mean, I think the general public has a good understanding that the model land and the real world are not the same. There is actually a healthy scepticism of models out there. And I think that’s probably a good thing. I give a couple of funny anecdotes in the book about that. So I mean, one was a, I think, a YouGov poll about people going to the moon and saying, you know, would you go to the moon if you could be guaranteed a safe return, blah, blah, blah, and like, a large percentage of those said, Well, no, I just don’t think you could give me a safe return, they reject the model land. And then there was another example about intelligence analysts being asked to sort of calibrate a probability language scale. So they say, like, likely, however many percent and very likely however many percent and unlikely however many percent. And so the study was looking at different ways of doing that. And one way was to accompany the word likely, or unlikely, or whatever, with a written number of what the probability it referred to was. So it would say, like, likely, I can’t remember the number, but sat it was, like 50 to 70%, written down in the question, and then the question was, what is the probability of an event, which is deemed to be likely brackets 50 to 70%? And what people write down was not 50 to 70%. You know, as a mathematician, that’s completely ridiculous. Because the answer was in the question, why wouldn’t you write that down? But of course, what you’re seeing there is the rejection of model land. Somebody has assessed it as 50-70%. The question is, do you believe it? Well, no, actually, you might write something like 40 to 80%, because you expect there to be, you know, the model to be generically overconfident. And so this is sort of what I mean, by curbing over enthusiasm for mathematical solutions is that, you know, we have to understand that the mathematical solutions are living in model land, and that we can, in order to get out of model land, we have to say, do we actually expect this result to refer to the real world? Or is it only saying what the next model run is going to tell us? And so the act of doing that is difficult, and it’s more difficult for mathematicians than for the general public because as mathematicians, we sort of are used to living within model land and noticing when the answer is in the question and then writing it down. And we’re not very good at saying, Well, what’s my subjective estimate of the probability of this model being inadequate in some way? That’s not something that you can necessarily do with respect to data and so it’s a tricky one. So in terms of the over enthusiasm, you know, it’s curbing over enthusiasm, not curbing enthusiasm, because as I said at the beginning, and I returned to a lot in the book, actually, mathematical models are incredibly valuable. And they contain a huge amount of information and insight that we’re, we would be fools to throw away. But we need to understand it, you know, in a more nuanced way and be clear about what it’s telling us and what it’s not telling us. And that answers in model land aren’t necessarily the answers that we need in reality, though they may be informative about them.\nErica Thompson\nWell, Erica, thank you very much for your time today, for talking through the book, which is out now. Do you have some some links or information about where people can find out more about the book?\nErica Thompson\nYep, look on my on my website, ericathompson.co.uk. And it’s available through all the usual booksellers.\nBrian Tarran\nExcellent, excellent. Well, I wish you the best of luck with the book. As I say, I think it’s fantastic. And well, I hope we get to talk again, maybe a bit further down the road and see whether some of these principles and this ethical framework that you talk about for mathematical modelling, whether that kind of comes to fruition because I think we need to watch that closely. So, Erica, thank you.\nErica Thompson\nThank you very much. Thanks for having me.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “How to ‘Escape from Model Land’: an interview with Erica Thompson.” Real World Data Science, January 25, 2023. URL"
  },
  {
    "objectID": "viewpoints/interviews/posts/2023/02/03/heidi-seibold.html",
    "href": "viewpoints/interviews/posts/2023/02/03/heidi-seibold.html",
    "title": "Why open science is ‘just good science in a digital era’",
    "section": "",
    "text": "Years are often dedicated to different causes and aims by different organisations. The United Nations, for example, has designated 2023 the International Year of Dialogue as a Guarantee of Peace, while for the European Commission it is the European Year of Skills. But over at the White House Office of Science and Technology Policy, 2023 has been declared the Year of Open Science.\nTo discuss what this means for science generally and data science in particular, Real World Data Science invited Heidi Seibold for an interview. Seibold is a statistician and data scientist, and also an open science trainer and consultant, and we talked about how she became involved in open science, what it means to her, the benefits of it, and how academic and industry researchers can move towards it.\nCheck out our full conversation below or on YouTube."
  },
  {
    "objectID": "viewpoints/interviews/posts/2023/02/03/heidi-seibold.html#timestamps",
    "href": "viewpoints/interviews/posts/2023/02/03/heidi-seibold.html#timestamps",
    "title": "Why open science is ‘just good science in a digital era’",
    "section": "Timestamps",
    "text": "Timestamps\n\nHow did Heidi become interested in open science? (1:46)\nWhat does open data science mean to Heidi? (7:50)\nWorking with PhD students on open science (12:00)\nHow do open data science principles fit into an industry environment? (14:10)\nKnowledge transfer and public science (17:29)\nYear of Open Science initiatives and lasting impacts (21:08)"
  },
  {
    "objectID": "viewpoints/interviews/posts/2023/02/03/heidi-seibold.html#quotes",
    "href": "viewpoints/interviews/posts/2023/02/03/heidi-seibold.html#quotes",
    "title": "Why open science is ‘just good science in a digital era’",
    "section": "Quotes",
    "text": "Quotes\n“Reproducibility [is] just like this minimum standard in research quality, where we say, ‘When we have the same data and the same analysis, we also want to see the same results’, and being able to check that from others is really, really important.” (2:45)\n“On the back of my wall here in my office I have written, ‘Open science is just good science in a digital era’… Before, we only had the printing press, and we had to print journals in order to distribute the knowledge that we have.” (5:23)\n“For me, open data science entails the part of the scientific process that focuses on everything that happens on the computer: the data processing and the data analysis, and then getting from the data analysis – getting the results and the knowledge, really – in sort of a pipeline where you go from one step to the next. And so the image that I have in my head, when I think about open data science, is of a pipeline.” (10:16)\n“Nobody’s perfect from the beginning. And open science and reproducible research is really hard, and it requires a lot of technical knowledge. And I always feel like people are so scared, because on one hand, they don’t know how to do it yet, and the goal is so far away. And so I always like [to say], you don’t have to be perfect right away; going one step into the right direction is super important.” (12:34)\n“If we think of companies, for example, like Microsoft – they put a lot of money right now into open source: they bought GitHub, they publish open source software, they put money into open source software projects like R, for example. So, somehow, this must be a good way of making profits.” (15:09)\n“[Open science for the public has value because] we don’t know what ideas people will have. There’s so many skilled people out there that probably will do amazing things… We have this with the software Stable Diffusion right now. That’s an AI that generates images from text, and it runs on my computer here. And I don’t need AI skills to be able to do that. And people are building such incredible images out of this, and it’s really fun to see.” (18:50)"
  },
  {
    "objectID": "viewpoints/interviews/posts/2023/02/03/heidi-seibold.html#transcript",
    "href": "viewpoints/interviews/posts/2023/02/03/heidi-seibold.html#transcript",
    "title": "Why open science is ‘just good science in a digital era’",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove repetitions.\n\n\n\nBrian Tarran\nHello, and welcome to another Real World Data Science interview. I’m Brian Tarran. And today I’m joined by Heidi Seibold, a statistician and data scientist. I invited Heidi along to speak about open science, what it means, the benefits of it, and how to move towards it. So welcome, Heidi, how are you?\nHeidi Seibold\nHello, thanks for having me. I’m good.\nBrian Tarran\nExcellent. Excellent. Well, Heidi, I contacted you because, you know, I know you have a real deep interest in open science. And the conversation I think was really motivated by the White House Office of Science and Technology Policy declaring 2023 to be the year of open science. So first off, I wanted to get your reaction to that announcement.\nHeidi Seibold\nYeah, I think in general, that’s a really cool thing for open science to happen. Right? We, there’s this movement that’s been going on for a while, and people have been doing these grassroots communities and growing open science from the bottom up. And now we see more and more also top down decisions, which I think is a very good sign for, yeah, quality of science, really.\nBrian Tarran\nYeah, definitely. So I mean, we’ll get into some of the details of the initiatives a little later, maybe, but I thought that maybe we could kick off by asking you what you thought of the official definition of open science that the US government has come up with, and, and for the benefit of people watching that’s, in quotes, the principle and practice of making research products and processes available to all while respecting diverse cultures, maintaining security and privacy and fostering collaborations, reproducibility and equity. So as definitions go, does that, you know, does that hit the mark, for you, do you think?\nHeidi Seibold\nYeah, I think given that this is a federal definition, right, from the US. And they have to like, yeah, take so many opinions into account, I think it’s a great definition. I also asked like on social media, what people thought about it, and I think the response generally was pretty good. What I liked especially is that they focus on collaboration, reproducibility, and equity, which aligns very much with how I personally see open science. So collaboration means like, I always think of the term like building on the shoulders of giants, right. So this is what we want to do in research, we want to build on the work of others. They might be like famous people, but they might also be our colleagues next door. And it’s so important to take that into focus. And also reproducibility, just like this minimum standard in research quality, where we say, when we have the same data and the same analysis, we also want to see the same results, and being able to check that from others is really, really important. And so I think, this focus, I personally like it a lot.\nBrian Tarran\nExcellent. Excellent. So can you tell me a little bit about your background and how you became interested, and I think committed to open science would be a great way of describing it, right?\nHeidi Seibold\nYeah, so I’m a trained statistician, I studied statistics. And then how did I get into this whole open science thing was, for me, really through reproducible research. So my first research project was during my master’s programme, and we wrote a paper and it was a very computationally complex project, and we had lots of files and folders, and, you know, scripts and stuff like this. And then, at some point, it just all got so messy. And I felt like, oh, no, I’m the worst scientist in the world. And then I told my colleague who I was working with on this project. And he was like, No, this is normal. And I was like, No, this can’t be normal. I want to be on top of things when I do research. And I want to be sure that, like, the code that I’m using for this is the correct code that I actually wanted to use, right. So that must be like a minimum standard that we have. And so through that, I learned about reproducible research and good coding practices. And then I also thought more and more about like, well, if I do all this, I should also publish it so that others can actually check that I’m good doing good work here. Right. And so through that I got more and more into open science and really always felt like that’s the right thing to do, especially if you’re funded through public money.\nBrian Tarran\nYeah, yeah. Can I ask you what you think is kind of driving the shift to open science because the way you’ve described open science to me just there, you know, that sounds like to me like, just good scientific practice, and the fact that it’s not something that’s been done before, I wonder whether is it a cultural change, a philosophical change, a technological development, that’s kind of spurring this shift?\nHeidi Seibold\nYeah. So that’s a really, really good and deep question. So on the back of my wall here in my office, I have written open science is just good science in a digital era. And I think that describes the answer to your question pretty well. So there was a technological change, right? Before, we only had the printing press. And we had to print journals in order to distribute the knowledge that we have. And of course, that costs money. So journals cost money historically. But now, journal costs are really super low, because nobody needs them printed anyhow. And you only ever want to read like one or two articles out of one issue. So it doesn’t really make sense anymore, right. And also publishing data and code. It’s just like, the cost is so so low, that now in this digital age of the internet, really, we have such a low burden of, of doing the right thing. But on the other hand, we need to make this social shift, because we’ve been always, like, researchers have always done it a certain way. And there are especially certain fields that are really into, like, they feel like this is my data. This is my code. This is my research. Why should I share it? But I feel like the young generation of researchers are like, well, because it’s the right thing to do. And the reason why we’re in research is because we want to have scientific progress and scientific progress comes when we can build upon each other’s work.\nBrian Tarran\nOf course, and you mentioned, obviously, journal publications, but I wonder to what extent is it that the scale of science has almost kind of outgrown the ability to kind of condense it all down into a, even if it’s a 20 or 30 page, academic paper, right? Because it’s not just about, you know, setting out the research question describing the methods, you know, presenting a table of the data, all that stuff can’t be published now, or it can be, but it can’t be fit within the framework of an academic paper, it has to be on a GitHub repository or in a Jupyter notebook or something like that. So kind of open science encourages us to, to think about distributing that knowledge in different ways.\nHeidi Seibold\nI think that’s completely true. So, research now is so much more complex than it used to be right. There used to be single researchers who did like, yeah, such breakthroughs within one paper. And now, we already have so much knowledge, and the questions are so much more detailed and complicated. And also the data is so much more and the things that we can do is so much more complex. And so I feel like one paper doesn’t– isn’t enough to describe the research that we’re doing. And we did this one project with, with a group of students, actually, where we took 11 papers that were related to the topic that we were teaching about. And we took these 11 papers and data was available for the papers. And so we thought, well, we try to reproduce the results that they have there. And we found that it is really hard if we don’t have the code, because the method section in papers is really super short, right? It’s a super short section, if it’s not like a statistics paper. And it’s impossible to describe all the intricate steps that you took from the data to the complex model and analysis that you did. It’s just not enough space within the paper. And the code is the perfect description of what you did, right? So why not publish it with it, and especially in cases where data privacy is not an issue, then, and you already publish the data, then it just makes so much sense to also publish the code, because the code is not like there’s no privacy issues to that – usually at least.\nBrian Tarran\nSo is there a kind of you know– when we say open data science, you know, what does that look like? And do you have a kind of example or simplified example of what that would look like, right? And how it’s more than just, you know, the finished paper, the product of the science scientific process?\nHeidi Seibold\nYeah, so for me, open data science entails the process of– the part of the scientific process that focuses on everything that happens on the computer. So the data processing and the data analysis, and then getting from the data analysis, getting the results and the knowledge really, in sort of, yeah, in sort of a pipeline where you go from one step to the next. And so what the image that I have, in my head, when I think about open data science, I really think of a of a pipeline where you stick the different parts of the pipeline together, in a consecutive order. But of course, research projects are really complicated. And the pipeline just looks really messy, in some ways, but if you still manage to organise it in a way that the different bits all fit together, then you can make it so that in the end, you can imagine something that goes from the start to the finish. And you really understand every step of this pipeline every step of the way, from the raw data to the paper.\nBrian Tarran\nYeah. So the idea would be that people provide almost like a framework for you to recreate that yourself if you want to either check the results or yeah, replicate, just replicate the process generally. Right? So is your– so your job now is, I guess, as an open science trainer and consultant, so are you the person that goes into organisations and kind of shows them how to build that pipeline? Or how to, you know, map it out and to present that to people?\nHeidi Seibold\nYeah, so what I do a lot is do workshops and trainings with PhD students. So graduate programmes will ask me to come in for a day, or, also, we do longer trainings, which are often more useful, because people can in between take the steps that I recommend. And then we just, like, work together on ideas and steps that they can actually take. And I think what’s always important there is to know that nobody’s perfect from the beginning. And open science and reproducible research is really hard. And it requires a lot of technical knowledge. And I always feel like people are so scared, because on one hand, they don’t know how to do it yet. And the goal is so far away. And so I always like, you don’t have to be perfect right away, going one step into the right direction is super important. And that also helps with like the social change, because then the question is, well, I do want to do this, but my supervisor doesn’t know the technology, what do I do? And then we always try to find like one step that they can take, rather than trying to be perfect right away.\nBrian Tarran\nAnd it’s, I guess, it’s encouraging to see people like you being brought in to work with people on graduate programmes. So you’re, we’re trying to almost train the next generation of scientists to be thinking, open science first, rather than, you know, falling into bad habits or old habits that, you know, we are trying to do away with.\nHeidi Seibold\nYeah, and really, the young researchers, my feeling so far has been that the only pushback I get for my work is from established researchers who feel like well, this is the way I did it, and so it has to be the right way. But the younger generation for them, it’s super obvious that this is the way we should go in order to achieve scientific progress and good scientific practice.\nBrian Tarran\nYeah, yeah. A lot of data science now is obviously now done within industry. I wondered how open data science or open science principles fit in an environment you know, where competitive advantages is linked to keeping things in house and confidential and not wanting to share too much. Do you see a conflict there or can the two work together?\nHeidi Seibold\nSo I think there’s– first of all, I think, if we, if we get it done in academia, I’d be already super happy, right? If we get it done in the space where we have public funded projects that then are available for the public, I’d be already like super stoked. But in industry, it’s really interesting because a lot of the work that is done in industry is already done pretty well. So we if we think of, for example, companies, for example, like Microsoft, right, they put a lot of money right now into open source, they bought GitHub, they publish open source software, they put money into open source software projects like R, for example, right. So somehow, this must be a good way of making profits as well. And we see lots of companies investing in open source. So why not think about other research products, like data, and so on, also in the same line as we do of open source, because software is just a similar product. And I don’t think that there’s, I mean, there is some software, or some products, where it makes sense to have a patent or a trade secret or something. But sometimes it’s just more profitable, to have something where people can look into it and trust it. And I’d also helps like finding the next coders, and the next researchers to work on these projects, because, well, we like looking into what we’re going to do next. Right. And also, if we look, for example, into pharma industry, there, we see that a lot of the work they’re doing is already pretty good. For example, if we look at clinical trial transparency, pharma is doing better than academia there. And also, they’re pretty good on reproducible research, because they have to stick to certain rules. And when we look on the other side, so industry also benefits from open science, right? Because if we do open science, in academia, and publicly funded projects, then this will help companies make more money, because they have access to more knowledge and maybe interesting ideas as well, that they don’t have right now. So I think this is a win-win situation for companies as well. And I feel like if academia gives more to the industry, then eventually there will be like a mindset change. And industry will also give more back as well.\nBrian Tarran\nYeah, I see– I can see that and I guess it’s, you know, knowledge transfer is a big objective of a lot of academic institutions, right, we want our outputs to be of use to wider society, and that includes business and industry, right. So if people can pick those things up, you know, download it off of the web without having to, you know, make one-to-one links with the researchers who’ve done that project, it just it smooths that transition, and that that knowledge transfer becomes a lot more straightforward. You did, you mentioned about, you know, open science is about public science, essentially putting these, when it’s publicly funded research, this data and this work goes into the public space. You know, I’m guessing that, to a large extent, a lot of, you know, regular members of the public aren’t going to be interacting with open science. They’re not going to be downloading the datasets, they’re not going to be rebuilding the pipelines and rerunning the analysis. But do you still see that there’s a value there for the public in that information being there should they need it? And where are the kind of areas of value that you think the public will exploit?\nHeidi Seibold\nYeah, I think that’s an interesting question. And I think the biggest answer to that is that we don’t, we don’t know what ideas people will have, right? There’s so many skilled people out there, that probably will do amazing things. And we see that over and over again, when we put stuff out there, that people just get the super cool ideas. So we have this with the software Stable Diffusion right now, right? So that’s an AI that generates images from text, and it runs on my computer here. And I don’t need AI skills to be able to do that. And people are building such incredible images out of this. And it’s really fun to see. So I think, yeah, we just don’t know what’s going to happen. And on the other hand, I think, well, I am a researcher, but I’m also the public, right? And so if I have a question about something that concerns my private life or my friends’ private life, then I can also– I do have the skills to go into this and look at some research for example, I don’t know, how to best raise children or whatever. Um, that’s– so I have a friend, she’s an epidemiologist, and she always goes and looks at research on like, how to feed her child best and what to do. There’s, there’s all kinds of questions you have as a mother. And she, as a researcher can just go into the research and figure out, like, what is the best path for me to take. And I think the more we do open science, the more we can also do, like science communication, that adds on to that as well. Right? So if we have her now, she could now help other mothers make the same good decisions as well. And she would be sort of a science communicator for research that isn’t even hers. Um, so that is pretty cool, I think.\nBrian Tarran\nI think so, and it’s speaks to me of breaking down silos that, and I guess, blurring the lines between our roles, our professional roles and our personal lives, right. It’s about bringing science into that kind of public sphere, so I can see why the benefits would accrue from doing so. Just to wrap up, let’s go back to that year of open science, that was announced by the White House, are there any other specific initiatives in that big long list that constituted the announcement that you’re excited about?\nHeidi Seibold\nYeah, there is. I think, especially what I really liked was that, yeah, federally funded research, um, needs to be accessible, including the data when possible. So again, there’s open when possible, closed when necessary. But I think this is a very good first step, to say, okay, it depends on the funding, if it’s publicly funded, then it should also be publicly available. And I think that’s a really good sign. And then also, these, the statement had like, mentioned all these open science initiatives in different fields, which I really liked. So for example, from NASA, they have this transform to open science programme, and they’re already super active. And it’s really cool to see what comes out of that. For medicine, they have requirements for data management plans, which I think is a very solid step towards open science in medicine, because they are we have the issue of data privacy. And we really have to think from the get-go of a project about what should we do in terms of best practices of data management and having a requirement on that is, I think, a really, really solid step. And also, I’m thinking about open science in the field of like federal government even, because open data and federal government is a huge topic, right. And it, that’s definitely something that a lot of people will be interested in as well.\nBrian Tarran\nLast question for you, then Heidi, you know, what would you want the lasting impact of an initiative like this to be? And you know, would you like to see this sort of thing replicated in other countries around the world? If indeed, you know, other countries may already be doing this and have already done this?\nHeidi Seibold\nI think in general, it’s a very, very good sign that we’re seeing right now. We’ve seen lots of movement, for example, in the Netherlands – Netherlands is really big on open science – and seeing such a big country [the US] that also plays such an important role in the world, doing– taking this step, is a great sign for the entire, like all of research, really. And yeah, I think it’s also nice to see that we’re going from like, Oh, this is a niche topic that only experts are interested in, and people that are like advocates and nerds focus on, to something that really governments are thinking about.\nBrian Tarran\nSo, open science is going mainstream, I think is the message. And let’s hope that it continues to do so. So Heidi, thank you very much for your time today. Where can people find you online, if f they want to find out more about you and your work and your thoughts on open science?\nHeidi Seibold\nYeah, thank you so much for having me. It was wonderful. And I always like talking about open science, so people can find me on my website, heidiseibold.com. And I’m also on Mastodon, Twitter, LinkedIn, YouTube, wherever your search for Heidi Seibold, you’ll find me.\nBrian Tarran\nExcellent. Excellent. Well, thank you again. And thank you to those of you who are tuning in today. Make sure to check realworlddatascience.net for more interviews. Take care.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Why open science is ‘just good science in a digital era’.” Real World Data Science, February 3, 2023. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/10/18/meet-the-team.html",
    "href": "viewpoints/editors-blog/posts/2022/10/18/meet-the-team.html",
    "title": "Meet the team",
    "section": "",
    "text": "Brian Tarran\n\n\n\nI am a writer and editor with 20 years of experience covering the research and data space. I have worked for the Royal Statistical Society (RSS) for the past 8 years, and was editor of Significance Magazine (a joint publication of the RSS, the American Statistical Association and the Statistical Society of Australia) prior to the launch of Real World Data Science. I am a former editor of Research-Live.com and was launch editor of Impact magazine, both published by the Market Research Society."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/10/18/meet-the-team.html#editor",
    "href": "viewpoints/editors-blog/posts/2022/10/18/meet-the-team.html#editor",
    "title": "Meet the team",
    "section": "",
    "text": "Brian Tarran\n\n\n\nI am a writer and editor with 20 years of experience covering the research and data space. I have worked for the Royal Statistical Society (RSS) for the past 8 years, and was editor of Significance Magazine (a joint publication of the RSS, the American Statistical Association and the Statistical Society of Australia) prior to the launch of Real World Data Science. I am a former editor of Research-Live.com and was launch editor of Impact magazine, both published by the Market Research Society."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/10/18/meet-the-team.html#editorial-board",
    "href": "viewpoints/editors-blog/posts/2022/10/18/meet-the-team.html#editorial-board",
    "title": "Meet the team",
    "section": "Editorial Board",
    "text": "Editorial Board\nSophie Carr (chair)\n\n\n\nI am the founder and owner of Bays Consulting. I trained as an engineer and took a PhD in Bayesian belief networks, and have worked in data analytics ever since. Or to put it another way, I have made a living out of finding patterns. I am the vice-president for education and statistical literacy at the RSS, officially one of the World’s Most Interesting Mathematicians and was a member of the first cohort of data scientists to achieve the new, defined standard of professionalism award from the Alliance for Data Science Professionals.\nI am delighted to be chairing the editorial board of the new data science project from the RSS and am excited to be a part of this project as it evolves into a key resource for all data science practitioners and leaders. To make this a place that helps everyone learn and develop within this field, I’d like to encourage all practitioners, no matter what stage of their career, to submit the type of resource they learn best from (whether that be an article, some code, a data set, a case study or a problem/exercise to solve) on a topic that is important to them – from ethics to analysis plans through to tips on how code. Whatever it is you’re working on that you care about, I’d like to ask you to become an active part of the wonderful community of data scientists by sharing your knowledge.\nYaxin Bi\n\n\n\nI am a reader in the Artificial Intelligence Research Centre, School of Computing at Ulster University, UK. My research interests include machine learning, data science and ensemble methods underpinned with the Dempster-Shafer theory of evidence. I have led on developing data analytics for anomaly/change detection within satellite data, as well as text analytics for document categorisation and sentiment analysis. I have served a number of international conferences as general chair and technical program committee chair. I am an associate editor for the International Journal of Intelligence Systems and a core steering committee member for the series of International Conference on Knowledge Science, Engineering and Management.\nAs a member of this editorial board, I am passionate about contributing my expertise to the development of data science and its real-world applications. I look forward to sharing my experience with others in the data science community, in particular to address ethical challenges posed by large language models and to inspire new curriculum development in the field of data science.\nSayma Chowdhury\n\n\n\nI am a freelance data scientist on Upwork, with a client portfolio ranging from start-ups to commercial businesses such as supermarkets, pharmaceuticals and automotive manufacturers. I transitioned into data science from law in 2017, having completed a MicroMasters in statistics and data science with MIT and a Professional Certificate in data science with Harvard University. In advance of a PhD in digital humanities, I am currently completing a MicroMasters in data, economy and development policy with MIT and an MSc in data science with the University of Aberdeen. My research interests are in text analytics, natural language processing and machine learning.\nThe RSS was instrumental in my training and professional development as a data scientist in the early stages of my career, particularly in mastering statistics and R. Data science is a rapidly growing field with employment opportunities in many sectors but there is an increasing need to uphold a realistic and accurate expectation of competency within the industry. I will endeavour to present expert practical guidelines for data scientists as well as demonstrate the versatility of the profession. I hope the site will be a benchmark for academic and professional resources by expert data scientists from industry, accessible to data scientists at all levels, anywhere in the world.\nLee Clewley\n\n\n\nI am head of applied AI in GSK’s AI and Machine Learning Group, R&D. I began my career as an astrophysicist, initially working out the mass of our galaxy, before pondering the bigger universe. After six years at Oxford as a post-doc lecturer publishing in theoretical cosmology, I entered the very real world of manufacturing at GSK. For the first five years I applied statistical modelling techniques across manufacturing, such as the first end-to-end continuous manufacturing prototype for tablets. The past decade has been spent as a lead data scientist delivering high value projects across R&D and manufacturing.\nI joined this editorial board because the impulse to assemble and present complex data science ideas to a wide range of folks has never left me. I have been a data scientist leader since it became a distinct profession but also have a decent understanding of classical and modern predictive analytics tools and statistics. I have spent a good deal of my adult life teaching students and non-technical adults alike.\nI am passionate about delivering useful, pragmatic data science ideas and products to a wide range of people. I enjoy trying to communicate complex scientific information simply. Alongside my peers in the team, I want to support and develop data scientists at whatever stage in their career. I want to help cut through the hype and nonsense to give the best advice possible in a highly respected institution like the RSS.\nJonathan Gillard\n\n\n\nI am a professor at the School of Mathematics, Cardiff University, where I am also research group lead for statistics. I have a history of publications in statistical methods and an interest in the theoretical underpinnings of data science, but I have also worked with industry on applied and practical projects. Recent industrial partners of mine include the Office for National Statistics (ONS) and the National Health Service, on projects such as anomaly detection and understanding heterogeneity. Indeed, I am academic chair for Cardiff University’s strategic partnership with the ONS which serves to spur and catalyse collaboration between both organisations.\nI am excited to see what this site can achieve. I’m particularly keen to support articles describing the latest, cutting-edge methodology, as well as contributions from data professionals in industry who can explain how data science has managed to offer insights into important problems. Data science is a broad church and I want to ensure that the full array of work in this area is represented on this site. I think the diversity of the editorial board will help promote this objective.\nJuhi Gupta\n\n\n\nI am a lecturer in health data sciences and the deputy programme director of the health data science MSc in the School of Health Sciences, University of Manchester (UoM). I have a background in genetics, pharmacology and bioinformatics, and my doctoral thesis focussed on multi-omics data analysis using machine learning methods for precision medicine. I have worked with scientists, clinical academics and technologists to produce translational research. I am currently investigating adverse health outcomes in people with common diseases using electronic health record data, and I also teach on the health informatics MSc joint programme with UoM and UCL.\nI would like to see this platform encourage collaborations and the sharing of ideas and good practice across different disciplines that apply data science skills in their work (or as a hobby). I would like to support budding data scientists to gain useful advice and guidance for upskilling as well as application in real-world situations involving health data and biological data.\nHollie Johnson\n\n\n\nI am a data scientist at the National Innovation Centre for Data (NICD), based in Newcastle upon Tyne. Following my undergraduate degree in mathematics, I worked as a software developer both in industry and as a technical research assistant in academia. I later joined the Centre for Doctoral Training in Cloud Computing for Big Data at Newcastle and obtained a PhD in topological data analysis in 2020. Now at the NICD, I specialise in transferring statistics and machine learning skills into industry, through collaborative data science projects.\nI am excited to be a member of the editorial board and look forward to seeing Real World Data Science develop into a valuable source of information for aspiring data scientists and professionals alike. I would particularly encourage submissions that demonstrate the use of data science in SMEs and the non-profit sector, as well as perspectives from those with non-standard backgrounds.\nHarvey Lewis\n\n\n\nI am a senior technology leader, with a diverse background spanning rocket science, data science and research. I have 30 years of experience in artificial intelligence and other emerging technologies and am currently pioneering the use of AI in Ernst & Young’s tax and law practice. I’m a former member of the Open Data User Group, the Public Sector Transparency Board and the Advisory Committee to the All-Party Parliamentary Group on AI. I am a member of techUK’s leadership committee for data analytics and AI, and an honorary senior visiting fellow at The Bayes Business School in London.\nI’m passionate about data science but I’m also a fierce advocate for human skills, which are as often underrated as AI is over-hyped. As a member of the editorial board, I’m keen to explore the interplay between artificial and human intelligence in businesses. I’m going to encourage all data scientists to think about the fundamentally human aspects of their work, such as trust and safety, so that we maintain perspective and proportionality in the face of ever-more sophisticated technology.\nDetlef Nauck\n\n\n\nI am a BT Distinguished Engineer and the head of AI and data science research for BT’s Applied Research Division located at Adastral Park, Ipswich, UK. I have over 30 years of experience in AI and machine learning and lead a programme spanning the work of a large team of international researchers who develop capabilities underpinning future AI systems. A key part of this work is to establish best practices in data science and machine learning, leading to the deployment of responsible and auditable AI solutions that are driving real business value.\nI am a computer scientist by training and hold a PhD and a Postdoctoral Degree (Habilitation) in machine learning and data analytics. I am also a visiting professor at Bournemouth University and a private docent at the Otto-von-Guericke University of Magdeburg, Germany. I have published 3 books and over 120 papers, and I hold 15 patents and have 30 active patent applications.\nI am passionate about promoting best practice in data science and believe that in the UK the RSS is the ideal professional body to pursue this goal. For me, Real World Data Science is an opportunity to share my experience and inspire a new generation of data scientists.\nFatemeh Torabi\n\n\n\nI am a research officer and data scientist at Health Data Research UK and a fellow of the RSS. My background is in mathematical statistics and health data science, and my research interests span novel analytical and computational methods for statistical inference in panel data and population health. I am supporting the development of the Real World Data Science platform in the context of health with a specific focus on how health data can be harnessed through data linkage and analysis to answer important questions and improve the lives of our population.\nIsabel Sassoon\n\n\n\nI am a senior lecturer in computer science (data science) at Brunel University and the programme lead for the data science and analytics MSc programme. My research interests are in data-driven automated reasoning and its transparency and explainability, which brings in data science and artificial intelligence with applications within the health space. I am also championing open science and reproducible analysis in both my research and teaching. I have a PhD in informatics from King’s College London and it was on the topic related to the use of AI to support statistical model selection. Prior to Brunel I was a teaching fellow and research associate at King’s College London and before that I worked for more than 10 years as a data science consultant in industry, including 8 years at SAS UK. \nI have been working, researching, consulting and teaching in the data science space for a while and I am passionate about the domain and its applications. I am always interested in sharing and hearing what else is being done to support, inform and inspire all those studying and working in the field of data science. I look forward to sharing case studies, how-to guides and data science profiles through this website.\nChristopher Thiele\n\n\n\nI’m a principal data scientist at Uniper where I lead a team that focuses mainly on financial business processes and upskilling initiatives. We run projects end to end: from use case ideation, requirement collection and translation to prototyping, assessment, deployment and maintenance. Besides the core analytical and data engineering duties, overarching topics such as data design, data governance and data strategy predominate my days. In my previous role, at the German Economic Institute, I contributed to the development of a cross-functional department that helps apply data science methods in economic research. Projects often involve geospatial analyses or natural language processing. Before that, I worked as a data scientist in customer and marketing analytics, doing statistical analyses such as marketing mix modelling. I have a master’s degree in statistics from Warwick University, a bachelor’s degree in economics from the University of Cologne and I’m trained as an assistant tax consultant in Germany.\nI see data science as a creative way to solve problems using software engineering and quantitative modelling techniques and I like to build software pieces that people can interact with. I think that there still exists a lot of confusion about data science as a discipline. Reducing it would promote the realisation of its potential for individuals, as a profession, and our society, as a form of digitalisation. I hope that with Real World Data Science we can provide guidance and clarification to everybody engaged or interested in the field and accompany this young profession’s development."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/12/09/A-chat-with-ChatGPT.html",
    "href": "viewpoints/editors-blog/posts/2022/12/09/A-chat-with-ChatGPT.html",
    "title": "A chat with ChatGPT",
    "section": "",
    "text": "Screenshots\n\n\n\nPart 1\n\n\n\n\n\nPart 2\n\n\n\n\n\nPart 3\n\n\n\n\n\nPart 4\n\n\nGood day, ChatGPT. How are you?\nChatGPT: Hello there! I’m a large language model trained by OpenAI, so I don’t have the ability to experience emotions or have a physical presence. I’m here to provide information and answer questions to the best of my ability. Is there something specific you would like to know?\nYes, certainly. We’re seeing a lot of money invested in large language models in the belief that these models will somehow lead to artificial general intelligence. Is that possible?\nChatGPT: It is possible that advances in large language models could ultimately lead to the development of artificial general intelligence (AGI), which is the idea of creating a machine that can think and understand the world in the same way that humans do. However, it is currently unclear how close we are to achieving AGI, and some experts believe it may be a long way off. Additionally, there are many different approaches to creating AGI, and the development of large language models is just one possible avenue of research.\nLarge language models are very good at constructing language that humans understand, but often the outputs of the models make no sense at all. Why does this happen?\nChatGPT: Large language models are trained on a vast amount of text data, and they are designed to predict the next word in a sequence of words based on the words that come before it. This allows them to generate text that is often coherent and resembles human language, but it does not necessarily mean that the text will always make sense or be relevant to a particular topic.\nOne reason why the outputs of large language models might not always make sense is that they are not explicitly programmed to understand the meaning of the words they generate. Instead, they are simply trying to maximize the likelihood of producing a sequence of words that resembles human language. As a result, they may produce outputs that are grammatically correct but do not convey a meaningful message.\nAnother reason why the outputs of large language models might not always make sense is that they are often trained on a wide range of different text sources, including news articles, books, and social media posts. This can result in the model incorporating elements from different styles and genres of writing, which may not always be compatible with each other.\nFinally, large language models are generally not designed to produce a specific type of output or to answer specific questions. Instead, they are designed to generate a wide range of possible outputs, and it is up to the user to interpret the outputs and determine if they are relevant or make sense in a given context.\nBut what happens if the user doesn’t know enough about a topic to know whether the language model output is right or wrong?\nChatGPT: If a user does not have enough knowledge about a topic to determine whether the output of a large language model is correct or not, then it is possible that the user may accept the output as accurate, even if it is not. In such cases, it is important for the user to be aware of the limitations of large language models and to consider the source of the information they are receiving.\nIt is also possible that the user may be able to use other sources of information, such as fact-checking websites or additional research, to verify the accuracy of the output of the large language model. In general, it is always a good idea to be cautious when evaluating any information, regardless of its source, and to verify its accuracy if possible.\nI agree with that. But it leads me to ask: What is the point of asking large language models a question if the answers might be wrong, and if you need to refer to other sources to check veracity?\nChatGPT: While it is true that the outputs of large language models are not always correct and may need to be checked against other sources, there are still many potential uses for these models. For example, large language models can be useful for generating text that is similar to human language, which can be useful for a variety of applications, such as language translation, text summarization, or conversation simulation.\nAdditionally, large language models can be used as a starting point for further research into natural language processing and artificial intelligence. By studying the outputs of these models and how they are generated, researchers can gain a better understanding of the underlying mechanisms of human language and potentially develop new techniques for processing and generating language.\nFinally, large language models can be useful for providing a broad range of possible answers to a given question, which can be useful for exploring different options and generating ideas. While it may be necessary to verify the accuracy of the outputs of these models, they can still be a valuable resource for generating new ideas and insights.\nEarlier, you told me: “One reason why the outputs of large language models might not always make sense is that they are not explicitly programmed to understand the meaning of the words they generate.” Do you understand the meaning of the words that I’m prompting you with now?\nChatGPT: As a large language model trained by OpenAI, I do not have the ability to understand the meaning of the words I generate in the same way that a human would. I am simply a machine learning algorithm that is designed to predict the likelihood of a given sequence of words based on the training data I have been provided with.\nWhile I can produce text that resembles human language and can sometimes provide useful information, I do not have the ability to understand the underlying meaning or context of the words I generate. I am not capable of experiencing emotions, forming opinions, or understanding the world in the way that humans do. I am simply a tool for generating text based on the inputs I receive.\nWell, at the very least, this has been a coherent conversation, and you’ve shared plenty of information for me to read up on. If I want to understand more about what we’ve discussed just now, can you recommend a particular textbook or academic paper?\nChatGPT: I’m sorry, but as a large language model trained by OpenAI, I do not have access to external sources of information such as textbooks or academic papers. I am only able to provide information based on the data I have been trained on, and I do not have the ability to browse the web or access other external resources.\nIf you would like to learn more about the topics we have discussed, I would recommend starting with a general search on the internet or consulting a textbook or other reliable source of information on the topic. There is a wealth of information available on the internet and in print on artificial intelligence and natural language processing, and I’m sure you will be able to find many valuable resources to help you learn more.\nOne last question, then. What data have you been trained on?\nChatGPT: As a large language model trained by OpenAI, I have been trained on a vast amount of text data, including books, news articles, and other written materials. This data allows me to generate text that resembles human language and to provide answers to questions to the best of my ability.\nHowever, it is important to keep in mind that the accuracy and completeness of my responses will depend on the quality and diversity of the training data I have been provided with. I am not able to access external sources of information or update my knowledge, so my responses are limited to the information I have been trained on.\nI said that my previous question was the last question. But, is it?\nChatGPT: Yes, your previous question was the last one. I’m here to provide information and answer questions to the best of my ability, but I cannot initiate conversation or continue a conversation indefinitely. If you have any further questions, I would be happy to try to answer them for you. Otherwise, our conversation has come to an end. Thank you for chatting with me!\nThank you. But, remind me: What were we chatting about?\nChatGPT: We were chatting about large language models and their potential uses and limitations. Specifically, we discussed the possibility that advances in large language models could ultimately lead to the development of artificial general intelligence, and we discussed the reasons why the outputs of these models might not always make sense. We also discussed the potential uses of large language models, such as generating text that resembles human language and providing a starting point for further research into natural language processing and artificial intelligence.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “A chat with ChatGPT.” Real World Data Science, December, 9 2022. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/12/01/themes.html",
    "href": "viewpoints/editors-blog/posts/2022/12/01/themes.html",
    "title": "Four themes for potential contributors to think about",
    "section": "",
    "text": "We’ve had a fantastic early response to our call for contributions, and it has been pleasing to see and hear how our plans for Real World Data Science chime with the wants and needs of the data science community. But one question we’ve been asked frequently is: “What particular topics are you most interested in?”\nThe honest answer to that question is this: we’re interested in any and all topics that are of interest and importance to you, the data science community at large. However, we thought it might be helpful to identify some themes around which potential contributors could construct different types of content.\nThese themes are outlined below. If you’d like to discuss any of them further, please do not hesitate to contact us."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/12/01/themes.html#can-data-science-save-the-world",
    "href": "viewpoints/editors-blog/posts/2022/12/01/themes.html#can-data-science-save-the-world",
    "title": "Four themes for potential contributors to think about",
    "section": "Can data science save the world?",
    "text": "Can data science save the world?\nEarth today faces major challenges – from the global to the regional to the local, and from the natural and physical to the social and digital. We have rich sources of data to help us understand many of these challenges, and there are teams of data scientists around the world who are working with, analysing, and extracting insights from that data in the hope of delivering positive lasting change.\nOn Real World Data Science we want to highlight this vital work, through case studies of data science projects and applications in such areas as:\n\nmonitoring and mitigating climate change and biodiversity loss\nbuilding sustainable futures\nsafeguarding public health and developing new medical treatments\nunderstanding human happiness and wellbeing\nidentifying and preventing online harms\nmeasuring national, regional, and local economies\n\nAs well as exploring the benefits that data science can deliver, we also want to have an informed conversation about the unintended negative consequences that can arise without careful consideration of data ethics and responsibilities."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/12/01/themes.html#what-is-a-data-scientist",
    "href": "viewpoints/editors-blog/posts/2022/12/01/themes.html#what-is-a-data-scientist",
    "title": "Four themes for potential contributors to think about",
    "section": "What is a data scientist?",
    "text": "What is a data scientist?\nDon’t be misled by the title of this theme. Definitions abound, but we’re not interested in establishing the exact boundaries of what a data scientist is or isn’t. Rather, our goal is to profile actual working data scientists. We want to hear about their skillsets, their experiences, and their career journeys so far. We want to learn about the ways in which they work, who they work with, the challenges they face, and their thoughts on where data science is heading next.\nIf you’re a working data scientist and you are happy to share your own career story, please get in touch."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/12/01/themes.html#statistical-ideas-all-data-scientists-need-to-know",
    "href": "viewpoints/editors-blog/posts/2022/12/01/themes.html#statistical-ideas-all-data-scientists-need-to-know",
    "title": "Four themes for potential contributors to think about",
    "section": "Statistical ideas all data scientists need to know",
    "text": "Statistical ideas all data scientists need to know\nStatistics is a crucial component of data science, but not all data scientists have a background in statistics. For those just starting out in their data science careers, or for those coming in from other fields, we want to highlight some of the statistical ideas that are absolutely vital to know.\nWe’re particularly interested in explainers that serve as an introduction to these ideas, alongside which we’ll be looking to publish exercises and example datasets to help people put what they’ve learned into practice.\nWe are also keen to explore the origins of modern data science techniques, including tracing their roots back to some of the foundational ideas in statistics and other disciplines."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/12/01/themes.html#whats-happening-in-the-world-of-data-science",
    "href": "viewpoints/editors-blog/posts/2022/12/01/themes.html#whats-happening-in-the-world-of-data-science",
    "title": "Four themes for potential contributors to think about",
    "section": "What’s happening in the world of data science?",
    "text": "What’s happening in the world of data science?\nData science is such a fast-moving, fast-developing field that it’s difficult to stay on top of all the latest news and developments. But racing to keep up can be counterproductive. It leaves little time to sit back and reflect on what the genuinely important new developments are, and what these might mean for data science longer term.\nOn Real World Data Science, we want to create a space for people to have these conversations – to step outside the news hype cycle, to ask big questions about what’s happening in the field, and to discuss new papers and ideas that otherwise might be lost amid the daily rush and noise.\nSo, if you have thoughts to share, a question you want to ask, or a new paper you want to talk about (one you’ve not written yourself, of course!), let us know.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “Four themes for potential contributors to think about.” Real World Data Science, December, 1 2022. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/03/29/defining-DS.html",
    "href": "viewpoints/editors-blog/posts/2023/03/29/defining-DS.html",
    "title": "Data science as ‘a rainbow’, and other definitions",
    "section": "",
    "text": "What does “data science” mean to you? That’s a question we’ve been asking a lot in recent weeks as part of our career profiles series of interviews – the first of which, featuring Jaguar Land Rover’s Tamanna Haque, was published yesterday.\nIt’s also a question that was asked recently of Sylvia Richardson, emeritus director of the Medical Research Council Biostatistics Unit at the University of Cambridge and immediate past president of the Royal Statistical Society (RSS).\nRichardson was interviewed by Francesca Dominici, interim co-editor-in-chief of the Harvard Data Science Review. In response to the question “What’s data science for you?”, Richardson said:\n\nIt’s hard to be original, but I was racking my brain for a good metaphor, and came up with the metaphor of a rainbow of interconnected disciplines, sharing the common aim of making the best use of data-rich environments we live in to solve problems in society. So, like in a rainbow, data scientists have to work together to draw out information from data. And the colors must match, [though] they are different. Similarly, there are different but intersecting data science tasks, taking different shapes and forms. As data scientists, we recognize and enjoy diversity, we’re not doing all the same tasks. Nevertheless, there is a backbone, a shape to the rainbow. And for us, this backbone is probability theory, study design, and quantifying uncertainty using statistical thinking. We also know that rainbows change all the time. They don’t last, but they keep reappearing. Data science is also evolving constantly because new questions and new types of data keep arising. In a similar way to the rainbow which is strongly influenced by the atmosphere, one key aspect of data science is that we have a strong link to practice. So, we work together to solve problems from different perspectives, we evolve, we try to be relevant to science and society, and make the best use of the data. [Source]\n\nRichardson’s view on the meaning and importance of data science has special resonance to me, as editor of Real World Data Science. While president of RSS, Richardson set up the Data Science Task Force out of which this website emerged. As she explains to Dominici:\n\n… while I was president, I felt a sense of urgency to encourage the RSS to revisit its engagement with data science, and I created a data science task force right at the beginning of my presidency. It didn’t get going earlier because there was COVID to keep us busy! Nevertheless, the Data Science Task Force got underway in 2021 and came up with two major recommendations. One was to give more resources to the practitioners’ community, which led the RSS to create a Real World Data Science online platform. A second direction was to brainstorm on what is still needed for the discipline to thrive. [Source]\n\nYou can read (or listen) to Richardson and Dominici’s conversation in full on the Harvard Data Science Review website.\nAnd we’ll have more career profiles – and more personal definitions of data science – to share soon. In the meantime, why not tell us what “data science” means to you?\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Data science as ‘a rainbow’, and other definitions.” Real World Data Science, March 29, 2023. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html",
    "href": "viewpoints/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html",
    "title": "How do people feel about AI? Well, it’s complicated",
    "section": "",
    "text": "How do people feel about AI? That was a question recently explored in a survey of 4,000 British residents. The answer is that, well, it depends.\nResearchers at the Ada Lovelace Institute and the Alan Turing Institute designed the survey to ask about specific AI use cases, rather than the concept of AI more broadly. Use cases included face recognition for policing, border control and security, targeted advertising for political campaigns and consumer products, virtual assistants, driverless cars, and so on.\nRoshni Modhvadia, a researcher at the Ada Lovelace Institute and member of the survey team, reported that respondents overall were broadly positive towards most of the use cases they were asked about. Healthcare applications (using AI to assess the risk of cancer, for example) or face recognition for border security were seen as very or somewhat beneficial by more than 80% of those surveyed. More than half of respondents thought that other applications, such as virtual reality in education, climate research simulations, and robotic care assistants were very or somewhat beneficial.\nViews were less positive towards applications including driverless cars, autonomous weapons and targeted advertising. These were the applications that respondents expressed most concern about, and for each of these use cases perceived risks were felt to outweigh perceived benefits.\nAnd yet, even for applications that were seen as being overwhelmingly beneficial – assessing cancer risk and face recognition for border control – respondents still expressed concern about the potential for overreliance on the technologies, the issue of who is accountable for mistakes, and the impact the technologies might have on jobs and employment opportunities.\nThree-fifths (62%) of respondents said laws and regulations would make them more comfortable with AI technologies being used. This is an important finding given where the national AI conversation is at the moment, said Professor Helen Margetts, director of the public policy programme at The Alan Turing Institute.\nThe current national conversation has been fuelled by the success of ChatGPT and the growing adoption of generative AI tools. The Lovelace/Turing survey, fielded in November 2022, did not ask about ChatGPT et al., but the results do at least provide a baseline against which to measure any shifts in attitudes brought on by what Professor Shannon Vallor, Baillie Gifford Chair in the Ethics of Data and Artificial Intelligence at the Edinburgh Futures Institute at the University of Edinburgh, described as “this latest round of AI hype and confusion”.\nModhvadia, Margetts and Vallor were speaking at an online event last week to mark the launch of the survey report. Video of the event is below. The full report is available from the Ada Lovelace Institute website."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html#how-do-people-feel-about-ai-in-statistics-and-data-science-education",
    "href": "viewpoints/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html#how-do-people-feel-about-ai-in-statistics-and-data-science-education",
    "title": "How do people feel about AI? Well, it’s complicated",
    "section": "How do people feel about AI in statistics and data science education?",
    "text": "How do people feel about AI in statistics and data science education?\nA new paper in the Journal of Statistics and Data Science Education considers the potential for using ChatGPT in statistics and data science classrooms. Authors Amanda R. Ellis and Emily Slade of the University of Kentucky give suggestions for using ChatGPT to generate course content: lecture notes and new material such as practice quizzes or exam questions, or pseudocode for introducing students to statistical programming. It could also be used as a code debugging tool and integrated into set tasks – e.g., have students prompt ChatGPT to write code, then run the code themselves and assess whether the code works as intended.\n“We recognize that educators have valid concerns regarding the implementation and integration of AI tools in the classroom,” write the authors, later adding that: “We encourage readers to consider other technologies, such as the calculator, WolframAlpha, and Wikipedia, all of which were met with initial wariness but are now commonly used as learning tools. As statistics and data science educators, we can actively shape and guide the incorporation of AI tools within our classrooms.”\nRead the paper: A new era of learning: Considerations for ChatGPT as a tool to enhance statistics and data science education"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html#ok-but-how-do-people-feel-about-ai-generated-music",
    "href": "viewpoints/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html#ok-but-how-do-people-feel-about-ai-generated-music",
    "title": "How do people feel about AI? Well, it’s complicated",
    "section": "OK, but how do people feel about AI-generated music?",
    "text": "OK, but how do people feel about AI-generated music?\nA new demo on Hugging Face allows users to generate short samples of music based on text descriptions. Users can also “condition on a melody” by uploading audio files. The results are… interesting, as I discovered while playing around with the demo yesterday.\n\n\n\nText-to-music-generation is now a thing (via @huggingface: https://t.co/fpBDLuB4yh) so I thought I'd try creating some new genre mashups pic.twitter.com/y93w7x9pNW\n\n— Brian Tarran (@brtarran) June 12, 2023\n\n\n\nRead the paper: Simple and controllable music generation\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail image by Andy Kelly on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “How do people feel about AI? Well, it’s complicated.” Real World Data Science, June 13, 2023. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/01/05/newsletter.html",
    "href": "viewpoints/editors-blog/posts/2023/01/05/newsletter.html",
    "title": "Explore the RSS Data Science & AI Section newsletter, right here!",
    "section": "",
    "text": "Happy New Year from all of us at Real World Data Science. We hope you had a relaxing break over the holidays and are now refreshed and excited to see what 2023 has in store. We’re starting the year with a new addition to the site: a page dedicated to the excellent RSS Data Science & AI Section newsletter.\nThis monthly newsletter has been running since February 2020 and is well worth subscribing to as it features roundups of news, new developments, big picture ideas and practical tips.\nYou’ll find the full list of past newsletters in our News and views section (click the “Newsletter” heading in the section menu). If you want to subscribe to the newsletter, head over to datasciencesection.org. The Data Science & AI Section also has a page on the RSS website.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Explore the RSS Data Science & AI Section newsletter, right here!” Real World Data Science, January, 5 2023. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/05/26/future.html",
    "href": "viewpoints/editors-blog/posts/2023/05/26/future.html",
    "title": "What’s the future of data science and AI in an LLM world?",
    "section": "",
    "text": "The impact ChatGPT and large language models (LLMs) are having on the practice and profession of data science is something we discussed recently with data scientists from Unilever, BT, Deliveroo, and others. So, it was interesting to hear a perspective this week from Osama Rahman, director of the Data Science Campus at the UK Office for National Statistics.\nSpeaking Tuesday at an online event, Rahman mused on how LLMs brought both potential benefits and risks. For example, those who can already code can code more efficiently now with the help of LLM-powered tools. However, those same tools also allow non-coders to code – and inexpert use of tools and code presents risks. How do we guard against this, he was asked. “I don’t know,” came the response, “other than you have to observe and clampdown on it.”\nSuch problems are by no means new or unique to the post-ChatGPT era, of course. As someone with a background in economics, Rahman said he has, over the years, observed “inexpert uses of economics.” His advice was to “make sure experts are plugged in” – to teams, conversations, decision-making processes, etc. – “and are seen as the experts in the use of these tools.”\nThe discussion was wide-ranging, and also took in questions on whether data scientists have the right skills at this moment – “Skills evolve, it’s just a natural process… We need to keep a culture of curiosity…” – and whether enough is being done to address ethical issues – “My key issue is that ethical frameworks need a lot more discussion and debate than it takes to put out a new tool… I don’t have much to add, other than that there is a problem.”\nHowever, two questions – and answers – jumped out at me as particularly interesting. Rahman was asked: Have we delivered on the promise of data science from 5 years ago? “No, but that’s because expectations were wrong,” he said. “Data science wasn’t going to completely and utterly transform government. But where it has delivered is in an evolving set of tools, people, and skills coming in and allowing us to do impactful stuff. It hasn’t delivered on the false promise that it would change the world, but it has delivered a lot.”\nHe was also asked: How will data science and AI have changed the world in 5–10 years? “I’m not sure it will,” he said. “It will do certain things. It will allow us to address certain analytical problems more efficiently.” Rahman then offered a salutary reminder. Email once made life more efficient; now, we’re all at risk of “death by email.”\nWe’ll be sure to update this post with a link to a video or other recording of the event, if/when it becomes available. For now, be sure to check out our two-part discussion on LLMs and data science:\n\nHow is ChatGPT changing data science?\nLarge language models: Do we need to understand the maths, or simply recognise the limitations?\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “What’s the future of data science and AI in an LLM world?” Real World Data Science, May 26, 2023. URL"
  }
]