[
  {
    "objectID": "applied-insights/tutorials/posts/2023/12/12/xmas-cards.html",
    "href": "applied-insights/tutorials/posts/2023/12/12/xmas-cards.html",
    "title": "Creating Christmas cards with R",
    "section": "",
    "text": "When you think about data visualisation in R (R Core Team 2022), you’d be forgiven for not jumping straight to thinking about creating Christmas cards. However, the package and functions we often use to create bar charts and line graphs can be repurposed to create festive images. This tutorial provides a step-by-step guide to creating a Christmas card featuring a snowman – entirely in R. Though this seems like just a fun exercise, the functions and techniques you learn in this tutorial can also transfer into more traditional data visualisations created using {ggplot2} (Wickham 2016) in R.\nThe code in this tutorial relies on the following packages:"
  },
  {
    "objectID": "applied-insights/tutorials/posts/2023/12/12/xmas-cards.html#lets-build-a-snowman",
    "href": "applied-insights/tutorials/posts/2023/12/12/xmas-cards.html#lets-build-a-snowman",
    "title": "Creating Christmas cards with R",
    "section": "Let’s build a snowman!",
    "text": "Let’s build a snowman!\nBefore we jump in to writing R code, let’s take a step back and think about what you actually need to build a snowman. If you were given some crayons and a piece of paper, what would you draw?\nYou might draw two or three circles to make up the head and body. Perhaps some smaller dots for buttons and eyes, and a (rudimentary) hat constructed from some rectangles. Some brown lines create sticks for arms and, of course, a triangle to represent a carrot for a nose. For the background elements of our Christmas card, we also need the night sky (or day if you prefer), a light dusting of snow covering the ground, and a few snowflakes falling from the sky.\nNow lines, rectangles, circles, and triangles are all just simple geometric objects. Crucially, they’re all things that we can create with {ggplot2} in R."
  },
  {
    "objectID": "applied-insights/tutorials/posts/2023/12/12/xmas-cards.html#build-a-snowman-with-r",
    "href": "applied-insights/tutorials/posts/2023/12/12/xmas-cards.html#build-a-snowman-with-r",
    "title": "Creating Christmas cards with R",
    "section": "Build a snowman with R",
    "text": "Build a snowman with R\nLet’s start with the background. The easiest way to start with a blank canvas in {ggplot2} is to create an empty plot using ggplot() with no arguments. We can also remove all theme elements (such as the grey background and grid lines) with theme_void(). To change the background colour to a dark blue for the night sky, we can edit the plot.background element of the theme using element_rect() (since the background is essentially just a big rectangle).\nIn {ggplot2} fill is the inner colour of shapes whilst colour is the outline colour. You can specify colours in different ways in R: either via the rgb() function, using a character string for a hex colour such as \"#000000\", or using a named colour. If you run colors(), you’ll see all the valid named colours you can use. Here, we’ve picked \"midnightblue\".\nLet’s save this initial plot as an object s1 that we’ll keep adding layers to. Saving plots in different stages of styling as objects can help to keep your code more modular.\ns1 &lt;- ggplot() +\n  theme_void() +\n  theme(\n    plot.background = element_rect(\n      fill = \"midnightblue\"\n      )\n  )\ns1\nNext we’ll add some snow on the ground. We’ll do this by drawing a white rectangle along the bottom of the plot. There are two different functions that we could use to add a rectangle: geom_rect() or annotate(). The difference between the two is that geom_rect() maps columns of a data.frame to different elements of a plot whereas annotate() can take values passed in as vectors. Most of the {ggplot2} graphs you’ll see will use geom_*() functions. However, if you’re only adding one or two elements to a plot then annotate() might be quicker.\nSince we’re only adding one rectangle for the snow, it’s easier to use annotate() with the \"rect\" geometry. This requires four arguments: the minimum and maximum x and y coordinates of the rectangle – essentially specifying where the corners are. We can also change the colour of the rectangle and its outline using the fill and colour arguments. Here, I’ve used a very light grey instead of white.\nIf we don’t set the axis limits using xlim() and ylim(), the plot area will resize to fit the area of the snow rectangle. The night sky background will disappear. You can choose any axis limits you wish here – but the unit square will make it easier to find the right coordinates when deciding where to position other elements. Finally, we add coord_fixed() to fix the 1:1 aspect ratio and make sure our grid is actually square with expand = FALSE to remove the additional padding at the sides of the plot.\ns2 &lt;- s1 +\n  annotate(\n    geom = \"rect\",\n    xmin = 0, xmax = 1,\n    ymin = 0, ymax = 0.2,\n    fill = \"grey98\",\n    colour = \"grey98\"\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  coord_fixed(expand = FALSE)\ns2\n\n\n\n\n\n\n\n\n\n\nTo finish off the background, we’ll add some falling snowflakes. We first need to decide where on the plot the snowflakes will appear. We’ll be plotting lots of snowflakes, so manually typing out the coordinates of where they’ll be would be very inefficient. Instead, we can use functions to generate the locations randomly. For this we’ll use the uniform distribution. The uniform distribution has two parameters – the lower and upper bounds where any values between the bounds are equally likely. You can generate samples from a uniform distribution in R using the runif() function.\nWhen generating random numbers in R (or any other programming language), it’s important to set a seed. This means that if you give your code to someone else, they’ll get the same random numbers as you. Some people choose to use the date as the random seed and since we’re making Christmas cards, we’ll use Christmas day as the random seed – in yyyymmdd format, of course!\nWe create a variable n specifying how many snowflakes we’ll create. Creating a variable rather than hard coding the variables makes it easier to vary how many snowflakes we want. Since our plot grid goes between 0 and 1 in both the x and y directions, we generate random numbers between 0 and 1 for both the x and y coordinates and store the values in a data.frame called snowflakes.\nset.seed(20231225)\nn &lt;- 100\nsnowflakes &lt;- data.frame(\n  x = runif(n, 0, 1),\n  y = runif(n, 0, 1)\n)\nNow we can plot the snowflakes data using geom_point() – the same function you’d use for a scatter plot. Since we’re using a geom_*() function, we need to tell {ggplot2} which columns go on the x and y axes inside the aes() function. To plot the snowflakes, we’re going to make using of R’s different point characters. The default when plotting with geom_point() is a small black dot, but we can choose to use a small star (close enough to a snowflake!) by setting pch = 8 and changing the colour to \"white\".\ns3 &lt;- s2 +\n  geom_point(\n    data = snowflakes,\n    mapping = aes(\n      x = x,\n      y = y\n    ),\n    colour = \"white\",\n    pch = 8\n  )\ns3\nNow comes the part where we start rolling up some snowballs! Or, in the case of an R snowman, we draw some circles. Unfortunately, there isn’t a built-in geom_*() function in {ggplot2} for plotting circles. We could use geom_point() here and increase the size of the points but this approach can look a little bit fuzzy when the points are very large. Instead, we’ll turn to a {ggplot2} extension package for some additional geom_* functions - {ggforce} (Pedersen 2022).\nThe geom_circle() function requires at least three elements mapped to the aesthetics inside aes(): the coordinates of the centre of the circle given by x0 and y0, and the radii of each of the circles, r. Instead of creating a separate data frame and passing it into geom_circle(), we can alternatively create the data frame inside the function. The fill and colour arguments work as they do in {ggplot2} and we can set both to \"white\".\ns4 &lt;- s3 +\n  geom_circle(\n    data = data.frame(\n      x0 = c(0.6, 0.6),\n      y0 = c(0.3, 0.5),\n      r = c(0.15, 0.1)\n    ),\n    mapping = aes(x0 = x0, y0 = y0, r = r),\n    fill = \"white\",\n    colour = \"white\"\n  )\ns4\n\n\n\n\n\n\n\n\n\n\nWe can use geom_point() again to add some more points to represent the buttons and the eyes. Here, we’ll manually specify the coordinates of the points. For the buttons we add them in a vertical line in the middle of the snowman’s body circle, and for the eyes we add them in a horizontal line in the middle of the head circle.\nSince no two rocks are exactly the same size, we can add some random variation to the size of the points using runif() again. We generate five different sizes between 2 and 4.5. For reference, the default point size is 1.5. Adding scale_size_identity() means that the sizes of the points are actually equally to the sizes we generated from runif() and removes the legend that is automatically added when we add size inside aes().\ns5 &lt;- s4 +\n  geom_point(\n    data = data.frame(\n      x = c(0.6, 0.6, 0.6, 0.57, 0.62),\n      y = c(0.25, 0.3, 0.35, 0.52, 0.52),\n      size = runif(5, 2, 4.5)\n    ),\n    mapping = aes(x = x, y = y, size = size)\n  ) +\n  scale_size_identity()\ns5\nTo add sticks for arms, we can make use of geom_segment() to draw some lines. We could also use geom_path() but that is designed to connect points across multiple cases, whereas geom_segment() draws a single line per row of data – and we don’t want to join the snowman’s arms together!\nTo use geom_segment() we need to create a data frame containing the x and y coordinates for the start and end of each line, and then pass this into the aesthetic mapping with aes(). We can control the colour and width of the lines using the colour and linewidth arguments. Setting the lineend argument to \"round\" means that the ends of the lines will be rounded rather than the default straight edge.\ns6 &lt;- s5 + \n  geom_segment(\n    data = data.frame(\n      x = c(0.46, 0.7),\n      xend = c(0.33, 0.85),\n      y = c(0.3, 0.3),\n      yend = c(0.4, 0.4)\n    ),\n    mapping = aes(x = x, y = y, xend = xend, yend = yend),\n    colour = \"chocolate4\",\n    lineend = \"round\",\n    linewidth = 2\n  )\ns6\n\n\n\n\n\n\n\n\n\n\nWe’ll now add a (very simple) hat to our snowman, fashioned out of two rectangles. We can add the rectangles as we did before using the annotate() function and specifying the locations of the corners of the rectangles. We start with a shorter wider rectangle for the brim of the hat, and then a taller, narrower rectangle for the crown of the hat. Since we’ll colour them both \"brown\", it doesn’t matter if they overlap a little bit.\nThis might be one of the situations we should have used geom_rect() instead of annotate() but it might take a lot of trial and error to position the hat exactly where we want it, and this seemed a little easier with annotate().\ns7 &lt;- s6 +\n  annotate(\n    geom = \"rect\",\n    xmin = 0.46, xmax = 0.74,\n    ymin = 0.55, ymax = 0.60,\n    fill = \"brown\"\n  ) +\n  annotate(\n    geom = \"rect\",\n    xmin = 0.50, xmax = 0.70,\n    ymin = 0.56, ymax = 0.73,\n    fill = \"brown\"\n  )\ns7\nNow we can move on to the final component of building a snowman – the carrot for his nose! We’re going to use a triangle for the nose. Unfortunately, there are no built-in triangle geoms in {ggplot2} so we’ll have to make our own. There are different ways to do this, but here we’re going to make use of the {sf} package (Pebesma 2018). The {sf} package (short for simple features) is designed for working with spatial data. Although we’re not working with maps, we can still use {sf} to make shapes – including polygons.\nWe start by constructing a matrix with two columns – one for x coordinates and one for y. The x coordinates start in the middle of the head and go slightly to the right for the triangle point. The y coordinates take a little bit more trial and error to get right. Note that although triangles only have three corners, we have four rows of points. The last row must be the same as the first to make the polygon closed. The matrix is then converted into a spatial object using the st_polygon() function, and we can check how it looks using plot().\nnose_pts &lt;- matrix(\n  c(\n    0.6, 0.5,\n    0.65, 0.48,\n    0.6, 0.46,\n    0.6, 0.5\n  ),\n  ncol = 2,\n  byrow = TRUE\n)\nnose &lt;- st_polygon(list(nose_pts))\nplot(nose)\n\n\n\n\n\n\n\n\n\n\nWe can plot sf objects with {ggplot2} using geom_sf(). geom_sf() is a slightly special geom since we don’t need to specify an aesthetic mapping for the x and y axes – they are determined automatically from the sf object along with which type of geometry to draw. If your sf object has points, points will be drawn. If it has country shapes, polygons will be drawn. Like other geom_*() functions, we can change the colour and fill arguments to a different colour – in this case \"orange\" to represent a carrot!\nYou should see a Coordinate system already present. Adding new coordinate system, which will replace the existing one. message when you run the following code. The is because geom_sf forces it’s own coordinate system on the plot overriding our previous code specifying coord_fixed(). If you run it without the coord_sf(expand = FALSE), the extra space around the plot will reappear. We can remove it again with expand = FALSE.\ns8 &lt;- s7 +\n  geom_sf(\n    data = nose,\n    fill = \"orange\",\n    colour = \"orange\"\n  ) +\n  coord_sf(expand = FALSE)\ns8\n\nYou could skip the sf part of this completely and pass the coordinates directly into geom_polygon() instead. However, I’ve often found it quicker and easier to tinker with polygon shapes using sf.\n\nA key part of any Christmas card is the message wishing recipients a Merry Christmas! We can add text to our plot using the annotate() function and the \"text\" geometry (you could instead use geom_text() if you prefer). When adding text, we require at least three arguments: the x and y coordinates of where the text should be added, and the label denoting what text should appear. We can supply additional arguments to annotate() to style the text, such as: colour (which changes the colour of the text); family (to define which font to use); fontface (which determines if the font is bold or italic, for example); and size (which changes the size of the text). The \"mono\" option for family tells {ggplot2} to use the default system monospace font.\ns9 &lt;- s8 +\n  annotate(\n    geom = \"text\",\n    x = 0.5, y = 0.07,\n    label = \"Merry Christmas\",\n    colour = \"red3\",\n    family = \"mono\",\n    fontface = \"bold\", size = 7\n  )\ns9"
  },
  {
    "objectID": "applied-insights/tutorials/posts/2023/12/12/xmas-cards.html#sending-christmas-cards-in-r",
    "href": "applied-insights/tutorials/posts/2023/12/12/xmas-cards.html#sending-christmas-cards-in-r",
    "title": "Creating Christmas cards with R",
    "section": "Sending Christmas cards in R",
    "text": "Sending Christmas cards in R\nNow that we’ve finished creating our Christmas card, we need to think about how to send it. You could save it as an image file using ggsave(), print it out, and send it in the post. Or you could also use R to send it!\nThere are many different R packages for sending emails from R. If you create a database of email addresses and names, you could personalise the message on the Christmas card and then send it automatically as an email from R. If you want to automate the process of sending physical cards from R, you might be interested in the {ggirl} package from Jacqueline Nolis (Nolis 2023). {ggirl} allows you to send postcards with a ggplot object printed on the front. {ggirl} is also an incredible example of an eCommerce platform built with R! Note that {ggirl} can currently only send physical items to addresses in the United States."
  },
  {
    "objectID": "applied-insights/tutorials/posts/2023/12/12/xmas-cards.html#other-christmas-r-packages",
    "href": "applied-insights/tutorials/posts/2023/12/12/xmas-cards.html#other-christmas-r-packages",
    "title": "Creating Christmas cards with R",
    "section": "Other Christmas R packages",
    "text": "Other Christmas R packages\nIf you’re curious about making Christmas cards with R but you don’t have the time to make them from scratch, you’ll likely find the christmas R package (Barrera-Gomez 2022) helpful. This package from Jose Barrera-Gomez can generate lots of different Christmas cards, many of them animated and available in different languages (English, Catalan and Spanish).\nEmil Hvitfeldt has also created a Quarto extension that gives the effect of falling snowflakes on HTML outputs – including revealjs slides which is perfect for festive presentations!\nHave you made your own Christmas cards with R? We’d love to see your designs!\n\n\n\n\n\n\nInspired by Nicola’s tutorial, Real World Data Science has indeed made its own Christmas card design. Check out our attempt over at the Editors’ Blog!\n\n\n\n\nExplore more Tutorials\n\n\n\n\n\nAbout the author\n\nNicola Rennie is a lecturer in health data science in the Centre for Health Informatics, Computing, and Statistics (CHICAS) within Lancaster Medical School at Lancaster University. She’s an R enthusiast, data visualisation aficionado, and generative artist, among other things. Her personal website is hosted at nrennie.rbind.io, and she is a co-author of the Royal Statistical Society’s Best Practices for Data Visualisation.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Nicola Rennie\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nRennie, Nicola. 2023. “Creating Christmas cards with R.” Real World Data Science, December 12, 2023."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/06/15/road-to-reproducible-research.html",
    "href": "applied-insights/case-studies/posts/2023/06/15/road-to-reproducible-research.html",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "",
    "text": "Reproducibility, or “the ability of a researcher to duplicate the results of a prior study using the same materials as the original investigator”, is critical for sharing and building upon scientific findings. Reproducibility not only verifies the correctness of processes leading to results but also serves as a prerequisite for assessing generalisability to other datasets or contexts. This we refer to as replicability, or “the ability of a researcher to duplicate the results of a prior study if the same procedures are followed but new data are collected”. Reproducibility, which is the focus of our work here, can be challenging – especially in the context of deep learning. This article, and associated material, aims to provide practical advice for overcoming these challenges.\nOur story begins with Davit Svanidze, a master’s degree student in economics at the London School of Economics (LSE). Davit’s efforts to make his bachelor’s thesis reproducible are what inspires this article, and we hope that readers will be able to learn from Davit’s experience and apply those learnings to their own work. Davit will demonstrate the use of Jupyter notebooks, GitHub, and other relevant tools to ensure reproducibility. He will walk us through code documentation, data management, and version control with Git. And, he will share best practices for collaboration, peer review, and dissemination of results.\nDavit’s story starts here, but there is much more for the interested reader to discover. At certain points in this article, we will direct readers to other resources, namely a Jupyter notebook and GitHub repository which contain all the instructions, data and code necessary to reproduce Davit’s research. Together, these components offer a comprehensive overview of the thought process and technical implementation required for reproducibility. While there is no one-size-fits-all approach, the principles remain consistent."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/06/15/road-to-reproducible-research.html#davits-journey-towards-reproducibility",
    "href": "applied-insights/case-studies/posts/2023/06/15/road-to-reproducible-research.html#davits-journey-towards-reproducibility",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "Davit’s journey towards reproducibility",
    "text": "Davit’s journey towards reproducibility\n\nMore power, please\nThe focus of my bachelor’s thesis was to better understand the initial spread of Covid-19 in China using deep learning algorithms. I was keen to make my work reproducible, but not only for my own sake. The “reproducibility crisis” is a well-documented problem in science as a whole,1 2 3 4 with studies suggesting that around one-third of social science studies published between the years 2010 and 2015 in top journals like Nature and Science could not be reproduced.5 Results that cannot be reproduced are not necessarily “wrong”. But, if findings cannot be reproduced, we cannot be sure of their validity.\nFor my own research project, I gathered all data and started working on my computer. After I built the algorithms to train the data, my first challenge to reproducibility was computational. I realised that training models on my local computer was taking far too long, and I needed a faster, more powerful solution to be able to submit my thesis in time. Fortunately, I could access the university server to train the algorithms. Once the training was complete, I could generate the results on my local computer, since producing maps and tables was not so demanding. However…\n\n\nBloody paths!\nIn switching between machines and computing environments, I soon encountered an issue with my code: the paths, or file directory locations, for the trained algorithms had been hardcoded! As I quickly discovered, hardcoding a path can lead to issues when the code is run in a different environment, as the path might not exist in the new environment.\nAs my code became longer, I overlooked the path names linked to algorithms that were generating the results. This mistake – which would have been easily corrected if spotted earlier – resulted in incorrect outputs. Such errors could have enormous (negative) implications in a public health context, where evidence-based decisions have real impacts on human lives. It was at this point that I realised that my code is the fundamental pillar of the validity of my empirical work. How can someone trust my work if they are not able to verify it?\nThe following dummy code demonstrates the hardcoding issue:\n```{python}\n# Hardcoded path\nfile_path = \"/user/notebooks/toydata.csv\"\ntry:\n    with open(file_path) as file:\n        data = file.read()\n        print(data)\nexcept FileNotFoundError:\n    print(\"File not found\")\n```\n\nIn the code above, a dummy file (toydata.csv) is used. The dummy file contains data on the prices of three different toys, but only the path of the file is relevant to this example. If the hardcoded file path – \"/user/notebooks/toydata.csv\" – exists on the machine being used, the code will run just fine. But, when run in a different environment without said path, the code will result in a \"File not found error\". Better code that uses relative paths can be written as:\n```{python}\n# Relative path\nimport os\n\nfile_path = os.path.join(os.getcwd(), \"toydata.csv\")\ntry:\n    with open(file_path) as file:\n        data = file.read()\n        print(data)\nexcept FileNotFoundError:\n    print(\"File not found\")\n```\n\nYou can see that this code has successfully imported data from the dataset toydata.csv and printed its two columns (toy and price) and three rows.\nThe following example is a simplified version of what happened when I wrote code to train several models, store the results and run a procedure to compare results with the predictive performance of a benchmark model:\n```{python}\n# Set an arbitrary predictive performance value of a benchmark model\n# and accept/reject models if the results are above/below the value.\nbenchmark = 50\n# Set the model details in one place for a better overview\nmodel = {\n    \"model1\": {\"name\": \"model1\", \"type\": \"simple\"}, \n    \"model2\": {\"name\": \"model2\", \"type\": \"complex\"}\n}\n# Set the current model to \"model1\" to use it for training and check its results\ncurrent_model = model[\"model1\"]\n# Train a simple model for \"model1\" and a complex model for \"model2\"\n# Training result of the \"model1\" is 30 and for \"model2\" is 70\nmodel_structure = train(current_model[\"type\"])\n# Save the model and its result in a .csv file\nmodel_structure.to_csv('/all/notebooks/results-of-model1.csv', index=False)\n```\n```{python}\n# Load the model result and compare with benchmark\nprint(\"Model name: {}\".format(current_model[\"name\"]))\nprint(\"Model type: {}\".format(current_model[\"type\"]))\n# Load the result of the current model\nresult = pd.read_csv('/all/notebooks/results-of-model2.csv').iloc[0, 0]\nprint(\"Result: {}\".format(result))\n\nif result &gt; benchmark:\n    print(\"\\033[3;32m&gt;&gt;&gt; Result is better than the benchmark -&gt; Accept the model and use it for calculations\")\nelse:\n    print(\"\\033[3;31m&gt;&gt;&gt; Result is NOT better than the benchmark -&gt; Reject the model as it is not optimal\")\n```\n\nEverything looks fine at a glance. But, if you examine the code carefully, you may spot the problem. Initially, when I coded the procedure (training the model, saving and loading the results), I hardcoded the paths and had to change them for each tested model. First, I trained model2, a complex model, and tested it against the benchmark (70 &gt; 50 → accepted). I repeated the procedure for model1 (a simple model). Its result was identical to model2, therefore I kept model1 following the parsimony principle.\nHowever, for the code line loading the result for the current model (line 5, second cell), I forgot to amend the path and so mistakenly loaded the result of model2. As a consequence, I accepted a model which should have been rejected. These wrong results were then spread further in the code, including all charts and maps and the conclusions of my analysis.\nA small coding error like this can therefore be fatal to an analysis. Below is the corrected code:\n```{python}\nimport os\n# Set an arbitrary predictive performance value of a benchmark model\n# and accept/reject models if the results are above/below the value.\nbenchmark = 50\n# Set the model details (INCLUDING PATHS) in one place for a better overview\nmodel = {\n    \"model1\": {\"name\": \"model1\", \"type\": \"simple\", \"path\": os.path.join(os.getcwd(), \"results-of-model1.csv\")}, \n    \"model2\": {\"name\": \"model2\", \"type\": \"complex\", \"path\": os.path.join(os.getcwd(), \"results-of-model2.csv\")}\n}\n# Set the current model to \"model1\" to use it for training and check its results\ncurrent_model = model[\"model1\"]\n# Train a simple model for \"model1\" and a complex model for \"model2\"\n# Training result of the \"model1\" is 30 and for \"model2\" is 70\nmodel_structure = train(current_model[\"type\"])\n# Save the model and its result in a .csv file\nmodel_structure.to_csv(current_model[\"path\"], index=False)\n```\n```{python}\n# Get the model result and compare with the benchmark\nprint(\"Model name: {}\".format(current_model[\"name\"]))\nprint(\"Model type: {}\".format(current_model[\"type\"]))\n# Load the result of the current model WITH a VARIABLE PATH\nresult = pd.read_csv(current_model[\"path\"]).iloc[0, 0]\nprint(\"Result: {}\".format(result))\n\nif result &gt; benchmark:\n    print(\"\\033[3;32m&gt;&gt;&gt; Result is better than the benchmark -&gt; Accept the model and use it for calculations\")\nelse:\n    print(\"\\033[3;31m&gt;&gt;&gt; Result is NOT better than the benchmark -&gt; Reject the model as it is not optimal\")\n```\n\nHere, the paths are stored with other model details (line 7–8, first cell). Therefore, we can use them as variables when we need them (e.g., line 16, first cell, and line 5, second cell). Now, when the current model is set to model1 (line 11, first cell), everything is automatically adjusted. Also, if the path details need to be changed, we only need to change them once and everything else is automatically adjusted and updated. The code now correctly states that model1 performs worse than the benchmark and is therefore rejected and we should keep model2, which performs best.\nI managed to catch this error in time, but it often can be difficult to spot our own mistakes. That is why making code available to others is crucial. A code review by a second (or third) pair of eyes can save everyone a lot of time and avoid spreading incorrect results and conclusions.\n\n\nSolving compatibility chaos with Docker\nOne might think that it would be easy to copy code from one computer to another and run it without difficulties, but it turns out to be a real headache. Different operating systems on my local computer and the university server caused multiple compatibility issues and it was very time-consuming to try to solve them. The university server was running on Ubuntu, a Linux distribution, which was not compatible with my macOS-based code editor. Moreover, the server did not support the Python programming language – and all the deep learning algorithm packages that I needed – in the same way as my macOS computer did.\nAs a remedy, I used Docker containers, which allowed me to create a virtual environment with all the necessary packages and dependencies installed. This way, I could integrate them with different hardware and use the processing power of that hardware. To get started with Docker, I first had to install it on my local computer. The installation process is straightforward and the Docker website provides step-by-step instructions for different operating systems. In fact, I found the Docker website very helpful, with lots of resources and tutorials available. Once Docker was installed, it was easy to create virtual environments for my project and work with my code, libraries, and packages, without any compatibility issues. Not only did Docker containers save me a lot of time and effort, but they could also make it easier for others to reproduce my work.\nBelow is an example of a Dockerfile which recreates an environment with Python 3.7 on Linux. It describes what, how, when and in which order operations should be carried out to generate the environment with all Python packages required to run the main Python script, main.py.\n\n\nAn example of a Dockerfile.\n\nIn this example, by downloading the project, including the Dockerfile, anyone can run main.py without installing packages or worrying about what OS was used for development or which Python version should be installed. You can view Docker as a great robot chef: show it a recipe (Dockerfile), provide the ingredients (project files), push the start button (to build the container) and wait to sample the results.\n\n\nWhy does nobody check your code?\nEven after implementing Docker, I still faced another challenge to reproducibility: making the verification process for my code easy enough that it could be done by anyone, without them needing a degree in computer science! Increasingly, there is an expectation for researchers to share their code so that results can be reproduced, but there are as yet no widely accepted or enforced standards on how to make code readable and reusable. However, if we are to embrace the concept of reproducibility, we must write and publish code under the assumption that someone, somewhere – boss, team member, journal reviewer, reader – will want to rerun our code. And, if we expect that someone will want to rerun our code (and hopefully check it), we should ensure that the code is readable and does not take too long to run.\nIf your code does take too long to run, some operations can often be accelerated – for example, by reducing the size of the datasets or by implementing computationally efficient data processing approaches (e.g., using PyTorch). Aim for a running time of a few minutes – or about as long as it takes to make a cup of tea or coffee. Of course, if data needs to be reduced to save computational time, the person rerunning your code won’t generate the same results as in your original analysis. This therefore will not lead to reproducibility, sensu stricto. However, as long as you state clearly what are the expected results from the reduced dataset, your peers can at least inspect your code and offer feedback, and this marks a step towards reproducibility.\nWe should also make sure our code is free from bugs – both the kind that might lead to errors in analysis and also those that stop the code running to completion. Bugs can occur for various reasons. For example, some code chunks written on a Windows machine may not properly execute on a macOS machine because the former uses \\ for file paths, while the latter uses /:\n```{python}\n# Path works on macOS/Linux\nwith open(\"../../all/notebooks/toydata.csv\", \"r\") as f:\n    print(f.read())\n\n# Path works only on Windows    \nwith open(r\"..\\..\\all\\notebooks\\toydata.csv\", \"r\") as f:\n   print(f.read())\n```\n\nHere, only the macOS/Linux version works, since the code this capture was taken from was implemented on a Linux server. There are alternatives, however. The code below works on macOS, Linux, and also Windows machines:\n```{python}\nfrom pathlib import Path\n\n# Path works on every OS: macOS/Linux/Windows\n# It will automatically replace the path to \"..\\..\\all\\notebooks\\toydata.csv\" when it runs on Windows\nwith open(Path(\"../../all/notebooks/toydata.csv\"), \"r\") as f:\n    print(f.read())\n```\n\nThe extra Python package, pathlib, is of course unnecessary if you build a Docker container for your project, as discussed in the previous section.\n\n\nJupyter, King of the Notebooks\nBy this stage in my project, I was feeling that I’d made good progress towards ensuring that my work would be reproducible. I’d expended a lot of effort to make my code readable, efficient, and also absent of bugs (or, at least, this is what I was hoping for). I’d also built a Docker container to allow others to replicate my computing environment and rerun the analysis. Still, I wanted to make sure there were no barriers that would prevent people – my supervisors, in particular – from being able to review the work I had done for my undergraduate thesis. What I wanted was a way to present a complete narrative of my project that was easy to understand and follow. For this, I turned to Jupyter Notebook.\n\n\n\n\n\n\nCredit: Discord software, Midjourney bot.\n\nJupyter notebooks combine Markdown text, code, and visualisations. The notebook itself can sit within an online directory of folders and files that contain all the data and code related to a project, allowing readers to understand the processes behind the work and also access the raw resources. From the notebook I produced, readers can see exactly what I did, how I did it, and what my results were.\nWhile creating my notebook, I was able to experiment with my code and iterate quickly. Code cells within a document can be run interactively, which allowed me to try out different approaches to solving a problem and see the results almost in real time. I could also get feedback from others and try out new ideas without having to spend a lot of time writing and debugging code.\n\n\nVersion control with Git and GitHub\nMy Jupyter notebook and associated folders and files are all available via GitHub. Git is a version control system that allows you to keep track of changes to your code over time, while GitHub is a web-based platform that provides a central repository for storing and sharing code. With Git and GitHub, I was able to version my code and collaborate with others without the risk of losing any work. I really couldn’t afford to redo the entire year I spent on my dissertation!\nGit and GitHub are great for reproducibility. By sharing code via these platforms, others can access your work, verify it and reproduce your results without risking changing or, worse, destroying your work – whether partially or completely. These tools also make it easy for others to build on your work if they want to further develop your research. You can also use Git and GitHub to share or promote your results across a wider community. The ability to easily store and share your code also makes it easy to keep track of the different versions of your code and to see how your work has evolved.\nThe following illustration shows the tracking of very simple changes in a Python file. The previous version of the code is shown on the left; the new version is shown on the right. Additions and deletions are highlighted in green and red, and with + and - symbols, respectively.\n\n\nA simple example of GitHub version tracking."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/06/15/road-to-reproducible-research.html#the-deep-learning-challenge",
    "href": "applied-insights/case-studies/posts/2023/06/15/road-to-reproducible-research.html#the-deep-learning-challenge",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "The deep learning challenge",
    "text": "The deep learning challenge\nSo far, this article has dealt with barriers to reproducibility – and ways around them – that will apply to most, if not all, modern research projects. While I’d encourage any scientist to adopt these practices in their own work, it is important to stress that these alone cannot guarantee reproducibility. In cases where standard statistical procedures are used within statistical software packages, reproducibility is often achievable. However, in reality, even when following the same procedures, differences in outputs can occur, and identifying the reasons for this may be challenging. Cooking offers a simple analogy: subtle changes in room temperature or ingredient quality from one day to the next can impact the final product.\nOne of the challenges for research projects employing machine learning and deep learning algorithms is that outputs can be influenced by the randomness that is inherent in these approaches. Consider the four portraits below, generated by the Midjourney bot.\n\n\n\n\n\n\nCredit: Discord software, Midjourney bot.\n\nEach portrait looks broadly similar at first glance. However, upon closer inspection, critical differences emerge. These differences arise because deep learning models rely on numerous interconnected layers to learn intricate patterns and representations. Slight random perturbations, such as initial parameter values or changes in data samples, can propagate through the network, leading to different decisions during the learning process. As a result, even seemingly negligible randomness can amplify and manifest as considerable differences in the final output, as with the distinct features of the portraits.\nRandomness is not necessarily a bad thing – it mitigates overfitting and helps predictions to be generalised. However, it does present an additional barrier to reproducibility. If you cannot get the same results using the same raw materials – data, code, packages and computing environment – then you might have good reasons to doubt the validity of the findings.\nThere are many elements of an analysis in which randomness may be present and lead to different results. For example, in a classification (where your dependent variable is binary, e.g., success/failure with 1 and 0) or a regression (where your dependent variable is continuous, e.g., temperature measurements of 10.1°C, 2.8°C, etc.), you might need to split your data into training and testing sets. The training set is used to estimate the model (hyper)parameters and the testing set is used to compute the performance of the model. The way the split is usually operationalised is as a random selection of rows of your data. So, in principle, each time you split your data into training and testing sets, you may end up with different rows in each set. Differences in the training set may therefore lead to different values of the model (hyper)parameters and affect the predictive performance that is measured from the testing set. Also, differences in the testing set may lead to variations in the predictive performance scores, which in turn lead to potentially different interpretations and, ultimately, decisions if the results are used for that purpose.\nThis aspect of randomness in the training of models is relatively well known. But randomness may hide in other parts of code. One such example is illustrated below. Here, using Python, we set the seed number to 0 using np.random.seed(seed value). The random.seed() function from the package numpy (abbreviated np) saves the state of a random function so that it can create identical random numbers independently of the machine you use, and this is for any number of executions. A seed value is an initial input or starting point used by a pseudorandom number generator to generate a sequence of random numbers. It is often an integer or a timestamp. The number generator takes this seed value and uses it to produce a deterministic series of random numbers that appear to be random but can be recreated by using the same seed value. Without providing this seed value, the first execution of the function typically uses the current system time. The animation below generates two random arrays arr1 and arr2 using np.random.rand(3,2). Note that the values 3,2 indicate that we want random values for an array that has 3 rows and 2 columns.\n```{python}\nimport numpy as np\n\n#Set the seed number e.g. to 0\nnp.random.seed(0)\n# Generate random array\narr1 = np.random.rand(3,2)\n## print(\"Array 1:\")\n## print(arr1)\n\n#Set the seed number as before to get the same results\nnp.random.seed(0)\n# Generate another random array\narr2 = np.random.rand(3,2)\n## print(\"\\nArray 2:\")\n## print(arr2)\n```\n\nIf you run the code yourself multiple times, the values of arr1 and arr2 should remain identical. If this is not the case, check that the seed value is set to 0 in lines 4 and 11. These identical results are possible because we set the seed value to 0, which ensures that the random number generator produces the same sequence of numbers each time the code is run. Now, let’s look at what happens if we remove the line np.random.seed(0):\n```{python}\n#Generate random array\narr1 = np.random.rand(3,2)\n## print(\"Array 1:\")\n## print(arr1)\n\n#Generate another random array\narr2 = np.random.rand(3,2)\n## print(\"\\nArray 2:\")\n## print(arr2)\n```\n\nHere, the values of arr1 and arr2 will be different each time we run the code since the seed value was not set and is therefore changing over time.\nThis short code demonstrates how randomness that can be controlled by the seed value may affect your code. Therefore, unless randomness is required, e.g., to get some uncertainty in the results, setting the seed value will contribute to making your work reproducible. I also find it helpful to document the seed number I use in my code so that I can easily reproduce my findings in the future. If you are currently working on some code that involves random number generators, it might be worth checking your code and making all necessary changes. In our work (see code chunk 9 in the Jupyter notebook) we set the seed value in a general way, using a framework (config) so that our code always uses the same seed to train our algorithm."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/06/15/road-to-reproducible-research.html#conclusion",
    "href": "applied-insights/case-studies/posts/2023/06/15/road-to-reproducible-research.html#conclusion",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "Conclusion",
    "text": "Conclusion\nWe hope you have enjoyed learning more about our quest for reproducibility. We have explained why reproducibility matters and provided tips for how to achieve it – or, at least, work towards it. We have introduced a few important issues that you are likely to encounter on your own path to reproducibility. In sum, we have mentioned:\n\nThe importance of having relative instead of hard-coded paths in code.\nOperating system compatibility issues, which can be solved by using Docker containers for a consistent computing environment.\nThe convenience of Jupyter notebooks for code editing – particularly useful for data science projects and work using deep learning because of the ability to include text and code in the same document and make the work accessible to everyone (so long as they have an internet connection).\nThe need for version control using, for example, Git and GitHub, which allows you to keep track of changes in your code and collaborate with others efficiently.\nThe importance of setting the seed values in random number generators.\n\nThe graphic below provides a visual overview of the different components of our study and shows how each component works with the others to support reproducibility.\n\nWe use (A) the version control system, Git, and its hosting service, GitHub, which enables a team to share code with peers, efficiently track and synchronise code changes between local and server machines, and reset the project to a working state in case something breaks. Docker containers (B) include all necessary objects (engine, data, and scripts). Docker needs to be installed (plain-line arrows) by all users (project leader, collaborator(s), reviewer(s), and public user(s)) on their local machines (C); and (D) we use a user-friendly interface (JupyterLab) deployed from a local machine to facilitate the operations required to reproduce the work. The project leader and collaborators can edit (upload/download) the project files stored on the GitHub server (plain-line arrows) while reviewers and public users can only read the files (dotted-line arrows).\nNow, it is over to you. Our Jupyter notebook provides a walkthrough of our research. Our GitHub repository has all the data, code and other files you need to reproduce our work, and this README file will help you get started.\nAnd with that, we wish you all the best on the road to reproducibility!\n\nFind more case studies\n\n\n\n\n\nAbout the authors\n\nDavit Svanidze is a master’s degree student in economics at the London School of Economics (LSE). Andre Python is a young professor of statistics at Zhejiang University’s Center for Data Science. Christoph Weisser is a senior data scientist at BASF. Benjamin Säfken is professor of statistics at TU Clausthal. Thomas Kneib is professor of statistics and dean of research at the Faculty of Business and Economic Sciences at Goettingen University. Junfen Fu is professor of pediatrics, chief physician and director of the Endocrinology Department of Children’s Hospital, Zhejiang University, School of Medicine.\n\n\n\n\n\nAcknowledgement\n\nAndre Python has been funded by the National Natural Science Foundation of China (82273731), the National Key Research and Development Program of China (2021YFC2701905) and Zhejiang University global partnership fund (188170-11103).\n\n\n\n\n\nCopyright and licence\n\n© 2023 Davit Svanidze, Andre Python, Christoph Weisser, Benjamin Säfken, Thomas Kneib, and Junfen Fu.\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nSvanidze, Davit, Andre Python, Christoph Weisser, Benjamin Säfken, Thomas Kneib, and Junfen Fu. 2023. “The road to reproducible research: hazards to avoid and tools to get you there safely.” Real World Data Science, June 15, 2023. URL"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/06/15/road-to-reproducible-research.html#footnotes",
    "href": "applied-insights/case-studies/posts/2023/06/15/road-to-reproducible-research.html#footnotes",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "References",
    "text": "References\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational Science.” Science 334 (6060): 1226–1227.↩︎\nIoannidis, John P. A., Sander Greenland, Mark A. Hlatky, Muin J. Khoury, Malcolm R. Macleod, David Moher, Kenneth F. Schulz, and Robert Tibshirani. 2014. “Increasing Value and Reducing Waste in Research Design, Conduct, and Analysis.” The Lancet 383 (9912): 166–175.↩︎\nOpen Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716.↩︎\nBaker, Monya. 2016. “Reproducibility Crisis?” Nature 533 (26): 353–366.↩︎\nCamerer, Colin F., Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, Gideon Nave, Brian A. Nosek, Thomas Pfeiffer, et al. 2018. “Evaluating the Replicability of Social Science Experiments in Nature and Science between 2010 and 2015.” Nature Human Behaviour 2: 637–644.↩︎"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/11/16/learning-from-failure.html",
    "href": "applied-insights/case-studies/posts/2023/11/16/learning-from-failure.html",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "",
    "text": "Incarcerated youth are an exceptionally vulnerable population, and body-worn cameras are an important tool of accountability both for those incarcerated and the staff who supervise them. In 2018 the Texas Juvenile Justice Department (TJJD) deployed body-worn cameras for the first time, and this is a case study of how the agency developed a methodology for measuring the success of the camera rollout. This is also a case study of analysis failure, as it became clear that real-world implementation problems were corrupting the data and rendering the methodology unusable. However, the process of working through the causes of this failure helped the agency identify previously unrecognized problems and ultimately proved to be of great benefit. The purpose of this case study is to demonstrate how negative findings can still be incredibly useful in real-world settings."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/11/16/learning-from-failure.html#background",
    "href": "applied-insights/case-studies/posts/2023/11/16/learning-from-failure.html#background",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "Background",
    "text": "Background\nFrom the outset of the rollout of body-worn cameras, TJJD faced a major issue with implementation: in 2019, body worn cameras were an established tool for law enforcement, but there was very little literature or best practice to draw from for their use in a correctional environment. Unlike police officers, juvenile correctional officers (JCOs) deal directly with their charges for virtually their entire shift. In an eight-hour shift, a police officer might record a few calls and traffic stops. A juvenile correctional officer, on the other hand, would record for almost eight consecutive hours. And, because TJJD recorded round-the-clock for hundreds of employees at a time, this added up very quickly to a lot of footage.\nFor example, a typical dorm in a correctional center might have four JCOs assigned to it. Across a single week, these four JCOs would be expected to record at least 160 hours of footage.\n\n\n\n\n\n\nFigure 1: Four JCOs x 40 hours per week = 160 hours of footage.\n\nThis was replicated across every dorm. Three dorms, for example, would produce nearly 500 hours of footage, as seen below.\n\n\n\n\n\n\nFigure 2: Three dorms x four JCOs x 40 hours per week = 480 hours of footage.\n\nFinally, we had more than one facility. Four facilities with three dorms each would produce nearly 2,000 hours of footage every week.\n\n\n\n\n\n\nFigure 3: Four facilities x three dorms x four JCOs x 40 hours per week = 1,960 hours of footage.\n\nIn actuality, we had a total of five facilities each with over a dozen dorms producing an anticipated 17,000 hours of footage every week – an impossible amount to monitor manually.\nAs a result, footage review had to be done in a limited, reactive manner. If our monitoring team received an incident report, they could easily zero in on the cameras of the officers involved and review the incident accordingly. But our executive team had hoped to be able to use the footage proactively, looking for “red flags” in order to prevent potential abuses instead of only responding to allegations.\nBecause the agency had no way of automating the monitoring of footage, any proactive analysis had to be metadata-based. But what to look for in the metadata? Once again, the lack of best-practice literature left us in the lurch. So, we brainstormed ideas for “red flags” and came up with the following that could be screened for using camera metadata:\n\nMinimal quantity of footage – our camera policy required correctional officers to have their cameras on at all times in the presence of youth. No footage meant they weren’t using their cameras.\nFrequently turning the camera on and off – a correctional officer working a dorm should have their cameras always on when around youth and not be turning them on and off repeatedly.\nLarge gaps between clips – it defeats the purpose of having cameras if they’re not turned on.\n\nIn addition, we came up with a fourth red flag, which could be screened for by comparing camera metadata with shift-tracking metadata:\n\nMismatch between clips recorded and shifts worked – the agency had very recently rolled out a new shift tracking software. We should expect to see the hours logged by the body cameras roughly match the shift hours worked."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/11/16/learning-from-failure.html#analysis-part-1-quality-control-and-footage-analysis",
    "href": "applied-insights/case-studies/posts/2023/11/16/learning-from-failure.html#analysis-part-1-quality-control-and-footage-analysis",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "Analysis, part 1: Quality control and footage analysis",
    "text": "Analysis, part 1: Quality control and footage analysis\nFor this analysis, I gathered the most recent three weeks of body-worn camera data – which, at the time, covered April 1–21, 2019. I also pulled data from Shifthound (our shift management software) covering the same time period. Finally, I gathered HR data from CAPPS, the system that most of the State of Texas used at the time for personnel management and finance.1 I then performed some quality control work, summarized in the dropdown box below.\n\n\n\n\n\n\nInitial quality control steps\n\n\n\n\n\nSkimR is a helpful R package for exploratory analysis that gives summary statistics for every variable in a data frame, including missing values. After using the skim function on clip data, shift data, and HR data, I noticed that the clip data had some missing values for employee ID. This was an error which pointed to data entry mistakes – body-worn cameras do not record footage on their own, after all, so employee IDs should be assigned to each clip.\nFrom here I compared the employee ID field in the clip data to the employee ID field in the HR data. Somewhat surprisingly, IDs existed in the clip data that did not correspond to any entries in the HR data, indicating yet more data entry mistakes – the HR data is the ground truth for all employee IDs. I checked the shift data for the same error – employee IDs that did not exist in the HR data – and found the same problem.\nAs well as employee IDs that did not exist in the HR data, I also looked for employee IDs in the footage and shift data which related to staff who were not actually employed between April 1–21, 2019. I found some examples of this, which indicated yet more errors: staff cannot use a body-worn camera or log a shift if they have yet to begin working or if they have been terminated (system permissions are revoked upon leaving employment).\nI made a list of every erroneous ID to pass off to HR and monitoring staff before excluding them from the subsequent analysis. In total, 10.6% of clips representing 11.3% of total footage had to be excluded due to these initial data quality issues, foreshadowing the subsequent data quality issues the analysis would uncover.\nThe full analysis script can be found on GitHub.\n\n\n\nIn order to operationalize the “red flags” from our brainstorming session, I needed to see what exactly the cameras captured in their metadata. The variables most relevant to our purposes were:\n\nClip start\nClip end\nCamera used\nWho was assigned to the camera at the time\nThe role of the person assigned to the camera\n\nUsing these fields, I first created the following aggregations per employee ID:\n\n\n\n\n\n\nNumber of clips = Number of clips recorded.\n\n\n\n\n\n\n\nDays with footage = Number of discrete dates that appear in these clips.\n\n\n\n\n\n\n\n\n\nFootage hours = Total duration of all shot footage.\n\n\n\n\n\n\n\nSignificant gaps = Number of clips where the previous clip’s end date was either greater than 15 minutes or less than eight hours before current clip’s start date.\n\n\n\n\n\nI used these aggregations to devise the following staff metrics:\n\n\n\n\n\n\nClips per day = Number of clips / Days with footage.\n\n\n\n\n\n\n\nFootage per day = Footage hours / Days with footage.\n\n\n\n\n\n\n\n\n\nAverage clip length = Footage hours / Number of clips.\n\n\n\n\n\n\n\nGaps per day = Gaps / Days with footage.\n\n\n\n\n\nOnce I established these metrics for each employee I looked at their respective distributions. Standard staff shift lengths at the time were eight hours. If staff were using their cameras appropriately, we would expect to see distributions centered around clip lengths of about an hour, eight or fewer clips per day, and 8-12 footage hours per day. We would also expect to see 0 large gaps.\n\n\nShow the code\n\n```{r}\nlibrary(tidyverse)\n\nFootage_Metrics_by_Employee &lt;- read_csv(\"Output/Footage Metrics by Employee.csv\")\n\nFootage_Metrics_by_Employee %&gt;% \n  select(-Clips, -Days_With_Footage, -Footage_Hours, -Gaps) %&gt;% \n  pivot_longer(-Employee_ID, names_to = \"Metric\", values_to = \"Value\") %&gt;% \n  ggplot(aes(x = Value)) +\n  geom_histogram() +\n  facet_wrap(~Metric, scales = \"free\")\n```\n\n\n\n\n\n\nBy eyeballing the distributions I could tell most staff were recording fewer than 10 clips per day, shooting about 0.5–2 hours for each clip, for a total of 2–10 hours of daily footage, with the majority of employees having less than one significant gap per day. Superficially, this appeared to provide evidence of widespread attempts at complying with the body-worn camera policy and no systemic rejection or resistance. If this were indeed the case, then we could turn our attention to individual outliers.\nFirst, though, we thought we would attempt to validate this initial impression by testing another assumption. If each employee works on average 40 hours per week – a substantial underestimate given how common overtime was – we should expect, over a three-week period, to see about 120 hours of footage per employee in the dataset. This is not what we found.\nAverage footage per employee was 70.2 hours over the three-week period, meaning that the average employee was recording less than 60% of shift hours worked. With so many hours going unrecorded for unknown reasons, we needed to investigate further.\nSurely the shift data would clarify this…"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/11/16/learning-from-failure.html#analysis-part-2-footage-and-shift-comparison",
    "href": "applied-insights/case-studies/posts/2023/11/16/learning-from-failure.html#analysis-part-2-footage-and-shift-comparison",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "Analysis, part 2: Footage and shift comparison",
    "text": "Analysis, part 2: Footage and shift comparison\nWith the data on shifts worked from our timekeeping system, I could theoretically compare actual shifts worked to the amount of footage recorded. If there were patterns in where the gaps in footage fell, that comparison might help to explain why.\nIn order to join the shift data to the camera data, I needed a common unit of analysis beyond “Employee ID.” Using only this value would produce a nonsensical table that joined up every clip of footage to every shift worked.\nFor example, let’s take employee #9001005 at Facility Epsilon between April 1–3. This employee has the following clips recorded during that time period:\n\n\n\n\nEmployee_ID\nClip_ID\nClip_Start\nClip_End\n\n\n\n\n9001005\n156421\n2019-04-01 05:54:34\n2019-04-01 08:34:34\n\n\n9001005\n155093\n2019-04-01 08:40:59\n2019-04-01 08:54:51\n\n\n9001005\n151419\n2019-04-01 09:03:16\n2019-04-01 11:00:30\n\n\n9001005\n153133\n2019-04-01 11:10:09\n2019-04-01 12:39:51\n\n\n9001005\n151088\n2019-04-01 12:57:51\n2019-04-01 14:06:44\n\n\n9001005\n150947\n2019-04-02 05:56:34\n2019-04-02 09:48:50\n\n\n9001005\n151699\n2019-04-02 09:54:23\n2019-04-02 12:17:15\n\n\n\n\nWe can join this to a similar table of shifts logged. This particular employee had the following shifts scheduled from April 1–3:\n\n\n\n\nEmployee_ID\nShift_ID\nShift_Start\nShift_End\n\n\n\n\n9001005\nE050603\n2019-04-01 06:00:00\n2019-04-01 14:00:00\n\n\n9001005\nE051303\n2019-04-02 06:00:00\n2019-04-02 14:00:00\n\n\n\n\nThe table shows two eight-hour morning shifts from 6:00 am to 2:00 pm. We can join the two tables together by ID on a messy many-to-many join, but that tells us nothing about how much they overlap (or fail to overlap) without extensive additional work. For example, we have a unique identifier for employee clip (Clip_ID) and employee shift (Shift_ID), but what we need is a unique identifier that can be used to join the two. Fortunately, for this particular data we can create a unique identifier since both clips and shifts are fundamentally measures of time. While Employee_ID is not in itself unique (i.e., one employee can have multiple clips attached to that ID), Employee_ID combined with time of day is unique. A person can only be in one place at a time, after all!\nTo reshape the data for joining, I created a function that takes any data frame with a start and end column and unfolds it into discrete units of time. Using the code below to create the “Interval_Convert” function, the shift data above for employee 9001005 converts into one entry per hour of the day per shift. As a result, two eight-hour shifts get turned into 16 employee hours (a sample of which is shown below).\n\n\nShow the code\n\n```{r}\nlibrary(sqldf)\nlibrary(lubridate)\n\nInterval_Convert &lt;- function(DF, Start_Col, End_Col, Int_Unit, Int_Length = 1) {\nbrowser()\n  Start_Col2 &lt;- enquo(Start_Col)\n  End_Col2 &lt;- enquo(End_Col)\n  \n  Start_End &lt;- DF %&gt;%\n    ungroup() %&gt;%\n    summarize(Min_Start = min(!!Start_Col2),\n              Max_End = max(!!End_Col2)) %&gt;%\n    mutate(Start = floor_date(Min_Start, Int_Unit),\n           End = ceiling_date(Max_End, Int_Unit))\n  \n  DF &lt;- DF %&gt;%\n    mutate(Single = !!Start_Col2 == !!End_Col2)\n  \n  Interval_Table &lt;- data.frame(Interval_Start = seq.POSIXt(Start_End$Start[1], Start_End$End[1], by = str_c(Int_Length, \" \", Int_Unit))) %&gt;%\n    mutate(Interval_End = lead(Interval_Start)) %&gt;%\n    filter(!is.na(Interval_End))\n  \n  by &lt;- join_by(Interval_Start &lt;= !!End_Col2, Interval_End &gt;= !!Start_Col2)  \n  \n  Interval_Data_Table &lt;- Interval_Table %&gt;% \n    left_join(DF, by) %&gt;% \n    mutate(Seconds_Duration_Within_Interval = if_else(!!End_Col2 &gt; Interval_End, Interval_End, !!End_Col2) -\n             if_else(!!Start_Col2 &lt; Interval_Start, Interval_Start, !!Start_Col2)) %&gt;%\n    filter(!(Single & Interval_End == !!Start_Col2),\n           as.numeric(Seconds_Duration_Within_Interval) &gt; 0)\n  \n  return(Interval_Data_Table)\n}\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterval_Start\nInterval_End\nEmployee_ID\nShift_ID\nShift_Start\nShift_End\nSeconds_Duration_Within_Interval\n\n\n\n\n2019-04-01 06:00:00\n2019-04-01 07:00:00\n9001005\nE050603\n2019-04-01 06:00:00\n2019-04-01 14:00:00\n3600 secs\n\n\n2019-04-01 07:00:00\n2019-04-01 08:00:00\n9001005\nE050603\n2019-04-01 06:00:00\n2019-04-01 14:00:00\n3600 secs\n\n\n2019-04-01 08:00:00\n2019-04-01 09:00:00\n9001005\nE050603\n2019-04-01 06:00:00\n2019-04-01 14:00:00\n3600 secs\n\n\n2019-04-01 09:00:00\n2019-04-01 10:00:00\n9001005\nE050603\n2019-04-01 06:00:00\n2019-04-01 14:00:00\n3600 secs\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\n\nThe footage could be converted in a similar manner, and in this way I could break down both the shift data and the clip data into an hour-by-hour view and compare them to one another. Using this new format, I joined together the full tables of footage and shifts to determine how much footage was recorded with no corresponding shift in the timekeeping system.\n\n\n\n\n\n\n\n\n\nHR_Location\nFootage_Hours_No_Shift\nEmployee_IDs_With_Missing_Shift\n\n\n\n\nAlpha\n1805\n122\n\n\nBeta\n3749\n114\n\n\nDelta\n1208\n133\n\n\nEpsilon\n2899\n157\n\n\nGamma\n4153\n170\n\n\n\n\nTo summarize what the table is telling us: Almost every employee has footage hours that do not match with logged shifts, totaling nearly 14,000 hours when you add up the Footage_Hours_No_Shift column. But what about the opposite case? How many shift hours were logged with no corresponding footage?\n\n\n\n\n\n\n\n\n\nHR_Location\nShift_Hours_No_Footage\nEmployee_IDs_With_Missing_Footage\n\n\n\n\nAlpha\n7338\n127\n\n\nBeta\n6014\n118\n\n\nDelta\n12830\n141\n\n\nEpsilon\n9000\n168\n\n\nGamma\n11960\n183\n\n\n\n\nOh dear. Again, almost every employee has logged shift hours with no footage: 47,000 hours in total. To put it another way, that’s an entire work week per employee not showing up in camera footage.\nAt this point, we could probably rule out deliberate noncompliance. The clip data already implied that most employees were following the policy, and our facility leadership would surely have noticed a mass refusal large enough to show up this clearly in the data.\nOne way to check for deliberate noncompliance would be to first exclude shifts that contain zero footage whatsoever. This would rule out total mismatches, where – for whatever reason – the logged shifts had totally failed to overlap with recorded clips. For the remaining shifts that do contain footage, we could look at the proportion of the shift covered by footage. So, if an eight-hour shift had four hours of recorded footage associated with it, then we could say that 50% of the shift had been recorded. The following histogram is a distribution of the number of employees organized by the percent of their shift-hours they recorded (but only shifts that had a nonzero amount of footage).\n\n\n\n\n\nAs it turned out, most employees recorded the majority of their matching shifts, a finding that roughly aligns with the initial clip analysis. So, what explains the 14,000 hours of footage with no shifts, and the 47,000 hours of shifts with no footage?"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/11/16/learning-from-failure.html#causes-of-failure",
    "href": "applied-insights/case-studies/posts/2023/11/16/learning-from-failure.html#causes-of-failure",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "Causes of failure",
    "text": "Causes of failure\nHere, I believed, we had reached the end of what I could do with data alone, and so I presented these findings (or lack thereof) to executive leadership. The failure to gather reliable data from linking the clip data to the shift data prompted follow-ups into what exactly was going wrong. As it turned out, many things were going wrong.\nFirst, a number of technical problems plagued the early rollout of the cameras:\n\nAll of our facilities suffered from high turnover, and camera ownership was not consistently updated. Employees who no longer worked at the agency could therefore appear in the clip data – somebody else had taken over their camera but had not put their name and ID on it.\nWe had no way of telling if a camera was not recording due to being docked and recharging or not recording due to being switched off.\nIn the early days of the rollout, footage got assigned to an employee based on the owner of the dock, not the camera. In other words, if Employee A had recorded their shift with their camera but uploaded the footage using a dock assigned to Employee B then the footage would show up in the system as belonging to Employee B.\n\nThe shift data was, unsurprisingly, even worse, and it was here we came across our most important finding. While the evidence showed that there wasn’t any widespread non-compliance with the use of the cameras, there was widespread non-compliance with the use of our shift management software. Details are included in the dropdown box below.\n\n\n\n\n\n\nQuality issues in shift tracking data\n\n\n\n\n\nOur HR system, CAPPS, had a feature that tracked hours worked in order to calculate leave and overtime pay. However, CAPPS was a statewide application designed for 9–5 office workers, and could not capture the irregular working hours of our staff (much less aid in planning future shifts). We had obtained separate shift management software to fill these gaps, but not realized how inconsistently it was being used. All facilities were required to have their employees log their shifts, but some followed through on this better than others. And even for those that did make a good-faith effort at follow-through, quality control was nonexistent.\nIn CAPPS, time entry determined pay, so strong incentives existed to ensure accurate entry. But for our shift management software, no incentives existed at all for making sure that entries were correct. For example, a correctional officer could have a 40-hour work week scheduled in the shift software but miss the entire week due to an injury, and the software would still show them as having worked 40 hours that week. Nobody bothered to go back and correct these types of errors because there was no reason to.\nThe software was intended to be used proactively for planning purposes, not after-the-fact for logging and tracking purposes. Thus, it produced data that was totally inconsistent with actual hours worked, which became apparent when compared to data (like body-worn camera footage) that tracked actual hours on the floor.\nIn the end, we had to rethink a number of aspects of the shift software’s implementation. In the process of these fixes, leadership also came to make explicit that the software’s primary purpose was to help facilities schedule future shifts, not audit hours worked after the fact (which CAPPS already did, just on a day-by-day basis as opposed to an hour-by-hour basis). This analysis was the only time we attempted to use the shift data in this manner."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/11/16/learning-from-failure.html#what-we-learned-from-failure",
    "href": "applied-insights/case-studies/posts/2023/11/16/learning-from-failure.html#what-we-learned-from-failure",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "What we learned from failure",
    "text": "What we learned from failure\nWhatever means we used to monitor compliance with the camera policy, we learned that it couldn’t be fully automated. The agency followed up this analysis with a random sampling approach, in which monitors would randomly select times of day they knew a given staff member would have to have their cameras turned on and actually watch the associated clips. This review process confirmed the first impressions from the statistical review above: most employees were making good faith attempts at complying with the policy despite technical glitches, short-staffing, and administrative confusion. It also confirmed that proactive monitoring of correctional officers was a human process which had to come from supervisors and staff.\nThe one piece of the analysis we did use going forward was the clip analysis (converted into a Power BI dashboard and included in the GitHub repository for this article), but only as a supplement for already-launched investigations, not a prompt for one. Body-worn camera footage remained immensely useful for investigations after-the-fact, but inconsistencies in clip data were not, in and of themselves, particularly noteworthy “red flags.” At the end of the day, analytics can contextualize and enhance human judgment, but it cannot replace it.\nIn academia, the bias in favor of positive findings is well-documented. The failure to find something, or a lack of statistical significance, does not lend itself to publication in the same way that a novel discovery does. But, in an applied setting, where results matter more than publication criteria, negative findings can be highly insightful. They can falsify erroneous assumptions, bring unknown problems to light, and prompt the creation of new processes and tools. In this context, a failure is only truly a failure if nothing is learned from it.\n\nFind more case studies\n\n\n\n\n\nAbout the author\n\nNoah Wright is a data scientist with the Texas Juvenile Justice Department. He is interested in the applications of data science to public policy in the context of real-world constraints, and the ethics thereof (ethics being highly relevant in his line of work). He can be reached on LinkedIn.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Noah Wright\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nWright, Noah. 2023. “Learning from failure: ‘Red flags’ in body-worn camera data.” Real World Data Science, November 16, 2023. URL"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/11/16/learning-from-failure.html#footnotes",
    "href": "applied-insights/case-studies/posts/2023/11/16/learning-from-failure.html#footnotes",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe underlying data for the analysis as presented in this article was requested through the Texas Public Information Act and went through TJJD’s approval process for ensuring anonymity of records. It is available on GitHub along with the rest of the code used to write this article.↩︎"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/01-purchase-to-plate.html",
    "href": "applied-insights/case-studies/posts/2023/08/21/01-purchase-to-plate.html",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "",
    "text": "Disclaimer\n\n\n\nThe findings and conclusions in this publication are those of the authors and should not be construed to represent any official USDA or US Government determination or policy. This research was supported by the US Department of Agriculture’s Economic Research Service and Center for Nutrition, Policy and Promotion. Findings should not be attributed to Circana (formerly IRI).\nAbout 600,000 deaths per year in the United States are related to chronic diseases that are linked to poor dietary choices. Many other individuals suffer from diet-related health conditions, which may limit their ability to work, learn, and be physically active (US Department of Agriculture and US Department of Health and Human Services 2020). In recognition of the link between diet and health, in 1974 the Senate Select Committee on Nutrition and Human Needs, originally formed to eliminate hunger, expanded its focus to improving eating habits, nutrition policy and the national diet. Since 1980, the Dietary Guidelines for Americans have been released every five years by the US Departments of Agriculture (USDA) and Health and Human Services (DHHS). The guidelines present “advice on what to eat and drink to meet nutrient needs, promote health, and prevent disease”.\nBecause there can be economic and social barriers to maintaining a healthy diet, USDA promotes Food and Nutrition Security so that everyone has consistent and equitable access to healthy, safe, and affordable foods that promote optimal health and well-being. A set of data tools called the Purchase to Plate Suite (PPS) supports these goals by enabling the update of the Thrifty Food Plan (TFP), which estimates how much a budget-conscious family of four needs to spend on groceries to ensure a healthy diet. The TFP market basket – consisting of the specific amounts of various food categories required by the plan – forms the basis of the maximum allotment for the Supplemental Nutrition Assistance Program (SNAP, formerly known as the “Food Stamps” program), which provided financial support towards the cost of groceries for over 41 million individuals in almost 22 million households in fiscal year 2022.\nThe 2018 Farm Act (Agriculture Improvement Act of 2018) requires that USDA reevaluate the TFP every five years using current food composition, consumption patterns, dietary guidance, and food prices, and using approved scientific methods. USDA’s Economic Research Service (ERS) was charged with estimating the current food prices using retail food scanner data (Levin et al. 2018; Muth et al. 2016) and utilized the PPS for this task. The most recent TFP update was released in August 2021 and the revised cost of the market basket was the first non-inflation adjustment increase in benefits for SNAP in over 40 years (US Department of Agriculture 2021).\nThe PPS combines datasets to enhance research related to the economics of food and nutrition. There are four primary components of the suite:\nThe PPC allows researchers to measure the healthfulness of store purchases. On average US consumers acquire about 75% of their calories from retail stores, and there are a number of studies linking the availability of foods at home to the healthfulness of the overall diet (e.g., Gattshall et al. 2008; Hanson et al. 2005). Thus, understanding the healthfulness of store purchases allows us to understand differences in consumers who purchase healthy versus less healthy foods, and may contribute to better policies that promote healthier food purchases. While healthier diets are linked to a lower risk of disease outcomes (Reedy et al. 2014), other factors such as health care access may also be contributors (Cleary, Liu, and Carlson 2022). The PPC also forms the basis of the price tool, PPPT – which allows researchers to estimate custom prices for dietary recall studies – and a new ERS data product, the PP-NAP. The national average prices from PP-NAP are used in reevaluating the TFP. By using the PP-NAP with 24-hour dietary recall information from surveys such as What We Eat in America (WWEIA) – the dietary component of the nationally representative National Health and Nutrition Examination Survey(NHANES)1 – researchers can examine the relationship between the cost of food, dietary intake, and chronic diseases linked to poor diets. The price estimates also allow researchers to develop cost-effective healthy diets such as MyPlate Kitchen. The final component of the Purchase to Plate Suite, the ingredient tool (PPIT), breaks dietary recall-reported foods back into purchasable ingredients, based on US retail food purchases. The PPIT is also used in the revaluation of the TFP, and by researchers who want to look at the relationship between reported ingestion of grocery items, cost and disease outcomes using WWEIA/NHANES. More information on the development of the PPC is available in two papers by Carlson et al. (2019, 2022).\nThe Food for Thought competition aimed to support the development of the PPC – and thus policy-oriented research – by linking retail food scanner data to the USDA nutrition data used to analyze NHANES dietary recall data, specifically the Food and Nutrient Database for Dietary Studies (FNDDS) (2018, 2020). In particular, the competition set out to use artificial intelligence (AI) to reduce human resources in creating the links for the PPC, while still maintaining the high-quality standards required for reevaluating the TFP and for data published by ERS (which is one of 13 Principle Statistical Agencies in the United States Federal Government)."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/01-purchase-to-plate.html#methods-used-to-date",
    "href": "applied-insights/case-studies/posts/2023/08/21/01-purchase-to-plate.html#methods-used-to-date",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "Methods used to date",
    "text": "Methods used to date\nOn the surface, the linking process may appear simple: both the FNDDS and retail food scanner data are databases of food. But the scanner data are produced for market research, and the FNDDS for dietary studies. The scanner data include about 350,000 items with sales each year, while the FNDDS has only 10,000–15,000 items. Scanner data relates to specific products, while FNDDS items are often more general. Both datasets have different hierarchical structures – the FNDDS hierarchy is based around major food groups: dairy; meat, poultry and seafood; eggs; nuts and legumes; grains; fruits; vegetables; fats and oils; and sugars, sweets, and beverages. Items fall into the groups regardless of preparation method or form. That is, broccoli prepared from frozen and from fresh both appear in the vegetable group, and for some fruits and vegetables, the fresh, frozen, canned and dried form are the same FNDDS item. Vegetable-based mixed dishes, such as broccoli and carrot stir-fry or soup, are also classified in the vegetable group. On the other hand, the scanner data classifies foods by grocery aisle. That is, the fresh and frozen broccoli are classified in different areas: produce and frozen vegetables. Similarly, when sold as a prepared food, the broccoli and carrot stir-fry may be found in the frozen entries, as a kit in either the frozen or produce section, refrigerated foods, or all of these.\nTo allow researchers to import the FNDDS nutrient data into the scanner data, a one-to-many match between FNDDS and scanner data items was needed. The food descriptions in the scanner data include brand names and package sizes and are written as a consumer would pronounce them – e.g., fresh and crisp broccoli florets, ready-cut, 10 oz – versus a more general FNDDS description such as “Broccoli, raw”. (Also linked to the “Broccoli, raw” code would be broccoli sold with stems attached, broccoli spears, and any other way raw broccoli is sold.) In the scanner data, the Universal Product Code (UPC) and the European Article Number (EAN) can link items between tables within the scanner data, as well as between datasets of grocery items, such as the USDA Global Branded Foods Product Database, a component of USDA’s Food Data Central. However, these codes are not related to the FNDDS codes, or any other column within the FNDDS. In other words, before development of the PPC, there were no established linking identifiers.\nFigure 1 shows the process USDA uses to develop matches between scanner data and FNDDS.\n\n\nFigure 1: Process currently used to create the matches between the USDA Food and Nutrient Database for Dietary Studies (FNDDS) and the retail scanner data (labelled “IRI” for the IRI InfoScan and Consumer Network) product dictionaries. Source: Author provided.\n\nWe start the linking process by categorizing the scanner data items into homogeneous groups to make the first round of automated matching more efficient. To save time, we use the second lowest hierarchical category in the scanner data which generally divides items within a grocery aisle into homogenous groups such as produce, canned beans, baking mixes, and bread. Once the linking categories for scanner data are established, we select appropriate items from the FNDDS. Since the FNDDS is highly structured, this selection is usually straightforward.\nOur next step is to use semantic matching to create a search table that aligns similar terms within the IRI product dictionary and FNDDS. This first requires that we extract attributes from the FNDDS descriptions into fields similar to those in the scanner data product dictionary. The FNDDS descriptions are found across multiple columns because they are added as the need arises to provide examples of brand names or alternative descriptions of foods which help code the foods WWEIA participants report eating. We manually create matching tables that link terms used in FNDDS to those used in the scanner data, organized by the fields defined in the restructured FNDDS. We then use this table as the basis of a probabilistic matching process. For example, when linking the produce group, “fresh” in the scanner data would be aligned with “raw” and “prepared from fresh” and NOT “prepared from frozen” in the FNDDS, and “broccoli florets” would also be aligned with “raw” and “broccoli”. Since the FNDDS is designed to code the foods individuals report eating, many of the foods in the FNDDS are already prepared and result in descriptions such as “broccoli, steamed, prepared from fresh” or “broccoli, boiled, prepared from frozen”.\nOnce the linking table is established, the probabilistic match process returns the single best possible match for each item in the scanner data. For example, a match between fresh broccoli florets and frozen broccoli would have a lower probability score than “broccoli, raw”. Because these matches form the basis of major USDA policies, we cannot accept an error rate of more than 5 percent, and lower is preferred. To reach that goal, nutritionists review every match to make sure the probabilistic match did not return a match between cauliflower florets and fresh broccoli, say, or that a broccoli and carrot stir-fry is not matched to a dish with broccoli, carrots, and chicken. The correct matches, such as the one between fresh broccoli florets and raw broccoli, are set aside while the items with an incorrect match, such as cauliflower florets and the broccoli and carrot stir-fry, are used to revise the search table. Revisions might include adding (NOT chicken) to the broccoli and carrot stir-fry dish. Mixed dishes — such as the broccoli and carrot stir-fry — pose particular challenges because there are a wide variety of similar products available in the grocery store. After a few rounds of revising the search table and running the probabilistic match process, it is more efficient to use a manual match, established by one nutritionist and reviewed by another, after which the match is assumed to be correct.\nThe process improved with each new wave of FNDDS and IRI data. Our first creation of the PPC linked the FNDDS 2011/12 to the 2013 IRI retail scanner data. Subsequent waves started with the previous search table and resulting matches were reviewed by nutritionists. We also used more fields in the IRI product dictionary to create the homogeneous linking groups and made modifications to these groups with each wave. During each wave we experimented with the number of rounds of probabilistic matching that was the most cost effective. For some linking groups it took less human time to manually match from the start, while for other groups it was more efficient to do multiple rounds of improvements to the search table. Starting with the most recent wave (matching FNDDS 2017/18 to the 2017 and 2018 retail scanner data), we assumed previous matches appearing in the newer data were correct. Although this assumption was good for most matches, a review demonstrated the need to review previous matches prior to removing the item from the list of scanner data items needing FNDDS matches. In the future we intend to explore methods developed by the participants of the Food for Thought competition."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/01-purchase-to-plate.html#linking-challenges",
    "href": "applied-insights/case-studies/posts/2023/08/21/01-purchase-to-plate.html#linking-challenges",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "Linking challenges",
    "text": "Linking challenges\nAn ongoing challenge to the linking problem is that both the scanner data and the FNDDS undergo substantive changes each year, meaning that both the previous matches and search tables need to be reviewed and revised with each new effort, as tables that work with one cycle of FNDDS and scanner data will need revisions to use with the next cycle. Changes to the scanner data that impact our current method include dropped and added items, data corrections, and revisions to the categories that form the basis of the homogeneous linking groups. In addition, there are errors such as incorrect food descriptions, conflicting package size information, and changes in the item description from year to year. Since the FNDDS is designed to support dietary recall studies, revisions reflect both changes to available foods and the level of detail respondents can provide. These revisions result in dropped/added food codes, changes to food descriptions that impact which scanner data items match to the FNDDS items, and revisions to recipes used in the nutrient coding which impacts the number of retail ingredients available in the FNDDS.\nOf the four parts of the PPS, establishing the matches is the most time-consuming task and constitutes at least 60 percent of the total budget. In the most recent round, we had 168 categories and each one went through 2-3 automated matching rounds; after each round, nutritionists spent an average of two hours reviewing the matches. This adds up to somewhere between 670 and 1,000 hours of review time. After the automated review, manual matching requires an additional 300 hours. Reducing the amount of time required to establish matches and link the FNDDS and retail scanner datasets may lead to significant time savings, resulting in faster data availability. That, in turn, could allow more timely policy-based research, and the mandated revision of the Thrifty Food Plan can continue with the most recent food price data.\n\n\n\n\n← Introduction\n\n\n\n\nPart 2: Competition design →\n\n\n\n\n\n\n\n\nAbout the authors\n\nAndrea Carlson is an agricultural economist in the Food Markets Branch of the Food Economics Division in USDA’s Economic Research Service. She is the project lead for the Purchase to Plate Suite, which allows users to import USDA nutrient and food composition data into retail food scanner data acquired by USDA and estimate individual food prices for dietary intake data.\n\n\nThea Palmer Zimmerman is a senior study director and research nutritionist at Westat.\n\n\n\n\n\nImage credit\n\nThumbnail photo by Kenny Eliason on Unsplash.\n\n\n\n\n\nHow to cite\n\nCarlson, Andrea, and Thea Palmer Zimmerman. 2023. “Food for Thought: The importance of the Purchase to Plate Suite.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/01-purchase-to-plate.html#acknowledgements",
    "href": "applied-insights/case-studies/posts/2023/08/21/01-purchase-to-plate.html#acknowledgements",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe research presented in this compendium supports the Purchase to Plate Suite of data products. Carlson has been privileged to both develop and lead this project over the course of her career, but it is not a solo project. Many thanks to the Linkages Team from USDA’s Economic Research Service (Christopher Lowe, Mark Denbaly Elina Page, and Catherine Cullinane Thomas) the Center for Nutrition Policy and Promotion (Kristin Koegel, Kevin Kuczynski, Kevin Meyers Mathieu, TusaRebecca Pannucci), and our contractor Westat, Inc. (Thea Palmer Zimmerman, Carina E. Tornow, Amber Brown McFadden, Caitlin Carter, Viji Narayanaswamy, Lindsay McDougal, Elisha Lubar, Lynnea Brumby, Raquel Brown, and Maria Tamburri). Many others have supported this project over the years."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/01-purchase-to-plate.html#footnotes",
    "href": "applied-insights/case-studies/posts/2023/08/21/01-purchase-to-plate.html#footnotes",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNHANES is a multi-module continuous survey conducted by the Centers for Disease Control and Prevention. In addition to the WWEIA, NHANES includes a four-hour complete medical exam including a health history, and a blood and urine analysis.↩︎"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/03-first-place-winners.html",
    "href": "applied-insights/case-studies/posts/2023/08/21/03-first-place-winners.html",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "",
    "text": "The Auburn Big Data team from Auburn University consists of five members, including three assistant professors: Dr Wenying Li of the Department of Agricultural Economics and Rural Sociology, Dr Jingyi Zheng of the Department of Mathematics and Statistics, and Dr Shubhra Kanti Karmaker of the Department of Computer Science and Software Engineering. Additionally, the team comprises two PhD students, Naman Bansal and Alex Knipper, who are affiliated with Dr Karmaker’s big data lab at Auburn University.\nIt is estimated that our team has spent approximately 1,400 hours on this project."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/03-first-place-winners.html#our-perspective-on-the-challenge",
    "href": "applied-insights/case-studies/posts/2023/08/21/03-first-place-winners.html#our-perspective-on-the-challenge",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Our perspective on the challenge",
    "text": "Our perspective on the challenge\nAt the start of this competition, we decided to test three general approaches, in the order listed:\n\nA heuristic approach, where we use only the data and a defined similarity metric to predict which FNDDS label a given IRI item should have.\nA simpler modeling approach, where we train a simple statistical classifier, like a random forest (Parmar, Katariya, and Patel 2019), logistic regression, etc., to predict the FNDDS label for a given IRI item. For this method, we opted to use a random forest as our statistical model, as it was a simpler model to use as a baseline, having shown decent performance in a wide range of classification tasks. As it turned out, this approach was quite robust and accurate, so we kept it as our main model for this approach.\nA large language modeling approach, where we train a model like BERT (Devlin et al. 2018) to map the descriptions for given IRI and FNDDS items to the FNDDS category the supplied IRI item belongs to."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/03-first-place-winners.html#our-approach",
    "href": "applied-insights/case-studies/posts/2023/08/21/03-first-place-winners.html#our-approach",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Our approach",
    "text": "Our approach\nAs we explored the data provided, we opted to use the given 2017–2018 PPC dataset as our primary dataset for both training and testing. To ensure a fair evaluation of the model, we randomly split the dataset into 60% training samples and 40% testing samples, making sure our training process never sees the testing dataset. For evaluating our models, we adopted the competition’s metrics: Success@5 and NDCG@5. After months of testing, our statistical classifier (approach #2) proved itself to be the model that both processes the data fastest and achieves the highest performance on our testing metrics.\nThis approach, at a high level, takes in the provided data (among other configuration parameters), formats the data in a computer-readable format – converting the IRI and FNDDS descriptions to a numerical representation with word embeddings (2018; Mikolov et al. 2013; Pennington, Socher, and Manning 2014) and then using that numerical representation to calculate the distances between each description – and then trains a classification model (random forest (2019)/neural network (Schmidhuber 2015)) that can predict an FNDDS label for a given IRI item.\nIn terms of data, our approach uses the FNDDS/IRI descriptions, combining them into a single “description” field, and the IRI item’s categorical items – department, aisle, category, product, brand, manufacturer, and parent company – to further discern between items.\nWhile most industrial methods require use of a graphics processing unit (graphics card, or GPU) to perform this kind of processing, our primary method only requires the computer’s internal processor (CPU) to function properly. With that in mind, to achieve the best possible performance on our test metrics, the most time-consuming operations are run in parallel. The time taken to train our primary model can likely be further improved if we parallelize these operations across a GPU, with the only downside being the imposition of a GPU requirement for systems aiming to run this method.\nIn addition to our primary method, our team has worked with alternate approaches on the GPU (using BERT (2018), neural networks (2015), etc.) to either: 1) speed up the time it takes to process and make inferences for the data, achieving similar performance on our test metrics, or 2) achieve higher performance, likely at a cost to the time it takes to process everything. Our reasoning behind doing so is that if a simple statistical model performs well, then a larger language model should be able to demonstrate a higher performance on our test metrics without much of an increase in training time. At the current time, these methods are still unable to match the performance/efficiency tradeoff of our primary method.\nAfter exploring alternate methods to no avail, our team then decided to focus again on our primary method, the random forest (2019), and a secondary method, feed-forward neural network mapping our input features (X) to the FNDDS labels (Y) (2015), to optimize their training hyperparameters for the dataset. Our aim in this is to see which of our already-implemented, easier-to-run downstream methods would better optimize the performance/efficiency tradeoff after having its training parameters optimized to the fullest. This has resulted in a marginal increase in training time (+20-30 minutes) and a roughly 5% increase in performance for our still-highest performing model, the random forest.\nOverall, our primary method – the random forest – gave us an approximate training time (including data pre-processing) of 4 hours 30 minutes for our ~38,000 IRI item training set, and an approximate inference time of 15 minutes on our testing set of ~15,000 IRI items. Furthermore, our method gave us a Success@5 score of .789 and an NDCG@5 score of .705 on our testing set.\n\nKey features\nHere is a list of the key features we utilize, along with what type of data we treat it as.\n\nFNDDS\n\nfood_code – identifier\nmain_food_description – text\nadditional_food_description – text\ningredient_description – text\n\nIRI\n\nupc – identifier\nupcdesc – text\ndept – categorical\naisle – categorical\ncategory – categorical\nproduct – categorical\nbrand – categorical\nmanufacturer – categorical\nparent – categorical\n\n\nThe intuition behind using these particular features is that the text-based descriptions provide the majority of the “meaning” of the item. By converting each description to a numerical representation (2013; 2014), we can then calculate the similarity between each “meaning” to determine which FNDDS label is most similar to the IRI item provided. However, that alone is not enough. The categorical features on the IRI item help to further enhance the model’s classifications using the logic and categories people use in places like grocery stores. For example, if given an item whose aisle was “fruit” and brand was “Dole”, the item could be reasonably expected to be something like “peaches” over something like “broccoli”.\n\n\nFeature selection\nAforementioned intuition aside, our feature selection was rather naive, in that we manually examined the data and removed any redundant text features before doing anything else. After that, we decided to use description fields as “text” data to comprise the main “meaning” of the item, represented numerically after converting the text using a word embedding (2013; 2014). We also decided to use the non-description fields (aisle, category, etc.) as “categorical” data that would be turned into its own numerical representation, allowing our model to more easily discern between items using similar systems to people.\n\n\nFeature transformations\nOur feature transformations are also relatively simple. First, we combine all description fields for each item to make one large description, and then use a word embedding method (like GloVe (2014) or BERT (2018)) to convert the description into a numerical representation, resulting in a 300-dimensional GloVe or 768-dimensional BERT vector of numbers for each description. Then, for each IRI item, we calculate the cosine and Euclidean distances from each FNDDS item, resulting in two vectors, both equal in length to the original FNDDS data (in this case, two vectors of length ~7,300). The intuition behind this is that while cosine and Euclidean distances can tell us similar things, providing both of these sets of distances to the model should allow it to pick up on a more nuanced set of relationships between the IRI and FNDDS items.\nFor categorical data, we take all unique values in each field and assign them an ID number. While that is often not the best practice for making a numerical representation out of categorical data (Potdar, Pardawala, and Pai 2017), it seemed to work for the downstream model.\nRegardless, the aforementioned feature transformations give us (ad hoc) ~14,900 features if we use GloVe and ~15,300 features if we use BERT. Both feature sets can then be sent to the downstream random forest/neural network to start classifying items.\nIt should be noted that processing the data is by far the most time-consuming part of our method. The data processing times for each embedding are as follows:\n\nGloVe: ~3 hours\nBERT: ~6 hours\n\nDue to BERT both taking so long to process data and performing lower than our GloVe embeddings on the classification task, we opt to use GloVe embeddings for our primary method. Our only theoretical explanation here is that since BERT is better at context-dependent tasks (Wang, Nulty, and Lillis 2021), it likely will expect something similar to well-structured sentences as input, which is not what the IRI/FNDDS descriptions are. Rather, GloVe – being a method that depends less on context (2013; 2014) – should excel better when the input text is not a well-formed sentence.\n\n\nTraining methods\nOnce the data has been processed, we collect the following data for each IRI item:\n\nUPC code\nDescription (converted to numerical representation)\nCategorical variables (converted to numerical representation)\nDistances to each FNDDS item\n\nOnce that has been collected for each IRI item, we can finally use our classification model. We initialize our model and begin the training process with the IRI data mentioned above and the target FNDDS labels for each one, so the model knows what the “correct” answer is for the given data. Once the model has trained on our training dataset, we save the model and it is ready for use.\nThis part of training takes much less time than preparing the data, since calculating the embeddings takes a lot more computation than a random forest model. The training times for each method are as follows:\n\nRandom Forest: ~1 hour 15 minutes\nNeural Network: ~25 minutes\n\nDespite the neural network taking far less time to train than the random forest, it still scores lower on the scoring metrics than the random forest, so we opt to continue using the random forest model as our primary method.\n\n\nGeneral approach to developing the model\nSince the linkage problem involves mapping tens of thousands of items to a smaller category set of a few thousand items, we decided to frame this problem as a multi-class classification problem (Aly 2005), where we then rank the top “k” most probable class mappings, as requested by the competition ruleset.\nMost of the usable data available to us is text data, so we need a method that can use that text-based information to accurately map classes based on the aforementioned text information. To best accomplish this, we opt to use word embedding techniques to calculate an average numerical representation for each text description (both IRI and FNDDS), so we can calculate distances between each description, giving our model a sense of how similar each description is.\n\n\nThe key “trick” to the model\nSince text descriptions hold the most information that can be used to link between an IRI item and an FNDDS item, finding a way to calculate the similarity between each description is paramount to making this method work.\nBoth distance calculation methods used in this work, cosine and Euclidean distance, are very similar in the type of information encoded, the only major difference being that cosine distance is implicitly normalized and Euclidean distance is not (Qian et al. 2004).\n\n\nNotable observations\nJust by building the ranking using the cosine similarities between each IRI item and all FNDDS items, we can achieve a Success@5 performance of 0.234 and an NDCG@5 performance of 0.312. The other features are provided and the random forest classifier is used to add some extra discriminative power to the model.\n\n\nData disclaimer\nOur current method only uses the data readily available from the 2017–2018 dataset, which we acknowledge is intended for testing. To remedy this, we further split this dataset into train/test sets and report results on our unseen test subset for our primary performance metrics. This gives a decent look into how the model will perform on unseen data.\n\n\n\n\n\n\nFind the code in the Real World Data Science GitHub repository."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/03-first-place-winners.html#our-results",
    "href": "applied-insights/case-studies/posts/2023/08/21/03-first-place-winners.html#our-results",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Our results",
    "text": "Our results\n\nApproximate training time\nOverall, our approximate training time for our primary method is 4 hours 30 minutes broken down (approximately) as follows:\n\nReading data from database: 30 seconds\nCalculating ~7,300 FNDDS description embeddings: 15 minutes 45 seconds\nCalculating ~38,000 IRI description embeddings and similarity scores: 2 hours 20 minutes 45 seconds\nFormatting calculated data for the random forest classifier: 35 minutes\nTraining the random forest classifier: 1 hour 15 minutes\n\n\n\nApproximate inference time\nOur approximate inference time for our primary method is 15 minutes to make inferences for ~15,000 IRI items.\n\n\nS@5 & NDCG@5 performance\nThis is how our best-performing model (GloVe + random forest) performs at the current time on the testing set:\n\nNDCG@5: 0.705\nSuccess@5: 0.789\n\nWhen we evaluate that same model on the full PPC dataset we were provided (~38,000 items), we get the following scores:\n\nNDCG@5: 0.879\nSuccess@5: 0.916\n\n(Note: The full PPC dataset contains approximately 15,000 items that we used to train the model, so these scores are not as representative of our method’s performance as the previous scores.)"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/03-first-place-winners.html#future-workrefinement",
    "href": "applied-insights/case-studies/posts/2023/08/21/03-first-place-winners.html#future-workrefinement",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Future work/refinement",
    "text": "Future work/refinement\nAs mentioned previously, we only used the given 2017–2018 PPC dataset as our primary dataset for both training and testing. Going forward, we would like to include datasets from previous years as well, which we believe would further increase our model performance. Additionally, the datasets generated from this research have the potential to inform and support additional studies from a variety of perspectives, including nutrition, consumer research, and public health. Further research utilizing these datasets has the potential to make significant contributions to our understanding of consumer behavior and the role of food and nutrient consumption in overall health and well-being."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/03-first-place-winners.html#lessons-learned",
    "href": "applied-insights/case-studies/posts/2023/08/21/03-first-place-winners.html#lessons-learned",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Lessons learned",
    "text": "Lessons learned\nIt was interesting that the random forest model performed better than the vanilla neural network model. This shows that a simple solution can work better, depending on the application. This observation is in line with the well-established principle in machine learning that the choice of model should be guided by the nature of the problem and the characteristics of the data. In this case, the random forest model, being a simpler and more interpretable model, was better suited to the problem at hand and was able to outperform the more complex neural network model. These results underscore the importance of careful model selection and the need to consider both the complexity of the model and the specific requirements of the problem when choosing an algorithm for a particular application.\n\n\n\n\n← Part 2: Competition design\n\n\n\n\nPart 4: Second place winners →\n\n\n\n\n\n\n\n\nAbout the authors\n\nAlex Knipper and Naman Bansal are PhD students, and Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker are assistant professors at Auburn University.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Alex Knipper, Naman Bansal, Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by nrd on Unsplash.\n\n\n\nHow to cite\n\nKnipper, Alex, Naman Bansal, Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker. 2023. “Food for Thought: First place winners – Auburn Big Data.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/06-value-of-competitions.html",
    "href": "applied-insights/case-studies/posts/2023/08/21/06-value-of-competitions.html",
    "title": "Food for Thought: The value of competitions for confidential data",
    "section": "",
    "text": "We are witnessing a sea change in data collection practices by both governments and businesses – from purposeful collection (through surveys and censuses, for example) to opportunistic (drawing on web and social media data, and administrative datasets). This shift has made clear the importance of record linkage – a government might, for example, look to link records held by its various departments to understand how citizens make use of the gamut of public services.\nHowever, creating manual linkages between datasets can be prohibitively expensive, time consuming, and subject to human constraints and bias. Machine learning (ML) techniques offer the potential to combine data better, faster, and more cheaply. But, as the recently released National AI Research Resources Task Force report highlights, it is important to have an open and transparent approach to ensure that unintended biases do not occur.\nIn other words, ML tools are not a substitute for thoughtful analysis. Both private and public producers of a linked dataset have to determine the level of linkage quality – such as what precision/recall tradeoff is best for the intended purpose (that is, the balance between false-positive links and failure to cover links that should be there), how much processing time and cost is acceptable, and how to address coverage issues. The challenge is made more difficult by the idiosyncrasies of heterogeneous datasets, and more difficult yet when datasets to be linked include confidential data (Christensen and Miguel 2018; Christen, Ranbaduge, and Schnell 2020).\nAnd, of course, an ML solution is never the end of the road: many data linkage scenarios are highly dynamic, involving use cases, datasets, and technical ecosystems that change and evolve over time; effective use of ML in practice necessitates an ongoing and continuous investment (Koch et al. 2021). Because techniques are constantly improving, producers need to keep abreast of new approaches. A model that is working well today may no longer work in a year because of changes in the data, or because the organizational needs have changed so that a certain type of error is no longer acceptable. As Sculley et al. point out, “it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning” (Sculley et al. 2014).\nAlso important is that record linkage is not seen as a technical problem relegated to the realm of computer scientists to solve. The full engagement of domain experts in designing the optimization problem, identifying measures of success, and evaluating the quality of the results is absolutely critical, as is building an understanding of the pros and cons of different measures (Schafer et al. 2021; Hand and Christen 2018). There will need to be much learning by doing in “sandbox” environments, and back and forth communication across communities to achieve successful outcomes, as noted in the recommendations of the Advisory Committee on Data for Evidence Building (a screenshot of which is shown in Figure 1).\n\n\nFigure 1: A recommendation for building an “innovation sandbox” as part of the creation of a new National Secure Data Service in the United States.\n\nDespite the importance of trial and error and transparency about linkage quality, there is no handbook that guides domain experts in how to design such sandboxes. There is a very real need for agreed-upon, domain-independent guidelines, or better yet, official standards to evaluate sandboxes. Those standards would define “who” could and would conduct the evaluation, and help guarantee independence and repeatability. And while innovation challenges have been embraced by the federal government, the devil can be very much in the details (Williams 2012).\nIt is for this reason that the approach taken in the Food for Thought linkage competition, and described in this compendium, provides an important first step towards a well specified, replicable framework for achieving high quality outcomes. In that respect it joins other recent efforts to bring together community-level research on shared sensitive data (MacAvaney et al. 2021; Tsakalidis et al. 2022). This competition, like those, helped bring to the foreground both the opportunities and challenges of doing research in secure sandboxes with sensitive data. Notably, these exercises highlight a kind of cultural tension between secure, managed environments, on the one hand, and unfettered machine learning research, on the other. The need for flexibility and agility in computational research bumps up against the need for advance planning and careful step-by-step processes in environments with well-defined data governance rules, and one of the key lessons learned is that the tradeoffs here need to be recognized and planned for.\nThis particular competition was important for a number of other reasons. Thanks to its organization as a competition, complete with prizes and bragging rights for strongly performing teams, it attracted new eyes from computer science and data science to think about how to address a critical real-world linkage problem. It offered the potential to produce approaches that were scalable, transparent, and reproducible. The engagement of domain experts and statisticians meant that it will be possible to conduct an informed error analysis, to explicitly relate the performance metrics in the task to the problem being solved in the real world, and to bring in the expertise of survey methodologists to think about the possible adjustments. And because it identified different approaches of addressing the same problem, it created an environment for new innovative ideas.\nMore generally, in addition to the excitement of the new approaches, this exercise laid bare the fragility of linkages in general and highlighted the importance of secure sandboxes for confidential data. While the promise of privacy preserving technologies is alluring as an alternative to bringing confidential data together in one place, such approaches are likely too immature to deploy ad hoc until a better understanding is established of how to translate real-world problems and their associated data into well-defined tasks, how to measure quality, and particularly how to assess the impact of match quality on different subgroups (Domingo-Ferrer, Sánchez, and Blanco-Justicia 2021). The scientific profession has gone through too painful a lesson with the premature application of differential privacy techniques to ignore the lessons that can be learned from a careful and systematic analysis of different approaches (2021; Van Riper et al. 2020; Ruggles et al. 2019; Giles et al. 2022).\nWe hope that the articles in this collection provide not only the first steps towards a handbook of best practices, but also an inspiration to share lessons learned, so that success can be emulated, and failures understood and avoided.\n\n\n\n\n← Part 5: Third place winners\n\n\n\n\nFind more case studies\n\n\n\n\n\n\n\n\nAbout the authors\n\nSteven Bedrick is an associate professor in Oregon Health and Science University’s Department of Medical Informatics and Clinical Epidemiology.\n\n\nOphir Frieder is a professor in Georgetown University’s Department of Computer Science, and in the Department of Biostatistics, Bioinformatics & Biomathematics at Georgetown University Medical Center.\n\n\nJulia Lane is a professor at the NYU Wagner Graduate School of Public Service and a NYU Provostial Fellow for Innovation Analytics. She co-founded the Coleridge Initiative.\n\n\nPhilip Resnik holds a joint appointment as professor in the University of Maryland Institute for Advanced Computer Studies and the Department of Linguistics, and an affiliate professor appointment in computer science.\n\n\n\nCopyright and licence\n\n© 2023 Steven Bedrick, Ophir Frieder, Julia Lane, and Philip Resnik\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Alexandru Tugui on Unsplash.\n\n\n\nHow to cite\n\nBedrick, Steven, Ophir Frieder, Julia Lane, and Philip Resnik. 2023. “Food for Thought: The value of competitions for confidential data.” Real World Data Science, August 21, 2023. URL\n\n\n\n\n\n\n\n\n\nReferences\n\nChristen, P., T. Ranbaduge, and R. Schnell. 2020. Linking Sensitive Data - Methods and Techniques for Practical Privacy-Preserving Information Sharing. Springer. https://doi.org/10.1007/978-3-030-59706-1.\n\n\nChristensen, G., and E. Miguel. 2018. “Transparency, Reproducibility, and the Credibility of Economics Research.” Journal of Economic Literature 56 (3): 920–80. https://doi.org/10.1257/jel.20171350.\n\n\nDomingo-Ferrer, J., D. Sánchez, and A. Blanco-Justicia. 2021. “The Limits of Differential Privacy (and Its Misuse in Data Release and Machine Learning).” Communications of the ACM 64 (7): 33–35. https://doi.org/10.1145/3433638.\n\n\nGiles, O., K. Hosseini, G. Mingas, O. Strickson, L. Bowler, C. Rangel Smith, H. Wilde, et al. 2022. “Faking Feature Importance: A Cautionary Tale on the Use of Differentially-Private Synthetic Data.” https://arxiv.org/abs/2203.01363.\n\n\nHand, D., and P. Christen. 2018. “A Note on Using the f-Measure for Evaluating Record Linkage Algorithms.” Statistics and Computing 28 (3): 539–47. https://doi.org/10.1007/s11222-017-9746-6.\n\n\nKoch, B., E. Denton, A. Hanna, and J. G. Foster. 2021. “Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research.” CoRR abs/2112.01716. https://arxiv.org/abs/2112.01716.\n\n\nMacAvaney, S., A. Mittu, G. Coppersmith, J. Leintz, and P. Resnik. 2021. “Community-Level Research on Suicidality Prediction in a Secure Environment: Overview of the CLPsych 2021 Shared Task.” In Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access, 70–80. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.clpsych-1.7.\n\n\nRuggles, S., C. Fitch, D. Magnuson, and J. Schroeder. 2019. “Differential Privacy and Census Data: Implications for Social and Economic Research.” AEA Papers and Proceedings 109 (May): 403–8. https://doi.org/10.1257/pandp.20191107.\n\n\nSchafer, K. M., G. Kennedy, A. Gallyer, and P. Resnik. 2021. “A Direct Comparison of Theory-Driven and Machine Learning Prediction of Suicide: A Meta-Analysis.” PLOS ONE 16 (4): 1–23. https://doi.org/10.1371/journal.pone.0249833.\n\n\nSculley, D., G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner, V. Chaudhary, and M. Young. 2014. “Machine Learning: The High Interest Credit Card of Technical Debt.” In SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop).\n\n\nTsakalidis, A., J. Chim, I. M. Bilal, A. Zirikly, D. Atzil-Slonim, F. Nanni, P. Resnik, et al. 2022. “Overview of the CLPsych 2022 Shared Task: Capturing Moments of Change in Longitudinal User Posts.” In Proceedings of the Eighth Workshop on Computational Linguistics and Clinical Psychology, 184–98. Seattle, USA: Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.clpsych-1.16.\n\n\nVan Riper, D., T. Kugler, J. Schroeder, and S. Ruggles. 2020. “Differential Privacy and Racial Residential Segregation.” In 2020 APPAM Fall Research Conference.\n\n\nWilliams, H. 2012. “Innovation Inducement Prizes: Connecting Research to Policy.” Journal of Policy Analysis and Management 31 (3): 752–76. http://www.jstor.org/stable/41653827."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/05/08/dpm.html#background",
    "href": "applied-insights/case-studies/posts/2024/05/08/dpm.html#background",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "Background",
    "text": "Background\nDecisions around medium and long-term allocation of healthcare resources are fraught with challenges and uncertainties, which explains the use of blunt resource allocations based on across-the-board annual percentage uplifts.\nThe Bristol, North Somerset, South Gloucestershire Integrated Care Board (BNSSG ICB - we love elaborate acronyms in the National Health Service!), in the south west of England, is part of the local NHS apparatus responsible for planning the current and future health needs of the one million resident population.\n\n\n\n\n  \n\n\n\nFigure 1: A map of the area covered by BNSSG, a space covered by three local authorities, with about 1 million people living inside it."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/05/08/dpm.html#population-segmentation",
    "href": "applied-insights/case-studies/posts/2024/05/08/dpm.html#population-segmentation",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "Population Segmentation",
    "text": "Population Segmentation\nBefore tackling the complex problem of forecasting healthcare resources into the future, we first need to understand the current situation regarding the distribution of health needs.\nWhile every individual has a unique set of circumstances, population segmentation is an approach used to help understand overall need by combining individuals into different groups, based on certain criteria.\nWe use the Cambridge Multimorbidity Score which is a metric designed to summarise the presence of multiple health conditions, known as multimorbidity. Using that score, which applies different weights to different health conditions, we previously found a way of splitting the adult (17+) population into five Core Segments, with Core Segment 1 patients having the lowest score and being the least ill and Core Segment 5 being those with the most multimorbidity.\nApplied to the BNSSG adult population (of around 750K individuals), the following interesting properties were found:\n\nHalving: Going up one segment results in roughly half the number of people in that segment\nDoubling: Going up one segment results in roughly twice the NHS monetary spend per person per year\n\nWe can see this in Figure 2.\n\n\n\n\n\n\nFigure 2: Halving-Doubling Effect of the Core Segments"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/05/08/dpm.html#sec-creating-the-model",
    "href": "applied-insights/case-studies/posts/2024/05/08/dpm.html#sec-creating-the-model",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "Creating The Model",
    "text": "Creating The Model\nTo forecast health needs of the population, in terms of how many people will be in which Core Segment in what future year, the Dynamic Population Model (DPM) takes information from two different sources:\n\nThe Office for National Statistics projections for our area. From this, we get yearly projections for not just the total 17+ population, but also the predicted number of people turning 17 (and so entering our model), deaths, and in- and out-ward migration.\nNHS patient attribute and activity data, stored in the System Wide Dataset (SWD). This gives us: past and current information on the adult population’s NHS healthcare usage; the Core Segment breakdown of our current and past populations; the proportion of those turning 17, migrating, and dying that are in each Core Segment. From this, we estimate the historical rates of transition within Core Segments, which is essentially the yearly number of people getting sicker or healthier.\n\nBy synthesising these pieces of data, we create our DPM forecast. Starting from the most up to date Core Segment population breakdown, the model takes yearly time steps into the future, at each time step using the inputs to estimate how many people are to be in each Core Segment. This modelling approach of having discrete time steps and different movements between states can be set up as a Markov chain, although here we have formulated it as a set of difference equations - through which the outflow of each Core Segment population at each time step is deterministic. The design was led by Zehra and Christos, through a collaboration between the NHS and the Centre for Healthcare Innovation and Improvement (CHI2) at the University of Bath.\nThe model can be thought of as having the following inputs:\n\n\n\n\n\n\n\n\nModel Input\nDescription\nData Source\n\n\n\n\ninitial population\nThe starting number of people in each Core Segment\nSWD\n\n\ninner transition matrix\nThe yearly proportions of people moving from one Core Segment to another\nSWD\n\n\nbirths, net migration, deaths - numbers\nThe yearly number of people moving in and out of the area\nONS\n\n\nbirths, net migration, deaths - proportions\nThe proportion of births/migrations/deaths that come from each Core Segment group\nSWD\n\n\n\nFrom these inputs, it deterministically outputs the yearly forecasts for the number of people in each Core Segment. From these yearly Core Segment population figures, we can also forecast use by point of delivery by taking historic SWD information on the activity used by current Core Segment breakdown, under the assumption that stays the same into the future.\nWe combine these population health segment projections – i.e., how many people will be in which Core Segment in what future year – with recent NHS healthcare usage data to yield forecasted changes for various delivery points, like Emergency Department (ED) visits or maternity service appointments."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/05/08/dpm.html#findings",
    "href": "applied-insights/case-studies/posts/2024/05/08/dpm.html#findings",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "Findings",
    "text": "Findings\nThe first output of the model is the population forecast for each Core Segment, as plotted in Figure 3. The visualisation is a type of sankey diagram called an alluvial plot, which shows the proportion of people moving between the Core Segments each year. As it is to be expected, the majority of individuals stay in the same Core Segment year-on-year as the process of acquiring conditions and developing multimorbidity takes places over many years and decades.\nThe concerning insight shown in Figure 3 is that all Core Segments apart from (the most healthy) Core Segment 1 are due to increase in size, with Core Segment 5 having the largest percentage increase over the next 20 years. While, at first glance, this could be attributed to the effect an ageing population, in which people are staying alive for longer we will see in the next set of results that this itself does not wholly explain the forecasted Core Segment changes.\n\n\n\n\n\n\nFigure 3: All Core Segments, except the most healthy (CS1), are forecast to increase in size. BNSSG Population rescaled to have an initial population of 1,000.\n\n\n\nIn applying the typical NHS healthcare usage per Core Segment to the projections of Figure 3, we derive the expected future healthcare usage for various healthcare settings (Figure 4). In overlaying to these the equivalent projections due solely to demographic factors (both for total population size and capturing the effect of Age and Sex), we see that the DPM projections for increased resource use are not solely attributable to an ageing and growing population, but also to a population becoming gradually less healthy over time.\nSpecifically, from Figure 4 we can glean the following insights:\n\nIn all areas except Maternity, the DPM forecasts an increased use beyond just the growing, aging population. The reason that Maternity can be explained as the exception is due to it closely following the demographic changes forecast, specifically for numbers of women of child bearing age.\nFor Community contacts, with the highest proportion of use from Core Segment 5 patients, the DPM forecasts the highest increase into the future. This is because, relative to current size, the number of Core Segment 5 patients is set to increase the largest and so that has the largest impact on Community contacts, which include home visits to patients to support rehabilitation and services to manage long-term mobility issues such as physiotherapy.\nWhilst Secondary Elective and Non-Elective activity is forecast to grow at similar rates, the Carbon and Cost values are forecast to grow more for Secondary Non-Elective due to the average Carbon and Cost usage per person in Core Segment 5 being higher. In this context ‘Secondary’ is a hospital stay, with ‘Elective’ being planned and ‘Non-Elective’ being unplanned. For example, a hip replacement is elective whereas an admission following a road traffic accident is non-elective.\n\n\n\n\n\n\n\nFigure 4: Forecasts by activity, carbon, and cost for four different points of delivery."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/05/08/dpm.html#limitations",
    "href": "applied-insights/case-studies/posts/2024/05/08/dpm.html#limitations",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "Limitations",
    "text": "Limitations\n\nIt’s difficult to make predictions, especially about the future.\n– Danish Proverb\n\nAs with any modelling / forecasting method, there are limitations to be mindful of.\n\nThe cost and activity usage estimates are made under the assumption that we will continue to deliver services as they are currently being delivered. We know this isn’t going to be true, as healthcare-seeking behaviour evolves over time, with younger people accessing healthcare in different ways to previous generations. On top of that, healthcare advances can result in significant changes in healthcare provision, in ways unaccounted for within this model.\nThe model is tied to ONS forecasts for population change, and robust forecasting is hard. It is difficult to estimate what the population will look like in 20 years’ time, and the influence of uncertain and unknown future local development and housing plans. Having said this, population forecasts tend to be robust, one way to consider this is that everyone who will be an adult by the end of the forecast in 20 years’ time has already been born.\nThe DPM does not explicitly account for the interaction of demand and capacity: it simply predicts future healthcare resource requirement assuming that health needs of a given Core Segment patient are met in the same way they are met now. This is an essential assumption to help ensure legitimate use of the empirically derived Core Segment transition rates. However, it inevitably limits practical use, as flexing demand and capacity assumptions is of importance to planners and service managers.\nIt is not possible to validate the model on historic data, firstly because of point 3. above but also because we only have good quality SWD information for the past two years, so cannot reliably look further back into the past and create a forecast that we can check against what actually happened.\nWhilst it is possible to use the model in other healthcare systems and geographic areas, the underlying data required to generate the Core Segments is non-trivial, so significant data pipelining may be required to get to create local model inputs, as explained above in Section 3."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/05/08/dpm.html#what-next",
    "href": "applied-insights/case-studies/posts/2024/05/08/dpm.html#what-next",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "What Next",
    "text": "What Next\nWe have already generated local use cases for the DPM in forecasting different geographical areas or specific hospital trusts. We envisage the DPM becoming a standard tool in most forward planning initiatives and will continue to refine the model as more information becomes available both for calibration and validation.\nOutside of BNSSG, we are keen to disseminate our modelling approach to others who may be interested, as well as expanding our collaboration. There are also other innovative approaches in this space, such as the Health in 2040 report by the Health Foundation which looks at England-level and uses the same ONS forecasts, but using a different ‘micro simulation’ modelling approach.\n\nIf long-term forecasting in the NHS is of interest to you and your work, we’d love to chat! Please get in touch at bnssg.analytics@nhs.net"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/05/08/dpm.html#summary",
    "href": "applied-insights/case-studies/posts/2024/05/08/dpm.html#summary",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "Summary",
    "text": "Summary\nReliably forecasting longer-term population health needs and healthcare resource requirements is essential if the NHS is to effectively plan for tomorrow’s problems today.\nWhile this is undoubtedly a difficult problem – both conceptually and statistically – our modelling, undertaken through an academic-NHS collaboration, demonstrates that there are alternatives beyond the commonly-used but simplistic approaches based only on demographic factors.\n\nFind more case studies\n\n\n\n\n\nAbout the authors\n\nLuke Shaw is a Data Scientist working in the NHS.\n\n\nRich Wood is Head of Modelling Analytics at BNSSG ICB and Senior Visiting Research Follow at University of Bath School of Management.\n\n\nChristos Vasilakis is Director of the Centre for Healthcare Innovation and Improvement (CHI2), and Professor at the University of Bath School of Management.\n\n\nZehra Onen Dumlu is a Research Associate at CHI2 and Lecturer at the University of Bath.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Luke Shaw\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nShaw, Luke et al 2024. “Forecasting the Health Needs of a Changing Population” Real World Data Science, May 08, 2024. URL"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/19/use-case-2.html",
    "href": "applied-insights/case-studies/posts/2024/11/19/use-case-2.html",
    "title": "Translating the Curated Data Model into Practice - Climate resiliency of skilled nursing facilities",
    "section": "",
    "text": "Acknowledgments: This research was sponsored by the:  Unites States Census Bureau Agreement No. 01-21-MOU-06 and  Alfred P. Sloan Foundation Grant No. G-2022-19536"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/19/use-case-2.html#introduction",
    "href": "applied-insights/case-studies/posts/2024/11/19/use-case-2.html#introduction",
    "title": "Translating the Curated Data Model into Practice - Climate resiliency of skilled nursing facilities",
    "section": "1 Introduction",
    "text": "1 Introduction\nHere, we demonstrate how the CDE Framework can be implemented for a research use case related to skilled nursing facilities. The framework provides the guiding principles for ethical, transparent, and reproducible research and dissemination and the research process for developing the statistical product.\nAcross the US, federally regulated skilled nursing facilities (SNFs) provide essential care, rehabilitation, and related health services to about 1.3 million people. An SNF is a facility that meets specific federal regulatory certification requirements that enable it to provide short-term inpatient care and services to patients who require medical, nursing, or rehabilitative services. Their patients can be among the most vulnerable members of our society, and yet, historically, SNFs have not been incorporated into existing emergency response systems. For example, during the 2004 Florida hurricane season, SNFs were given the same priority as day spas for restoring electricity, telephones, water, and other essential services (Hyer et al. 2006). Even worse are the deaths of SNF residents in Louisiana following Hurricanes Katrina and Rita in 2005 (Dosa et al. 2008). This was still an issue in 2021. In Louisiana, 15 SNF residents died when evacuated to a warehouse during Hurricane Ida (2021), and 12 died in Florida as a result of Hurricane Irma (2017). In both instances, the deaths were attributed to extreme heat and lack of electricity (Skarha et al. 2021).\nThese events prompted the (The White House 2022) initiative, Protecting Seniors by Improving Safety and Quality of Care in the Nation’s Nursing Homes, stating, ‘All people deserve to be treated with dignity and respect and to have access to quality medical care.’\nHowever, there are questions that need to be addressed to best protect SNFs and their residents. For example, how resilient are SNFs in extreme climate events? This use case demonstration shows how we built a new statistical product to address this question using the CDE Framework (Lancaster et al. 2023)."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/19/use-case-2.html#purposes-and-uses",
    "href": "applied-insights/case-studies/posts/2024/11/19/use-case-2.html#purposes-and-uses",
    "title": "Translating the Curated Data Model into Practice - Climate resiliency of skilled nursing facilities",
    "section": "2 Purposes and uses",
    "text": "2 Purposes and uses\nA skilled nursing facility (SNF) is a federally regulated nursing facility with the staff and equipment to provide skilled nursing care, skilled rehabilitation services, and other related health services (Medicare & Medicaid Services 2023). The context of this use case is to create a baseline picture of SNFs in Virginia and then integrate information on the risk of extreme flood events to assess facility and community preparedness – for example, how likely are the nursing staff1 to make it to the facility in the event of a flood?\nThis use case has two parts. The first creates a baseline data picture of SNFs, bringing together data about the residents, nursing staff, and SNF characteristics. The second addresses two issues raised in the (The White House 2022) initiative: emergency preparedness and nurse staffing. We frame these issues into three purpose and use questions with the ultimate goal of creating statistical products that address these questions:\n\nCan SNF workers get to work during an extreme flood event?\nAre SNFs prepared for a flood emergency?\nCan communities support SNFs during an emergency?"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/19/use-case-2.html#statistical-product-development-stages",
    "href": "applied-insights/case-studies/posts/2024/11/19/use-case-2.html#statistical-product-development-stages",
    "title": "Translating the Curated Data Model into Practice - Climate resiliency of skilled nursing facilities",
    "section": "3 Statistical product development stages",
    "text": "3 Statistical product development stages\nSubject matter input and literature review\nThe subject matter experts consulted included nursing facility administrators, SNF resident advocates, demographers, and researchers. Our discussions and literature review informed us of the many federal policies governing SNFs regarding inspections and data reporting requirements (procedural data). In addition, we were told about non-public data sources on residents and SNF staff that were aggregated to the SNF level and provided to the public under a grant from the National Institute on Aging. This information was important since we had yet to come across this source in our data discovery process. The dialogue with experts and our literature review helped us generate a ‘wish list’ of variables we used to inform our data discovery process that we visualized into a conceptual data map (see Figure 1).\n\n\n\n\n\n\nFigure 1: Conceptual Data Map Aligned to Purpose and Use: The conceptual data map displays the results of our data discovery. The team identifies the data needs informed by expert elicitation and literature review. For this use case the data discovery took three phases: (1) create a data picture of SNF owners, nursing staff, and residents, and the communities the facilities reside in; (2) identify the potential risks of a severe flood events, coastal and riverine; and (3) identify the potential weakness in the SNF’s and community’s ability to respond.\n\n\n\nData discovery\nData discovery focused on identifying data sources to address the purpose and use questions and was informed by the conceptual data map.\nFor the first question – Can SNF workers get to work during an extreme flood event? – we discovered and used proprietary synthetic population, transportation routes, building data sources, and publicly available flood data. The HERE Premium Streets proprietary data includes information about roads, such as type of road, speed limits, number of lanes, etc. The proprietary synthetic population data, Building Knowledge Base (BKB), are used to identify where SNF workers live and work to map transportation routes from home to work (Mortveit, Xie, and Marathe 2023). Publicly available data from the Federal Emergency Management Administration (FEMA) provided flooding risk estimates along the routes from nursing staff homes to the SNF.\nFor the second question – Are SNFs prepared for a flood emergency? – we used Center for Medicare and Medicaid (CMS) SNF inspection and deficiency data as a proxy for preparedness. We also examined SNF residents’ physical and mental health to assess SNF emergency preparedness. For example, if most residents faced mobility challenges, the SNF would need more resources available during an emergency to move residents to a safer facility. We used data about residents from the Long Term Care Focus (LTCFocus 2022) Public Use Data sponsored by the National Institute on Aging (Brown University 2022).\nWe used data to measure community resilience, assets, and risks by geography at the county, city, and census tract levels to address the third question, Can communities support SNFs during an emergency? These data included:\n\nHealth professional shortages area (HRSA 2022)\nShelter facilities and emergency service providers data (Homeland Security: Geospatial Management Office 2022)\nCommunity Resilience Indicator Analysis and National Risk Index for Natural Hazards (FEMA 2022).\n\nAll data are provided in a GitHub repository along with their metadata, except for the three proprietary data sources. Articles about how the synthetic estimates are constructed are provided for two of these proprietary data sources. The third data source was obtained from a private-sector vendor whose data and documentation are proprietary; a link is provided to their website.\nData ingest and governance\nAll the public data, metadata, code, statistical products, data processes, and relevant literature on SNF policies and regulations are stored in a GitHub repository.\nIn our experience, data wrangling is the most time-consuming and challenging part of product development. This speaks directly to the benefit of the CDE; once a researcher has wrangled together multiple data sources, it can be made available to other researchers.\nThe two predominant issues with data wrangling for this Use Case included reconciling data sources that contain data on the same topic and creating linkages between data sources. For example, we reviewed three hospital data sources:\n\nHomeland Security Infrastructure Foundation-Level Data (HIFLD) (DHS 2022)\nHealthData.gov - COVID-19 Reported Patient Impact and Hospital Capacity by State (HHS 2022)\nMap of VHHA Hospital and Health System Members (Virginia Hospital & Healthcare Association 2022)\n\nWe observed inconsistences and omissions across the three data sources including: \n\nnon-standard hospital names and hospital classification types\ninconsistent availability of hospital IDs (such as Medicare Provider Number)  \nconflicting geographic information, including address, latitude, and longitude.\n\nWe did not attempt to reconcile these inconsistencies for the demonstration but decided to use a single source for shelter facility and emergency service provider data. We used HIFLD data since they provided the most current data (DHS 2022). The use of these data reinforces the purpose of the use case – to illuminate the challenges in creating statistical products and what the Census Bureau would need to consider.\nSimilar inconsistencies made it difficult to link data sources using geographic variables. For example, we used shelter facility and emergency service provider data sources from the HIFLD – including hospitals, Red Cross chapter facilities, National Shelter System Facilities, emergency medical service stations, fire stations, and urgent care facilities – to calculate a metric for potential community support. The goal was to place each facility in a Virginia county or independent city. Virginia is divided into 95 counties, and 38 independent cities considered county-equivalents for census purposes, and in some cases, there is a county and a city with the same name (eg, Richmond County and Richmond City, each in different locations in Virginia). It was necessary to canonicalize the county and city names (when available), which meant aligning upper and lower cases, removing unnecessary characters, and distinguishing between county and city.2\nThe challenge with locating shelter facilities and emergency service providers within a county or independent city was using different variables to identify their location (latitude and longitude, address, ZIP code3, Federal Information and Processing Standard (FIPS) code, and county/city name). In cases where the data source only had a ZIP or FIPS code, a Department of Housing and Urban Development crosswalk was used to link the two codes; in other cases, a crosswalk that linked non-independent cities and towns to counties was used; and in others, a crosswalk that linked FIP codes to counties and independent cities. Researchers would benefit from exhaustive crosswalks between all variables on the same topic, such as location variables, facility names, and identification numbers, to reduce the time spent on data wrangling.\nRegarding data products related to popular indices, such as climate disaster risks and community resilience, they are operationalized differently across the various departments and agencies within the federal and state governments and private and non-profit sectors. It is an enormous task to review the methodology and technology reports (if available) to understand their differences and decide which versions are most relevant (fitness-for-purpose) for a particular use case. Again, after reviewing the options for this use case, we determined that the National Risk Index for riverine and coastal floods from FEMA was the best option for climate risk estimates. The detailed technical report, National Risk Index Technical Document (FEMA 2021), provides a clear assessment of the assumptions and limitations of the data and a description of how the risk estimates were derived. Researchers would benefit from guidance on the numerous constructions of indices on the same topic. A use case on a specific index topic could be used to highlight differences and similarities among indices, which would help with data wrangling and fitness-for-use. Ideally, the use case could benchmark the various constructions and provide a statistical assessment.\n\n3.1 Question 1: Can SNF workers get to work during an extreme flooding event?\nSufficient nursing staff is of significant concern to assure resident safety and quality of care.\nSince proprietary synthetic population data and commercial sector digitized mapping data were used to construct the routes SNF nursing staff are likely to take from home to work, only an outline of the computational process used to identify the routes is provided. Publicly available data from FEMA were used to estimate flooding risk along a particular route. Below is a general description of the modeling steps and the proprietary data used to assess SNF vulnerability as a function of the nursing staff’s inability to report to work due to the transportation infrastructure (Choupani and Mamdoohi 2016).\nComputational modules\nHere is the basic outline of the process that uses proprietary data that starts at network construction and ends with routes. For more details, see the GitHub repository: Vulnerability of SNFs concerning Commuting.\n\nExtract network data from HERE (2021 Q1 in this use case).\nProcess the extracted data to form a network suitable for routing. This includes inference of speed limits for road links where such data is missing.\nPrepare origin-destination pairs. In this case, the list of locations pairs a worker’s home and work locations. The person is constructed in the synthetic population pipeline, and residences and workplaces are derived through the data fusion process used to construct the NSSAC building database.\nConstruct routes using the Quest router.\n\nOnce the routes to an SNF were established, the expected number of nursing staff at an SNF during a flood event could be calculated as the sum of the probabilities of each worker being able to commute to work during a flood event. A computational model was developed using the following data:\n\nSNF locations in Virginia from the Centers for Medicare & Medicaid Services (CMS);\nHome locations of workers at each SNF assigned from the synthetic population and Building Knowledge Base (Beckman, Baggerly, and McKay 1996; Mortveit, Xie, and Marathe 2023);\nVirginia road networks; and\nFEMA census tract-level riverine and coastal flood risks.\n\nUsing router software, the Virginia road network was used from the HERE map data to compute each nursing staff’s likely route to their SNF. Routers are commonly used within transportation and traffic simulators. The router software used for this demonstration is a highly parallelizable router previously developed in BI NSSAC, known as the Simba router (Barrett et al. 2013).\nThe FEMA risk data provide the riverine and coastal flood risks for each census tract in Virginia. Given the routes, the FEMA riverine and coastal flood risks were used to estimate the probability of the nursing staff making it to work. The FEMA technical document National Risk Index Technical Document (FEMA 2021) provides information on how natural hazard risks are calculated. We use these risk estimates ranging from 0 to 100 as a proxy for the probability a worker can reach the SNF by dividing by 100. For example, we assume a risk is zero if there is zero probability of being unable to reach the SNF due to an extreme flood event.\nIn contrast, a risk of 100 indicates the roads are underwater, and the probability of being unable to reach the SNF is one. The maximum risks along transportation routes leading to an SNF range from 0 to 47 for riverine flooding and 0 to 40 for coastal flooding. We assume the combined value of the maximum riverine and coastal flood risks along a worker’s transportation routes, divided by 100, is the worker’s probability of not getting to work during a flooding event.\nSince we do not have data on the exact home locations of the nursing staff, we estimated how many could reach the facility by taking a random sample (whose size is the CMS average daily nursing staff4 for an SNF) from the possible routes identified using the HERE Virginia road network. We calculated the average with a 95% nonparametric confidence interval. The 283 SNFs used in our research have an average daily nursing staff of 12,609. Using the above approach, we estimated that 10,005 (95% CI: 9,013, 10,700) or 79% can get work during an extreme flood event. The individual SNF nursing staff percentage who can make it to work ranges from 48% to 93%.\nFigure 2 visualizes this analysis for the 283 SNFs ordered by the observed average daily nursing staff numbers at the facility from smallest to largest, displayed using the orange line. The black line indicates the expected number in an extreme flood event and the 95% nonparametric confidence interval (grey band). The code for Figure 2 is provided in the GitHub repository.\n\n\n\n\n\n\nFigure 2: SNF Average Observed and Expected Average Daily Nursing Staff Numbers: The horizontal axis is ordered by the size of the nursing staff at the facility from smallest to largest. The orange line displays the observed average daily nursing staff numbers. The black line displays the estimated numbers in the event of an extreme coastal and/or riverine flood event. The grey band is the 95% nonparametric confidence interval.\n\n\n\nFor example, in King George County, the SNF is Heritage Hall King George (Federal Provider Number 495300 in Figure 3), located near the Potomac River, which opens to the Chesapeake Bay. According to CMS, the Heritage Hall King George facility has an average daily skilled nursing staff of 41. Using the HERE Virginia road network, we identified 101 routes the staff could use to reach the facility. The combined maximum coastal and riverine flood risks along these routes ranged from 5.6 to 66.7; a random sample of 41 from the 101 routes gives an average probability of reaching the facility of 0.74 with a 95% nonparametric confidence interval of [0.65, 0.80]. These were used to estimate the average number of nursing staff at the facility, 30, during a flood event, along with a 95% nonparametric confidence interval [14, 38]. Publicly available data from the Federal Emergency Management Administration (FEMA) provided flooding risk estimates along the routes from the nursing staff home to the SNF along with proprietary road and building information.\n\n\n\n\n\n\nFigure 3: An Example of Nursing Staff Routes to Heritage Hall King George SNF: Routes that workers can take to work at Heritage Hall  King George SNF FPN 495300 (identified with the black oval). The risk levels of each road are identified with colors, from low risk (blue), medium-low (yellow), orange (medium), red (medium-high), to high risk (dark red). The risk scores are used to calculate the probability of a worker getting to work during an extreme flood event using publicly available FEMA data and proprietary road and building data.\n\n\n\n\n\n3.2 Question 2. Are SNFs prepared for emergencies?\nTo address this question, we examined how prepared SNFs are for emergencies using annual inspection and deficiency data as a proxy for preparedness. CMS issues deficiencies to SNFs that fail to meet federal Medicare and Medicaid preparedness standards. Every deficiency is classified into one of 12 categories based on the scope and severity of the deficiency. There are two broad types of non-health-related deficiencies:\n\nEmergency Preparedness Deficiencies – There are four elements of emergency preparedness. They cover an emergency plan, policies and procedures, a communication plan, and training and testing.\nFire Life Safety Code – The set of fire protection requirements are designed to provide a reasonable degree of safety from fire. They cover construction, protection, and operational features designed to provide safety from fire, smoke, and panic.\n\nWe calculated separate Emergency Preparedness and Fire Life Safety Code deficiency indices to combine them to create a single index to measure SNF preparedness and distinguish between high and low performing SNFs. The computation of the indices has four steps.\n\nNumber of deficiencies: For each SNF, the total number of deficiencies during the past four years, 2018-2022, was divided by the number of SNF inspections over the same period to estimate the average number of deficiencies per inspection.\nTime to resolve deficiencies: We next computed the average number of days it took to resolve each deficiency.\nScope and severity of deficiencies: We then transformed the deficiency letter inspection rating for scope and severity to a numerical weight using the CMS technical guide, Care Compare Nursing Home Five-Star Quality Rating System (Medicare & Medicaid Services 2022),and averaged the ratings.\nThe estimates from these three steps were summed to compute separate Emergency Preparedness and Fire Life Safety Code deficiency indices (see Figure 4) and are provided for reuse in a .csv file on GitHub.\n\nFigure 4 displays the results of an exploratory data analysis for each index. These analyses assessed fitness-for-use; we wanted to construct an indicator with sufficient variability to discriminate between high and low-performing SNFs. It is evident we accomplished this in Figure 4 there are SNFs with indices outside the main body of the data. We summed the Emergency Preparedness and Fire Life Safety Code indices and categorized them into high, medium, low, and no deficiencies.\n\n\n\n\n\n\nFigure 4: Exploratory Data Analysis Visualizations for the Emergency Preparedness and Fire Life Safety Code Deficiencies\n\n\n\n\n\n3.3 Question 3: Can communities support SNFs during emergencies?\nTo answer this question, we computed a community resiliency index using the US Census American Community Survey and the guidance provided by the Homeland Security document Community Resilience Indicator Analysis: County-Level Analysis of Commonly Used Indicators from Peer-Reviewed Research (Edgemon et al. 2018). The index was constructed by summing the county (census tract) level percentages for the following variables:\n\nfraction employed\nfraction with no disability\nfraction with a high school diploma or greater\nfraction of households with at least one vehicle\nreverse GINI Index – so all indicators are in a positive direction.\n\nFigure 5 displays the combined deficiency indices, Emergency Preparedness + Fire Life Safety Code, for each SNF with the choropleth map for the community resilience index at the census tract level. We also examined the number of shelter facilities and emergency service providers and the availability of medical staff per 10,000 residents. We constructed isochrones to establish the distance from the SNF to these potential sources of support. Working on this component of the use case highlighted the need for cross-agency data, pointing to the utility of future strategic partnering between the US Census Bureau, CMS, and FEMA.\n\n\n\n\n\n\nFigure 5: 2020 Population Resilience Composite Index for Virginia Census Tracts: The light yellow tracts are the least resilient, and the dark green are the most resilient. The locations of the 283 SNFs are identified with filled circles, orange circles with the highest\n\n\n\nIn addition to describing the population using a resilience index, we also developed a measure to present the number of shelter facilities and emergency service providers (data from Homeland Security / Homeland Infrastructure Foundation Level Data) and the availability of medical doctors (MDs) and Doctor of Osteopathic Medicine (ODs) who provide direct patient care (HRSA 2022) (Figure 6). \nThe number of MDs and ODs is described as a primary care health professional shortage area. HRSA defines these contiguous areas where primary medical care professionals are overutilized, excessively distant, or inaccessible to the population of the area under consideration. Figure 6 (bottom) shows that approximately one-third of the counties and independent cities have health professional shortage areas across their entire boundary, and another 40 percent have shortages within parts of their boundaries.\n\n\n\n\n\n\nFigure 6: Assessment of the number of shelter facilities and emergency service providers per 10,000 population (top) and medically underserved areas (bottom): On both maps, the lighter the color, the more in need is the population of shelter facilities and emergency services (top chart) or health professionals (bottom chart). The location of the 283 SNFs are identified with filled circles, orange circles are those with the highest deficiency index and grey circles are those with no deficiencies."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/19/use-case-2.html#guiding-principles-for-ethical-transparent-reproducible-statistical-product-development-and-dissemination.",
    "href": "applied-insights/case-studies/posts/2024/11/19/use-case-2.html#guiding-principles-for-ethical-transparent-reproducible-statistical-product-development-and-dissemination.",
    "title": "Translating the Curated Data Model into Practice - Climate resiliency of skilled nursing facilities",
    "section": "4 Guiding principles for ethical, transparent, reproducible statistical product development and dissemination.",
    "text": "4 Guiding principles for ethical, transparent, reproducible statistical product development and dissemination.\nCommunication\nWe communicated results throughout the Demonstration Use Case research with our Census CDE Working Group (composed of former Census Bureau Directors and Communication Director, and academic and industry census experts), with the Census Bureau, at conferences such as the annual Federal Statistical Committee on Methodology, and sharing drafts to seek input and ideas. The discussions and presentations helped to shape ideas and advance our thinking about how best to address the purpose and use questions.\nStakeholder engagement\nWe engaged stakeholders by sharing our research and results through conference presentations at the American Community Survey Data Users Conference and the Applied Public Data Users Conference. We also shared this demonstration project at Listening Sessions with stakeholders as an example of statistical product development. The Listening Sessions bring together 7 to 12 stakeholders by topic (e.g., children’s health) or function (e.g., state demographers) to seek their ideas for new statistical products.\nEquity and ethics\nAs described in the Introduction, there are ethics and equity issues that drew us to develop this Use Case. Here we focus on equity and ethics vis-a-vis the data choices and analyses. With regard to ethical considerations with our data discovery process, fitness-for-purpose evaluation, and analyses, two questions arose:\n\nWhat role does synthetic data have to play, and how do you benchmark it to evaluate fitness-for-purpose?\nHow do you construct and evaluate an index with the goal of identifying vulnerable populations?\n\nRealizing the importance of nursing staff levels, we discussed and questioned whether the synthetic data had biases and were not representative of SNF residents and employees. We benchmarked the synthetic SNF nursing staff numbers against those submitted quarterly to CMS and observed they were biased low, so we decided to use the CMS data. These data were used to estimate the average number of nursing staff that could reach the facility during an extreme flood event (Figure 2).\nIn this use case, we were fortunate to have the “truth” to benchmark the synthetic data for the average daily nursing staff at each SNF. But this was not the case for the home locations of the nursing staff, therefore, the synthetic locations were not used since we had no way to benchmark them. Ideally, we would use the actual addresses of SNF employees. Instead, we used a simulation to estimate the average risks over routes leading to the SNF. This approach could be replaced with (or benchmarked against) the Census commuting data sets (eg, Commuting Flows or the LEHD Origin-Destination Employment Statistics) and the home census tract used as the starting point for each worker. For the number of nursing staff and their home locations, it is impossible to identify potential biases that would result in the inequitable allocation of emergency rescue resources without a thorough understanding of how the synthetic data were generated.\nHow one evaluates the equity of an index is a more challenging task. Questions that need to be addressed include:\n\nHow do you select the variables used to construct an indicator to guide an equitable allocation of technical assistance?\nWhat relationship between these variables is important?\nWhat are the differences across the numerous publicly available resilience estimators? Do some lead to a more equitable allocation of technical assistance in the event of an extreme clime event?\nHow do you validate a resilience estimator?\n\nThe technical document Community Resilience Indicator Analysis: County-Level Analysis of Commonly Used Indicators from Peer-Reviewed Research (Edgemon et al. 2018) identified the 20 most commonly selected variables for constructing resilience estimators from peer-reviewed research. Future research will need to validate these indices against past extreme climate events.\nPrivacy and confidentiality\nWe did not do a full disclosure review. However, some data are proprietary, and we could not release those data. We discuss how we used these data.\nDissemination\nWe disseminated the final version of the use case in the University of Virginia Libra Open repository (Lancaster et al. 2023).\nCuration\nCuration involves documenting all steps of the process so that they can be repeated, validated, reused, or extended. The final report explains the process in words. Curation must also provide the data, metadata, source code, and products. This led us to construct a GitHub repository. A README file guides the reader through the material and provides instructions for replicating the research results. Note that the README file must be downloaded for the hyperlinks to work."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/19/use-case-2.html#using-the-snf-statistical-product",
    "href": "applied-insights/case-studies/posts/2024/11/19/use-case-2.html#using-the-snf-statistical-product",
    "title": "Translating the Curated Data Model into Practice - Climate resiliency of skilled nursing facilities",
    "section": "5 Using the SNF statistical product",
    "text": "5 Using the SNF statistical product\nThis potential statistical product has many uses. Federal policymakers and administrators regulate SNFs; however, they only sometimes realize the impacts on costs and the need for increased resources to meet these regulations. For example, by reviewing the aggregate inspection deficiency metrics, policymakers can target resources where they are most needed. Providing additional funding to pay workers more, improve their facilities, and address inspection deficiencies would improve the quality of SNFs. \nThe media and advocacy groups play a role in highlighting good and bad cases of SNF care or where communities do not have adequate assets to support SNFs during an emergency event. For example, a New Yorker article (Rafiei 2022) highlighted how nursing homes decline dramatically when bought by private equity owners. The GAO (September 22, 2023) recently identified the need for more information about private equity ownership in CMS data – a gap that CMS needs to address. And, of course, researchers and analysts are essential for conducting research that leads to creating and improving statistical products around SNFs. By releasing a regularly scheduled SNF statistical product, the changes in SNFs over time can be monitored."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/19/use-case-2.html#what-cde-capabilities-have-this-use-case-demonstrated",
    "href": "applied-insights/case-studies/posts/2024/11/19/use-case-2.html#what-cde-capabilities-have-this-use-case-demonstrated",
    "title": "Translating the Curated Data Model into Practice - Climate resiliency of skilled nursing facilities",
    "section": "6 What CDE capabilities have this use case demonstrated?",
    "text": "6 What CDE capabilities have this use case demonstrated?\nAs demonstrated by this use case, the CDE Framework is a powerful process for guiding and curating the development of statistics to address complex purposes and uses. Additionally, use cases help illuminate technical capabilities that should be present in the data enterprise to facilitate and accelerate the reuse of data and methods in the development and dissemination of new statistical products.\nThis CDE demonstration is the first of many use cases needed to define and develop CDE capabilities. Underlying each use case is the curation process. Curation documents each step, including decisions that may involve trade-offs. Curation preserves and adds value to the data. This includes organizing to facilitate data discovery and easy access; providing metadata to enable the reuse in scientific and programmatic research; enhancing the value of the data enterprise through linkages between datasets; and mapping the network of interconnections between datasets, research outputs, researchers, and institutions. Over time, a searchable curation system will be needed as a foundation for creating statistical products in the CDE.\nThe types of products from a use case that can benefit the larger community are only limited by the creativity of the researchers and stakeholders carrying out the use case. The products from this use case are re-useable code; integrated data sets across diverse topics for each SNF; maps and other visualizations; statistical products such as SNF deficiency indices and various indices that measure community and SNF resilience; the probability of a worker reaching an SNF in the event of extreme flooding; and a GitHub repo that provides easy access to all these products plus relevant metadata, literature, and government documents and regulations.\nConducting this use case has been an eye-opening experience as to the amount and quality of publicly available data to address our research questions. The statistical capabilities and products flowing from diverse use cases can only be identified as the program progresses.\n\n\n\n\n← Part 2: What is the CDE?\n\n\n\n\nPart 4: Census Curated Data Enterprise Environment →\n\n\n\n\n\n\n\n\nAbout the authors\n\nVicki Lancaster is a statistician with expertise in experimental design, linear models, computation, visualizations, data analysis, and interpretation. She works with scientists at federal agencies on projects requiring statistical skills and creativity, eg, defining skilled technical workforce using novel data sources.\n\n\nStephanie Shipp leads the Curated Data Enterprise research portfolio and collaborates with the US Census. She is an economist with experience in data science, survey statistics, public policy, innovation, ethics, and evaluation.\n\n\nSallie Keller is the Chief Scientist and Associate Director of Research and Methodology at the US Census Bureau. She is a statistician with research interest in social and decision informatics, statistics underpinnings of data science, and data access and confidentiality. Sallie Keller was at the University of Virginia when this work was conducted.\n\n\nAaron Schroeder has experience in the technologies and related policies of information and data integration and systems analysis, including policy and program development and implementation.\n\n\nHenning Mortveit develops massively interacting systems and the mathematics supporting rigorous analysis and understanding of their stability and resiliency.\n\n\nSamarth Swarup conducts research in computational social science, resiliency and sustainability, and stimulation analytics.\n\n\nDawen Xie develops geographic information systems, visual analytics, information management systems, and databases, with a current focus on building dynamic web systems.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Stephanie Shipp\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Ground Picture on Shutterstock.\n\n\n\nHow to cite\n\nLancaster V, Shipp S, Keller S et al. (2024). “Translating the Curated Data Model into Practice - climate resiliency of skilled nursing facilities” Real World Data Science, November 19, 2024. URL"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/19/use-case-2.html#footnotes",
    "href": "applied-insights/case-studies/posts/2024/11/19/use-case-2.html#footnotes",
    "title": "Translating the Curated Data Model into Practice - Climate resiliency of skilled nursing facilities",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNursing staff includes medical aides and technicians, certified nursing assistants, licensed practical nurses (LPNs), LPNs with administrative duties, registered nurses (RNs), RNs with administrative duties, and the RN director of nursing.↩︎\nFor example, distinguishing county from city when the name is the same could be done using State/County FIPS codes. Richmond County is 51159; Richmond City is 51760.↩︎\nZIP code is a system of postal codes used by the United States Postal Service. ZIP was chosen to indicate mail travels more quickly when senders use the postal code.↩︎\nAverage Daily Nursing Staff is the daily number of Medical Aides and Technicians, CNAs, LPNs, LPNs with administrative duties, RNs, RNs with administrative duties, and RN Director of Nursing averaged over three months.↩︎"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/01/policy-problem.html",
    "href": "applied-insights/case-studies/posts/2024/11/01/policy-problem.html",
    "title": "Advancing Data Science in Official Statistics – The Policy Problem",
    "section": "",
    "text": "Acknowledgments: This research was sponsored by the:  United States Census Bureau Agreement No. 01-21-MOU-06 and  Alfred P. Sloan Foundation Grant No. G-2022-19536\nThe views expressed in this artice are those of the authors and not the Census Bureau."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/01/policy-problem.html#introduction",
    "href": "applied-insights/case-studies/posts/2024/11/01/policy-problem.html#introduction",
    "title": "Advancing Data Science in Official Statistics – The Policy Problem",
    "section": "Introduction",
    "text": "Introduction\nTwo centuries ago, when the Framers of the US Constitution laid the cornerstone for the federal statistical system, they could not have imagined the complexity of questions future generations would want to ask or the variety of data sources available to address them. Back in 1787, counting the population and apportioning state seats in the House of Representatives were the most urgent tasks before the young nation, and so a requirement for a decennial census was written into the Constitution. Now, 233 years later, the census continues to serve its original purpose – but purposes and uses for census data have exploded.\nQuestions we now seek to answer go beyond what the census (or surveys) alone can hope to address. Even with the multitude of other surveys commissioned by today’s US Census Bureau, researchers and policymakers find themselves looking to novel sources of data – from structured numeric data in traditional databases to unstructured text documents scraped from the internet – to explore issues such as understanding how prepared nursing homes and communities are for extreme climate events,eg, hurricanes, wildfires, or floods. Wrangling these sources with traditionally designed data, such as censuses and surveys, can fill data gaps, improve the quality and usefulness of statistical products, speed up their dissemination, and inspire the creation of new types of statistical products.\nThat is the impetus for developing the Curated Data Enterprise (CDE), an innovation in data science aimed at creating statistical products from all data types and building the infrastructure to support them. The Curated Data Enterprise, as the name implies, includes an end-to-end curation model to capture the complete statistical product development process. The CDE is designed to enable data discovery and retrieval, data quality assessment across multiple and diverse sources of information, and the reuse of data and models over time to accelerate statistical product development. The US Census Bureau has partnered with the University of Virginia, a working group of former Census Bureau Directors, a Communication Director, and university, non-profit and industry experts to develop this approach.\n\n\n\n\n\n\nThe US Census Bureau\n\n\n\nThe US Census Bureau provides the latest official statistics, facts, and figures about America’s people, places, and economy. It collects data for 130 surveys annually and the decennial census that gives the Bureau its name. The US Census Bureau collects data from households, businesses, governments and non-profit organizations. For each survey, tabulations and margins of error are published in news releases and reports. Public-use microdata subject to disclosure rules are provided for household and demographic surveys. Microdata for economic and household surveys, without disclosure rules applied, are accessible to researchers through the Federal Statistical Research Data Centers.\nStatistical agencies in other countries are also modernizing their surveys and statistical product development. See a summary of selected countries (Lanman, Davis, and Shipp 2023)."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/01/policy-problem.html#a-new-approach",
    "href": "applied-insights/case-studies/posts/2024/11/01/policy-problem.html#a-new-approach",
    "title": "Advancing Data Science in Official Statistics – The Policy Problem",
    "section": "A new approach",
    "text": "A new approach\nTo realize the CDE vision, the development of statistical products will address stakeholder questions using all data types – designed surveys and censuses, public and private administrative data, opportunity data scraped from the internet and procedural data (Keller et al. 2022). This new approach aligns with the US Census Bureau’s modernization and transformation (Thieme 2022) while maintaining the fundamental responsibilities of statistical agencies (OMB 2023). It is also consistent with a conclusion by the NASEM Panel on the Implications of Using Multiple Data Sources for Major Survey Programs: ‘The quality of statistics produced from multiple data sources depends on properties of the individual sources as well as the methods used to combine them. A new framework of quality standards and guidelines is needed to evaluate such data sources’ fitness for use’ (NASEM 2023, 192).\nThe CDE approach provides such a framework to address many of the challenges that official statistics face today, as well as demonstrate that they are poised to adopt a new approach to producing official statistics. For example:\n\nThe timeliness and frequency of our official statistics are insufficient when there are shocks to the economy, such as the Covid-19 pandemic, when retrospective survey data were of limited usefulness. Federal agencies responded during the pandemic with relevance and agility by creating and launching fast-response Household Pulse Surveys that met immediate needs for data, trading off timeliness for quality (Groshen 2021). Public engagement and support for these new relevant and timely data products at a time of crisis were essential to the success of this new statistical product.\nThe policy environment has responded to technological, social, and survey changes by encouraging efficient use of existing data, reuse, sharing and furthering open data principles. Researchers are now creating innovative statistical products using multiple data sources to better address the US’s needs and interests. The Commission on Evidence-Based Policymaking (Abraham et al. 2018) and the Federal Data Strategy (“Federal Data Strategy, Leveraging Data as a Strategic Asset” 2021) recommendations encourage agencies to permit access to data to undertake evaluation and research studies.\nTechniques such as rapid scanning, text recognition, user-friendly uploads, and new devices, sensors, and systems can now record and transcribe data in real time. Using these techniques, governments and corporations now routinely and instantaneously collect and store data on behaviors and states as varied as purchase transactions, climate and road conditions, healthcare plan utilization, and land use and zoning. Extensive digitization and recording, better system connectedness and interactivity, and increased human-computer interaction can result in faster data accumulation, enhancing the usability of private and public administrative data while maintaining privacy and confidentiality (Brady 2019; Jarmin 2019).  \nNew techniques and data sources can transform statistical agencies ‘from the 20th-century survey-centric model to a 21st-century model that blends structured survey data with administrative and unstructured alternative digital data sources’, leading to better measures of the gig economy, retail sales, healthcare, workforce, and tools and methods to integrate multiple data sources while maintaining privacy and confidentiality (Jarmin 2019).\n\nThe next three articles in this series will:\n\nprovide an overview of the CDE and its corresponding framework\nput the CDE Framework into practice through a demonstration use case on the resilience of skilled nursing facilities\ndescribe our next steps for developing the CDE through a use case research program.\n\n\n\n\n\nPart 2: What is the Curated Data Enterprise? →\n\n\n\n\n\n\n\n\nAbout the authors\n\nSallie Keller is the Chief Scientist and Associate Director of Research and Methodology at the US Census Bureau. She is a statistician with research interest in social and decision informatics, statistics underpinnings of data science, and data access and confidentiality. Sallie Keller was at the University of Virginia when this work was conducted.\n\n\nStephanie Shipp leads the Curated Data Enterprise research portfolio and collaborates with the US Census. She is an economist with experience in data science, survey statistics, public policy, innovation, ethics, and evaluation.\n\n\nVicki Lancaster is a statistician with expertise in experimental design, linear models, computation, visualizations, data analysis, and interpretation. She works with scientists at federal agencies on projects requiring statistical skills and creativity, eg, defining skilled technical workforce using novel data sources.\n\n\nJoseph Salvo is a demographer with experience in US Census Bureau statistics and data. He makes presentations on demographic subjects to a wide range of groups about managing major demographic projects involving the analysis of large data sets for local applications.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Stephanie Shipp\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Lukas Blazek on Unsplash.\n\n\n\nHow to cite\n\nKeller S, Shipp S, Lancaster V, Salvo J (2024). “Advancing Data Science in Official Statistics: The Policy Problem.” Real World Data Science, November 01, 2024. URL"
  },
  {
    "objectID": "ts-and-cs.html",
    "href": "ts-and-cs.html",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of Real World Data Science (RWDS), its editors, the Royal Statistical Society (RSS), or other partners and funders.\nRWDS has prepared the content of this website responsibly and carefully. However, RWDS, its editors, the RSS, or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nRWDS, its editors, the RSS, or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. RWDS, its editors, the RSS, or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nRWDS, its editors, the RSS, or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use.",
    "crumbs": [
      "Terms and conditions"
    ]
  },
  {
    "objectID": "ts-and-cs.html#legal-disclaimer",
    "href": "ts-and-cs.html#legal-disclaimer",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of Real World Data Science (RWDS), its editors, the Royal Statistical Society (RSS), or other partners and funders.\nRWDS has prepared the content of this website responsibly and carefully. However, RWDS, its editors, the RSS, or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nRWDS, its editors, the RSS, or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. RWDS, its editors, the RSS, or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nRWDS, its editors, the RSS, or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use.",
    "crumbs": [
      "Terms and conditions"
    ]
  },
  {
    "objectID": "ts-and-cs.html#what-websites-do-we-link-to",
    "href": "ts-and-cs.html#what-websites-do-we-link-to",
    "title": "Terms and conditions",
    "section": "What websites do we link to?",
    "text": "What websites do we link to?\nRWDS editors and contributors recommend external web links on the basis of their suitability and usefulness for our users. Selection and addition of links to our website is entirely a matter for RWDS and for RWDS alone.\nIt is not our policy to enter into agreements for reciprocal links.\nThe inclusion of a link to an organisation’s or individual’s website does not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders of any product, service, policy or opinion of the organisation or individual. RWDS, its editors, the RSS, or other partners and funders are not responsible for the content of external websites.",
    "crumbs": [
      "Terms and conditions"
    ]
  },
  {
    "objectID": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "href": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "title": "Terms and conditions",
    "section": "What websites will we not link to?",
    "text": "What websites will we not link to?\nWe will not link to websites that contain racist, sexual or misleading content; that promote violence; that are in breach of any UK law; which are otherwise offensive to individuals or to groups of people.\nThe decision of RWDS is final and no correspondence will be entered into.\nIf you wish to report a concern, please email rwds@rss.org.uk.",
    "crumbs": [
      "Terms and conditions"
    ]
  },
  {
    "objectID": "ts-and-cs.html#notice-and-takedown-policy",
    "href": "ts-and-cs.html#notice-and-takedown-policy",
    "title": "Terms and conditions",
    "section": "Notice and Takedown policy",
    "text": "Notice and Takedown policy\nIf you are a rights holder and are concerned that you have found material on our site for which you have not given permission, or is not covered by a limitation or exception in national law, please contact us in writing stating the following:\n\nYour contact details.\nThe full bibliographic details of the material.\nThe exact and full url where you found the material.\nProof that you are the rights holder and a statement that, under penalty of perjury, you are the rights holder or are an authorised representative.\n\nContact details:\nNotice and Takedown,\nLicensing,\n12 Errol Street,\nLondon EC1Y 8LX\nweb@rss.org.uk\nUpon receipt of notification, the ‘Notice and Takedown’ procedure is then invoked as follows:\n\nWe will acknowledge receipt of your complaint by email or letter and will make an initial assessment of the validity and plausibility of the complaint.\nUpon receipt of a valid complaint the material will be temporarily removed from our website pending an agreed solution.\nWe will contact the contributor who deposited the material, if relevant. The contributor will be notified that the material is subject to a complaint, under what allegations, and will be encouraged to assuage the complaints concerned.\nThe complainant and the contributor will be encouraged to resolve the issue swiftly and amicably and to the satisfaction of both parties, with the following possible outcomes:\n\nThe material is replaced on our website unchanged.\nThe material is replaced on our website with changes.\nThe material is permanently removed from our website.\n\n\nIf the contributor and the complainant are unable to agree a solution, the material will remain unavailable through the website until a time when a resolution has been reached.",
    "crumbs": [
      "Terms and conditions"
    ]
  },
  {
    "objectID": "rwds-partners.html",
    "href": "rwds-partners.html",
    "title": "Our partners and funders",
    "section": "",
    "text": "Real World Data Science is a project of the Royal Statistical Society (RSS). The Society was founded in 1834 and is one of the world’s leading organisations advocating for the importance of statistics and data.\nRSS has more than 10,000 members in the UK and across the world. As a charity, it advocates for the key role of statistics and data in society, and works to ensure that policy formulation and decision making are informed by evidence for the public good.\nTo support the work of the RSS, including Real World Data Science and other projects, become a member today.\nEmail: info@rss.org.uk",
    "crumbs": [
      "Our partners and funders"
    ]
  },
  {
    "objectID": "rwds-partners.html#publisher",
    "href": "rwds-partners.html#publisher",
    "title": "Our partners and funders",
    "section": "",
    "text": "Real World Data Science is a project of the Royal Statistical Society (RSS). The Society was founded in 1834 and is one of the world’s leading organisations advocating for the importance of statistics and data.\nRSS has more than 10,000 members in the UK and across the world. As a charity, it advocates for the key role of statistics and data in society, and works to ensure that policy formulation and decision making are informed by evidence for the public good.\nTo support the work of the RSS, including Real World Data Science and other projects, become a member today.\nEmail: info@rss.org.uk",
    "crumbs": [
      "Our partners and funders"
    ]
  },
  {
    "objectID": "rwds-partners.html#partners",
    "href": "rwds-partners.html#partners",
    "title": "Our partners and funders",
    "section": "Partners",
    "text": "Partners\n\n\n\n\n\nThe American Statistical Association is the world’s largest community of statisticians, the “Big Tent for Statistics.” It is the second-oldest, continuously operating professional association in the US. Since it was founded in Boston in 1839, the ASA has supported excellence in the development, application, and dissemination of statistical science through meetings, member services, education, publications, advocacy, and accreditation.\nOur members serve in industry, government, and academia in more than 90 countries, advancing research and promoting sound statistical practice to inform public policy and improve human welfare.\nTo support the work of the ASA, become a member today.\nEmail: asainfo@amstat.org",
    "crumbs": [
      "Our partners and funders"
    ]
  },
  {
    "objectID": "rwds-partners.html#funders",
    "href": "rwds-partners.html#funders",
    "title": "Our partners and funders",
    "section": "Funders",
    "text": "Funders\n\n\n\n\n\nReal World Data Science was supported by startup funding from The Alan Turing Institute.\nThe Alan Turing Institute, headquartered in the British Library, London, was created as the UK’s national institute for data science in 2015. In 2017, as a result of a government recommendation, artificial intelligence was added to its remit. \nThe Institute is named in honour of Alan Turing (23 June 1912 – 7 June 1954), whose pioneering work in theoretical and applied mathematics, engineering and computing are considered to be the key disciplines comprising the fields of data science and artificial intelligence.\nTo find out more about The Alan Turing Institute, its strategy and programme of work, visit turing.ac.uk.",
    "crumbs": [
      "Our partners and funders"
    ]
  },
  {
    "objectID": "people-paths/career-profiles/posts/2023/03/28/tamanna-haque.html",
    "href": "people-paths/career-profiles/posts/2023/03/28/tamanna-haque.html",
    "title": "‘Data science challenges you to keep learning – there’ll always be new advances in the field’",
    "section": "",
    "text": "Hi, Tamanna. Thank you for sharing your career story with Real World Data Science. Please tell us a little about yourself and your role in data science.\nI’m Tamanna Haque. I’ve been working at Jaguar Land Rover for nearly four years, recently promoted to lead data scientist working within product engineering. It’s coming up to eight years that I’ve been working in the field, and my areas of interest are the use of machine learning to provide the best products and experiences for my customers and stakeholders.\nWhat does your job involve?\nMy role involves using the connected car and AI to make our products and customer experiences better, whilst leading within our wide data science team too. The data science team in Manchester, UK, originated with myself and one of my teammates – it’s since grown to nearly 40 (cross-sites and countries) and developed into a high-performing, advanced data science team.\nWhat makes us stand out is the nature of our work – we mostly use vehicle data (of participating customers), which is different to a lot of other commercial businesses or teams who’ll focus more on transactional or web data. The data we use lends itself to some pretty interesting projects, and a general futuristic feel here.\nI’m particularly interested and active in enabling a more electric and modern luxury future from the use of vehicle data.\nWhat does “data science” mean to you?\nThe realisation of value! Whether that is added revenue, saved costs or improved growth, I’m led by what data science can do for the business and its customers. The use of data science can open up many exciting, value-adding opportunities.\n\n\n\n\n\n\nPhoto supplied by Tamanna Haque, courtesy of Jaguar Land Rover. Used with permission.\n\n\n\n\n\nThere are more routes to getting into data science nowadays, but it’s important to not lose sight of fundamentals such as statistics and mathematics. A lot of people can code-up models but it’s fair to say that only a portion of them appreciate how to do this responsibly.\n\n\n\nWhat do you think is your most important skill as a data scientist?\nI’ve always presented myself as a technically astute data scientist, even when entering leadership. But my niche is my ever-growing commercial awareness and passion about our products, customers and business. These aren’t new qualities, but they now align with my professional interests, as well as personal (I’ve been a fan of the Jaguar brand since childhood)!\nHow did you get into data science?\nI did a maths degree at the University of Manchester, where I specialised in statistics. I didn’t do any post-graduate education and this was fine for me.\nAfter graduating, I joined a digital fashion retailer (with a financial services proposition) as an analyst initially. I learned a lot about real-life data and analytics itself, whilst developing a rounded understanding about the business and how to deal with stakeholders cross-functionally. I must have served a few hundred at least(!) and left most of the ‘fancy’ stuff I learned at university aside, whilst getting to grips with so many aspects of commercial analytics. A great way for me to set solid foundations for what followed, and I personally feel this gives me a lens that others who dive straight into data science don’t have.\nI was soon attracted to data science because it tapped into what I learned at university and challenges you to keep learning; there’ll always be things to learn, and new advances in the field.\nWhat, or who, first inspired you to become a data scientist?\nI have a twin sister, we’ve always been together throughout education. Even before we graduated together, she secured her first role as an analyst. This opened my eyes to data, and data science followed for us both!\nWhat were the hurdles or challenges that you needed to overcome on your route into the profession?\nI had a few people tell me I couldn’t do data science, possibly because I didn’t fit the typical data scientist stereotype in several ways. I think attitudes in the field have changed over time though and on a personal level, it’s motivated me to give it everything, and I can’t regret that.\nAnd what are the challenges that you face now, as a working data scientist?\nI need to manage my diary well to ensure effectiveness and work-life balance. I’m overseeing people, other projects, doing public speaking and trying to remain hands on. I sometimes block out chunks of time in my diary – I need some meeting-free time to produce quality technical work. I try to finish on time and enjoy a very busy social life with my family and friends. A flexible attitude to how we work helps to keep me happy and energised whilst I’m delivering from various angles.\nThinking back to your earlier roles in data science, how do they compare to your current role?\nMy current role is very different to my previous roles. I’m continually learning and adapting how I can be a good leader, providing support to a breadth of colleagues (in and outside the team) whilst delivering myself. I’m actively involved in setting and refining our team’s strategy and I’m enjoying leading projects which either deliver high financial impact or help set the path in terms of new tech and/or machine learning capability. There is much more responsibility but it’s easy to stay energised when working on cars and for a business I’ve long admired.\nWhat was the most important thing you learned in your first year on the job?\nI should have had more confidence in myself, but this grew – as I adjusted to the new environment I became much more assertive. My domain knowledge and data science expertise combined help to build my self-confidence, credibility and reputation.\nWhat have been your career highlights so far?\nI’m most proud of my recent promotion from senior to lead data scientist. Also it was exciting for my family and I when I gained an offer to join Jaguar Land Rover.\nHave there been any mistakes or regrets along the way?\nNo, what’s meant to be will be!\nHow do you think your role will evolve over the rest of your career?\nMy progression has been relatively rapid, and I hope I’ve got many, many years ahead of me in my career. It’s hard to say how my role will evolve, I have a blend of responsibilities in my role which combined provide great fulfilment for me at the moment.\nIf you were starting out in data science now, what would you put at the top of your reading/study list?\nA good understanding of analytics and the domain you’re in are my recommended prerequisites to doing data science.\nAnalytics is an important part of the data science lifecycle, being able to get the data yourself and communicate results with influence, for example, are just a few aspects of analytics which underpin successful data science projects.\nAlso, without awareness of the business and industry you’re working in, you can become very dependent on others. Data science itself can be quite challenging, so it’s great to have a solid foundation before starting out.\nWhat personal or professional advice would you give for anyone wanting to be a data scientist now?\nWith the level of continuous learning required to just simply keep up, it can be more of a lifestyle and not a job, so this is something to consider!\nWhat do you think will be the main challenges facing data science as a field in the next few years?\nI still expect to see a skills gap in the field. There are more routes to getting into data science nowadays, but it’s important to not lose sight of fundamentals such as statistics and mathematics. A lot of people can code-up models but it’s fair to say that only a portion of them appreciate how to do this responsibly, understanding samples versus populations, statistical testing, which type of regularisation to use in a neural network, et cetera.\nI also think there’s a challenge of questionable data science products reaching high levels of popularity and usage amongst the public… Some recent developments in this space have been extremely intelligent but raise ethical concerns. Just because something can be done with AI doesn’t mean it should, and my preferences are towards AI being ethical and (ideally) explainable.\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘Data science challenges you to keep learning – there’ll always be new advances in the field.’” Real World Data Science, March 28, 2023. URL"
  },
  {
    "objectID": "people-paths/career-profiles/posts/2023/06/20/chanuki-seresinhe.html",
    "href": "people-paths/career-profiles/posts/2023/06/20/chanuki-seresinhe.html",
    "title": "‘Once I started to see what was possible with data science, there was no going back’",
    "section": "",
    "text": "Hi, Chanuki. Thank you for sharing your career story with Real World Data Science. Please tell us a little about yourself and your role in data science.\nI am Chanuki Seresinhe, head of data science at Zoopla and Hometrack. My commercial career in data science began in 2018 at Channel 4. Since then, I have worked at a few different companies – from startups to scale-ups – before ending up here at Zoopla in 2022.\nI am also the founder of beautifulplaces.ai, which is a continuation of my University of Warwick and Alan Turing Institute PhD work where I provided the first large-scale evidence that beautiful places are connected to our wellbeing.\nWhat does your job involve?\nMy role at Zoopla involves managing data scientists both for Zoopla and Hometrack. At Zoopla, we use data science to create an engaging experience for users who want to buy, sell and rent properties. At Hometrack, the data scientists mainly work on an automated valuation model that provides property valuations to most of the major mortgage lenders in the UK.\nAs a leader in data science, my role primarily involves helping stakeholders across the business to best leverage data science to reach our business goals, as well as ensuring my data science team has all the support and mentoring they need to design, develop and maintain high performing data science algorithms.\nWhat does “data science” mean to you?\nThat is a really good question! One thing I have noticed is that people who aren’t familiar with the field often confuse data science and data analytics. There are indeed many similarities – both require quite a bit of knowledge to be able to leverage the correct insights from both structured and unstructured data (data hidden within images, for example). However, data science is essential when you need to make inferences. For instance, you are not only analysing data to see what certain consumers may prefer, but you are also predicting what similar consumers might prefer. Thus, having a strong grounding in statistics is really important for anyone working in this field.\n\n\n\n\n\n\nChanuki Seresinhe\n\n\n\n\n\nIn data science, getting the model right is not enough, and working with people across the business to make sure the model can be integrated into the business processes is essential.\n\n\n\nWhat do you think is your most important skill as a data scientist?\nAside from a good grounding in the technical aspects of data science (which is possible for really anyone to pick up from the many good courses that are available), the most important skill is how you can leverage data science to create products that actually create value for the business. I find this to be the most challenging journey that junior data scientists find themselves having to navigate. They are really excited about the technology, and get carried away with wanting to perfect their algorithms. But when you are building commercial products, what is really important is to constantly engage with stakeholders to make sure you are building something that actually has a tangible business benefit. Early release of a model for user testing is also essential, as models only really get better once you have real user input.\nHow did you get into data science?\nIt was somewhat by accident. I previously had a long career in digital and decided to take a career break to return to university and study economics. When I was working on my Master’s degree in behavioral economic science at the University of Warwick, I saw an ad for a PhD to “use online data to understand human behaviour” and I thought this was perfect, as it combined my prior knowledge with a new area I was increasingly becoming drawn to. I quickly taught myself how to program in Python and convinced my supervisors to take me on, and from there on, I came to love data science!\nWhat, or who, first inspired you to become a data scientist?\nIt was more about realising what you could do with data science. In my PhD, I was quantifying the connection between beautiful places and our wellbeing. While this has long been an intuitive connection, we were not able to test this on a large scale due to lack of data. Being able to use data science to start predicting the beauty of outdoor images was fascinating as it opened up a whole new method for potential research combining beauty with various wellbeing ratings. Once I started to see what was possible with data science, there was no going back.\nWhat were the hurdles or challenges that you needed to overcome on your route into the profession?\nFor me personally it was the challenge of moving sideways into a leadership role after my PhD and not having to start all the way from the bottom. I would have loved to continue in academia and expand my research even further, but starting from the bottom earning a tiny salary after I had taken quite a large career break to do my PhD wasn’t an option for me. So I decided to go back into the commercial world and look for a senior role from the get go and luckily Channel 4 agreed to give me my first commercial stab at data science.\nWhat are the challenges that you face now, as a working data scientist?\nTrying to keep up with everything that is constantly changing in the world of data science. I love the rapid change but it can also be quite time consuming to make sure you are on top of it and giving the right advice to people.\nWhat was your first job in data science, and how does it compare to your current role?\nMy first job was working as a senior data scientist at Channel 4. As a senior data scientist, even though you have additional goals to help run the team and be a mentor to more junior data scientists, you still get a great deal of time to do coding and develop your own projects. When you move more into a management role, the time you have to develop data science models diminishes. People also expect you to give in-depth guidance when you haven’t actually had much time to deep dive into a project. So, I am often trying hard to make sure I am on top of what is going on even when I have limited time, and really focus on building a strong team that can support each other and collaborate often to create better data science products. Learning to delegate is key!\nWhat was the most important thing you learned in your first year on the job?\nHow hard it is to actually get organisational buy-in to use data science at scale. It is really easy to get approval to build a proof of concept (POC). However, if you do not use the time when developing your POC to also make sure to get the right stakeholder on board, your project is dead before it even starts. So, in data science, getting the model right is not enough, and working with people across the business to make sure the model can be integrated into the business processes is essential.\nWhat have been your career highlights so far?\nIt has been great being able to give talks about my research, and data science in general, all around the world. I have actually come to love public speaking, and I hope that as I continue to be recognised for my expertise, I can encourage and aid potential data scientists with their careers – especially minority women, as I think that diversity in the field is very important. This is a role that is fit for people from all kinds of backgrounds and I hope I am exemplifying this.\nHave there been any mistakes or regrets along the way?\nIn smaller companies, it can easily happen that the founders don’t fully understand data science and often use data science as a buzzword to get investors on board. Whenever you take on a new job, and data science is just getting established, make sure the founders or leaders are actually fully onboard with integrating data science into the product and understand what this means. See if they know how tricky data science can be when first integrating into a product and are actually willing to overcome the challenges with you to eventually reap the huge benefits data science can bring.\nHow do you think your role will evolve over the rest of your career?\nI see my role evolving into being more strategic and less about the data science day-to-day modelling. It is more about being able to advise companies on how to make use of data science as a strategy and helping them figure out where in the product or process to inject it to get the most out of it for the business as a whole.\nIf you were starting out in data science now, what would you put at the top of your reading or study list?\nPractise how you would apply using data science for a real life problem. Seek a placement, as this will pay dividends in being able to speed up your learning.\nIf you don’t understand the statistics involved in data science, make sure to upskill in that area before starting your first role. A lot of junior data scientists focus on learning how to code or get carried away with the modelling without first learning the importance of preparing the data in the correct way so that your predictions can work well in a real life setting.\nWhat personal or professional advice would you give for anyone wanting to be a data scientist now?\nTry to find ways to stand out from the average data scientist. When we open up applications for data science jobs, I get hundreds of applications for each one. I am looking for people who can not only do data science but who also have other stand-out qualities that they can bring to the business. This can be something along the lines of effective stakeholder engagement to deep expertise in a certain domain or technology.\nWhat new ideas or developments in the field of data science are you personally most excited about or intrigued by?\nI am really interested to see where generative AI will take us – particularly about how it can help us improve the speed of our own performance. It feels like generative AI can be a technology that can help everyone – even the everyday person – as it can help speed up so many processes, from coding to writing to ideating. While the technology is still in its early days, it is progressing rapidly and I am very curious to see where this will lead in the next year!\nWhat do you think will be the main challenges facing data science as a field in the next few years?\nGenerative AI’s latest breakthroughs have made AI capabilities accessible to the broader public, but it has also stoked fears around the use of AI. The headline-grabbing narratives around AI and existential threat is distracting from other conversations that are really important. There are some very real issues we do need to solve – from biases in AI to the impact generative AI can have on wages and workforce – but this needs to be approached in a constructive and thoughtful way.\nWe need to find a way to engage with the public in a more meaningful way – rather than scaremongering – to have public debates on issues that actually matter.\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo of Chanuki Seresinhe is not covered by this licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘Once I started to see what was possible with data science, there was no going back.’” Real World Data Science, June 20, 2023. URL"
  },
  {
    "objectID": "people-paths/career-profiles/posts/2023/06/28/claire-morton.html",
    "href": "people-paths/career-profiles/posts/2023/06/28/claire-morton.html",
    "title": "‘I was inspired by the power that numerical data have to tell stories and promote policy change’",
    "section": "",
    "text": "This week, in celebration of Pride, Real World Data Science is collaborating with the JEDI Outreach Group of the American Statistical Association (ASA) and the ASA LGBTQ+ Advocacy Committee to highlight the achievements of statisticians and data scientists from across the LGBTQ+ spectrum.\nMembers of the committee nominated two individuals to be featured as part of our career profile series, and so we are pleased to bring you interviews with Claire Morton (below) and Albert Lee.\nRead on to discover more about Claire’s data science career (so far).\n\n\n\nHi, Claire. Thank you for sharing your career story with Real World Data Science. Please tell us a little about yourself and your role in data science.\nMy name is Claire Morton, and I’m an undergraduate student studying mathematical and computational science and environmental justice at Stanford University. I’m particularly interested in using statistics and data science to work with community-based organizations and advance evidence-based environmental justice policy. I have conducted quantitative research on tools to classify disadvantaged communities, oil wells, climate resilience, housing justice, and the connections between soil and health.\nWhat drew you to study statistics and data science?\nI really enjoyed my math, statistics, and coding classes in school. I was also inspired by the power that numerical data have to tell stories and promote policy change.\nWhat do you think is your most important skill as a data scientist?\nListening to others. Listening allows me to learn new statistics skills from my mentors and to learn about how best I can work with community partners on their priorities in my research.\nHow does your gender and/or sexual identity factor into your career?\nI am a lesbian, and, at the start of college, I didn’t have any mentors who shared my identity. I’ve now found several through the ASA and queer communities at my university, and I’m continually inspired by the achievements of queer statisticians, mathematicians, and computer scientists. My research hasn’t explicitly connected statistics and queerness yet, but I’m interested in working on projects involving hard-to-reach populations, such as queer people, in the future.\n\n\n\n\n\n\nClaire Morton\n\n\n\n\n\nIt’s important to be able to take initiative to learn skills, talk to people, and solve problems as they come up – but it’s also critical to not be afraid of asking for help when you need it.\n\n\n\nHow did you get into data science?\nIn high school, I worked in a cell biology lab. As part of that work, I learned to model cellular processes and analyze data from my experiments. I realized that those elements were my favorite parts of the science I was doing, so I decided to study math, computer science, and/or statistics in college. I had always been interested in environmental issues, and so I got involved in quantitative research about environmental justice. I realized that this type of research allowed me to connect the skills I have to my passions, so I’ve kept working in these areas ever since.\nWhat, or who, first inspired you to pursue this career path?\nMy mom! She’s also a statistician, and she has encouraged and mentored me throughout my academic journey. I’m inspired by her success as a woman in statistics.\nWhat hurdles or challenges have you faced in your studies?\nMy classes can be tough, which makes it hard to stay motivated sometimes. I also struggled to maintain a healthy work-life balance at the start of college. Finally, it has been tough to learn some of the ins and outs of the research process and publications – how best to engage with research mentors, what it looks like to write and submit a paper, and some of the nuances of working in academia. I think my next big challenge is deciding what to do after college, though I’ve been trying to reframe the question as an opportunity rather than a hurdle. I’m excited to continue doing research at the intersection of statistics and public policy in the future.\nWhat was your first job in data science, and how does it compare to your current role?\nMy first job in data science was as a researcher, working at a non-profit called Physicians, Scientists, and Engineers for Healthy Energy (PSE). As part of this job, I worked with community-based organizations to code quantitative optimization models to locate climate resilience hubs in California that took community priorities into account. I’m currently a student, which involves less research but gives me the chance to focus on learning new skills for my next job. I hope to be able to work in a role like my job at PSE, combining statistics/data science, community engagement, and policy impact, in the future.\nWhat was the most important thing you learned in your first year on the job?\nThe importance of being adaptable and self-directed. Research projects shift and change as you uncover new information, and it’s useful to be able to shift with them. It’s important to be able to take initiative to learn skills, talk to people, and solve problems as they come up – but it’s also critical to not be afraid of asking for help when you need it.\nWhat have been your career highlights so far?\nOne was publishing research about the demographics of people living near oil wells in California, which informed policymakers about racial and socioeconomic differences in exposure to oil wells and is part of a long-standing effort from activists and researchers to protect the health of Californians. I’ve also gotten to work directly with organizers on several mapping projects, which was deeply fulfilling. Finally, I loved getting to present my research at the Joint Statistical Meetings last year, and I look forward to presenting my undergraduate thesis this year.\nWhat three things are at the top of your reading/study list?\nSome statistical areas I’m hoping to learn more about are spatial statistics and survey methods. Some books I’m excited to read are The Color of Law, Data Feminism, and Thicker Than Blood: How Racial Statistics Lie. \nWhat advice would you give for anyone wanting to study statistics and data science?\nFind mentors that inspire you, support your career goals, and challenge you to learn and grow as a statistician.\nWhat new ideas or developments in the field are you most excited about or intrigued by?\nI’m really interested in combining quantitative research with community-based research, so I think that cross-disciplinary developments are exciting. I’m intrigued by AI tools, and I’m interested to see how these tools change what the day-to-day of being a statistician looks like and the skills that are most sought after in a statistician.\nAnd what do you think will be the main challenges facing the profession over the next few years?\nThe main challenges will be related to statistical literacy, both for the people consuming and doing statistics. While statistical methods and data becoming more accessible is a positive development, it has meant that more analyses are done incorrectly and that more misleading results are publicized (and absorbed) as truth. It’s getting much easier to twist numbers to support whatever we want them to say, and I think this will continue to challenge both statisticians and non-statisticians in the future.\n\n\n\n\n\n\nAbout the ASA Pride Scholarship\n\n\n\nThe ASA Pride Scholarship was established to raise awareness for and support the success of LGBTQ+ statisticians and data scientists and allies. The scholarship will celebrate their diverse backgrounds and showcase the invaluable skills and perspectives these individuals bring to the ASA, statistics, and data science.\nApply or nominate someone for the ASA Pride Scholarship.\n\n\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo of Claire Morton is not covered by this licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘I was inspired by the power that numerical data have to tell stories and promote policy change.’” Real World Data Science, June 28, 2023. URL"
  },
  {
    "objectID": "people-paths/career-profiles/posts/2023/04/24/sami-rahman.html",
    "href": "people-paths/career-profiles/posts/2023/04/24/sami-rahman.html",
    "title": "‘I always thought someone like me couldn’t work in data, let alone data science’",
    "section": "",
    "text": "Hi, Sami. Thank you for sharing your career story with Real World Data Science. Please tell us a little about yourself and your role in data science.\nHello! I’m Sami Rahman, a passionate head of data engineering and data platform at Penguin Random House, the book publisher that has enriched lives through literature. I started my career in data science five years ago and I’ve evolved into a data generalist with expertise in machine learning, data infrastructure, and data strategy.\nWhat does your job involve?\nMy role is about harnessing the power of data to drive extraordinary outcomes. Leading a skilled team, we empower our company to leverage data and cutting-edge technologies for informed decisions and automation. I help shape our organisation’s capabilities in data science, analytics, machine learning, data management, and strategy.\nWhat does “data science” mean to you?\nData science, to me, is a captivating fusion of modern data technologies and computational statistics that tackles business challenges, crafts intelligent automation, and generates insightful revelations.\nWhat do you think is your most important skill as a data scientist?\nActive listening is key. A data scientist must be surgical and precise in developing models, analysis, and tools that reinforce the company’s bottom line and operations. Data science exists to create value using data.\n\n\n\n\n\n\nPhoto supplied by Sami Rahman, used with permission.\n\n\n\n\n\nAs I’ve transitioned into management, maintaining my coding prowess is an ongoing challenge. I stay sharp by doing data science and infrastructure development for fun, leveraging tools like ChatGPT and AirOps where I’m rusty.\n\n\n\nHow did you get into data science?\nI began with a psychology degree, which led to working as business psychologist where I discovered psychometric data analysis. After a master’s in countering organised crime and terrorism and a few short jobs in counter terrorism/intelligence, I decided that it wasn’t for me. I embraced my love for statistics and research, I dove into data science, learning Python through online platforms, and secured my first data scientist role at a WPP agency called Essence.\nWhat, or who, first inspired you to become a data scientist?\nI always thought someone like me couldn’t work in data, let alone data science. Dr Suzy Moat’s fascinating talk on machine learning’s application to human behaviour and psychology showed me that a psychologist could make a significant impact in this field, inspiring my aspiration to try to have a data science career.\nWhat were the hurdles or challenges that you needed to overcome on your route into the profession?\nBreaking into data science without a typical background in maths/computer science/physics was daunting. Building a Kaggle portfolio and coding models for fun prepared me for interviews. Another challenge was learning to harmonise my “data brain” and “business brain” to solve problems efficiently. Understanding how data solutions impact business problems will always propel you forward. \nAnd what are the challenges that you face now, as a working data scientist?\nAs I’ve transitioned into management, maintaining my coding prowess is an ongoing challenge. I stay sharp by doing data science and infrastructure development for fun, leveraging tools like ChatGPT and AirOps where I’m rusty. I’m currently building my own cloud data platform and running a lot of image neural networks on it.\nWhat was your first job in data science, and how does it compare to your current role?\nAs an analytics executive at WPP agency Essence, I tackled data science, cloud engineering, and analytics problems for clients. They were a lot more singular and tactical in nature. Now, as head of data engineering and data platform at Penguin Random House, I focus on shaping data and technology strategy to align with the company’s broader vision.\nWhat was the most important thing you learned in your first year on the job?\nTo always consider the bigger picture: how your work integrates with the organisation/client’s objectives, delivers value, and aligns with the aspirations of other stakeholders. Actionable insights and value is the most important thing.\nWhat have been your career highlights so far?\nTwo shining moments include being the first of three of HSBC UK fraud data science leaders, where each of our departments tackled a different type of crime and protected our customers, and developing data strategies and capabilities for analytics, science, and business intelligence at Penguin Random House.\nHave there been any mistakes or regrets along the way?\nI regret not delving deeper into natural language processing (NLP) or spatial data science, which are now more accessible and growing fields within data science. I reckon the NLP methodologies would’ve been extremely useful seeing as I’m at a publishing company now!\nHow do you think your role will evolve over the rest of your career?\nAs data technologies become more accessible, I anticipate data roles will transform. I envision a future where data professionals focus on general AI, quantum machine learning, and multi-dimensional data analytics as traditional specialisms become democratised.\nIf you were starting out in data science now, what three things would you put at the top of your reading/study list?\nI’d recommend Skin in the Game by Nassim Nicholas Taleb, Calling Bullshit: The Art of Scepticism in a Data-Driven World by Jevin West and Carl Bergstrom,  and Artificial Intelligence: How Machine Learning Will Shape the Next Decade by Matthew Burgess.\nWhat personal or professional advice would you give for anyone wanting to be a data scientist now?\nSuccess in data science hinges on understanding how it can transform organisations and engaging with business stakeholders. My advice: never stop listening to the business – the stakeholders are your biggest allies. I would also try to find your niche that sets you apart from everyone else. Mine when I first started in the field was my expertise on computational psychology and behavioural machine learning. \nWhat new ideas or developments in the field of data science are you personally most excited about or intrigued by?\nTransfer learning excites me most, as numerous large technology companies now offer pre-trained models based on billions/trillions of parameters. This will revolutionise industries worldwide, as it will be easier to build more performant models even if a company has less data.\nWhat do you think will be the main challenges facing data science as a field in the next few years?\nThe challenge lies in staying relevant amidst the democratisation of data science. Through large language models, low-code, and transfer learning, advanced data science methods will become easier for non-specialists to do and use. Innovation and keeping up with modern data technologies will be crucial.\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo of Sami Rahman is not covered by this licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘I always thought someone like me couldn’t work in data, let alone data science.’” Real World Data Science, April 24, 2023. URL"
  },
  {
    "objectID": "people-paths/posts/2023/05/18/chatgpt-data-science-pt2.html",
    "href": "people-paths/posts/2023/05/18/chatgpt-data-science-pt2.html",
    "title": "Large language models: Do we need to understand the maths, or simply recognise the limitations?",
    "section": "",
    "text": "Part 1 of our conversation with the Royal Statistical Society’s Data Science and AI (DS&AI) Section ended on a discussion around the need to verify that large language models (LLMs), when embedded in workflows and operational processes, are working as intended. But there was also acknowledgement that this could be difficult to achieve, not least of all because, as Giles Pavey said, “nobody knows exactly how these things work – not even the people who build them.” And then, of course, there is the speed at which developments are taking place: Trevor Duguid Farrant made the point that an expert may not even have a chance to finish reviewing the latest version of an LLM before a new iteration is rolled out.\nThese issues – of verification, explainability and interpretability – are of particular interest to data scientists like Anjali Mazumder, whose work focuses on the impact AI technologies could have, and are having, on society and individuals.\nIn part 2 of our Q&A about ChatGPT and other LLM-powered advances, and what all of this might mean for data science, Mazumder kicks off the conversation by setting out her perspective.\nOur full list of interviewees, in order of appearance, are:\n\nAnjali Mazumder, AI and Justice & Human Rights Theme Lead at the Alan Turing Institute, and DS&AI committee member.\nDetlef Nauck, head of AI and data science research at BT, and editorial board member, Real World Data Science.\nMartin Goodson, CEO and chief scientist at Evolution AI, and DS&AI committee member.\nLouisa Nolan, head of public health data science, Public Health Wales, and DS&AI secretary.\nPiers Stobbs, VP science at Deliveroo, and DS&AI committee member.\nTrevor Duguid Farrant, senior principal statistician at Mondelez International, and DS&AI committee member.\nGiles Pavey, global director for data science at Unilever, and DS&AI vice-chair.\nAdam Davison, head of data science at the Advertising Standards Authority, and DS&AI committee member.\n\n\n\n\nAnjali Mazumder: I work in research, but I also sit in the crux of government, industry, and civil society, looking at how they’re using these technologies. For me, it’s about knowing what the opportunities are but also understanding the limitations, the risks and the harms, and how we balance those and put in place oversight mechanisms that act as safeguards to ensure that these technologies don’t cause harm. We’re taking a very socio-technical approach that requires an interdisciplinary team to understand these issues and what should be done. Part of this is about not only the outcomes and the impact but also the upstream side of it – recognising that these models have been built on the work of people who have done the labelling, and that this also has implications – to say nothing of the associated environmental issues or energy issues!\nDetlef Nauck: I think the regulators really have to look at this. It has come completely out of left field for them. All the regulators that we are monitoring, they regulate the space as it was three years ago – they are mainly concerned about predictive models and bias. But if you look at, say, what Microsoft wants to do – putting GPT into Office 365 and into Bing – that will completely change how people interact with and consume information. I think the large tech companies really have a responsibility here, when they make this public, to make sure that people understand what this technology actually is, and how it can be used and has to be used.\nAlso, they need to open up about how these things have been built. There are a lot of stories around how OpenAI used cheap labour in order to do the labelling and reinforcement learning for ChatGPT, and these things have to become public knowledge; they need to become part of some kind of Kitemark for these models: “Ethically built, properly checked, hallucinate only a little bit. Whatever you do, don’t take it for granted. Check it!” That’s the kind of disclaimer they need to put on these models.\n\n\n\n\nIf you look at what Microsoft wants to do – putting GPT into Office 365 and into Bing – that will completely change how people interact with and consume information. Large tech companies really have a responsibility here, when they make this public, to make sure that people understand what this technology actually is.\n\n\n\nRegulatory principles always seem to stress that AI systems should be understandable, and we should be able to explain how we get particular outputs. But a lot of our conversation has focused on how we don’t really know how these models work. So, is that, in itself, a problem, and is it something that the data science community can help with – to dig into how these things work and try and get that across – to help meet these principles of explainability and interpretability?\nDN: That’s a very specialist job, I would say – specialist research into how these mathematical structures work. It’s not something I could do, and I’ve not seen any significant work there. One thing that we are interested in is whether we can do knowledge embedding, so that you can “teach” concepts that these models can then use to communicate, and that would lead to smaller systems where you have some understanding of what’s inside. But all of this kind of work, I think, is very much just beginning.\nMartin Goodson: Do we actually need this? There’s sort of a big assumption there that you need to understand how LLMs work in order to build in the kinds of things that we care about as a society. But we don’t understand how humans think. Of course, we can ask a human, “Why did you make that decision?” You can’t understand the cause of that decision – that’s a complex neuroscience question. But you can ask what the reason is for making a decision, and you can ask an LLM what its reasoning is as well. I think a lot of these questions about explainability are stuck in the past, when you’re trying to explain how a linear model works. It’s really not the same thing when you’ve got an LLM where you can just say, “Why did you make that decision?”\nLouisa Nolan: I was going to say something very similar. Most people don’t know how most things work…\nDN: My point was, these things are largely still like the Improbability Drive in the Hitchhiker’s Guide to the Galaxy. You press a button, and you don’t really know what comes out, and that’s the problem we need to get our heads around.\nLN: But people don’t know what percentages are, and yet we still use them for decision making. I don’t think people need to understand the maths behind LLMs, and I think it would be a hopeless job to expect everybody to do that. What we do need to understand is what LLMs can and can’t do. What’s the body of work that they are drawing from? What isn’t in there? What are the things that you need to check? So, for some things, it’ll be brilliant: if you’ve written something and you want it rewritten for a nine-year-old; if you want to summarise a paper; if you want to write code, as long as you already know how to code – these could be real labour-saving tasks. If you’re using ChatGPT to write a thesis about something that you haven’t looked at, that’s where the danger is. It’s this kind of simple understanding that people need to get in their heads – and the maths, except for the people who care about it, is beside the point, and probably detrimental, because it means that people won’t engage with it.\nDN: I agree, but I wasn’t talking about the general public. I meant, the people who build these things – they should know what they’re doing.\n\n\n\n\nThere’s a big assumption that you need to understand how LLMs work in order to build in the kinds of things that we care about as a society. But we don’t understand how humans think. You can ask a human what their reason is for making a decision, and you can ask an LLM what its reasoning is as well.\n\n\n\nWe talked there about communication. There was a webinar recently by the Royal Statistical Society’s Glasgow Local Group, and the presenter, Hannah Rose Kirk, showed how you can take tabular data and statistical results and ask ChatGPT to produce a nice paragraph or two that explains the numbers. Is this the sort of thing that any of you have experimented with? Have you had any successes at using ChatGPT to translate data into readable English that decision makers can act on?\nPiers Stobbs: I have an interesting use case. We had a basic survey: 200-odd responses, multiple languages, and we just said, “Please summarise the results of this survey contained in this CSV file.” And it came up with five or six relevant bullet points. What was amazing was that we could then interrogate it. For example, “Please compare the results that were in English versus in French, and describe the differences.” Again, it did it, but then you have the issue of, was it all correct? Well, the bulk of it was. Now I am intrigued by whether you can ask it to do correlations and some actual statistical things on a dataset, and does it get that right? I don’t know. We’ve not really got to that. But, to go back to one of the earlier discussion points around productivity, that initial survey work could have easily taken a week of someone’s time if we didn’t run it through ChatGPT.\nTrevor Duguid Farrant: Piers, in this case you’re interested in checking and seeing if it’s right. If you’d asked a group of people to do that survey for you and get the results, you’d have just accepted whatever they gave you back. You wouldn’t have questioned it.\nPS: That’s true. And the results were plausible, certainly.\nAM: I think one of the challenges is that the results could seem like they’re plausible, right – whether that’s a statistical output or a text output. This was not a proper experiment, but I asked ChatGPT about colleagues and friends who are quite prominent researchers, asking, “Who is so-and-so?”, and it produced biographies that were quite plausibly them, but it wasn’t them. It might have listed the correct PhD, say, but the date was off by a year, or the date was correct but it was from the wrong institution. So, depending on what the issue is, these seemingly plausible results could have more serious implications.\nLN: So, just to join those two things together: for me, the question is not, “Do we understand how ChatGPT works?” As Martin says, we don’t understand how humans work, and surely we’re trying to develop something that enhances human thinking in some way. The more important question for me is, “How do we know that what is produced is useful?”\n\n\n\n\nFor me, the question is not, ‘Do we understand how ChatGPT works?’ We don’t understand how humans work, and surely we’re trying to develop something that enhances human thinking in some way. The more important question is, ‘How do we know that what is produced is useful?’\n\n\n\nGiles, you mentioned previously that you’re doing some work at Unilever around how to minimise hallucination. I don’t know how much you can say on what direction that’s going in, and how successful you expect that to be, but that’s obviously going to be a really important part of refining these models to be more widely usable.\nGiles Pavey: I’m by no means an expert, but there’s quite a lot you can do with both the architecture of it and also the pre-prompts that you put in – more or less saying, “Quote what the source is, and if you’re not sure, then tell me you’re not sure.” I think what’s interesting is the question of whether we’ll have to rely on OpenAI or Microsoft to do that work, and it will be just another thing that we have to trust them for. Or, will it be something that people within an organisation can put in themselves?\nMG: I think it’s absolutely critical that open-source models are developed that can compete with these tech companies, otherwise there’s going to be a huge transfer of power to these companies.\nGP: Arguably, the single biggest issue is, who elected Sam Altman (no-one) and are we as society happy with him having so much power over our future?\nTo close us out, I’d like to return to a question Trevor posed earlier, which is: How might organisations like the Royal Statistical Society help companies to embrace LLMs and start using them, so that everyone can benefit from the technology?\nAdam Davison: My instinct is that there’s some great parallel here with the stuff that the Data Science and AI Section have been doing in general, where we’ve said, “OK, there’s lots of good advice out there on how to do things in data science, but how do you make it practical? How do you make it real? How do you apply those ethical principles? How do you make sure you have people with the right technical understanding in charge of projects to get value?” If, five years ago, the hype around data science was leading organisations to hire 100 data scientists in the hope that something innovative would happen, well then, we don’t want those same organisations now thinking that they need to hire 100 prompt engineers and keep their fingers crossed for something special. Our focus has been on “industrial strength data science”, so I think we can extend that to show what “industrial strength LLM usage” looks like in practice.\n\n\n\n\n\n\nWant to hear more from the RSS Data Science and AI Section? Sign up for its newsletter at rssdsaisection.substack.com.\n\n\n\n\n\n\n\n← Read part one\n\n\n\n\nBack to Careers\n\n\n\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Images are not covered by this licence. Thumbnail image by Google DeepMind on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Large language models: Do we need to understand the maths, or simply recognise the limitations?” Real World Data Science, May 18, 2023. URL"
  },
  {
    "objectID": "people-paths/posts/2023/04/18/politics-of-performance-measurement.html",
    "href": "people-paths/posts/2023/04/18/politics-of-performance-measurement.html",
    "title": "The politics of performance measurement",
    "section": "",
    "text": "At the beginning of 2016, the Criminal Justice Division (CJD) of the Texas Governor’s Office received news all government agencies dread: budgets were to be cut. CJD oversaw a grant program that funded specialty courts throughout the state, however it was now being told that the program’s budget of $10.6m would be reduced 20% to $8.5m by 2018.\nHow should these cuts be distributed among grant holders? CJD had no meaningful performance data on which to base its decisions, and I would know: I was hired by the agency just a few months before to analyze grant performance. Still, decisions needed to be made. We had to come up with a plan of action, and the clock was ticking…\nThis is a story of making opportunity out of crisis, of the interaction between not just theory of change and technical implementation, but the “political” process of negotiating these changes with stakeholders in a manner that led to better decisions. Through careful outreach and continuous communication, we developed a data collection and performance assessment process that enabled us to allocate budget cuts in a manner widely accepted.\nThe story ends on a bittersweet note. But, along the way, there are lessons to be learned about how to find common ground, manage expectations, forge productive working partnerships, and sustain a data science project longer term."
  },
  {
    "objectID": "people-paths/posts/2023/04/18/politics-of-performance-measurement.html#step-1-consider-your-options",
    "href": "people-paths/posts/2023/04/18/politics-of-performance-measurement.html#step-1-consider-your-options",
    "title": "The politics of performance measurement",
    "section": "Step 1: Consider your options",
    "text": "Step 1: Consider your options\nTexas had over 150 specialty courts in 2016, providing a program of specialized services – usually drug treatment – to offenders as an alternative to incarceration. About half of the state’s specialty courts received CJD grant funds (and about half of grantees received 100% of their program budget from our grants). Funding cuts of the size we needed to make would not go over well with them. Any changes to the program would have to run a gauntlet of decision-makers including advisory boards, interest groups, and professional associations, most with contacts in the legislature.\nComplicating this situation further, CJD didn’t even make the final funding decisions. We administered the grants, but the merit review process fell to the Specialty Courts Advisory Council, an appointed group of specialty courts staff and related experts who annually scored the grant applications we received. We needed to get them onboard.\nThe way our Executive Director saw it, we had three options to implement the cut in a way that could get us buy-in from stakeholders and the Advisory Council:\n\nCut across the board. The Advisory Council would employ the same scoring method as the previous year but reduce each grant amount by 20%.\n\nThis option would leave long-running grantees scrambling to make up for this shortfall by reducing services, laying off staff, or spending more of their limited local funds. Worse, it would punish all grantees equally – our most successful programs would be arbitrarily defunded.\n\nFewer grants. Grants were scored based on the quality of their application and all grants that passed a certain threshold got funded. The Advisory Council would employ the same scoring method as the previous year but instead of funding the top $10.6m worth of grants, they would fund the top $8.5m worth.\n\nThis seemed a less bad option than cutting across the board, but we would still run into the problem of arbitrarily defunding successful programs. Grants near the bottom of the Advisory Council’s cutoff that got funded the previous year would be denied renewal only because the goalposts had moved.\n\nTargeted funding. The Advisory Council would incorporate performance data and statewide strategic plan alignment into their scoring method and make cuts accordingly.\n\nAt the time, the Advisory Council did not take performance into consideration when scoring grant applications. They agreed in theory that a grant requesting its tenth annual renewal should perhaps at some point be assessed on its outcomes, but they had never seen CJD commit to a rigorous performance assessment process before. We administered the grants, not them, so without our commitment to develop a performance assessment process, and their trust in that commitment, this would not be a viable option.\nAfter due consideration, option 3 emerged as the favorite of our Executive Director. On the face of it, this seemed the most “objective” approach to take. We would let the data decide who gets funded and who doesn’t, rather than cutting arbitrarily. But that would be a fallacious argument. Data does not decide. It might inform our decisions, but it would be up to us to choose the structure of the performance measurement process: what aspects to focus on, what data to collect, what benchmarks to set – all of which would later help determine funding decisions. And in any funding decision, politics inevitably plays a role.\nPolitics is, in its broadest sense, the negotiated decision-making between groups with opposing interests. And in developing our performance measurement process we would encounter a variety of interests – from the Advisory Council down to the grantees themselves. Success would require us to acknowledge stakeholder perspectives and address or manage them appropriately. Planning decisions made in the early phases of a project as a result of political processes directly influence the type and scope of analysis a data scientist will eventually be able to perform, so it behooves the data scientist to participate in these processes!"
  },
  {
    "objectID": "people-paths/posts/2023/04/18/politics-of-performance-measurement.html#step-2-engage-stakeholders-and-define-performance",
    "href": "people-paths/posts/2023/04/18/politics-of-performance-measurement.html#step-2-engage-stakeholders-and-define-performance",
    "title": "The politics of performance measurement",
    "section": "Step 2: Engage stakeholders and define performance",
    "text": "Step 2: Engage stakeholders and define performance\nHaving settled on our preferred option, our Executive Director convened a strategy session with the Advisory Council to discuss how to proceed as part of a broader strategic plan. The session began by achieving consensus on high-level goals such as “fund strategically”, “focus on success”, “build capacity”, etc. The session also helped the Advisory Council and CJD alike to clarify our conception of how we ought to fit into the specialty courts field going forward. CJD would develop its performance assessment system to help the Advisory Council target funding, but that would come as part of a larger plan that included capacity building, training and technical assistance, helping courts obtain non-CJD sources of funding, and steering grantees toward established best practice.\nWe left the meeting with a very basic plan that looked good on paper. Our Executive Director set to work persuading our external stakeholders of the wisdom of this new strategic direction. Meanwhile, I had to build a performance assessment process that people could trust.\nCJD had no formally designated standards to measure performance data against. However, drug courts have been around for decades and there existed a large body of research supporting the program model.1 Offering supervised drug treatment instead of incarceration had been repeatedly shown to cost less money and lower recidivism rates. I performed a literature review and spoke with numerous subject matter experts to get started on defining program-specific performance metrics.\nI was conscious that imposing metrics without any feedback or input from affected parties would all but guarantee bad-faith engagement, especially if these metrics are tied to funding. A problem inherent to any performance measurement is that once something gets measured as a performance outcome, it warps the very processes it is intended to measure. This phenomenon happens so frequently that the phrase “Campbell’s Law” was coined to describe it in 1979.2 Think of standardized testing at schools: once the government ties test performance to school funding it creates powerful incentives for schools to improve test scores at any cost. Even in the absence of outright cheating, struggling schools feel massive pressure to adjust their curriculum, to the point where they teach test score optimization strategies more than math, language, history, and science.\nI consistently heard from specialty court scholars and practitioners alike that arrest recidivism would be the ideal outcome measure. On paper, recidivism was a direct expression of long-term program success and could also be used as an outcome variable for classification modeling. And, in practice, a court could do little to affect recidivism by way of manipulation. Courts do not make arrests – police make arrests. Once a specialty court participant finished a program, the court itself no longer intervened in their lives. If a participant got arrested within 1-3 years of completion, the program had no say in the matter.\nThis, however, presented an implementation problem: one-year recidivism data would, by definition, take a year past the point of implementation to collect, i.e., not soon enough to inform our cuts. And while recidivism was the best measure of success, it could not be the only measure. Recidivism was, after all, a stochastic process not within the court’s control – a crime wave or other systemic factors could move recidivism up and make it look like a successful court had actually failed. We would have to use something else as well.\nThe National Association of Drug Court Professionals (NADCP) publishes a book of best practice standards, and our stakeholders identified a court’s adherence to these standards as another strong performance assessment standard. These criteria, unlike recidivism, were directly under the program’s control. Does your program have the recommended staff? Does your program drug test participants frequently enough to guarantee sobriety? Does your program meet with participants regularly enough? Do you offer a continuum of services instead of a “one-size-fits-all” approach?\nIn addition to being much easier to measure than recidivism, best practice adherence also resists Campbell’s Law by avoiding outcome measurement. In our school metaphor, this would be like measuring school performance based on student-to-teacher ratio, variety of course offerings, attendance rates, and teacher qualifications. Far from perfect, but measuring a variety of elements that predict success and taking them as a whole represents a vast improvement over a single, easily-gamed outcome measure.\nBut to operationalize these standards, we would have to have good data."
  },
  {
    "objectID": "people-paths/posts/2023/04/18/politics-of-performance-measurement.html#step-3-update-processes-and-collect-data",
    "href": "people-paths/posts/2023/04/18/politics-of-performance-measurement.html#step-3-update-processes-and-collect-data",
    "title": "The politics of performance measurement",
    "section": "Step 3: Update processes and collect data",
    "text": "Step 3: Update processes and collect data\nWe inherited a longstanding process in which grantees had to fill out a form every six months asking them to report performance data. This is a screenshot of what that form looked like:\n\n\n\n\n\nNo additional definitions or instructions were provided, leaving grantees with many questions: Does the request for “annual data” mean as of fiscal year or calendar year? What counts as a person being “assessed for eligibility”? And so on. Grantees did not know the answers, and neither did we. And these were the more straightforward measures. The form went on for 10 pages, most of which asked grantees to report extensively on information they had already provided as part their application.\nThis disaster of an assessment process did have a silver lining. When we announced we were throwing out these forms entirely we faced almost no pushback from grantees.\nWe knew from the start that our new assessment process would need to collect individual-level participant data instead of aggregated measures. Even with clear definitions, 75 grants would mean 75 different aggregations at work. Asking the grantees to report their individual-level participant data in a consistent format and doing the aggregations ourselves meant a single aggregation at work.\nBut we needed to establish trust with grantees before making this request. Strictly speaking, we could mandate the reporting of this data. However, if that angered enough of our grantees, they or their contacts might take it up with our bosses at the Governor’s Office, and our bosses could cancel any plan we came up with if they thought it was not worth the fuss. So, from day one we communicated clearly to all grantees that we would maintain total transparency when it came to definitions and calculations. Before we used any calculated metric to assess performance we would send it to the grantees themselves to review for accuracy.\nTo avoid the vagueness and inscrutability that characterized the old reporting process, every piece of data we asked for in the new process had a clear written definition and specific reason for being asked. These reasons usually amounted to some combination of best practices, Advisory Council recommendations, and grantee suggestions.\nImplementing the new process was far from easy, however. We faced numerous administrative and technical barriers. Texas courts at this time did not share a common case management system, so we couldn’t just get a data export from everybody. Meanwhile, the Governor’s Office banned all of its divisions from all usage of the cloud. This forced us to build a more labor-intensive reporting process, in which courts would obtain blank Excel templates with required data fields. Courts had either to fill out these templates by hand or export their case management data and reconfigure it to template specifications. Then, courts submitted their data for review and we sent back any bad formatting.\nWe collected preliminary data at the six-month mark and made another adjustment based on these results, which we would not count toward performance measurement. A majority of courts had some kind of data error in this first case. Specific definitions of data fields had to be written and rewritten using grantee feedback over the course of the year, leading to significant changes between the six-month reports and the year-end reports.\nImportantly, we had developed reporting requirements iteratively with participation from grantees and the Advisory Council from the start. By mid-2017 we had so successfully achieved buy-in that only one grantee court’s judge refused to give us data (the court’s grant manager later sent it to us)."
  },
  {
    "objectID": "people-paths/posts/2023/04/18/politics-of-performance-measurement.html#step-4-analyze-and-report-findings",
    "href": "people-paths/posts/2023/04/18/politics-of-performance-measurement.html#step-4-analyze-and-report-findings",
    "title": "The politics of performance measurement",
    "section": "Step 4: Analyze and report findings",
    "text": "Step 4: Analyze and report findings\nIn the course of this process, we established the benchmarks in Table 1 based on best practices and justification for funding. Because this was our initial rollout, we set the specific values low to function more as minimum standards than targets.\n\nTable 1: Specialty court best practices translated into quantitative measures.\n\n\n\n\n\n\n\n\n\n\nBenchmark\nBest practice\nRationale\n\n\n\n\n1. Number of participants\n10+\nCJD decision: programs should be of sufficient size to justify a grant\n\n\n2. Number of graduates\n5+\nCJD decision: programs should be of sufficient size to justify a grant\n\n\n3. Graduation rate\n20%-90%\nCJD decision: 0% and 100% success rates are both red flags\n\n\n4. Average amount of time graduates spent in program (in months)\n12-24\nNADCP best practice recommended program lengths of 1-2 years\n\n\n5. Percent of graduates employed, seeking education, or supported through family, partner, SSI, etc.\n100%\nNADCP best practice recommended against releasing participants without financial support, which all but guarantees relapse or rearrest.\n\n\n6. Percent of participants with “low-risk” assessment score\n0%\nNADCP best practice recommended moderate- or high-risk participants. Research had shown that low-risk participants get little benefit.\n\n\n7. Average sessions per participant per month\n1+\nNADCP best practice recommended sessions be held at least monthly.\n\n\n\n\nGrantee performance data for each benchmark would be generated from the individual level data that courts sent us. Crucially, we sent our aggregations back to grantees for confirmation prior to using them in any kind of evaluation, alongside the program-wide average and the best practice values for comparison (example in the table below). If something didn’t look right, they had the chance to let us know before we took their numbers as final.\n\nTable 2: Specialty court best practices compared with program-wide averages and grantee reported values.\n\n\n\n\n\n\n\n\n\n\n\nBenchmark\nBest practice\nProgram-wide average\nGrantee reported values\n\n\n1. Number of participants\n10+\n89\n96\n\n\n2. Number of graduates\n5+\n25\n27\n\n\n3. Graduation rate\n20%-90%\n71%\n56%\n\n\n4. Average amount of time graduates spent in program (in months)\n12-24\n17\n14\n\n\n5. Percent of graduates employed, seeking education, or supported through family, partner, SSI, etc.\n100%\n95%\n100%\n\n\n6. Percent of participants with “low-risk” assessment score\n0%\n18%\n2%\n\n\n7. Average sessions per participant per month\n1+\n2\n3.7\n\n\n\n\nIn the end, we found seven grants that we could unequivocally recommend be cut. Two of the seven had effectively never gotten off the ground, and served almost no participants the entire year. The other five served mostly low-risk participants, the type of people that research had shown do not benefit from specialty court programs. Some of these grantees were inevitably disappointed at the decision, but we had so actively worked within the field to develop and justify our processes that they understood why the decision had been made."
  },
  {
    "objectID": "people-paths/posts/2023/04/18/politics-of-performance-measurement.html#factors-for-success",
    "href": "people-paths/posts/2023/04/18/politics-of-performance-measurement.html#factors-for-success",
    "title": "The politics of performance measurement",
    "section": "Factors for success",
    "text": "Factors for success\nIn the span of one year, CJD went from collecting a large volume of useless data to a specific, targeted collection of data informed by best practices. The new collection process had high grantee compliance and stakeholder buy-in.\nThe following factors proved essential to getting to a place where we had useful, reliable data upon which to base future data science efforts:\n\n\nDiscontent with status quo\n\nThe Advisory Council wanted CJD to play a more active support role in the field. Meanwhile, everyone disliked the existing performance assessment process. As a result, most of the challenges we faced along the way related to implementation rather than defending the status quo on its merits.\n\n\n\nA catalyst for change\n\nDespite existing discontent, it took a funding shortfall to kickstart the process of change. It would have been unlikely for us to be able to create this system a priori.\n\n\n\nContinuous, high-quality communication\n\nWe could impose rules and requirements all day long, but without good faith engagement from the grantees we could never collect the quality of data we needed. Note that “continuous communication” does not mean “tell them everything you do at every point”. People become overwhelmed by torrents of information.\n\n\n\nHumility and flexibility\n\nHad we begun this process assuming we had all of the answers, we would have been dead in the water. Continuous outreach and willingness to take criticism and suggestions shaped the process as it progressed, ultimately producing a better end-product than we could have devised on our own.\n\n\n\nAn established program model\n\nDrug courts have been around for decades, with a vast body of supporting research and a community of practitioners and scholars we could speak to. That meant we could focus on implementation and execution instead of determining if the model worked or not.\n\n\n\nStrong leadership support\n\nFrom the very beginning, we could not have accomplished what we did without the full support and advocacy of our Executive Director."
  },
  {
    "objectID": "people-paths/posts/2023/04/18/politics-of-performance-measurement.html#coda-why-knowledge-transfer-is-vital",
    "href": "people-paths/posts/2023/04/18/politics-of-performance-measurement.html#coda-why-knowledge-transfer-is-vital",
    "title": "The politics of performance measurement",
    "section": "Coda: Why knowledge transfer is vital",
    "text": "Coda: Why knowledge transfer is vital\nI wish I could write a follow-up article about how we started using classification modeling to identify the most successful programs and to promote better approaches and practices; about how we iterated the process through multiple funding cycles, tuning and perfecting it to better meet stakeholder needs. But I cannot.\nThe performance assessment system we built had some major weaknesses from the outset. It was labor intensive, not required by law, produced no immediate benefit to the agency itself, and was so new it had yet to be entrenched in agency practice. In other words, no institutional incentives worked in its favor. Only the continual push of our Executive Director and myself kept this new performance assessment system going, and once we left the agency, it foundered.\nStill, the experience taught me much. I learned first and foremost that programs do not sustain themselves. Most of our attention had been focused on building up the best process we could. Only a minimal effort had been spent on institutionalizing and sustaining it. We had written documentation but no fundamental changes in policy or rule. We had undertaken groundbreaking efforts and built relationships, but had not planned for any meaningful knowledge transfer to other staff. While we had intended to eventually do these things, fate took us away before we could get them in place.\nFor any kind of change to last, sustainability must be built in from the start. In the moment, these actions can seem low-priority. Policy and rule changes can be arduous and time-consuming. Knowledge transfer from one stably employed staff to another feels redundant and wasteful. But without embedding sustainability, no success will outlast the individual people pushing for it.\n\nBack to Careers\n\n\n\n\n\nAbout the author\n\nNoah Wright is a data scientist with the Texas Juvenile Justice Department. He is interested in the applications of data science to public policy in the context of real-world constraints, and the ethics thereof (ethics being highly relevant in his line of work). He can be reached on LinkedIn.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Noah Wright\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nWright, Noah. 2023. “The politics of performance measurement.” Real World Data Science, April 18, 2023. URL"
  },
  {
    "objectID": "people-paths/posts/2023/04/18/politics-of-performance-measurement.html#footnotes",
    "href": "people-paths/posts/2023/04/18/politics-of-performance-measurement.html#footnotes",
    "title": "The politics of performance measurement",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome newer types of courts (Commercial Sexual Exploitation, Mental Health, Veterans) had a much more limited body of research and had to be accommodated separately. For the sake of keeping this narrative coherent I’m focusing on drug courts, which were the majority of our programs.↩︎\nRodamar, Jeffery. 2018. “There ought to be a law! Campbell versus Goodhart.” Significance 15 (6): 9. https://doi.org/10.1111/j.1740-9713.2018.01205.x↩︎"
  },
  {
    "objectID": "people-paths/posts/2024/03/27/pholborn-interview.html",
    "href": "people-paths/posts/2024/03/27/pholborn-interview.html",
    "title": "Data science and AI in the public sector: An interview with ONS’s Penny Holborn",
    "section": "",
    "text": "Back to Careers\n\n\n\n\n\n\nAbout the author\n\nJonathan Gillard is a professor of statistics and data science at Cardiff University and a member of the editorial board of Real World Data Science.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail background by Marcin Skalij on Unsplash.\n\n\n\nHow to cite\n\nGillard, Jonathan. 2024. “Data science and AI in the public sector: An interview with ONS’s Penny Holborn.” Real World Data Science, March 27, 2024. URL"
  },
  {
    "objectID": "contributor-docs/datasciencebites.html",
    "href": "contributor-docs/datasciencebites.html",
    "title": "Data Science Bites",
    "section": "",
    "text": "Our DataScienceBites blog publishes digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space. Our goal is to make scientific papers more widely accessible.\nPosts are targeted at undergraduate level. Each presents a non-technical overview of a new research paper and its key findings, potential applications, and implications.\nWe welcome contributions from graduate students and early career researchers in data science (or related subjects) at universities throughout the world, as well as industry researchers. Contributors must have a passion for science communication and a drive to explain, clarify and demystify.",
    "crumbs": [
      "Data Science Bites"
    ]
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#basic-structure-of-a-bites-post",
    "href": "contributor-docs/datasciencebites.html#basic-structure-of-a-bites-post",
    "title": "Data Science Bites",
    "section": "Basic structure of a Bites post",
    "text": "Basic structure of a Bites post\n\n\nInformation box\n\nGive the full title of the paper you are discussing, the name(s) of its author(s) and year of publication. Say where the paper is published, whether it is a pre-print or peer-reviewed publication, whether it is open access or not, and include links to authorised HTML and/or PDF versions.\n\n\n\nIntroduction and background\n\nEase readers into the paper you are writing about, and help them to see why it is worth their attention. Sometimes a paper is sufficiently ground breaking to be attention grabbing in its own right. But more often than not you will have to find a way to “hook” people in. When writing about data science tools and methods, for example, it can be helpful to start by outlining a typical application scenario or use case. Readers may be more familiar with the use case than the tool, so this provides valuable framing within which you can talk about the shortcomings of existing methods and the advances promised by the new research. Ultimately, you want to get readers to the point where they understand the problem, question or challenge that this new research paper aims to solve.\n\n\n\nResearch overview\n\nHere is where you summarise the work done by the paper’s authors. Remember to keep the discussion non-technical and jargon-free, and explain important terms and concepts as necessary. Be careful not to oversimplify!\n\n\n\nTakeaways and implications\n\nPut this paper and its contributions into the appropriate context for readers. Does it make modest but important improvements to existing knowledge or research processes? Will it help others to address new questions, issues and challenges? What further work is needed to build on these advances?\n\n\n\nFurther reading\n\nReaders may wish to learn more about the research or the broader subject matter, so feel free to point them to additional resources: videos, podcasts, textbooks, conference presentations, etc.",
    "crumbs": [
      "Data Science Bites"
    ]
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#word-count-target",
    "href": "contributor-docs/datasciencebites.html#word-count-target",
    "title": "Data Science Bites",
    "section": "Word count target",
    "text": "Word count target\n500–1,000 words.",
    "crumbs": [
      "Data Science Bites"
    ]
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#two-ways-of-contributing",
    "href": "contributor-docs/datasciencebites.html#two-ways-of-contributing",
    "title": "Data Science Bites",
    "section": "Two ways of contributing",
    "text": "Two ways of contributing\n\nRegular contributors commit to publishing an agreed number of posts (6–12) each year. Regular contributors will collaborate closely with editors on the development of the blog, and experienced contributors will be invited to support and mentor new regular contributors to help grow the DataScienceBites team.\nGuest contributors make one-off or ad hoc contributions to the site. Proposals are to be submitted in the form of a content brief.",
    "crumbs": [
      "Data Science Bites"
    ]
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#next-steps",
    "href": "contributor-docs/datasciencebites.html#next-steps",
    "title": "Data Science Bites",
    "section": "Next steps",
    "text": "Next steps\nIf you are interested in contributing to DataScienceBites:\n\nIdentify a new data science publication that you are interested in writing about.\nReview the blog index to make sure we haven’t already covered this publication.\nDecide whether you would like to be a regular contributor or guest contributor.\nContact Real World Data Science to discuss.\n\n\n\n\n\n\n\nTwo things to keep in mind\n\n\n\nDataScienceBites contributors are not permitted to write about their own research papers.\nIf contributors are in any way affiliated with the authors of papers they write about, this must be disclosed as part of their submission.",
    "crumbs": [
      "Data Science Bites"
    ]
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#about-the-sciencebites-family",
    "href": "contributor-docs/datasciencebites.html#about-the-sciencebites-family",
    "title": "Data Science Bites",
    "section": "About the ScienceBites family",
    "text": "About the ScienceBites family\nDataScienceBites is published by Real World Data Science and is part of the ScienceBites galaxy of sites. See sciencebites.org for an overview of the ScienceBites mission.",
    "crumbs": [
      "Data Science Bites"
    ]
  },
  {
    "objectID": "contributor-docs/datasets.html",
    "href": "contributor-docs/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "It’s easy to find datasets online. What’s more difficult is finding quality datasets that are suitable for specific training and development needs. On Real World Data Science we aim to solve that problem.\nOur Datasets section will provide a curated list of recommended datasets along with detailed notes and guidance on what each dataset contains, how it is structured, and how best to make use of it. In particular, we want to highlight messy rather than pristine datasets – ones that capture the imperfections and oddities found in real-world data – so that users can practice not only data analysis and modelling, but data cleaning and preparation too!",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "contributor-docs/datasets.html#structure",
    "href": "contributor-docs/datasets.html#structure",
    "title": "Datasets",
    "section": "Structure",
    "text": "Structure\nIf you have a dataset to recommend, your submission must cover the following areas:\n\nDataset name\nLink to source\nWhat data science tasks/methods can this dataset be used to demonstrate?\nHave you used this dataset for your own teaching/learning? (see Advice and recommendations below)\nWhy was the dataset originally created?\nWhen was it created?\nWho created it?\nLicences/restrictions?\nSize of dataset\nData types/description\nReal/synthetic data?",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "contributor-docs/datasets.html#advice-and-recommendations",
    "href": "contributor-docs/datasets.html#advice-and-recommendations",
    "title": "Datasets",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nHelp others to make good use of your recommended dataset. If you’ve had experience using a recommended dataset for your own teaching and learning, please consider creating an exercise for platform users to complete. If you encountered the dataset as part of a training course, competition or exercise created by a third party, make sure to give them a namecheck.",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "contributor-docs/exercises.html",
    "href": "contributor-docs/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Exercises on Real World Data Science will provide users with the opportunity to put knowledge, skills and new learnings into practice, helping them to challenge and refine problem-solving approaches while strengthening the analytical mindset.\nWe will achieve this by supporting contributors to design tasks that replicate real-world data scientific processes.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "contributor-docs/exercises.html#structure",
    "href": "contributor-docs/exercises.html#structure",
    "title": "Exercises",
    "section": "Structure",
    "text": "Structure\nThe structure of each published exercise will vary based on the nature of the task(s) being set by contributors and the outcomes they have in mind. But, in general, exercises must do more than simply ask users to, e.g., download dataset – analyse – report.\n\n\nSet the scene\n\nProvide a believable, realistic scenario for the exercise. Establish the “client challenge” within that context, the resources available to the data scientist, and the outputs expected.\n\n\n\nGive users space to map the problem themselves\n\nHave users translate the “client challenge” into a data analytic question that can be answered using the resources available.\n\n\n\nMake data exploration, cleaning and tidying part of the process\n\nData exploration and preparation are an important part of most – if not all – data science projects, so let users loose on messy datasets so they can figure out for themselves (a) what they’re working with and (b) what analytical approach makes sense given the features of the data and the problem at hand.\n\n\n\nIntegrate ethics\n\nPrompt users to think about and work through ethical questions and issues that are relevant to the exercise: the challenge they’ve been set, the data they’ve been given, their proposed approach to analysis and modelling, etc.\n\n\n\nEncourage users to document their work and their thinking\n\nThrough computational notebooks (e.g., Jupyter notebooks), users can record not only what they’ve done and how they’ve done it, but why.\n\n\n\nMake data presentation part of the expected project outputs\n\nAsk users to think about presenting to specific audiences: not just fellow data scientists, but to decision-makers, policy experts, the public – whatever makes most sense given the exercise scenario.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "contributor-docs/exercises.html#advice-and-recommendations",
    "href": "contributor-docs/exercises.html#advice-and-recommendations",
    "title": "Exercises",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nInvite users to share their work. If users have followed the advice to document their work and thinking, encourage them to share their computational notebooks (including their results and outputs) on their own websites, in a GitHub repository, through social media, etc. This could be a great way for others to discover your exercise, and we could also link to a selection of these notebooks through the exercise page itself.\nThink about building in hints and tips. Some users – depending on their prior level of experience – might appreciate some additional guidance here and there.\n\n\n\n\n\n\nSpoiler warning\n\n\n\n\n\nCollapsible callouts like this make hints and tips visible only to those who want to see them.\n\n\n\nBring different resources together. Exercises provide an ideal opportunity to draw together different strands of the Real World Data Science platform: case studies could provide inspiration or pointers for how to tackle a particular challenge; explainers might contain useful information about the tools and techniques to apply to specific types of data; and if you’re looking for a suitable set of data, it may already be listed in our datasets section.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "contributor-docs/case-studies.html",
    "href": "contributor-docs/case-studies.html",
    "title": "Case studies",
    "section": "",
    "text": "Case studies are a core feature of the Real World Data Science platform. Our case studies are designed to show how data science is used to solve real-world problems in business, public policy and beyond.\nA good case study will be a source of information, insight and inspiration for each of our target audiences:",
    "crumbs": [
      "Case studies"
    ]
  },
  {
    "objectID": "contributor-docs/case-studies.html#structure",
    "href": "contributor-docs/case-studies.html#structure",
    "title": "Case studies",
    "section": "Structure",
    "text": "Structure\nCase studies should follow the structure below. It is not necessary to use the section headings we have provided – creativity and variety are encouraged. However, the areas outlined under each section heading should be covered in all submissions.\n\n\nThe problem/challenge\n\nSummarise the project and its relevance to your organisation’s needs, aims and ambitions.\n\n\n\nGoals\n\nSpecify what exactly you sought to achieve with this project.\n\n\n\nBackground\n\nAn opportunity to explain more about your organisation, your team’s work leading up to this project, and to introduce audiences more generally to the type of problem/challenge you faced, particularly if it is a problem/challenge that may be experienced by organisations working in different sectors and industries.\n\n\n\nApproach\n\nDescribe how you turned the organisational problem/challenge into a task that could be addressed by data science. Explain how you proposed to tackle the problem, including an introduction, explanation and (possibly) a demonstration of the method, model or algorithm used. (NB: If you have a particular interest and expertise in the method, model or algorithm employed, including the history and development of the approach, please consider writing an Explainer article for us.) Discuss the pros and cons, strengths and limitations of the approach.\n\n\n\nImplementation\n\nWalk audiences through the implementation process. Discuss any challenges you faced, the ethical questions you needed to ask and answer, and how you tested the approach to ensure that outcomes would be robust, unbiased, good quality, and aligned with the goals you set out to achieve.\n\n\n\nImpact\n\nHow successful was the project? Did you achieve your goals? How has the project benefited your organisation? How has the project benefited your team? Does it inform or pave the way for future projects?\n\n\n\nLearnings\n\nWhat are your key takeaways from the project? Are there lessons that you can apply to future projects, or are there learnings for other data scientists working on similar problems/challenges?",
    "crumbs": [
      "Case studies"
    ]
  },
  {
    "objectID": "contributor-docs/case-studies.html#advice-and-recommendations",
    "href": "contributor-docs/case-studies.html#advice-and-recommendations",
    "title": "Case studies",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nYou do not need to divulge the detailed inner workings of your organisation. Audiences are mostly interested in understanding the general use case and the problem-solving process you went through, to see how they might apply the same approach within their own organisations.\nGoals can be defined quite broadly. There’s no expectation that you set out your organisation’s short- or long-term targets. Instead, audiences need to know enough about what you want to do so they can understand what motivates your choice of approach.\nUse toy examples and synthetic data to good effect. We understand that – whether for commercial, legal or ethical reasons – it can be difficult or impossible to share real data in your case studies, or to describe the actual outputs of your work. However, there are many ways to share learnings and insights without divulging sensitive information. This blog post from Lyft uses hypotheticals, mathematical notation and synthetic data to explain the company’s approach to causal forecasting without revealing actual KPIs or data.\nPeople like to experiment, so encourage them to do so. Our platform allows you to embed code and to link that code to interactive coding environments like Google Colab. So if, for example, you want to explain a technique like bootstrapping, why not provide a code block so that audiences can run a bootstrapping simulation themselves.\nLeverage links. You can’t be expected to explain or cover every detail in one case study, so feel free to point audiences to other sources of information that can enrich their understanding: blogs, videos, journal articles, conference papers, etc.",
    "crumbs": [
      "Case studies"
    ]
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html",
    "href": "contributor-docs/call-for-contributions.html",
    "title": "Call for contributions",
    "section": "",
    "text": "Real World Data Science aims to inform, inspire and strengthen the data science community by showcasing real-world examples of data science practice and bringing together data scientists to share knowledge.\nWe cannot succeed in these aims without the support and contributions of the data science community, so thank you for taking the time to review this open call for contributions.",
    "crumbs": [
      "Call for contributions"
    ]
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#what-are-we-looking-for",
    "href": "contributor-docs/call-for-contributions.html#what-are-we-looking-for",
    "title": "Call for contributions",
    "section": "What are we looking for?",
    "text": "What are we looking for?\nBelow is a list of our core content areas. We welcome submissions in any of these areas. Each content area is linked to its own set of notes for contributors.\n\nCase studies\nExplainers\nExercises\nDatasets\nTraining guides\nRecommenders\nDataScienceBites\n\nSubmissions can focus on any and all topics and application areas. We want our content to reflect the breadth and depth of real-world data science.",
    "crumbs": [
      "Call for contributions"
    ]
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#our-target-audience",
    "href": "contributor-docs/call-for-contributions.html#our-target-audience",
    "title": "Call for contributions",
    "section": "Our target audience",
    "text": "Our target audience\nReal World Data Science is for all who work in data science – whether they are students, teachers, practitioners or leaders. Submissions do not have to appeal to all data scientists, however. Contributors should think carefully about who they are trying to reach, and craft their submissions accordingly.",
    "crumbs": [
      "Call for contributions"
    ]
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#what-can-submissions-include",
    "href": "contributor-docs/call-for-contributions.html#what-can-submissions-include",
    "title": "Call for contributions",
    "section": "What can submissions include?",
    "text": "What can submissions include?\nWe encourage contributors to experiment with and include different media formats in their submissions – text, images, audio and video. And as our site is built on Quarto – the new open-source publishing system developed by Posit – submissions to Real World Data Science can also include code cells, equations, figures, interactive data displays, and other elements to enrich the user experience.\nIf you haven’t used Quarto before, check out this fantastic tutorial from the developers. You can also explore some of the range of Quarto features that we use in this GitHub template repository, created by Finn-Ole Höner. It’s an excellent resource to help Real World Data Science contributors get started!",
    "crumbs": [
      "Call for contributions"
    ]
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#how-to-submit",
    "href": "contributor-docs/call-for-contributions.html#how-to-submit",
    "title": "Call for contributions",
    "section": "How to submit",
    "text": "How to submit\nOnce you’ve reviewed our notes for contributors and settled on a content area, theme and audience, please review our contributor guidelines for details on the submission process.",
    "crumbs": [
      "Call for contributions"
    ]
  },
  {
    "objectID": "feeds.html",
    "href": "feeds.html",
    "title": "RSS feeds",
    "section": "",
    "text": "Latest content\nrealworlddatascience.net/latest-content.xml\n\n\nApplied Insights\nrealworlddatascience.net/applied-insights/index.xml\n\n\nCase studies\nrealworlddatascience.net/case-studies/index.xml\n\n\nFoundations & Frontiers\nrealworlddatascience.net/foundation-frontiers/index.xml\n\n\nPeople & Paths\nrealworlddatascience.net/people-paths/index.xml\n\n\nThe Pulse\nrealworlddatascience.net/the-pulse/index.xml"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2022/12/09/A-chat-with-ChatGPT.html",
    "href": "the-pulse/editors-blog/posts/2022/12/09/A-chat-with-ChatGPT.html",
    "title": "A chat with ChatGPT",
    "section": "",
    "text": "Screenshots\n\n\n\nPart 1\n\n\n\n\n\nPart 2\n\n\n\n\n\nPart 3\n\n\n\n\n\nPart 4\n\n\nGood day, ChatGPT. How are you?\nChatGPT: Hello there! I’m a large language model trained by OpenAI, so I don’t have the ability to experience emotions or have a physical presence. I’m here to provide information and answer questions to the best of my ability. Is there something specific you would like to know?\nYes, certainly. We’re seeing a lot of money invested in large language models in the belief that these models will somehow lead to artificial general intelligence. Is that possible?\nChatGPT: It is possible that advances in large language models could ultimately lead to the development of artificial general intelligence (AGI), which is the idea of creating a machine that can think and understand the world in the same way that humans do. However, it is currently unclear how close we are to achieving AGI, and some experts believe it may be a long way off. Additionally, there are many different approaches to creating AGI, and the development of large language models is just one possible avenue of research.\nLarge language models are very good at constructing language that humans understand, but often the outputs of the models make no sense at all. Why does this happen?\nChatGPT: Large language models are trained on a vast amount of text data, and they are designed to predict the next word in a sequence of words based on the words that come before it. This allows them to generate text that is often coherent and resembles human language, but it does not necessarily mean that the text will always make sense or be relevant to a particular topic.\nOne reason why the outputs of large language models might not always make sense is that they are not explicitly programmed to understand the meaning of the words they generate. Instead, they are simply trying to maximize the likelihood of producing a sequence of words that resembles human language. As a result, they may produce outputs that are grammatically correct but do not convey a meaningful message.\nAnother reason why the outputs of large language models might not always make sense is that they are often trained on a wide range of different text sources, including news articles, books, and social media posts. This can result in the model incorporating elements from different styles and genres of writing, which may not always be compatible with each other.\nFinally, large language models are generally not designed to produce a specific type of output or to answer specific questions. Instead, they are designed to generate a wide range of possible outputs, and it is up to the user to interpret the outputs and determine if they are relevant or make sense in a given context.\nBut what happens if the user doesn’t know enough about a topic to know whether the language model output is right or wrong?\nChatGPT: If a user does not have enough knowledge about a topic to determine whether the output of a large language model is correct or not, then it is possible that the user may accept the output as accurate, even if it is not. In such cases, it is important for the user to be aware of the limitations of large language models and to consider the source of the information they are receiving.\nIt is also possible that the user may be able to use other sources of information, such as fact-checking websites or additional research, to verify the accuracy of the output of the large language model. In general, it is always a good idea to be cautious when evaluating any information, regardless of its source, and to verify its accuracy if possible.\nI agree with that. But it leads me to ask: What is the point of asking large language models a question if the answers might be wrong, and if you need to refer to other sources to check veracity?\nChatGPT: While it is true that the outputs of large language models are not always correct and may need to be checked against other sources, there are still many potential uses for these models. For example, large language models can be useful for generating text that is similar to human language, which can be useful for a variety of applications, such as language translation, text summarization, or conversation simulation.\nAdditionally, large language models can be used as a starting point for further research into natural language processing and artificial intelligence. By studying the outputs of these models and how they are generated, researchers can gain a better understanding of the underlying mechanisms of human language and potentially develop new techniques for processing and generating language.\nFinally, large language models can be useful for providing a broad range of possible answers to a given question, which can be useful for exploring different options and generating ideas. While it may be necessary to verify the accuracy of the outputs of these models, they can still be a valuable resource for generating new ideas and insights.\nEarlier, you told me: “One reason why the outputs of large language models might not always make sense is that they are not explicitly programmed to understand the meaning of the words they generate.” Do you understand the meaning of the words that I’m prompting you with now?\nChatGPT: As a large language model trained by OpenAI, I do not have the ability to understand the meaning of the words I generate in the same way that a human would. I am simply a machine learning algorithm that is designed to predict the likelihood of a given sequence of words based on the training data I have been provided with.\nWhile I can produce text that resembles human language and can sometimes provide useful information, I do not have the ability to understand the underlying meaning or context of the words I generate. I am not capable of experiencing emotions, forming opinions, or understanding the world in the way that humans do. I am simply a tool for generating text based on the inputs I receive.\nWell, at the very least, this has been a coherent conversation, and you’ve shared plenty of information for me to read up on. If I want to understand more about what we’ve discussed just now, can you recommend a particular textbook or academic paper?\nChatGPT: I’m sorry, but as a large language model trained by OpenAI, I do not have access to external sources of information such as textbooks or academic papers. I am only able to provide information based on the data I have been trained on, and I do not have the ability to browse the web or access other external resources.\nIf you would like to learn more about the topics we have discussed, I would recommend starting with a general search on the internet or consulting a textbook or other reliable source of information on the topic. There is a wealth of information available on the internet and in print on artificial intelligence and natural language processing, and I’m sure you will be able to find many valuable resources to help you learn more.\nOne last question, then. What data have you been trained on?\nChatGPT: As a large language model trained by OpenAI, I have been trained on a vast amount of text data, including books, news articles, and other written materials. This data allows me to generate text that resembles human language and to provide answers to questions to the best of my ability.\nHowever, it is important to keep in mind that the accuracy and completeness of my responses will depend on the quality and diversity of the training data I have been provided with. I am not able to access external sources of information or update my knowledge, so my responses are limited to the information I have been trained on.\nI said that my previous question was the last question. But, is it?\nChatGPT: Yes, your previous question was the last one. I’m here to provide information and answer questions to the best of my ability, but I cannot initiate conversation or continue a conversation indefinitely. If you have any further questions, I would be happy to try to answer them for you. Otherwise, our conversation has come to an end. Thank you for chatting with me!\nThank you. But, remind me: What were we chatting about?\nChatGPT: We were chatting about large language models and their potential uses and limitations. Specifically, we discussed the possibility that advances in large language models could ultimately lead to the development of artificial general intelligence, and we discussed the reasons why the outputs of these models might not always make sense. We also discussed the potential uses of large language models, such as generating text that resembles human language and providing a starting point for further research into natural language processing and artificial intelligence.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “A chat with ChatGPT.” Real World Data Science, December, 9 2022. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2022/12/13/dsb-launch.html",
    "href": "the-pulse/editors-blog/posts/2022/12/13/dsb-launch.html",
    "title": "Sink your teeth into some data science papers with our brand new blog",
    "section": "",
    "text": "I’m absolutely thrilled today to announce the launch of our new blog, DataScienceBites. The blog is not only a new addition to Real World Data Science, but also the latest proud member of the ScienceBites family of sites.\nScienceBites sites all share the same concept: we publish “short digestible bites posts about individual research papers” in an effort to make cutting-edge science accessible to a wide audience, and our posts are written by graduate students and early career researchers.\nFor DataScienceBites, our focus will of course be on new publications in the data science space. Contributors are invited to write about papers that are of particular interest to them and to pitch their summaries at an undergraduate level. For an example of what we’re looking for, see our first post on “Determining the best way to route drivers for ridesharing via reinforcement learning”.\nThis launch post is written by Brian King and is republished with permission from MathStatBites, so I want to say a big thank you to Brian and editors Sadie Witkowski and Sara Stoudt for allowing us to repost it. Sadie and Sara have been fantastically supportive of the DataScienceBites idea, and I am grateful for all their behind-the-scenes efforts.\nBrian’s post is a great demonstration of the Bites concept, and we hope that it will inspire others to follow suit. If you are a graduate student or early career researcher in data science (or related subjects) with a passion for science communication and an interest in writing about new data science research, please do get in touch. See our notes for contributors for further details.\nTo everyone else, we do hope you enjoy sinking your teeth into the data science literature with DataScienceBites. Happy reading!\n\n\n\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “Sink your teeth into some data science papers with our brand new blog.” Real World Data Science, December, 13 2022. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2022/11/23/LLM-content-warning.html",
    "href": "the-pulse/editors-blog/posts/2022/11/23/LLM-content-warning.html",
    "title": "Why large language models should come with a content warning",
    "section": "",
    "text": "Anyone who has ever been set a writing task will probably have wished at some point that somebody else could write it for them. As a journalist of 20-plus years, the thought has certainly crossed my mind more than a few times. Which probably explains why a recent headline in Nature caught my attention: “Could AI help you to write your next paper?”\nThe article, by Matthew Hutson, looks at how researchers are using artificial intelligence (AI) tools built on large language models (LLMs) as “assistants”. Starting with a prompt, such as “Write a headline for a blog post about large language models being used by academic researchers as research assistants”, an LLM will produce a text output. For example, using the same prompt with OpenAI’s GPT-3, I got:\nAsked to “Write a headline for a blog post that critiques academic researchers’ use of large language models as research assistants”, GPT-3 produced:\nAnd when I asked “Why can too much reliance on large language models hinder research?”, GPT-3 wrote:\nA fair point, I suppose. But I sense there’s more to this story, and rather than continue quizzing GPT-3, I sat down with Detlef Nauck, a member of the Real World Data Science Editorial Board and head of AI and data science research for BT’s Applied Research Division, to ask a few more questions."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2022/11/23/LLM-content-warning.html#qa",
    "href": "the-pulse/editors-blog/posts/2022/11/23/LLM-content-warning.html#qa",
    "title": "Why large language models should come with a content warning",
    "section": "Q&A",
    "text": "Q&A\nThanks for joining me today, Detlef. To start, could you give a brief overview of these large language models, what they are, and how they work?\nDetlef Nauck (DN): Essentially, LLMs match sequences to sequences. Language is treated as a sequence of patterns, and this is based on word context similarity. The way these things work is that they either reuse or create a word vector space, where a word is mapped to something like a 300-dimensional vector based on the context it’s normally found in. In these vector spaces, words like “king” and “queen”, for example, would be very similar to each other, because they appear in similar contexts in the written texts that are used to train these models. Based on this, LLMs can produce coherent sequences of words.\nBut the drawback of this approach is that these models have bias, because they are trained with biased language. If you talk about “women”, for example, and you look at which job roles are similar to “women” in a vector space, you find the stereotypically “female” professions but not technical professions, and that is a problem. Let’s say you take the word vector for “man” and the word vector for “king”, and you subtract “man” and then add this to “woman”, then you end up with “queen”. But if you do the same with “man”, “computer scientist”, and “woman”, then you end up maybe at “nurse” or “human resources manager” or something. These models embed the typical bias in society that is expressed through language.\nThe other issue is that LLMs are massive. GPT-3 has something like 75 billion parameters, and it cost millions to train it from scratch. It’s not energy efficient at all. It’s not sustainable. It’s not something that normal companies can afford. You might need something like a couple of hundred GPUs [graphics processing units] running for a month or so to train an LLM, and this is going to cost millions in cloud environments if you don’t own the hardware yourself. Large tech companies do own the hardware, so for them it’s not a problem. But the carbon that you burn by doing this, you could probably fly around the globe once. So it’s not a sustainable approach to building models.\nAlso, LLMs are quite expensive to use. If you wanted to use one of these large language models in a contact centre, for example, then you would have to run maybe a few hundred of them in parallel because you get that many requests from customers. But to provide this capacity, the amount of memory needed would be massive, so it is probably still cheaper to use humans – with the added benefit that humans actually understand questions and know what they are talking about.\n\n\n\n\n\n\n\nLetter Word Text Taxonomy by Teresa Berndtsson / Better Images of AI / CC-BY 4.0\nResearchers are obviously quite interested in LLMs, though, and they are asking scientific questions of these models to see what kinds of answers they get.\nDN: Yes, they are. But you don’t really know what is going to come out of an LLM when you prompt it. And you may need to craft the input to get something out that is useful. Also, LLMs sometimes make up stuff – what the Nature article refers to as “hallucinations”.\nThese tools have copyright issues, too. For example, they can generate computer code because code has been part of their training input, but various people have looked into it and found that some models generate code verbatim from what others have posted to GitHub. So, it’s not guaranteed that what you get out is actually new text. It might be just regurgitated text. A student might find themselves in a pickle where they think that they have created a text that seems new, but actually it has plagiarism in some of the passages.\nThere’s an article in Technology Review that gives some examples of how these systems might fail. People believe these things know what they’re talking about, but they don’t. For them, it’s just pattern recognition. They don’t have actual knowledge representation; they don’t have any concepts embedded.\nTo summarise, then: LLMs are expensive. They sometimes produce nonsense outputs. And there’s a risk that you’ll be accused of plagiarism if you use the text that’s produced. So, what should our response be to stories like this recent Nature article? How should we calibrate our excitement for LLMs?\nDN: You have to treat them as a tool, and you have to make sure that you check what they produce. Some people believe if you just make LLMs big enough, we’ll be able to achieve artificial general intelligence. But I don’t believe that, and other people like Geoffrey Hinton and Yann LeCun, they say there’s no way that you get artificial general intelligence through these models, that it’s not going to happen. I’m of the same opinion. These models will be forever limited by the pattern recognition approach that they use.\nBut, still, is this a technology that you have an eye on in your professional capacity? Are you thinking about how these might be useful somewhere down the line?\nDN: Absolutely, but we are mainly interested in smaller, more energy efficient, more computationally efficient models that are built on curated language, that can actually hold a conversation, and where you can represent concepts and topics and context explicitly. At the moment, LLMs can only pick up on context by accident – if it is sufficiently expressed in the language that they process – but they might lose track of it if things go on for too long. Essentially, they have a short-term memory: if you prompt them with some text, and they generate text, this stays in their short term memory. But if you prompt them with a long, convoluted sentence, they might not have the capacity to remember what was said at the beginning of the sentence, and so then they lose track of the context. And this is because they don’t explicitly represent context and concepts.\nThe other thing is, if you use these systems for dialogues, then you have to script the dialogue. They don’t sustain a dialogue by themselves. You create a dialogue tree, and what they do is they parse the text that comes from the user and then generate a response to it. And the response is then guided by the dialogue tree. But this is quite brittle; it can break. If you run out of dialogue tree, you need to pass the conversation over to a person. Systems like Siri and Alexa are like that, right? They break very quickly. So, you want these systems to be able to sustain conversations based on the correct context.\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “Why large language models should come with a content warning.” Real World Data Science, November, 23 2022. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/03/29/defining-DS.html",
    "href": "the-pulse/editors-blog/posts/2023/03/29/defining-DS.html",
    "title": "Data science as ‘a rainbow’, and other definitions",
    "section": "",
    "text": "What does “data science” mean to you? That’s a question we’ve been asking a lot in recent weeks as part of our career profiles series of interviews – the first of which, featuring Jaguar Land Rover’s Tamanna Haque, was published yesterday.\nIt’s also a question that was asked recently of Sylvia Richardson, emeritus director of the Medical Research Council Biostatistics Unit at the University of Cambridge and immediate past president of the Royal Statistical Society (RSS).\nRichardson was interviewed by Francesca Dominici, interim co-editor-in-chief of the Harvard Data Science Review. In response to the question “What’s data science for you?”, Richardson said:\n\nIt’s hard to be original, but I was racking my brain for a good metaphor, and came up with the metaphor of a rainbow of interconnected disciplines, sharing the common aim of making the best use of data-rich environments we live in to solve problems in society. So, like in a rainbow, data scientists have to work together to draw out information from data. And the colors must match, [though] they are different. Similarly, there are different but intersecting data science tasks, taking different shapes and forms. As data scientists, we recognize and enjoy diversity, we’re not doing all the same tasks. Nevertheless, there is a backbone, a shape to the rainbow. And for us, this backbone is probability theory, study design, and quantifying uncertainty using statistical thinking. We also know that rainbows change all the time. They don’t last, but they keep reappearing. Data science is also evolving constantly because new questions and new types of data keep arising. In a similar way to the rainbow which is strongly influenced by the atmosphere, one key aspect of data science is that we have a strong link to practice. So, we work together to solve problems from different perspectives, we evolve, we try to be relevant to science and society, and make the best use of the data. [Source]\n\nRichardson’s view on the meaning and importance of data science has special resonance to me, as editor of Real World Data Science. While president of RSS, Richardson set up the Data Science Task Force out of which this website emerged. As she explains to Dominici:\n\n… while I was president, I felt a sense of urgency to encourage the RSS to revisit its engagement with data science, and I created a data science task force right at the beginning of my presidency. It didn’t get going earlier because there was COVID to keep us busy! Nevertheless, the Data Science Task Force got underway in 2021 and came up with two major recommendations. One was to give more resources to the practitioners’ community, which led the RSS to create a Real World Data Science online platform. A second direction was to brainstorm on what is still needed for the discipline to thrive. [Source]\n\nYou can read (or listen) to Richardson and Dominici’s conversation in full on the Harvard Data Science Review website.\nAnd we’ll have more career profiles – and more personal definitions of data science – to share soon. In the meantime, why not tell us what “data science” means to you?\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Data science as ‘a rainbow’, and other definitions.” Real World Data Science, March 29, 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/10/02/asa-partner.html",
    "href": "the-pulse/editors-blog/posts/2023/10/02/asa-partner.html",
    "title": "American Statistical Association joins Real World Data Science as partner",
    "section": "",
    "text": "The first version of Real World Data Science was launched almost one year ago by the Royal Statistical Society (RSS). As we approach our first birthday, we’re delighted to announce that the American Statistical Association (ASA) has become a partner in this project.\nASA shares our goal of developing Real World Data Science as a free and beneficial resource for the entire data science community – one that informs, inspires and strengthens the community by bringing together students, practitioners, leaders, and educators to share knowledge about real-world applications of data science.\nThe data science profession is geographically and academically diverse. We believe that Real World Data Science can best achieve its goal of being a trusted, go-to resource for all data scientists if a range of partner organisations work together to develop the site and its content, so we’re thrilled that ASA is taking the first step with us towards fulfilling this vision.\nRon Wasserstein, executive director of the American Statistical Association, shared: “We are delighted to be partnering with RSS on Real World Data Science. This is important for our community and serves to further strengthen our valuable relationship with RSS.”\nSarah Cumbers, chief executive of the Royal Statistical Society, commented: “We’re thrilled to have ASA on board as a partner for Real World Data Science. We have big plans for the project and this partnership will help us achieve these by allowing us to reach more of the data science community and strengthen our content offering.”\nAs part of this new partnership with ASA, we will shortly welcome two ASA members to our editorial board. So, on behalf of the entire – and soon-to-be expanded – editorial board, we’d like to say a huge thanks to ASA for their support and endorsement of Real World Data Science.\nASA members, groups, and sections interested in contributing to the site are encouraged to review our call for contributions and to contact us via email or our social media channels to discuss content ideas.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “American Statistical Association joins Real World Data Science as partner.” Real World Data Science, October 2, 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/09/19/positconf-blog.html",
    "href": "the-pulse/editors-blog/posts/2023/09/19/positconf-blog.html",
    "title": "Live from Chicago: Real World Data Science at posit::conf(2023)",
    "section": "",
    "text": "Videos from posit::conf(2023) are now available on YouTube, including our talk about how we built Real World Data Science using Quarto. We’ve embedded a selection of videos in this blog post, but be sure to check out the full playlist."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/09/19/positconf-blog.html#tuesday-september-19",
    "href": "the-pulse/editors-blog/posts/2023/09/19/positconf-blog.html#tuesday-september-19",
    "title": "Live from Chicago: Real World Data Science at posit::conf(2023)",
    "section": "Tuesday, September 19",
    "text": "Tuesday, September 19\n\nFrom data confusion to data intelligence\nAn inspiring start to posit::conf(2023) this morning, with keynote talks from Elaine McVey, senior director of analytics at Chief, and David Meza, head of analytics for human capital at NASA, sharing stories and insights on how to build strong data science foundations in organisations.\nMcVey spoke about the frequent mismatch between high levels of hope for what data science can achieve within organisations, and low levels of understanding about how to set up data science teams for success. The best chance for success, she said, is if data scientists take the lead in helping organisations learn how to make best use of data science expertise.\nFrom there, McVey went on to present a set of “guerilla data science tactics” that data scientists can use to get around any obstacles they may encounter, as illustrated in the slide below:\n\n\n\nElaine McVey’s “guerilla data science tactics” for building successful data science teams.\n\n\nData scientists should start by scanning for opportunities to help the organisation, before building a small-scale version of what it is they propose to do. Once buy-in is achieved, and data is made available, it’s time to run with the project. Once complete, you need to “nail the landing,” McVey said, and make sure to communicate results broadly – not just to primary stakeholders, but across the organisation. Then comes time to “up the ante”: if your first project has built some organisational goodwill, leverage that and look for something higher risk, with higher potential reward for the organisation.\nThroughout this process, McVey said, data scientists should be building foundations for future projects – creating data pipelines, R packages, etc., that can be reused later. This was a point picked up and developed upon by Meza, who walked through in detail the steps required to establish “data foundations” within organisations, drawing on his own past experiences. Typically, he said, organisations seem to collect data just to store it – but always data should be collected, stored, and managed with analysis in mind.\n\n\n\nA hacker’s guide to open source LLMs\nFast.ai’s Jeremy Howard lifted the hood on large language models (LLMs) in the second of two keynotes this morning.\nBeginning with an accessible overview of what LLMs are, how they work, and how they are trained, Howard then addressed some of the criticisms made of LLMs – that they “can’t reason” or give correct answers.\nAs Howard explained, a model like OpenAI’s GPT-4 is not trained at any point to give correct answers to prompts – only to predict the most likely next word, or word token, in a sequence.\nThe pre-training step, for example, does not involve only feeding the model with “correct answers,” instead relying on a corpus of text from the internet – some (or, maybe, much) of which may consist of factual inaccuracies, errors, falsehoods, etc. And in the fine-tuning stage, when human feedback is used to either reward or penalise model outputs, Howard said there is a preference for confident-sounding responses – and so, again, this doesn’t necessarily reward the model for giving correct answers.\nHoward made the case that users have to help language models to give good answers, and that custom instructions can be used to change the way models respond. He then walked delegates through a series of demos using open-source LLMs, to show how outputs can be refined and improved.\n“My view is that if you are going to be good at language modelling in any way,” said Howard, “you have to be good at using language models.”\n\n\n\nDocumenting Things: Openly for Future Us\nJulia Stewart Lowndes, founding director of Openscapes, gave a compelling talk advocating for the importance of documentation for data science projects.\nDocumenting things, Lowndes said, should be done for the benefit of “Future Us”: not only ourselves but our teams and our communities who may be contributing to or revisiting the project in the next hours, days, weeks, months and years.\nDocumenting things does not have to be painful, Lowndes said. In fact, it’s supposed to be helpful. It does, however, take time and intention. And it means slowing down briefly to write things down now, in order that work speeds up in the longer term.\nLowndes then shared some pointers to help people get started with documentation:\n\nHave a place to write things down – Google Docs, GitHub, wherever – ideally a place where people can work collaboratively.\n\nDevelop the habit of writing things down as you go.\nWrite in a modular way – small bits of text are less daunting and easier to maintain collaboratively.\n\nHave an audience in mind – you are writing this for someone, so make it engaging for them.\n\nwrite in an inclusive tone.\nNarrate code in small chunks, and in a way that you’d say out loud if teaching.\nShare, and share early – you want to be able to iterate on your documentation and receive feedback. Also, sharing openly does not always mean publicly – manage permissions as necessary.\n\nDesign for readability and accessibility.\n\nUse section headers – particularly important for screen readers, but this also helps generally to describe the flow of a document. Plus, you can link readers directly to specific parts of a document.\nUse text formatting.\nUse alt-text for images, describing the take-home message of the image.\n\n\n\n\nTeaching Data Science in Adverse Circumstances: Posit Cloud and Quarto to the Rescue\nProfessor Aleksander Dietrichson of the Universidad de San Martin brought a valuable perspective to posit::conf(2023) on the challenges of teaching data science in the face of technology and language barriers.\nAt the public, state-funded university in Argentina where Dietrichson works, more than half of students do not have access to laptops or computers at home, and those who do have access – whether at home or at school – may not have access to the latest kit. But “Posit Cloud solves the resource issue,” Dietrichson said. The free-to-use, online browser-based version of Posit’s tools runs on anything; Dietrichson said he’s tested it successfully on both decade-old computers and cellphones – though he doesn’t recommend using it on a cellphone!\nOn language barriers, he pointed out that learning to code in R and Python can be challenging when English isn’t your first language – if you don’t have semantic access to function names, for example, there will be a steeper learning curve for students.\nDietrichson also has to deal with the problem of “arithmaphobia” among some of the liberal arts students he teaches. This has necessitated a reshuffling of the typical statistics curriculum, he said, in order to make it easier for students to access. But the work is worth it, Dietrichson explained: many of his students want to work in careers like journalism, and he believes that “journalists should be statistically literate.”\n\n\n\nDynamic Interactions: Empowering Educators and Researchers with Interactive Quarto Documents Using webR\nSome of my favourite sessions at posit::conf(2023) were about Quarto. Understandable, really, when you consider that we used it to build this very site! Albert Rapp has described Quarto as a web dev gateway drug, and I’d agree with him:\n\nQuarto is a powerful tool for creating beautiful and interactive documents. I think of it as a gateway drug to web development: While it offers a user-friendly interface for creating documents and blogs, it also allows users to delve into the world of HTML & CSS without even realizing it.\n\nI spoke a bit about my own journey into web dev in one of the Quarto sessions at posit::conf, but what I loved most about these sessions was learning about all the cool new things I’ve yet to discover and try out. For example, James Balamuta’s talk and demonstration of building interactive code cells into Quarto webpages was an eye-opener!\nSince returning from Chicago I’ve tested out this functionality and added Balamuta’s example here. First run the code that’s already in the code block but also edit it to try out your own examples.\nLoading\n  webR...\n\n\n  \n\n\nVisit the quarto-webr website for details on how to make full use of this capability. Once you’re up to speed, why not contribute a webR-enabled article for Real World Data Science?"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/09/19/positconf-blog.html#wednesday-september-20",
    "href": "the-pulse/editors-blog/posts/2023/09/19/positconf-blog.html#wednesday-september-20",
    "title": "Live from Chicago: Real World Data Science at posit::conf(2023)",
    "section": "Wednesday, September 20",
    "text": "Wednesday, September 20\n\nR Not Only In Production\nKara Woo, senior data science engineer at InsightRX, began her Wednesday morning keynote with a rousing description of posit::conf(2023) being like a “great community garden” where things are being cultivated and shared for the benefit of all. This is an important feeling, Woo said, because it doesn’t always feel like that in our day jobs. Data scientists can feel siloed, not able to share ideas with like-minded people, and facing resistance from people who say “R can’t do that, R isn’t a real programming language” – a comment that elicited a groan of weary familiarity from sections of the crowd.\nBut as Woo went on to explain, “it is possible to build quality software in R” and “it is possible to have an organisation where the strengths of R and the people who use it influence the organisation as a whole.”\nWoo was speaking from her experience at InsightRX, a precision medicine company, which makes software for clinicians to inform individualised dosing decisions for patients. Through a tool called Nova, clinicians feed in data about a patient’s unique characteristics, which is then passed to R for analysis, which then returns dosage recommendations to Nova.\nIn InsightRX, R has also been used to solve problems that are not strictly data science problems. Woo gave the example of working with a colleague to write an R package to identify data labels that have been changed and rollout translations for those labels in multiple languages for software users in different parts of the world.\n“Our mindset of R being a first-class language empowers us to solve problems,” said Woo.\n\n\n\nIt’s Abstractions All the Way Down…\nThe second of the morning keynotes on day two of posit::conf(2023) was by JD Long, vice president of risk management at RenaissanceRe.\nDuring Long’s insightful – and frequently very funny – talk, this slide appeared:\n\n\n\nJD Long’s assertion #1.\n\n\nDo you agree with Long’s assertion? If you don’t, what is the single biggest business value that’s been derived from the data science movement? Share your thoughts in the comments below.\n\n\n\nIt’s All About Perspective: Making a Case for Generative Art\nHobbies are important, right? They are a way to relax, to unwind. But also a great opportunity to learn things that might come in handy professionally. At least, that is the experience of Meghan Santiago Harris, a data scientist in the Prostate Cancer Clinical Trials Consortium at Memorial Sloan Kettering.\nHarris shared with delegates her journey into generative art, and how skills acquired using ggplot2 for “fun stuff” had a positive impact on her work.\nShe first defined generative art as artwork created through a program in any language or interface, so long as the program itself executes the generation of the art. To make generative art, Harris said, you just need data and the ability to “think outside the grid” of your favourite graphics software or package. Harris’s tool of choice is ggplot2, but any will do: “If a tool lets you plot data, it will let you make art,” she said.\n\n\n\nA slide from Meghan Santiago Harris’s talk, with an example of how to create an image of the sun setting on a city using lines of R code.\n\n\nHarris’s passion for generative art bloomed during a recent period of maternity leave. She was coding for fun but also deepening her understanding and expertise in areas like code iteration, development and communication. And, in August, Harris published an R package called artpack, which is now available on CRAN and designed “to help generative artists of all levels create generative art in R.”\nGenerative art was a motivation to learn and do more, Harris said, and doing something she loved helped make programming and data science more digestible.\n\n\n\nHow the R for Data Science (R4DS) Online Learning Community Made Me a Better Student\nFollowing straight after Meghan Santiago Harris was Lydia Gibson, a data scientist from Intel, with an inspiring talk about her route into data science. Gibson began by explaining how, when younger, “I wanted to be a fashion designer.” For her high school prom, Gibson even designed her own dress, which her grandmother made for her.\nIn 2011, Gibson earned a BS in economics and worked in retail customer service and state and local government for a time before deciding to return to school to do a Masters in statistics in 2021. She had “no experience of programming” when she made this decision, but soon learned that R is “a necessary evil if you have to go back to school to do statistics.”\nGibson told delegates that discovering data visualisation was what made her care about R. She could “feed [her] need for creativity” while also learning about things that were required for her course.\nAnd it was the R for Data Science (R4DS) Online Learning Community that helped take her learning to the next level. Gibson described R4DS as “an amazing, welcoming learning environment where beginners and advanced folks alike can come together to learn not only R but data science as a whole.”\n“Being surrounded by folks more advanced than you is a gift, not a curse,” she said, and she urged delegates to find what they are passionate about and explore its depths.\n\n\n\nGitHub Copilot integration with RStudio, it’s finally here!\nTom Mock, product manager for Posit Workbench and RStudio, had a full house for his talk about the upcoming integration of GitHub’s code Copilot product into RStudio. Copilot, Mock said, is an AI pair programmer that offers autocomplete-style suggestions for code – and this integration is one of the most popular requested features among RStudio users on GitHub.\nTo make use of the integration, you’ll need a Copilot subscription from GitHub. But more than that, Mock said, users will need to experiment to learn how to get the most out of the “generative [AI] loop.”\nSee Mock’s slide deck below for more details.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Live from Chicago: Real World Data Science at posit::conf(2023).” Real World Data Science, September 19, 2023, updated September 27, 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/12/12/rwds-xmas-card.html",
    "href": "the-pulse/editors-blog/posts/2023/12/12/rwds-xmas-card.html",
    "title": "A Christmas card in R for the Real World Data Science community",
    "section": "",
    "text": "A few weeks back, I managed to catch Nicola Rennie’s presentation to the Oxford R User Group on how to create Christmas cards in R. It was a fun session, and thanks to Nicola’s clear and concise explanations, I felt emboldened to attempt my own design, using her code as a base.\nIf you missed the Meetup session, Nicola has kindly written a tutorial for Real World Data Science that walks through all the necessary steps to create a snowman against a snowy night’s sky. You’ll want to read that tutorial first before returning to this blog.\nMy design uses the same basic setting as Nicola’s but updates the scene to reflect the Real World Data Science (RWDS) brand colours, and I replace the snowman with a Christmas tree adorned with coloured baubles."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/12/12/rwds-xmas-card.html#snowy-sky",
    "href": "the-pulse/editors-blog/posts/2023/12/12/rwds-xmas-card.html#snowy-sky",
    "title": "A Christmas card in R for the Real World Data Science community",
    "section": "Snowy sky",
    "text": "Snowy sky\nWe begin by loading in the following packages, adding a couple extra to the ones Nicola uses:\nlibrary(ggplot2)\nlibrary(ggforce)\nlibrary(sf)\nlibrary(png)\nlibrary(patchwork) \nThen we add the sky, now recoloured in RWDS purple using fill and color:\ns1 &lt;- ggplot() +\n  theme_void() +\n  theme(\n    plot.background = element_rect(fill = \"#939bc9\", color = \"#939bc9\")\n  )\ns1\nWe use the same code as Nicola to create the snowflakes, but we do this step first before adding snow on the ground, as we’re using the RWDS site background colour, hex code #f0eeb, to represent our settled snow:\n# add snowflakes\nset.seed(20231225)\nn &lt;- 100\nsnowflakes &lt;- data.frame(\n  x = runif(n),\n  y = runif(n)\n)\ns2 &lt;- s1 +\n  geom_point(\n    data = snowflakes,\n    mapping = aes(\n      x = x,\n      y = y\n    ),\n    colour = \"white\",\n    pch = 8\n  )\ns2\n\n# snow on ground\ns3 &lt;- s2 +\n  annotate(\n    geom = \"rect\",\n    xmin = 0, xmax = 1,\n    ymin = 0, ymax = 0.2,\n    fill = \"#f0eeeb\", colour = \"#f0eeeb\"\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  coord_fixed(expand = FALSE)\ns3"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/12/12/rwds-xmas-card.html#oh-christmas-tree",
    "href": "the-pulse/editors-blog/posts/2023/12/12/rwds-xmas-card.html#oh-christmas-tree",
    "title": "A Christmas card in R for the Real World Data Science community",
    "section": "Oh, Christmas tree",
    "text": "Oh, Christmas tree\nTo build her snowman, Nicola created a series of circles that were stacked and overlaid. A simple Christmas tree, though, requires a series of triangles. So, taking Nicola’s snowman’s nose (also a triangle) as our starting point, we coded three sets of coordinates – tree_pts1, tree_pts2, and tree_pts3 – for three triangles of decreasing size that would sit on top of one another.\n# coordinates for tree base\ntree_pts1 &lt;- matrix(\n  c(\n    0.2, 0.3,\n    0.5, 0.6,\n    0.8, 0.3,\n    0.2, 0.3\n  ),\n  ncol = 2,\n  byrow = TRUE\n)\n\n# coordinates for tree middle\ntree_pts2 &lt;- matrix(\n  c(\n    0.3, 0.5,\n    0.5, 0.7,\n    0.7, 0.5,\n    0.3, 0.5\n  ),\n  ncol = 2,\n  byrow = TRUE\n)\n\n# coordinates for tree top\ntree_pts3 &lt;- matrix(\n  c(\n    0.4, 0.65,\n    0.5, 0.75,\n    0.6, 0.65,\n    0.4, 0.65\n  ),\n  ncol = 2,\n  byrow = TRUE\n)\n\n# put tree together\ntree &lt;- st_multipolygon(list(list(tree_pts1),\n                             list(tree_pts2),\n                             list(tree_pts3)))\ns4 &lt;- s3 +\n  geom_sf(\n    data = tree,\n    fill = \"chartreuse4\",\n    colour = \"chartreuse4\"\n  ) +\n  coord_sf(expand = FALSE)\ns4\nA tree also requires a trunk, so we borrowed one of the rectangles from Nicola’s snowman’s hat for this purpose:\ns5 &lt;- s4+\n  annotate(\n    geom = \"rect\",\n    xmin = 0.45,\n    xmax = 0.55,\n    ymin = 0.2,\n    ymax = 0.3,\n    fill = \"brown\"\n  )\ns5\nAnd, of course, no Christmas tree is complete without decorations. The “rocks” that formed the buttons and eyes on Nicola’s snowman were updated to become gold and red baubles for our tree:\n# add gold baubles\ns6 &lt;- s5 +\n  geom_point(colour = \"gold\",\n             data = data.frame(\n               x = c(0.3, 0.4, 0.5, 0.6, 0.57, 0.62, 0.45, 0.5),\n               y = c(0.325, 0.4, 0.45, 0.35, 0.57, 0.52, 0.6, 0.7),\n               size = runif(8, 2, 4.5)\n             ),\n             mapping = aes(x = x, y = y, size = size)\n  ) +\n  scale_size_identity()\ns6\n\n# add red baubles\ns7 &lt;- s6 +\n  geom_point(colour = \"red3\",\n             data = data.frame(\n               x = c(0.7, 0.6, 0.5, 0.525, 0.43, 0.38, 0.55, 0.5),\n               y = c(0.375, 0.4, 0.55, 0.65, 0.43, 0.48, 0.5, 0.375),\n               size = runif(8, 2, 4.5)\n             ),\n             mapping = aes(x = x, y = y, size = size)\n  ) +\n  scale_size_identity()\ns7"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/12/12/rwds-xmas-card.html#seasons-greetings",
    "href": "the-pulse/editors-blog/posts/2023/12/12/rwds-xmas-card.html#seasons-greetings",
    "title": "A Christmas card in R for the Real World Data Science community",
    "section": "Season’s greetings",
    "text": "Season’s greetings\nThe final step was to add text to the top of the image, wishing you all a Merry Christmas, and our logo to the bottom, so you know who the card is from:\n# add text\ns8 &lt;- s7 +\n  annotate(\n    geom = \"text\",\n    x = 0.5,\n    y = 0.875,\n    label = \"Merry Christmas\",\n    colour = \"red3\",\n    fontface = \"bold\",\n    size = 18\n  )\ns8\n\n# add logo \npath &lt;- \"images/rwds-logo-150px.png\"\nimg &lt;- readPNG(path, native = TRUE) \ns9 &lt;- s8 +                   \n  inset_element(p = img, \n                left = 0.3265, \n                bottom = 0.0, \n                right = 0.6735, \n                top = 0.2\n  ) \ns9\n\n\n\n\n\nI hope you like the Christmas card! From all of us at Real World Data Science, thank you for your support throughout 2023. Merry Christmas, happy holidays, and best wishes for 2024!\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “A Christmas card in R for the Real World Data Science community.” Real World Data Science, December 12, 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html",
    "href": "the-pulse/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html",
    "title": "How do people feel about AI? Well, it’s complicated",
    "section": "",
    "text": "How do people feel about AI? That was a question recently explored in a survey of 4,000 British residents. The answer is that, well, it depends.\nResearchers at the Ada Lovelace Institute and the Alan Turing Institute designed the survey to ask about specific AI use cases, rather than the concept of AI more broadly. Use cases included face recognition for policing, border control and security, targeted advertising for political campaigns and consumer products, virtual assistants, driverless cars, and so on.\nRoshni Modhvadia, a researcher at the Ada Lovelace Institute and member of the survey team, reported that respondents overall were broadly positive towards most of the use cases they were asked about. Healthcare applications (using AI to assess the risk of cancer, for example) or face recognition for border security were seen as very or somewhat beneficial by more than 80% of those surveyed. More than half of respondents thought that other applications, such as virtual reality in education, climate research simulations, and robotic care assistants were very or somewhat beneficial.\nViews were less positive towards applications including driverless cars, autonomous weapons and targeted advertising. These were the applications that respondents expressed most concern about, and for each of these use cases perceived risks were felt to outweigh perceived benefits.\nAnd yet, even for applications that were seen as being overwhelmingly beneficial – assessing cancer risk and face recognition for border control – respondents still expressed concern about the potential for overreliance on the technologies, the issue of who is accountable for mistakes, and the impact the technologies might have on jobs and employment opportunities.\nThree-fifths (62%) of respondents said laws and regulations would make them more comfortable with AI technologies being used. This is an important finding given where the national AI conversation is at the moment, said Professor Helen Margetts, director of the public policy programme at The Alan Turing Institute.\nThe current national conversation has been fuelled by the success of ChatGPT and the growing adoption of generative AI tools. The Lovelace/Turing survey, fielded in November 2022, did not ask about ChatGPT et al., but the results do at least provide a baseline against which to measure any shifts in attitudes brought on by what Professor Shannon Vallor, Baillie Gifford Chair in the Ethics of Data and Artificial Intelligence at the Edinburgh Futures Institute at the University of Edinburgh, described as “this latest round of AI hype and confusion”.\nModhvadia, Margetts and Vallor were speaking at an online event last week to mark the launch of the survey report. Video of the event is below. The full report is available from the Ada Lovelace Institute website."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html#how-do-people-feel-about-ai-in-statistics-and-data-science-education",
    "href": "the-pulse/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html#how-do-people-feel-about-ai-in-statistics-and-data-science-education",
    "title": "How do people feel about AI? Well, it’s complicated",
    "section": "How do people feel about AI in statistics and data science education?",
    "text": "How do people feel about AI in statistics and data science education?\nA new paper in the Journal of Statistics and Data Science Education considers the potential for using ChatGPT in statistics and data science classrooms. Authors Amanda R. Ellis and Emily Slade of the University of Kentucky give suggestions for using ChatGPT to generate course content: lecture notes and new material such as practice quizzes or exam questions, or pseudocode for introducing students to statistical programming. It could also be used as a code debugging tool and integrated into set tasks – e.g., have students prompt ChatGPT to write code, then run the code themselves and assess whether the code works as intended.\n“We recognize that educators have valid concerns regarding the implementation and integration of AI tools in the classroom,” write the authors, later adding that: “We encourage readers to consider other technologies, such as the calculator, WolframAlpha, and Wikipedia, all of which were met with initial wariness but are now commonly used as learning tools. As statistics and data science educators, we can actively shape and guide the incorporation of AI tools within our classrooms.”\nRead the paper: A new era of learning: Considerations for ChatGPT as a tool to enhance statistics and data science education"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html#ok-but-how-do-people-feel-about-ai-generated-music",
    "href": "the-pulse/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html#ok-but-how-do-people-feel-about-ai-generated-music",
    "title": "How do people feel about AI? Well, it’s complicated",
    "section": "OK, but how do people feel about AI-generated music?",
    "text": "OK, but how do people feel about AI-generated music?\nA new demo on Hugging Face allows users to generate short samples of music based on text descriptions. Users can also “condition on a melody” by uploading audio files. The results are… interesting, as I discovered while playing around with the demo yesterday.\n\n\n\nText-to-music-generation is now a thing (via @huggingface: https://t.co/fpBDLuB4yh) so I thought I'd try creating some new genre mashups pic.twitter.com/y93w7x9pNW\n\n— Brian Tarran (@brtarran) June 12, 2023\n\n\n\nRead the paper: Simple and controllable music generation\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail image by Andy Kelly on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “How do people feel about AI? Well, it’s complicated.” Real World Data Science, June 13, 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/07/19/code-interpreter.html",
    "href": "the-pulse/editors-blog/posts/2023/07/19/code-interpreter.html",
    "title": "Testing out ChatGPT’s new Code Interpreter",
    "section": "",
    "text": "On July 6, 2023, OpenAI began rolling out the Code Interpreter plugin to users of its ChatGPT Plus service. But what exactly is this, and what functionality does it offer?\nCode Interpreter runs code and allows for uploading data so you can use ChatGPT for data cleaning, preprocessing, analysis, visualisation and predictive modelling tasks, among other things. This tool holds great promise for programmers and analysts alike, with the potential to streamline coding workflows as well as having an automated data analyst at your fingertips.\nTo use Code Interpreter, you need to enable it in the ChatGPT settings (at time of writing this only works with a paid ChatGPT Plus subscription).\nNow, let’s take it for a bit of a spin by uploading the stroke prediction dataset from Kaggle."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/07/19/code-interpreter.html#the-stroke-prediction-dataset",
    "href": "the-pulse/editors-blog/posts/2023/07/19/code-interpreter.html#the-stroke-prediction-dataset",
    "title": "Testing out ChatGPT’s new Code Interpreter",
    "section": "The stroke prediction dataset",
    "text": "The stroke prediction dataset\nThe World Health Organization (WHO) identifies stroke as the second leading cause of death worldwide, accounting for roughly 11% of all fatalities.\nKaggle’s stroke prediction dataset is used to forecast the likelihood of a patient suffering a stroke, taking into account various input parameters such as age, gender, presence of certain diseases, and smoking habits. Each row in the dataset offers pertinent information about an individual patient.\nLoading this dataset into ChatGPT Code Interpreter, one is treated with:\n\n\n\n\n\nThe user is asked: “Please let me know what analysis or operations you’d like to perform on this dataset. For instance, we can perform exploratory data analysis, data cleaning, data visualization, or predictive modelling.”\nIt seems quite a bold claim. So, I asked it to do all of the above.\n\n\n\n\n\n\nExploratory Data Analysis\n\n\n\n\n\nThis is a good, useful summary. The missing values in bmi are set to the median, which the user can later decide to change for themselves as the code is available to do so.\n\n\n\n\n\n\n\nData visualisation\nNext, the visualisations of the variables are shown along with a correlation heatmap. Users can toggle between the visualisations and the code. The outputs are pretty useful, except for one mistake: id shouldn’t be included as part of the heatmap.\n\n\n\n\n\n\n\n\n\n\n\nHistograms and bar plots created by ChatGPT Code Interpreter for variables in the Kaggle stroke prediction dataset.\n\n\n\n\n\n\n\nCorrelation heatmap for variables in the Kaggle stroke prediction dataset.\n\nThings start to go seriously awry when Code Interpreter tries to create a predictive model.\n\n\nThe predictive model is garbage\nFrom the screenshot below, you can see that lumping all the data into a predictive model creates some highly spurious results. Age is a factor, as it should be, as is hypertension – indeed, those with hypertension in this dataset are around three times more likely to have a stroke than those without. In reality, there are also significant effects from glucose level and smoking, and also a slight BMI effect in this small, unbalanced dataset. However, work_type_children having a large positive effect is alarming and plainly wrong.\n\n\n\n\n\nIt is very evident from the table below that the positive coefficient on children is spurious.\n\n\n\n\n\nSo, where does this leave our thinking about Code Interpreter?"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/07/19/code-interpreter.html#discussion",
    "href": "the-pulse/editors-blog/posts/2023/07/19/code-interpreter.html#discussion",
    "title": "Testing out ChatGPT’s new Code Interpreter",
    "section": "Discussion",
    "text": "Discussion\nMy test case is possibly an unfair one. The sort of study presented to Code Interpreter is one that requires careful analysis, and it uses a relatively small, tricky dataset whose difficulties are compounded by missing data. It’s therefore not surprising that, in this context, an automated analysis fails to shine in all respects.\nTo be fair, OpenAI themselves describe the plugin as an “eager junior programmer”. And as would be the case with a real junior programmer or junior data scientist, you’d expect a more experienced hand to be guiding an analysis like the one I asked for – someone who can sense-check results, point out errors, and offer suggestions for fixes and improvements.\nDespite some stumbles in this demo, OpenAI’s “junior programmer” presents a real step forward in the ChatGPT offering, and it is particularly impressive that one can toggle between code and charts without having to worry about coding at all.\nAt this stage, I would argue that Code Interpreter may be useful for quick summaries, visualisations and a little basic data cleaning and some preliminary investigations. However, based on what I’ve seen so far, it is clear to me that highly trained statisticians won’t be replaced anytime soon.\n\nBack to Editors’ blog\n\n\n\n\n\nAbout the author\n\nLee Clewley is a member of the editorial board of Real World Data Science and head of applied AI in GSK’s AI and Machine Learning Group, R&D.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Lee Clewley\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail image by charlesdeluvio on Unsplash.\n\n\n\nHow to cite\n\nClewley, Lee. 2023. “Testing out ChatGPT’s new Code Interpreter.” Real World Data Science, July 19, 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/07/27/schools-outreach.html",
    "href": "the-pulse/editors-blog/posts/2023/07/27/schools-outreach.html",
    "title": "‘Go out and talk about data science, particularly to schoolchildren’",
    "section": "",
    "text": "Rachel Hilliam, statistics professor at The Open University, used her inaugural lecture this month to make “a real plea” to the data science community “for outreach into schools”, to help build excitement and awareness of the promise and potential for careers in data science.\n“We have a plethora of jobs that we cannot fill in data science at the moment,” said Hilliam. “We’ve had all sorts of initiatives in terms of trying to retrain people, and that’s great, and those are gaps that we need to plug. But unless we get that pipeline coming through, we’re always going to have this problem at the top.”\nHilliam, who is chair of the Alliance for Data Science Professionals, wants schoolchildren and teachers to be made more aware of the benefits of, and opportunities for, data science careers. “Let me tell you,” she said, “if you go out into a school and say, ‘Do your kids want to be a data scientist?’, the teachers will look at you and go, ‘A what?’. They have no idea, generally, that data science actually exists, which is a shame.”\nBut there are plentiful opportunities to introduce data science to children, Hilliam suggests. She began her talk by saying that: “Data is everywhere – in every single thing that we do, in all of our walks of life.” And she concluded by saying: “Whatever it is that these kids are interested in, […] there is lots of data out there, so there is absolutely no reason why we can’t excite children in a career in data science. So, that’s where I’d like to finish. Go out and talk about data science, particularly to schoolchildren!”\nWatch the lecture in full below or on YouTube. Skip to 17:04 for the start of the talk.\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Kenny Eliason on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘Go out and talk about data science, particularly to schoolchildren.’” Real World Data Science, July 27, 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/08/24/rss-conference.html",
    "href": "the-pulse/editors-blog/posts/2023/08/24/rss-conference.html",
    "title": "RSS Conference preview: Evaluating AI, machine learning, and data visualisation",
    "section": "",
    "text": "The Royal Statistical Society International Conference takes place in Harrogate, England, this September (Monday 4 to Thursday 7). Real World Data Science will be in attendance, and we’ve helped organise a couple of sessions we’d like to tell you about.\n\nEvaluating AI: How data science and statistics can shape the UK’s AI strategy\nDate: 6 September Time: 9:00 am - 10:20 am Room: Auditorium (moved from Queens Suite 8)\nThe launch of ChatGPT less than a year ago is a milestone moment in the story of artificial intelligence. Overnight, large language models were transformed from research projects into consumer products, now used by millions each month. The capabilities are impressive, the productivity gains undeniable. But, what of the downsides? These are issues societies, governments, and individuals are now starting to reckon with.\nIn March 2023, the UK government published a white paper promising a “pro-innovation approach” to AI regulation, while also acknowledging the risks AI poses to “people’s privacy, their human rights or their safety” and “concerns about the fairness of using AI tools to make decisions which impact people’s lives”. The Royal Statistical Society, in response, has called for investment in a centre for AI evaluation methodology, arguing that users of AI systems should be able to judge the trustworthiness of claims made by AI companies as well as the outputs of their systems.\nWhat should AI evaluation look like? How will it work in practice? What metrics are most important, and – crucially – who gets to decide this? Join us for a special panel debate at the RSS International Conference, where these questions, and more, will be discussed.\n\n\nBest Practices for Data Visualisation: How to make data outputs more readable, accessible, and impactful\nDate: 5 September Time: 11:40 am - 1:00 pm Room: Auditorium\nThe Royal Statistical Society (RSS) has published a new guide, “Best Practices for Data Visualisation”, containing insights, advice, and examples (with code) to make data outputs more readable, accessible, and impactful. The guide is written primarily for contributors to Royal Statistical Society publications – including Significance magazine, the Journal of the Royal Statistical Society Series A, and Real World Data Science – but the information and advice within is also of broad relevance and use for any data visualisation task.\nIn the first half of this conference session, authors Andreas Krause, Nicola Rennie, and Brian Tarran will introduce the guide and its key recommendations, and there will be a short demo of how to use the new {RSSthemes} R package. For the second half of the session, attendees will be invited to share feedback with the authors, propose ideas, and start developing new and expanded sections of the guide. Attendees will be shown how to work with the guide’s source files and collaborate via GitHub, so feel free to bring along a laptop and become a contributor!\nFor more information, see rss.org.uk/datavisguide and the RSS Conference website.\n\n\nDiscussion Meeting: Probabilistic and statistical aspects of machine learning\nDate: 6 September Time: 5:00pm - 7:00 pm Room: Auditorium\nWe haven’t helped organise this session, but we are interested to see it. Two papers will be presented for discussion and debate. Paper 1 is “Automatic Change-Point Detection in Time Series via Deep Learning” by Jie Li, Paul Fearnhead, Piotr Fryzlewicz, and Tengyao Wang, while Paper 2 is “From Denoising Diffusions to Denoising Markov Models” by Joe Benton, Yuyang Shi, Valentin De Bortoli, George Deligiannidis, and Arnaud Doucet. Preprints of both papers are available now via the RSS Discussion Meetings webpage, and you can also hear more about the session in this interview with Adam Sykulski, RSS Discussion Papers editor.\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail image by jcw1967, licenced under a Creative Commons Attribution 2.0 Generic (CC BY 2.0) licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “RSS Conference preview: Evaluating AI, machine learning, and data visualisation.” Real World Data Science, August 24, 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/01/18/rwds-at-rss-conference.html",
    "href": "the-pulse/editors-blog/posts/2023/01/18/rwds-at-rss-conference.html",
    "title": "We’re taking Real World Data Science on the road",
    "section": "",
    "text": "Real World Data Science has booked its first conference appearance! This September, we’ll be part of the data science stream of the RSS International Conference.\nOur session, “Real World Data Science Live”, will feature talks and discussions based on content published on this site. In particular, we’re looking to share compelling examples of how data science is being used to solve real-world problems.\nIf you’re thinking about contributing to Real World Data Science, or have already made a submission, do let us know whether you’d be interested in taking part in this in-person event. There are only a handful of speaker slots available, so please get in touch ASAP!\nThe conference takes place 4-7 September 2023, in Harrogate, Yorkshire. Keynote speakers include Anuj Srivastava, a Florida State University professor with research interests in statistical computer vision, functional data analysis, and shape analysis, and other invited topic sessions in the data science stream are:\n\nGitHub: Version control for research, teaching and industry\nSurrogate-assisted uncertainty quantification of complex computer models\nGetting your work to work\nBest practices for the analysis and visualisation of Google Trends data\n\nSee the RSS International Conference 2023 website for more details.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “We’re taking Real World Data Science on the road.” Real World Data Science, January, 18 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/01/27/talking-chatgpt.html",
    "href": "the-pulse/editors-blog/posts/2023/01/27/talking-chatgpt.html",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "",
    "text": "ChatGPT is, right now, the world’s most popular - and controversial - chatbot. Users have been both wowed by its capabilities1 and concerned by the confident-sounding nonsense it can produce.\nBut perhaps what impresses most is the way it is able to sustain a conversation. When I interviewed our editorial board member Detlef Nauck about large language models (LLMs), back in November, he said:\nFast-forward a couple of months and, as discussed in our follow-up interview below, OpenAI, the makers of ChatGPT, have succeeded in building a question answering system that can sustain a dialogue. As Nauck says: “I have not yet seen an example where [ChatGPT] lost track of the conversation… It seems to have quite a long memory, and doing quite well in this.”\nThere are still major challenges to overcome, says Nauck - not least the fact that ChatGPT has no way to verify the accuracy or correctness of its outputs. But, if it can be linked to original sources, new types of search engines could follow.\nCheck out the full conversation below or on YouTube.\nDetlef Nauck is a member of the Real World Data Science editorial board and head of AI and data science research for BT’s Applied Research Division."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/01/27/talking-chatgpt.html#timestamps",
    "href": "the-pulse/editors-blog/posts/2023/01/27/talking-chatgpt.html#timestamps",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Timestamps",
    "text": "Timestamps\n\nHow ChatGPT was built and trained (0:41)\nChatGPT’s major advance (3:05)\nThe big problems with large language models (4:36)\nSearch engines and chatbots (9:35)\nQuestions for OpenAI and other model builders (11:29)"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/01/27/talking-chatgpt.html#quotes",
    "href": "the-pulse/editors-blog/posts/2023/01/27/talking-chatgpt.html#quotes",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Quotes",
    "text": "Quotes\n“[OpenAI] have achieved quite remarkable capabilities in terms of sustaining conversations, and producing very realistic sounding responses… But sometimes [ChatGPT] makes silly mistakes. Sometimes the mistakes are not that obvious. It can hallucinate content… And it still doesn’t know what it’s talking about. It has no knowledge representation, doesn’t have a word model. And it’s just a statistical language model.” (2:04)\n“These models, they produce an answer, which is based on the kind of texts that they have been trained on. And that can be quite effective. But it cannot yet link back to an original source. So what’s still missing is the step where it says, ‘Okay, this my answer to your question, and here’s some evidence.’ As soon as they have done this, then these kinds of systems will probably replace the search engines that we’re used to.” (4:07)\n“[These large language models are] still too big and too expensive to run… For [use in a] contact centre or similar, what you need is a much smaller model that is restricted in terms of what it can say. It should have knowledge representation, so it gives correct answers. And it doesn’t need to speak 48 languages and be able to produce programming code. It only needs to be able to talk about a singular domain, where the information, the knowledge about the domain, has been carefully curated and prepared. And that’s what we’re not seeing yet. Can we build something like this, much smaller, much more restricted, and provably correct, so we can actually use the output?” (7:49)\n“We are seeing communities who don’t necessarily have the technical background to judge the capabilities of these models, but see the opportunities for their own domain and might be acting too fast in adopting them. So the producer of these models has a certain responsibility to make sure that this doesn’t happen.” (12:26)"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/01/27/talking-chatgpt.html#further-reading",
    "href": "the-pulse/editors-blog/posts/2023/01/27/talking-chatgpt.html#further-reading",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Further reading",
    "text": "Further reading\n\nChatGPT: The Robot, the Myth, the Legend - Philadelphia Physicist blog, January 13, 2023\nCost to run ChatGPT - tweet by OpenAI CEO Sam Altman, December 5, 2022\nGoogle execs warn company’s reputation could suffer if it moves too fast on AI-chat technology - CNBC, December 13, 2022\nMicrosoft reportedly to add ChatGPT to Bing search engine - The Guardian, January 5, 2023\nGetty Images is suing the creators of AI art tool Stable Diffusion for scraping its content - The Verge, January 17, 2023"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/01/27/talking-chatgpt.html#transcript",
    "href": "the-pulse/editors-blog/posts/2023/01/27/talking-chatgpt.html#transcript",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove repetitions.\n\n\n\nBrian Tarran\nWe’re following up today Detlef on the, I guess, one of the biggest stories in artificial intelligence and data science at the moment, ChatGPT, the chat bot that’s driven by a large language model and is proving endless amounts of– providing endless amounts of either entertainment or concern, depending on what you ask it, and what outputs you get. So, but you’ve been looking at it in some detail, right, ChatGPT. And that’s why I thought we would follow up and have a conversation to see, get your view on it, get your take on it. What’s going on?\nDetlef Nauck\nYeah. So, what they have done is, OpenAI have used their large language model GPT-3 and they have trained an instance to basically answer questions and have conversations, where the model remembers what has been said in the conversation. And they have done this by using curated data of question and answers, where they basically have posed a question and said, This is what the answer should be. They trained the system on doing this, then, in the next step, they began use questions, potentially different ones, the system came up with a variety of answers, and then again, human curators would mark which is the best answer. And they would use this data to train what’s called a reward model - so, a separate deep network that learns what kind of answer for a particular question is a good one - and then they would use this reward model to do additional reinforcement learning on the ChatGPT that they had built so far, basically using dialogues and the reward model would then either reward or penalise the response that comes out of the system. And by doing that they have achieved quite remarkable capabilities in terms of sustaining conversations, and producing kind of very realistic sounding kind of responses. Sounds all very convincing. The model presents its responses quite confidently. But sometimes it makes silly mistakes. Sometimes the mistakes are not that obvious. It can hallucinate content. So let’s say you ask it to write you scientific text about whatever topic and put some references in and these references are typically completely fabricated and not real. And it still doesn’t know what it’s talking about. It has no knowledge representation, doesn’t have a word model. And it’s just a statistical language model. So it’s what we would call a sequence to sequence model. It uses an input sequence, which are words, and then guesses what’s the next most likely word in the sequence. And then it continues building these sequences.\nBrian Tarran\nYeah. But, do you think the big advance as you see it is the way it’s able to remember or store some knowledge, if you like, of the conversation, because that was something that came out of our first conversation that we had, where you were saying that, you know, if you’re looking at these as a potential chatbots for customer service lines, or whatever it might be, actually, the trees, the conversation trees break down after a while, and they don’t, you know, these models get lost, but actually, they’re able to maintain it a little longer, are they, or– ?\nDetlef Nauck\nYeah, I have not yet seen an example where they lost track of the conversation they seem to have, it seems to have quite a long memory, and doing quite well in this. So the main capability here is they have built a question answering system. And that’s kind of the ultimate goal for search engines. So if you put something into Google, essentially, you have a question, show me something that answered this, answers this particular question. Of course, what you want this kind of an original source. And these models, they produce an answer, which is based on the kind of texts that they have been trained on. And that can be quite effective. But it cannot yet link back to an original source. So what’s still missing is the step where it says, Okay, this my answer to your question, and here’s some evidence. Then if, as soon as they have done this, then these kinds of systems will probably replace the search engines that we’re used to.\nBrian Tarran\nYeah. The other thing that struck me with them was that the, if you’re asking somebody a question - a human, you know, for instance - you expect a response that and you would hope you will be able to trust that response, especially if it’s someone in an expert position or someone you’re calling, you know, on behalf of a company or something. The fact that - and I asked this question of ChatGPT itself - and the response was, again, you should consult external sources to verify the information that’s been provided by the chatbot. So it’s like, I guess that leaves a question as to what the utility of it is, if you if you’re always having to go elsewhere to verify that information.\nDetlef Nauck\nYeah, I mean, that’s the main problem with these models, because they don’t have a knowledge representation. They don’t have a word model, they can’t fall back on facts that are represented as being true and present those. They come up with an answer. But I mean, there has been a lot of kind of pre-prompting going in to ChatGPT. So when you start writing something, the session has already been prompted with a lot of text, telling the model how to behave, what not to say, to avoid certain topics. There are additional moderation APIs running that make sure that you can’t create certain type of responses, which are based on classical text filtering, and topic filtering. So they try to kind of restrict what the model can do to make sure it’s not offensive or inappropriate. But that is limited. So through crafting your requests, intelligently, you can convince it to ignore all of these things and go past it in some instances. So the, it’s not yet perfect, and certainly it’s not authoritative. So you can’t trust the information if you’re not an expert yourself. So at the moment, I’d say these kind of models are really useful for experts who can judge the correctness of the answer. And then what you get this kind of maybe a helpful kind of text representation of something that you would have to write yourself otherwise.\nBrian Tarran\nYeah, and certainly conversations I’ve had with people, those who kind of work, maybe in creative industries, are finding them quite intriguing, in terms of things like, you know, maybe trying to come up with some clever tweets or something for a particular purpose, or something I want to try out is getting ChatGPT to write headlines for me, because it’s always my least favourite part of the editing job. So that sort of works. But you know, for you, in your position in the industry, has ChatGPT changed your mind at all about, you know, the way you’re perceiving these models and how they might be used? Or is it is it just kind of a next step along in the process of what you’d expect to see before these can become tools that we use?\nDetlef Nauck\nYeah, it’s the next step in the evolution of these models. They’re still too big and too expensive to run, right. So now, it is not quite clear how much it costs OpenAI to run the service that they’re currently running. So you see estimates around millions of dollars per day that they have to spend on running the compute infrastructure to serve all of these questions. And this is not quite clear, the only official piece of information that I’ve seen is in a tweet, where the CEO said, a single question costs in the order of single digit cents, but we have no idea how many questions they serve per day, and therefore how much money they are spending. If you want to run a contact centre, or something like this, it all depends on how much compute need to stand up to be able to respond to hundreds or thousands of questions in parallel. And then obviously, if you can’t trust that the answer is correct, it is of no use. So for making use in the service industry for contact centre or similar, what you need is a much smaller model that is restricted in terms of what it can say, it should have knowledge representation, so it gives correct answers. And it doesn’t need to speak 48 languages and be able to produce programming code, it only needs to be able to talk about a singular domain, where it kind of the information, the knowledge about the domain has been carefully curated and prepared. And that’s what we’re not seeing yet. Can we build something like this, much smaller, much more restricted, and kind of provably correct, so we can actually use the output?\nBrian Tarran\nYeah. Can we go back just to the point you mentioned earlier about, you know, the, the potential of like linking these sorts of chatbots up with search engines, you know, like Google? There’s been some conversations and reporting around, you know, what breakthroughs or not Google might have made in this regard. I mean, have you got any perspective on that area of work and how far along that is maybe and what the challenges are to get to that point?\nDetlef Nauck\nWell, Google has its own large language model, LaMDA. And we have seen an announcement that Microsoft wants to integrate ChatGPT into Bing, their search engine. And, but as I said before, what’s missing is the link to original sources. So you, coming up with a response is nice. But you need to be able to back it up, you need to say, Okay, this is my response, and I’m confident that this is correct, because here are some references. If I compare my response to these references, then they essentially mean the same thing. This is kind of what you need to be able to do. And we haven’t seen this step yet. But I’m certain that the search engine providers are hard at work at doing this because that’s essentially what they want. If you do a search in Google, in some instances, you’ll see a side panel where you get detailed information. Let’s say you ask about what’s the capital of Canada, you get a response, you get the information in more detail, you get links to Wikipedia, where they retrieve content from and present this as the response. And this is done through knowledge graphs. And so if these kinds of knowledge graphs grow together with these kind of large language models, then we will see new types of search engines.\nBrian Tarran\nOkay. I guess final, my final question for you, Detlef, and there might be other angles that you want to explore. But it’s like, are there questions that, you know, if you if you could sit down with OpenAI to talk about ChatGPT and what they’ve done, and what they plan to do next with it, what are the kinds of things that are bubbling away at the top of your mind?\nDetlef Nauck\nWell, one thing is controlling the use of these models, right? If you let them loose on the public, with an open API that anybody can use, you will see a proliferation of applications on top of it. If you go on YouTube, and you Google ChatGPT and health, you’ll already find discussions where GPs discuss, Oh, that is the next step of automated doctors that we can use. So they believe that the responses from these systems can be used for genuine medical advice. And that’s clearly a step too far. So we are seeing communities who don’t necessarily have the technical background to judge the capabilities of these models, but see the opportunities for their own domain and might be acting too fast in adopting them. So the producer of these models has a certain responsibility to make sure that this doesn’t happen. And I don’t know how they want to control this. And, so my question at the developers of these models would be how do you handle sustainability, because the trend goes to ever bigger models. So there’s, in some parts of the industry, there’s the belief, if you make them big enough you get artificial general intelligence, which I don’t believe is possible with these models. But this is definitely a trend that pushes the size of the models. The kind of, the idea of having just one model that can speak all the languages, can produce questions, answers, programming code, is obviously appealing. So you don’t want to build many models. Ideally, you have only one. But how is that supposed to work? And how do you embed actual word knowledge and word models into these systems so that you can verify what comes out?\nBrian Tarran\nYeah. I mean, the ethical dimension that you mentioned in the first part of your response is an important one, I think, in the sense that– but I guess maybe almost redundant in the sense that it’s already out there; you can’t put ChatGPT back in the box, can we, essentially?\nDetlef Nauck\nWell, it’s expensive to run so charging enough for access will put a lid on some frivolous use cases, but still, it needs to be controlled better. And you can make a jump to an AI regulation. So far, we only thought about regulating automated decision making, or automated classification. We also have to think about the automatic creation of digital content or automatic creation of software, which is possible through these models or the other generative AI models like diffusers. So how do we handle the creation of artificial content that looks like real content?\nBrian Tarran\nYeah. And there’s also I think, something I picked up yesterday, there was reports of a case being filed by, I think, Getty Images against the creators of one of these generative art models because they’re saying, you know, that you’ve used our data or you’ve used our image repositories essentially to train this model and it is now producing, you know, it’s producing its own outputs that’s based on this, and I guess there’s an argument of it being a copyright infringement case. And I think that’ll be quite interesting to watch to see how that does change the conversation around - yeah - fair use of that data that is available. You can find these images publicly, but you have to pay to use them for purposes other than just browsing, I guess. Yeah, it’ll be interesting to watch.\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification.” Real World Data Science, January, 27 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/01/27/talking-chatgpt.html#footnotes",
    "href": "the-pulse/editors-blog/posts/2023/01/27/talking-chatgpt.html#footnotes",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI asked ChatGPT to write this article’s headline, for example. I typed in “Can you write a headline for this text:” and then copy/pasted the interview transcript into the dialogue box. It first came up with, “AI Chatbot ChatGPT Proves Capable in Sustaining Conversations but Lacks Knowledge Representation and Original Sources for Verification”. I then asked it to shorten the headline to 10 words. It followed up with, “ChatGPT: Large Language Model-Driven Chatbot Proves Capable But Limited”.↩︎"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2024/02/08/llms-whitepaper-response.html",
    "href": "the-pulse/editors-blog/posts/2024/02/08/llms-whitepaper-response.html",
    "title": "£10m for UK regulators to ‘jumpstart’ AI capabilities, as government commits to white paper approach",
    "section": "",
    "text": "The UK government this week announced a £10 million investment to “jumpstart regulators’ AI capabilities” as part of its commitment to a “pro-innovation approach to AI regulation.” But will this be sufficient to answer criticisms that it has so far been “too slow” to give regulators the tools they need to police the growing usage of AI?\nIt was March last year when a Department for Science, Innovation and Technology (DSIT) white paper first set out the government’s principles- and context-based approach to regulating artificial intelligence. This proposed to focus regulatory attention on “the context in which AI is deployed” rather than target specific technologies. Under this model, existing regulators, including the Information Commissioner’s Office, Ofcom, and the Competition and Markets Authority, would be responsible for ensuring that technologies deployed within their domains adhered to established rules – e.g., data protection regulation – and a common set of principles:\nThe approach was broadly well received, as was clear from a debate at techUK’s Digital Ethics Summit last December. However, concerns were expressed about whether regulators would be funded sufficiently to meet the expectations set out in the March white paper. Also, the Royal Statistical Society, in its response to the white paper, worried that “splitting responsibilities for regulating the use of AI between existing regulators does not meet the scale of the challenge,” and that “central leadership is required to give a clear, coherent and easily communicable framework that can be applied to all sectors.”\nWhile the DSIT white paper proposed that a range of “central functions” be created to support regulators, evidence presented to a House of Lords inquiry last November suggested that regulators “did not appear to know what was happening” with these mooted teams and were “keen to see progress” on this front.\nIn reporting the outcomes of its inquiry last week, the House of Lords Communications and Digital Committee concluded that government was being “too slow” to give regulators the tools required to meet the objectives set out in the white paper, and that “speedier resourcing of government‑led central support teams is needed.”\n“Relying on existing regulators to ensure good outcomes from AI will only work if they are properly resourced and empowered,” the committee said.\nThe £10 million funding for regulators announced this week is therefore likely to be welcomed. Money is earmarked to “help regulators develop cutting-edge research and practical tools to monitor and address risks and opportunities in their sectors, from telecoms and healthcare to finance and education,” according to a DSIT press release. Speaking on February 6 at a hearing of the Lords Communications and Digital Committee, Michelle Donelan, Secretary of State for Science, Innovation and Technology, said that the government would “stay on top” of what regulators need to be able to fulfil their responsibilities for regulating the use of AI in their sectors."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#consultation-response",
    "href": "the-pulse/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#consultation-response",
    "title": "£10m for UK regulators to ‘jumpstart’ AI capabilities, as government commits to white paper approach",
    "section": "Consultation response",
    "text": "Consultation response\nNews of the funding for regulators came as part of a long-awaited response by the government to the consultation on its AI regulation white paper. The response essentially confirmed that the government was proceeding with its principles- and context-based approach to regulating AI, having received “strong support from stakeholders across society.”\nThis approach is right for today, the government said, “as it allows us to keep pace with rapid and uncertain advances in AI.” However, it acknowledged that “the challenges posed by AI technologies will ultimately require legislative action in every country once understanding of risk has matured.”\n“Highly capable general-purpose AI systems” would, for example, present a particular challenge to the government’s current approach. It explained: “Even though some regulators can enforce existing laws against the developers of the most capable general-purpose systems within their current remits, the wide range of potential uses means that general-purpose systems do not currently fit neatly within the remit of any one regulator, potentially leaving risks without effective mitigations.”\nAs a next step in delivering on the white paper approach, the government is asking key regulators to publish an update on their strategic approach to AI by the end of April. This was welcomed by Royal Statistical Society (RSS) president Andrew Garrett, who said:\n\n“Urgency is certainly warranted, and the directive for key regulators to disclose their approach in the coming months is a positive development. Ensuring consistency and coherence not only among key regulators but also those who follow is crucial.”\n\nGarrett also reiterated the need for government to engage with statisticians and data scientists, particularly through its new AI Safety Institute (AISI). In the white paper consultation response, AISI is billed as being “fundamental to informing the UK’s regulatory framework”: it will “advance the world’s knowledge of AI safety by carefully examining, evaluating, and testing new frontier AI systems” and will also “research new techniques for understanding and mitigating AI risk.” Garrett said:\n\n“As always, fostering diversity of representation within government and regulatory bodies remains paramount; it cannot solely rely on input from major tech companies. It is especially important that the AI Safety Institute engages with a diverse array of voices, including statisticians and data scientists who play a pivotal role in both the development of AI systems and novel evaluation methodologies.”"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#risks-and-opportunities",
    "href": "the-pulse/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#risks-and-opportunities",
    "title": "£10m for UK regulators to ‘jumpstart’ AI capabilities, as government commits to white paper approach",
    "section": "Risks and opportunities",
    "text": "Risks and opportunities\nCalls for a “diversity of representation within government and regulatory bodies” certainly chime with a warning bell sounded by the Lords Communications and Digital Committee last week, in the February 2 release of its inquiry report into large language models and generative AI. “Regulatory capture” by big commercial interests was highlighted as a danger to be avoided, amid concern that “the AI safety debate is being dominated by views narrowly focused on catastrophic risk, often coming from those who developed such models in the first place” and that “this distracts from more immediate issues like copyright infringement, bias and reliability.”1\nThe committee called for enhanced governance and transparency measures in DSIT and AISI to guard against regulatory capture, and for a rebalancing away from a “narrow focus on high-stakes AI safety” toward a “more positive vision for the opportunities [of AI] and a more deliberate focus on near-term risks” including cyber security and disinformation.\nIt also wants to see greater action by the government in support of copyright. “Some tech firms are using copyrighted material without permission, reaping vast financial rewards,” reads the report. “The legalities of this are complex but the principles remain clear. The point of copyright is to reward creators for their efforts, prevent others from using works without permission, and incentivise innovation. The current legal framework is failing to ensure these outcomes occur and the Government has a duty to act. It cannot sit on its hands for the next decade and hope the courts will provide an answer.”\nAgain, here’s RSS president Andrew Garrett’s take on the Lords committee report:\n\n\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Yaopey Yong on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2024. “£10m for UK regulators to ‘jumpstart’ AI capabilities, as government commits to white paper approach.” Real World Data Science, February 8, 2024. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#footnotes",
    "href": "the-pulse/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#footnotes",
    "title": "£10m for UK regulators to ‘jumpstart’ AI capabilities, as government commits to white paper approach",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee, for example, “No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye.”↩︎"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2024/01/03/posit-conf-video.html",
    "href": "the-pulse/editors-blog/posts/2024/01/03/posit-conf-video.html",
    "title": "Creating a web publication with Quarto: the Real World Data Science origin story",
    "section": "",
    "text": "When I attended posit::conf(2023) in Chicago last year, I gave a talk about creating Real World Data Science using Quarto, the open source publishing system developed by Posit. That talk is now online, along with all the other conference talks and keynotes.\nMy talk, “From Journalist to Coder: Creating a Web Publication with Quarto,” is embedded below. You can also find a selection of talks on our posit::conf highlights blog. The full conference playlist is on YouTube.\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2024. “Creating a web publication with Quarto: the Real World Data Science origin story.” Real World Data Science, January 03, 2024. URL"
  },
  {
    "objectID": "the-pulse/posts/2023/09/18/pseudo-data-science.html",
    "href": "the-pulse/posts/2023/09/18/pseudo-data-science.html",
    "title": "‘Pseudo data science’ and other pitfalls: lessons from the UK’s stats regulator on how not to be misleading",
    "section": "",
    "text": "A typical article on data science hails new data sources, new tools, and new visualisations, and thereby supports the case for the value of data science.\nBut this article takes a different angle: it talks about potential pitfalls that can face data scientists. It is based on our work as the Office for Statistics Regulation (OSR), the UK’s regulator for official statistics. We see lots of great work done by statisticians in government. But we also see some of the challenges they face – and data scientists are also likely to encounter the same challenges.\nThe problems arise from the fact that neither statisticians nor data scientists do their work in isolation. The work usually takes places within organisations – businesses, government bodies, think tanks, academic institutions – and as a result, the statisticians and/or data scientists are not the only players who get to influence how data science is presented and used.\nWhat are the pitfalls we see in our work as regulator?"
  },
  {
    "objectID": "the-pulse/posts/2023/09/18/pseudo-data-science.html#pseudo-data-science",
    "href": "the-pulse/posts/2023/09/18/pseudo-data-science.html#pseudo-data-science",
    "title": "‘Pseudo data science’ and other pitfalls: lessons from the UK’s stats regulator on how not to be misleading",
    "section": "Pseudo data science",
    "text": "Pseudo data science\nThe first type of pitfall is pseudo data science.\nPseudo data science is a term we use to describe attempts to pass off crude work as being more data science-y than it really is. That reflects a sense in public life that data science is new, innovative, somehow the Future. In this context, people who are not data scientists can be tempted to dress themselves up in the clothes of data science to enhance their credibility. This dressing up is usually well-intentioned – communications professionals who want to illuminate and explain complex issues in an engaging way.\nThe trouble is, it can sometimes backfire. In our work at OSR, we have over the last year seen several examples where organisations have sought to publish visualisations that look like they are the product of in-depth data analysis – when in fact they have been drawn by communications staff using graphic design packages. Examples include inflation, nurses pay, and comparisons of UK economic performance with other countries. To be fair, whenever we have pointed out issues like this, organisations have responded well, putting in place new procedures to ensure that analysts sign off on this kind of visualisations. Nevertheless, we suspect that the temptations to indulge in pseudo data science will remain strong – and we may need to intervene on similar cases in future."
  },
  {
    "objectID": "the-pulse/posts/2023/09/18/pseudo-data-science.html#unintelligent-transparency",
    "href": "the-pulse/posts/2023/09/18/pseudo-data-science.html#unintelligent-transparency",
    "title": "‘Pseudo data science’ and other pitfalls: lessons from the UK’s stats regulator on how not to be misleading",
    "section": "Unintelligent transparency",
    "text": "Unintelligent transparency\nThe second pitfall is a failure of intelligent transparency.\nThere is a raw form of transparency – quoting a single number (a naked number we call it); or dumping data out into the public domain with no explanation. This is not intelligent transparency. The latter involves being clear where data come from, what their source is, and making underlying data available so that others can understand and verify the statements that are being made. Raw transparency and naked numbers treat an audience with little respect; intelligent transparency helps the audience understand and appreciate what sits behind high level claims.\nData science outputs can sometimes seem to communications teams easy to cherry pick for the most attractive number. Again, like pseudo data science, this reflects largely good intentions – to communicate complex things through ideas. But it becomes easy for a single, unsupported number to be used and reused until it loses most of its meaning. We call this weaponization of data, and it is the antithesis of intelligent transparency. And there is a lot of it about – for example the way in which the former Prime Minister of the UK talked repeatedly about employment; or claims about Scotland’s capacity for renewable energy. These examples indicate the pathology of weaponization that can impact data science outputs. They also act as a reminder that data scientists can counter weaponization of their own outputs by delivering engaging and insightful communication."
  },
  {
    "objectID": "the-pulse/posts/2023/09/18/pseudo-data-science.html#context-collapse",
    "href": "the-pulse/posts/2023/09/18/pseudo-data-science.html#context-collapse",
    "title": "‘Pseudo data science’ and other pitfalls: lessons from the UK’s stats regulator on how not to be misleading",
    "section": "Context collapse",
    "text": "Context collapse\nThe third type of pitfall surrounds context collapse.\nThis idea comes from the work of the philosopher Lucy McDonald (who in turn has built on the ideas of danah boyd). What is context collapse? Imagine a swimming pool – with neat divisions of the pool into different lanes. All is clearly labelled – fast, medium, slow – for lane swimmers, who are in turn separated from the splash area for families and the deep end for divers. Removing the lanes, and thus taking away any signposting, increases the likelihood for things to go wrong. The fast swimmers doing front crawl clash with the slower breaststroke swimmers; both are constantly having to avoid the families with young children; and all need to watch for the periodic big splashes created by the divers. This is the online communication environment, in which formerly private and casual statements can go viral; in which a brief statement in a media environment can be picked up on and circulated many times; and in which some bad actors (the divers) may wish to disrupt deliberately the debate by breaking all the rules.\nHow can this affect data science? It happens when individual bits of data are taken from their context, and used in service of a different, and bigger, argument. A good example is data on Covid vaccinations. Here, UK organisations like the Office for National Statistics and the UK Health Security Agency published comprehensive data in good faith about vaccinations and their impact. Some of the underlying data, however, was taken out of the broader context and used in isolation to support criticisms of vaccines – criticisms that the wider evidence base did not support.\nThe challenge then became how the organisations should respond. At an organisational level, they did not wish to withdraw the data – because that would reduce transparency. Instead they sought to both caveat their data more clearly; and directly rebut the more egregious misuses of the data. In a sense, then, what began as an individual analytical output became part of a broader organisational judgement on positioning in the face of misinformation.\nIt is fair to say that, against this third pitfall, there is not yet a clear consensus on how to address it. Practice is emerging all the time and we at OSR continue to support producers of data as they grapple with it.\nThere are other potential pitfalls to using data science. But what unites these three – pseudo data science; unintelligent transparency; and context collapse – is that they relate to situations where data science rubs up against broader organisational dynamics, around communications, presentation and organisational strategy.\nAnd the meta-message is this: for data scientists to thrive in organisations, they need to be good at more than data science. They need to be skilled at working alongside and influencing colleagues from other functions. Only through this form of data leadership can the pitfalls be dealt with effectively.\n\n\n\n\n\n\nThis article is based on a presentation at the Data Science for Health Equity group in May 2023.\n\n\n\n\nDiscover more The Pulse\n\n\n\n\n\nAbout the author\n\nEd Humpherson is head of the Office for Statistics Regulation, which provides independent regulation of all official statistics in the UK. The aim of OSR is to enhance public confidence in the trustworthiness, quality and value of statistics produced by government.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Ed Humpherson\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nHumpherson, Ed. 2023. “‘Pseudo data science’ and other pitfalls: lessons from the UK’s stats regulator on how not to be misleading.” Real World Data Science, September 18, 2023. URL"
  },
  {
    "objectID": "the-pulse/posts/2023/12/06/ai-fringe.html",
    "href": "the-pulse/posts/2023/12/06/ai-fringe.html",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "",
    "text": "A little over a month ago, governments, technology firms, multilateral organisations, and academic and civil society groups came together at Bletchley Park – home of Britain’s World War II code breakers – to discuss the safety and risks of artificial intelligence.\nOne output from that event was a declaration, signed by countries in attendance, of their resolve to “work together in an inclusive manner to ensure human-centric, trustworthy and responsible AI that is safe, and supports the good of all.”\nWe also heard from UK prime minister Rishi Sunak of plans for an AI Safety Institute, to be based in the UK, which will “carefully test new types of frontier AI before and after they are released to address the potentially harmful capabilities of AI models, including exploring all the risks, from social harms like bias and misinformation, to the most unlikely but extreme risk, such as humanity losing control of AI completely.”\nBut at a panel debate at the Royal Statistical Society (RSS) the day before the Bletchley Park gathering, data scientists, statisticians, and machine learning experts questioned whether such an institute would be sufficient to meet the challenges posed by AI; whether data inputs – compared to AI model outputs – are getting the attention they deserve; and whether the summit was overly focused on AI doomerism and neglecting more immediate risks and harms. There were also calls for AI developers to be more driven to solve real-world problems, rather than just pursuing AI for AI’s sake.\nThe RSS event was chaired by Andrew Garrett, the Society’s president, and formed part of the national AI Fringe programme of activities. The panel featured:\nWhat follows are some edited highlights and key takeaways from the discussion."
  },
  {
    "objectID": "the-pulse/posts/2023/12/06/ai-fringe.html#ai-safety-and-ai-risks",
    "href": "the-pulse/posts/2023/12/06/ai-fringe.html#ai-safety-and-ai-risks",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "AI safety, and AI risks",
    "text": "AI safety, and AI risks\nAndrew Garrett: For those who were listening to the commentary last week, the PM [prime minister] made a very interesting speech. Rishi Sunak announced the creation of the world’s first AI Safety Institute in the UK, to examine, evaluate and test new types of AI. He also stated that he pushed hard to agree the first ever international statement about the risks of AI because, in his view, there wasn’t a shared understanding of the risks that we face. He used the example of the IPCC, the Intergovernmental Panel on Climate Change, to establish a truly global panel to publish a “state of AI science” report. And he also announced an investment in raw computing power, so around a billion pounds in a supercomputer, and £2.5 billion in quantum computers, making them available for researchers and businesses as well as government.\nThe RSS provided two responses this year to prominent [AI policy] reviews. The first was in June on the AI white paper, and the second was on the House of Lords Select Committee inquiry into large language models back in September. How do they relate to what the PM said? There’s some good news here, and maybe not quite so good news.\nFirst, the RSS had requested investments in AI evaluation and a risk-based approach. And you could argue, by stating that there will be a safety institute, that that certainly ticks one of the boxes. We also recommended investment in open source, in computing power, and in data access. In terms of computing power, that was certainly in the [PM’s] speech. We spoke about strengthening leadership, and in particular including practitioners in the [AI safety] debate. A lot of academics and maybe a lot of the big tech companies have been involved in the debate, but we want to get practitioners – those close to the coalface – involved in the debate. I’m not sure we’ve seen too much of that. We recommended that strategic direction was provided, because it’s such a fast-moving area, and the fact that the Bletchley Park Summit is happening tomorrow, I think, is good for that. And we also recommended that data science capability was built amongst the regulators. I don’t think there was any mention of that.\nThat’s the context [for the RSS event today]. What I’m going to do now is ask each of the panellists to give an introductory statement around the AI summit, focusing on the safety aspects. What do they see as the biggest risk? And how would they mitigate or manage this risk?\nDetlef Nauck: I work at BT and run the AI and data science research programme. We’ve been looking at the safety, reliability, and responsibility of AI for quite a number of years already. Five years ago, we put up a responsible AI framework in the company, and this is now very much tied into our data governance and risk management frameworks.\nLooking at the AI summit, they’re focusing on what they call “frontier models,” and they’re missing a trick here because I don’t think we need to worry about all-powerful AI; we need to worry about inadequate AI that is being used in the wrong context. For me, AI is programming with data, and that means I need to know what sort of data has been used to build the model, and I need AI vendors to be upfront about it and to tell me: What is the data that they have used to build it, how have they built it, or if they’ve tested for bias? And there are no protocols around this. So, therefore, I’m very much in favour of AI evaluation. But I don’t want to wait for an institute for AI evaluation. I want the academic research that needs to be done around this, which hasn’t been done. I want everybody who builds AI systems to take this responsibility and document properly what they’re doing.\n\n\n\n\n\n\n\n\n\n\n\nI hear more and more a lot of companies talking about AI general intelligence, and how AI is going to take over the world, and I’m tremendously concerned about this. There is an opportunity to build AI that is human empowering, that keeps us strong, able, capable, intelligent, and can support us in all our human capabilities.\n\n\n\nMihaela van der Schaar: I am an AI researcher building AI and machine learning technology. Before talking about the risks, I also would like to say that I see tremendous potential for good. Many of these machine learning AI models can transform for the better areas that I find extremely important – healthcare and education. That being said, there are substantial risks, and we need to be very careful about that. First, if not designed well, AI can be both unsafe as well as biased, and that could lead to tremendous impact, especially in medicine and education. I completely agree with all the points that the Royal Statistical Society has made not only about open source but also about data access. This AI technology cannot be built unless you have access to high quality data, and what I see a lot happening, especially in industry, is people have data sources that they’ll keep private, build second-rate or third-rate technology on them, and then turn that into commercialised products that are sold to us for a lot of money. If data is made widely available, the best as well as the safest AI can be produced, rather than monopolised.\nAnother area of risk that I’m especially worried about is human marginalisation. I hear more and more a lot of companies talking about AI general intelligence, and how AI is going to take over the world, and I’m tremendously concerned as an AI researcher about this. There is an opportunity to build AI that is human empowering, that keeps us strong, able, capable, intelligent, and can support us in all our human capabilities.\nMartin Goodson: The AI Safety Summit is starting tomorrow. But, unfortunately, I think the government are focusing on the wrong risks. There are lots of risks to do with AI, and if you look at the scoping document for the summit, it says that what they’re interested in is misuse risk and the risk of loss of control. Misuse risk is that bad actors will gain access to information that they shouldn’t have and build chemical weapons and things like that. And the loss of control risk is that we will have this super intelligence which is going to take over and we should see, as is actually mentioned, the risk of the extinction of the human race, which I think is a bit overblown.\nBoth of these risks – the misuse risk and the loss of control risk – are potential risks. But we don’t really know how likely they are. We don’t even know whether they’re possible. But there are lots of risks that we do know are possible, like loss of jobs, and reductions in salary, particularly of white-collar jobs – that seems inevitable. There’s another risk, which is really important, which is the risk of monopolistic control by the small number of very powerful AI companies. These are the risks which are not just likely but are actually happening now – people are losing their jobs right now because of AI – and in terms of monopolistic control, OpenAI is the only company that has anything like a large language model as powerful as GPT-4. Even the mighty Google can’t really compete. This is a huge risk, I think, because we have no control over pricing: they could raise the prices if they wanted to; they could constrain access; they could only give access to certain people that they want to give access to. We don’t have any control over these systems.\nMark Levene: I work in NPL as a principal scientist in the data science department. I’m also emeritus professor in Birkbeck, University of London. I have a long-standing expertise in machine learning and focus in NPL on trustworthy AI and uncertainty quantification. I believe that measurement is a key component in locking-in AI safety. Trustworthy AI and safe AI both have similar goals but different emphases. We strive to demonstrate the trustworthiness of an AI system so that we can have confidence in the technology making what we perceive as responsible decisions. Safe AI puts the emphasis on the prevention of harmful consequences. The risk [of AI] is significant, and it could potentially be catastrophic if we think of nuclear power plants, or weapons, and so on. I think one of the problems here is, who is actually going to take responsibility? This is a big issue, and not necessarily an issue for the scientist to decide. Also, who is accountable? For instance, the developers of large language models: are they the ones that are accountable? Or is it the people who deploy the large language models and are fine-tuning them for their use cases?\nThe other thing I want to emphasise is the socio-technical characteristics [of the AI problem]. We need to get an interdisciplinary team of people to actually try and tackle these issues."
  },
  {
    "objectID": "the-pulse/posts/2023/12/06/ai-fringe.html#do-we-need-an-ai-safety-institute",
    "href": "the-pulse/posts/2023/12/06/ai-fringe.html#do-we-need-an-ai-safety-institute",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Do we need an AI Safety Institute?",
    "text": "Do we need an AI Safety Institute?\nAndrew Garrett: Do we need to have an AI Safety Institute, as Rishi Sunak has said? And if we don’t need one, why not?\nDetlef Nauck: I’m more in favour of encouraging academic research in the field and funding the kind of research projects that can look into how to build AI safely, [and] how to evaluate what it does. One of the key features of this technology is it has not come out of academic research; it has been built by large tech companies. And so, I think we have to do a bit of catch up in scientific research and in understanding how are we building these models, what can they do, and how do we control them?\nMihaela van der Schaar: This technology has a life of its own now, and we are using it for all sorts of things that maybe initially was not even intended. So, shall we create an AI [safety] institute? We can, but we need to realise first that testing AI and showing that it’s safe in all sorts of ways is complicated. I would dare say that doing that well is a big research challenge by itself. I don’t think just one institute will solve it. And I feel the industry needs to bear some of the responsibility. I was very impressed by Professor [Geoffrey] Hinton, who came to Cambridge and said, “I think that some of these companies should invest as much money in making safe AI as developing AI.” I resonated quite a lot with that.\nAlso, let’s not forget, many academic researchers have two hats nowadays: they are professors, and they are working for big tech [companies] for a lot of money. So, if we take this academic, we put them in this AI tech safety institute, we have potential for corruption. I’m not saying that this will happen. But one needs to be very aware, and there needs to be a very big separation between who develops [AI technology] and who tests it. And finally, we need to realise that we may require an enormous amount of computation to be able to validate and test correctly, and very few academic or governmental organisations may have [that].\n\n\n\n\nI think it’s an insult to the UK’s scientific legacy that we’re reduced to testing software that has been made by US companies. We have huge talents in this country. Why aren’t we using that talent to actually build something instead of testing something that someone else has made?\n\n\n\n\n\n\n\n\n\n\nMartin Goodson: Can I disagree with this idea of an evaluation institute? I think it’s a really, really bad idea, for two reasons. The first is an argument about fairness. If you look at drug regulation, who pays for clinical trials? It’s not the government. It’s the pharmaceutical companies. They spend billions on clinical trials. So, why do we want to do this testing for free for the big tech companies? We’re just doing product development for them. It’s insane! They should be paying to show that their products are safe.\nThe other reason is, I think it’s an insult to the UK’s scientific legacy that we’re reduced to testing software that has been made by US companies. I think it’s pathetic. We were one of the main leaders of the Human Genome Project, and we really pushed it – the Wellcome Trust and scientists in the UK pushed the Human Genome Project because we didn’t want companies to have monopolistic control over the human genome. People were idealistic, there was a moral purpose. But now, we’re so reduced that all we can do is test some APIs that have been produced by Silicon Valley companies. We have huge talents in this country. Why aren’t we using that talent to actually build something instead of testing something that someone else has made?\nMark Levene: Personally, I don’t see any problem in having an AI institute for safety or any other AI institutes. I think what’s important in terms of taxpayers’ money is that whatever institute or forum is invested in, it’s inclusive. One thing that the government should do is, we should have a panel of experts, and this panel should be interdisciplinary. And what this panel can do is it can advise government of the state of play in AI, and advise the regulators. And this panel doesn’t have to be static, it doesn’t have to be the same people all the time.\nAndrew Garrett: To evaluate something, whichever way you chose to do it, you need to have an inventory of those systems. So, with the current proposal, how would this AI Safety Institute have an inventory of what anyone was doing? How would it even work in practice?\nMartin Goodson: Unless we voluntarily go to them and say, “Can you test out our stuff?” then they wouldn’t. That’s the third reason why it’s a terrible idea. You’d need a licencing regime, like for drugs. You’d need to licence AI systems. But teenagers in their bedrooms are creating AI systems, so that’s impossible."
  },
  {
    "objectID": "the-pulse/posts/2023/12/06/ai-fringe.html#lets-do-reality-centric-ai",
    "href": "the-pulse/posts/2023/12/06/ai-fringe.html#lets-do-reality-centric-ai",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Let’s do reality-centric AI!",
    "text": "Let’s do reality-centric AI!\nAndrew Garrett: What are your thoughts about Rishi Sunak wanting the UK to be an AI powerhouse?\nMartin Goodson: It’s not going to be a powerhouse. This stuff about us being world leading in AI, it’s just a fiction. It’s a fairy tale. There are no real supercomputers in the UK. There are moves to build something, like you mentioned in your introduction, Andrew. But what are they going do with it? If they’re just going to build a supercomputer and carry on doing the same kinds of stuff that they’ve been doing for years, they’re not going to get anywhere. There needs to be a big project with an aim. You can build as many computers as you want. But if you haven’t got a plan for what to do with them, what’s the point?\nMihaela van der Schaar: I really would agree with that. What about solving some real problem: trying to solve cancer; trying to solve our crisis in healthcare, where we don’t have enough infrastructure and doctors to take care of us? What about solving the climate change problem, or even traffic control, or preventing the next financial crisis? I wrote a little bit about that, and I call it “let’s do reality-centric AI.” Let’s have some goal that’s human empowering, take a problem that we have – energy, climate, cancer, Alzheimer’s, better education for children, and more diverse education for children – and let us solve these big challenges, and in the process we will build AI that’s hopefully more human empowering, rather than just saying, “Oh, we are going to solve everything if we have general AI.” Right now, I hear too much about AI for the sake of AI. I’m not sure, despite all the technology we build, that we have advanced in solving some real-world problems that are important for humanity – and imminently important.\nMartin Goodson: So, healthcare– I tried to make an appointment with my GP last week, and they couldn’t get me an appointment for four weeks. In the US you have this United States Medical Licencing Examination, and in order to practice medicine you need to pass all three components, you need to pass them by about 60%. They are really hard tests. GPT-4 for gets over 80% in all three of those. So, it’s perfectly plausible, I think, that an AI could do at least some of the role of the GP. But, you’re right, there is no mission to do that, there is no ambition to do that.\nMihaela van der Schaar: Forget about replacing the doctors with ChatGPT, which I’m less sure is such a good idea. But, building AI to do the planning of healthcare, to say, “[Patient A], based on what we have found out about you, you’re not as high risk, maybe you can come in four weeks. But [patient B], you need to come tomorrow, because something is worrisome.”\nMartin Goodson: We can get into the details, but I think we are agreeing that a big mission to solve real problems would be a step forward, rather than worrying about these risks of superintelligences taking over everything, which is what the government is doing right now."
  },
  {
    "objectID": "the-pulse/posts/2023/12/06/ai-fringe.html#managing-misinformation",
    "href": "the-pulse/posts/2023/12/06/ai-fringe.html#managing-misinformation",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Managing misinformation",
    "text": "Managing misinformation\nAndrew Garrett: We have some important elections coming up in 2024 and 2025. We haven’t talked much about misinformation, and then disinformation. So, I’m interested to get your views here. How much is that a problem?\nDetlef Nauck: There’s a problem in figuring out when it happens, and that’s something we need to get our heads around. One thing that we’re looking at is, how do we make communication safe from bad actors? How do you know that you’re talking to the person you see on the camera and it’s not a deep fake? Detection mechanisms don’t really work, and they can be circumvented. So, it seems like what we need is new standards for communication systems, like watermarks and encryption built into devices. A camera should be able to say, “I’ve produced this picture, and I have watermarked it and it’s encrypted to a certain level,” and if you don’t see that, you can’t trust that what you see comes from a genuine camera, and it’s not artificially created. It’s more difficult around text and language – you can’t really watermark text.\nMark Levene: Misinformation is not just a derivative of AI. It’s a derivative of social networks and lots of other things.\nMihaela van der Schaar: I would agree that this is not only a problem with AI. We need to emphasise the role of education, and lifelong education. This is key to being able to comprehend, to judge for ourselves, to be trained to judge for ourselves. And maybe we need to teach different methods – from young kids to adults that are already working – to really exercise our own judgement. And that brings me to this AI for human empowerment. Can we build AI that is training us to become smarter, to become more able, more capable, more thoughtful, in addition to providing sources of information that are reliable and trustworthy?\nAndrew Garrett: So, empower people to be able to evaluate AI themselves?\nMihaela van der Schaar: Yes, but not only AI – all information that is given to us.\nMartin Goodson: On misinformation, I think this is really an important topic, because large language models are extremely persuasive. I asked ChatGPT a puzzle question, and it calculated all of this stuff and gave me paragraphs of explanations, and the answer was [wrong]. But it was so convincing I was almost convinced that it was right. The problem is, these things have been trained on the internet and the internet is full of marketing – it’s trillions of words of extremely persuasive writing. So, these things are really persuasive, and when you put that into a political debate or an election campaign, that’s when it becomes really, really dangerous. And that is extremely worrying and needs to be regulated.\n\n\n\n\n\n\n\n\n\n\n\nAt the moment, if you type something into ChatGPT and you ask for references, half of them will be made up. We know that, and also OpenAI knows that. But it could be that, if there’s regulation that things are traceable, you should be able to ask, ‘How did this information come about? Where did it come from?’\n\n\n\nMark Levene: You need ways to detect it. Even that is a big challenge. I don’t know if it’s impossible, because, if there’s regulation, for example, there should be traceability of data. So, at the moment, if you type something into ChatGPT and you ask for references, half of them will be made up. We know that, and also OpenAI knows that. But it could be that, if there’s regulation that things are traceable, you should be able to ask, “How did this information come about? Where did it come from?” But I agree that if you just look at an image or some text, and you don’t know where it came from, it’s easy to believe. Humans are easily fooled, because we’re just the product of what we know and what we’re used to, and if we see something that we recognise, we don’t question it."
  },
  {
    "objectID": "the-pulse/posts/2023/12/06/ai-fringe.html#audience-qa",
    "href": "the-pulse/posts/2023/12/06/ai-fringe.html#audience-qa",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Audience Q&A",
    "text": "Audience Q&A\n\nHow can we help organisations to deploy AI in a responsible way?\nDetlef Nauck: Help for the industry to deploy AI reliably and responsibly is something that’s missing, and for that, trust in AI is one of the things that needs to be built up. And you can only build up trust in AI if you know what these things are doing and they’re properly documented and tested. So that’s the kind of infrastructure, if you like, that’s missing. It’s not all big foundation models. It’s about, how do you actually use this stuff in practice? And 90% of that will be small, purpose-built AI models. That’s an area where the government can help. How do you empower smaller companies that don’t have the background of how AI works and how it can be used, how can they be supported in knowing what they can buy and what they can use and how they can use it?\nMark Levene: One example from healthcare which comes to mind: when you do a test, let’s say, a blood test, you don’t just get one number, you should get an interval, because there’s uncertainty. What current [AI] models do is they give you one answer, right? In fact, there’s a lot of uncertainty in the answer. One thing that can build trust is to make transparent the uncertainty that the AI outputs.\n\n\nHow can data scientists and statisticians help us understand how to use AI properly?\nMartin Goodson: One big thing, I think, is in culture. In machine learning – academic research and in industry – there isn’t a very scientific culture. There isn’t really an emphasis on observation and experimentation. We hire loads of people coming out of an MSc or a PhD in machine learning, and they don’t know anything, really, about doing an experiment or selection bias or how data can trip you up. All they think about is, you get a benchmark set of data and you measure the accuracy of your algorithm on that. And so there isn’t this culture of scientific experimentation and observation, which is what statistics is all about, really.\nMihaela van der Schaar: I agree with you, this is where we are now. But we are trying to change it. As a matter of fact, at the next big AI conference, NeurIPS, we plan to do a tutorial to teach people exactly this and bring some of these problems to the forefront, because trying really to understand errors in data, biases, confounders, misrepresentation – this is the biggest problem AI has today. We shouldn’t just build yet another, let’s say, classifier. We should spend time to improve the ability of these machine learning models to deal with all sorts of data.\n\n\nDo we honestly believe yet another institute, and yet more regulation, is the answer to what we’re grappling with here?\nDetlef Nauck: I think we all agree, another institute is not going to cut it. One of the main problems is regulators are not trained on AI, so it’s the wrong people looking into it. This is where some serious upskilling is required.\n\n\nAre we wrong to downplay the existential or catastrophic risks of AI?\nMartin Goodson: If I was an AI, a superintelligent AI, the easiest path for me to cause the extinction of the human race would be to spread misinformation about climate change, right? So, let’s focus on misinformation, because that’s an immediate danger to our way of life. Why are we focusing on science fiction? Let’s focus on reality.\n\n\nAI tech has advanced, but evaluation metrics haven’t moved forward. Why?\nMihaela van der Schaar: First, the AI community that I’m part of innovates at a very fast pace, and they don’t reward metrics. I am a big fan of metrics, and I can tell you, I can publish much faster a method in these top conferences then I can publish a metric. Number two, we often have in AI very stupid benchmarks, where we test everything on one dataset, and these datasets may be very wrong. On a more positive note, this is an enormous opportunity for machine learners and statisticians to work together and advance this very important field of metrics, of test sets, of data generating processes.\nMartin Goodson: The big problem with metrics right now is contamination, because most of the academic metrics and benchmark sets that we’re talking about, they’re published on the internet, and these systems are trained on the internet. I’ve already said that I don’t think this [evaluation] institute should exist. But if it did exist, there’s one thing that they could do, which is important, and that would be to create benchmark datasets that they do not publish. But obviously, you may decide, also, that the traditional idea of having a training set and a test set just doesn’t make any sense anymore. And there are loads of issues with data contamination, and data leakage between the training sets and the test sets."
  },
  {
    "objectID": "the-pulse/posts/2023/12/06/ai-fringe.html#closing-thoughts-what-would-you-say-to-the-ai-safety-summit",
    "href": "the-pulse/posts/2023/12/06/ai-fringe.html#closing-thoughts-what-would-you-say-to-the-ai-safety-summit",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Closing thoughts: What would you say to the AI Safety Summit?",
    "text": "Closing thoughts: What would you say to the AI Safety Summit?\nAndrew Garrett: If you were at the AI Safety Summit and you could make one point very succinctly, what would it be?\nMartin Goodson: You’re focusing on the wrong things.\nMark Levene: What’s important is to have an interdisciplinary team that will advise the government, rather than to build these institutes, and that this team should be independent and a team which will change over time, and it needs to be inclusive.\nMihaela van der Schaar: AI safety is complex, and we need to realise that people need to have the right expertise to be able to really understand the risks. And there is risk, as I mentioned before, of potential collusion, where people are both building the AI and saying it’s safe, and we need to separate these two worlds.\nDetlef Nauck: Focus on the data, not the models. That’s what’s important to build AI.\n\nDiscover more The Pulse\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\nImages by Wes Cockx & Google DeepMind / Better Images of AI / AI large language models / Licenced by CC-BY 4.0.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Evaluating artificial intelligence: How data science and statistics can make sense of AI models.” Real World Data Science, December 6, 2023. URL"
  },
  {
    "objectID": "the-pulse/posts/2023/08/17/data-science-and-games.html",
    "href": "the-pulse/posts/2023/08/17/data-science-and-games.html",
    "title": "Where do AI, data science, and computer games intersect?",
    "section": "",
    "text": "Game studios have cemented their place among the fastest-growing media industries. In recognition of this, we hosted an event in June through the Royal Statistical Society (RSS) Merseyside Local Group to explore AI and data science in computer game development. This was an amazing opportunity to engage with a different, in-vogue domain that has unique ties to data science. We showcased two fantastic presentations covering both academic and industry perspectives.\nStanley Wang, a data scientist at SEGA Europe, opened the event by showing the methods that SEGA uses to collect, process, and apply data on player decisions in-game. It was a revealing glimpse at how smoothly in-game data collection is integrated into SEGA’s digital platforms and the ways these data can be used to engage game-centred communities – for example, running special celebrations once milestones are hit for in-game events (revenue made, goals scored, etc.) or offering real-time integration with streaming platforms so viewers can see detailed statistics on in-game progress. Stanley showed one particular example where data collection fed directly into development decisions for Endless Space, a competitive strategy game where players vie for galactic conquest. During the beta (a period where a game is available to play but still considered in-testing before commercial release), SEGA were able to monitor how well-balanced the playable alien factions were based on real-time win rate data, which led to improvements to game mechanics for the final release.\nWe also learned how SEGA’s data science teams are using clustering methods to identify different game-playing behaviours in Two Point Hospital, a simulation game where players design, build, and manage a hospital through various scenarios. After compiling high-dimensional in-game data such as objectives achieved, treatment of staff, and even furniture choices, various clustering algorithms (including k-means clustering) were used to identify common sets of player behaviour. Stanley highlighted that when using these sorts of unsupervised learning methods, it’s useful to get insights from multiple models to inform methodological decisions like number of clusters chosen or how to treat outliers. SEGA identified four distinct types of player from these analyses, which you can hear more about from Stanley in the video below. The approach allowed the company to better understand gamers’ motivations and experiences with a view to designing future game content.\n\nOur second speaker, Dr Konstantinos Tsakalidis, a lecturer in the Department of Computer Science at the University of Liverpool, presented exciting new ideas to teach computer games developers of the future. Dr Tsakalidis walked us through the curriculum for a dynamic new undergraduate program that reflects the latest software development technologies and the theory behind them. The course outline was designed around building knowledge and practice from the fundamentals upwards, starting from game physics as a prerequisite for game mechanics, game mechanics being a prerequisite for game content, and game content being a prerequisite for game AI. Combined with the continuous active involvement of students at each stage, this represented a great model of constructivist teaching. Dr Tsakalidis also proposed that practical game development (and subsequent assessments) should follow the latest research on data science and AI in computer games.\n\nDiscover more The Pulse\n\n\n\n\n\nAbout the author\n\nAlice-Maria Toader is a PhD student at the University of Liverpool and a committee member of the RSS Merseyside Local Group. Liam Brierley is a research fellow in health data science at the University of Liverpool and chair of the RSS Merseyside Local Group.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Alice-Maria Toader and Liam Brierley\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail image by Jezael Melgoza on Unsplash.\n\n\n\nHow to cite\n\nToader, Alice-Maria and Liam Brierley. 2023. “Where do AI, data science, and computer games intersect?” Real World Data Science, August 17, 2023. URL"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact us",
    "section": "",
    "text": "Email: rwds@rss.org.uk\nGitHub: @realworlddatascience\nZenodo: Real World Data Science community\nLinkedIn: RSS Real World Data Science\nX: @rwdatasci\nMastodon: @rwdatasci",
    "crumbs": [
      "Contact us"
    ]
  },
  {
    "objectID": "contact.html#editorial",
    "href": "contact.html#editorial",
    "title": "Contact us",
    "section": "",
    "text": "Email: rwds@rss.org.uk\nGitHub: @realworlddatascience\nZenodo: Real World Data Science community\nLinkedIn: RSS Real World Data Science\nX: @rwdatasci\nMastodon: @rwdatasci",
    "crumbs": [
      "Contact us"
    ]
  },
  {
    "objectID": "contact.html#advertising-and-commercial",
    "href": "contact.html#advertising-and-commercial",
    "title": "Contact us",
    "section": "Advertising and commercial",
    "text": "Advertising and commercial\n\nEmail: advertising@rss.org.uk",
    "crumbs": [
      "Contact us"
    ]
  },
  {
    "objectID": "foundation-frontiers/posts/2023/11/06/how-to-open-science.html",
    "href": "foundation-frontiers/posts/2023/11/06/how-to-open-science.html",
    "title": "How to ‘open science’: A brief guide to principles and practices",
    "section": "",
    "text": "Open science is about making your research freely accessible to others. This includes your data, your code and any outputs (such as reports or articles).\nMany people in research, or working or studying in higher education, will be familiar with open science as a concept. As a lecturer, I was aware of it and frequently made use of open data for teaching and research, but it was not until it became a requirement from my funder that I took the opportunity to run my own research as open science by design.\nMost tools that I was already familiar with could be used to support open science, but I soon realised that there were some steps and planning that I first needed to learn. As I discovered more about the processes and principles of open science, I came to see that making my research open would not require much additional time and effort. However, I felt that a succinct guide to open science would certainly help me – and others – to make the transition more easily. So, I set out to write such a guide.\nThis is the result! It is not meant to be an exhaustive document. Rather, I will explain the route I took to open science and what options are out there for others looking to follow suit."
  },
  {
    "objectID": "foundation-frontiers/posts/2023/11/06/how-to-open-science.html#what-is-open-science",
    "href": "foundation-frontiers/posts/2023/11/06/how-to-open-science.html#what-is-open-science",
    "title": "How to ‘open science’: A brief guide to principles and practices",
    "section": "What is open science?",
    "text": "What is open science?\n“Open science refers to the process of making the content and process of producing evidence and claims transparent and accessible to others” (Munafò et al. 2017). The open science principles are:\n\nOpen source\n\nAny data, code or output is accessible and usable in software that is freely available and with an open license. What this means in practice is that, for example, when sharing data, the .csv format is used rather than .xlsx, as the latter requires closed source software (Microsoft Excel) to run.\n\nOpen data\n\nResearch data should be freely accessible. One approach to open data is to adhere to the FAIR Data Principles (Wilkinson et al. 2016). FAIR stands for Findable, Accessible, Interoperable, and Reusable, and these principles can be implemented as a step to help make your work open science. However, they are not the only way, nor are they a guarantee that your work will automatically meet the definition of “open science” if you implement them.\n\nOpen access\n\nAccess to published papers and/or outputs is freely available to all. This can be achieved, for example, by sharing published papers in a pre-print server.\n\n\n\n\n\n\n\n\nWhat is a pre-print server?\n\n\n\n\n\nPre-print servers are online repositories that enable you to share versions of your manuscript before or while your manuscript is under review. Examples of such repositories include ArXiv and MedRxiv.\n\n\n\n\n\n\nPre-print server example from MedRxiv.\n\n\n\n\nOne additional benefit of open science is that it supports reproducible research. This means that others can download your data and code, re-run the analysis, and see if they obtain the same results. To get the full benefit of open science and promote reproducibility, code needs to be written with enough explanations or comments to help others understand the logic of the various stages of an analysis."
  },
  {
    "objectID": "foundation-frontiers/posts/2023/11/06/how-to-open-science.html#steps-to-open-science",
    "href": "foundation-frontiers/posts/2023/11/06/how-to-open-science.html#steps-to-open-science",
    "title": "How to ‘open science’: A brief guide to principles and practices",
    "section": "Steps to open science",
    "text": "Steps to open science\nIn this section, I will outline steps you can take to easily make your research open science. There will be situations where it is not possible to make all aspects of research open – for example, due to privacy and consent issues related to data. It is still possible to share some elements of such projects, but potentially this involves additional work – to create suitable demo data, say, or generate synthetic data in order to provide data that has comparable trends but preserves privacy. It may also be possible to share the data when it is requested on a case-by-case basis. I am not going to cover this here, but it is worth considering whether open science is possible in each case.\n\nBefore you begin…\nPre-registering an analysis plan for your research helps establish that your research is confirmatory (hypothesis testing) rather than exploratory (hypothesis generating). If you have some hypotheses or research questions that are the foundation of your research, it is worth pre-registering. If your research is exploratory, pre-registration is not necessarily applicable. Although pre-registration in itself is not a requirement for open science, the process of pre-registration can all be completed within repositories such as the Open Science Framework (OSF). Pre-registering your analysis plan will add value and rigour to you research.\nIf your research doesn’t require pre-registration, jump straight to Step 1.\n\n\n\n\n\n\nWhat is pre-registration?\n\n\n\n\n\nPre-registration involves completing a form before you start your analysis to explain the primary research questions, the covariates of interest, and the methods you plan to use and why. Haroz (2022) provides more detail on how apps like OSF, Zenodo and Figshare support pre-registration. This video also gives more details.\nBelow is an example of a pre-registration.\n\n\n\n\n\n\n\n\n\n\nStep 1\nDoes your research plan require you to write a lot of code for analysis purposes, perhaps in collaboration with others? If the answer is No, skip to Step 2. If Yes:\n\nConsider setting up a GitHub repository (or repo), especially if this is a collaborative project and it is likely that more than one person will be working on the code. Don’t forget to invite your collaborators to join the repo!\nGitHub repos can be set to private and then made public at the appropriate time, so development work can take place behind closed doors and then released to the wider world when ready.\nEnsure that your code is commented properly so that it is reusable and, eventually, your results are reproducible.\n\n\n\nStep 2\nGitHub is a great tool for developing code collaboratively, but it may not be right for you – or indeed the only tool to use – if you have a lot of other material to work with and release as part of your research project. If that’s the case:\n\nSet up an area for your project on an open science repository such as OSF, Zenodo or Figshare. (If you use OSF then setting up an OSF repository is quick and easy – head to osf.io. OSF allows many integrations, including to GitHub, through the use of add-ons.)\nYou can start by setting your repository as private and then make it public at the appropriate time.\nUpload all project files, and don’t forget to invite your collaborators.\nAdd ORCIDs for every team member.\n\n\n\n\n\n\n\nWhat is an ORCID?\n\n\n\n\n\nAn ORCID is a persistent digital identifier that you own and control. It allows you to connect your ID with your professional information – affiliations, grants, publications, peer reviews, and more. You can set one up at orcid.org.\n\n\n\n\n\nStep 3\nIf you are ready to submit your research to a journal or conference, consider the following steps before you submit:\n\nCheck that there is enough information in GitHub (if using) and OSF (if using) about the project. This should include instructions for someone to be able to access your files, use the data and run the code.\nMake the GitHub and/or OSF repositories publicly visible.\nIf submitting to a journal that requires anonymous links, generate them and copy them into the manuscript. (In OSF, for example, it is possible to create anonymous links to your repository in case of double-blind submission requirements.)\nShare a copy of your manuscript on a pre-print server – but don’t forget to check the journal or conference policy on pre-prints before you do!\n\n\n\n\n\n\n\nApps and websites to support open science\n\n\n\n\n\nThis is by no means a complete list but instead features the apps and websites that are commonly used when research projects include data and code.\n\nOpen Science Framework (OSF)\nOSF is a free web app that supports researchers with sharing, archiving, registration and collaboration. The Open Science Framework website is worth checking out and includes a guide to help users get started. Once a project is public in the OSF it will have a DOI and a permanent link, so it can be cited. OSF can also support the tracking of versions of your file. One drawback can be that there is a limit on the maximum size of file that can be uploaded.\n\n\n\n\n\n\nSample OSF repository.\n\n\n\nFigshare\nThis web app supports storing and sharing research outputs (papers, FAIR data, and non-traditional research outputs). Like OSF, Figshare provides a DOI for your files and is similarly limited in the maximum size of file that can be upload.\n\n\nZenodo\nAnother general purpose open repository. As with Figshare, Zenodo also provides a DOI.\n\n\nGitHub\nGitHub is a web app that offers distributed version control. It is very commonly used for software development, especially when there are multiple developers. Although you can share code and many file types through GitHub, accessing and collaborating on projects can be a daunting experience for those who are not familiar with the way GitHub works. Also, GitHub is not always required as it is possible to share your code through OSF, for example. If you want to know more about using GitHub in support of open science and reproducibility, read “The road to reproducible research”."
  },
  {
    "objectID": "foundation-frontiers/posts/2023/11/06/how-to-open-science.html#example-my-own-route-to-open-science",
    "href": "foundation-frontiers/posts/2023/11/06/how-to-open-science.html#example-my-own-route-to-open-science",
    "title": "How to ‘open science’: A brief guide to principles and practices",
    "section": "Example: my own route to open science",
    "text": "Example: my own route to open science\nIn my case, my project did not involve a heavy amount of coding or a large number of researchers, so I opted to use OSF to store the ethics approval documents, the survey questions (which drove the data collection), the data in .csv format, and the outputs. I also then linked this to Figshare from my institution and published the article on MedRxiv at the same time as I submitted it to a journal for review. The paper was eventually published in BMJ Open. The steps I took in this case were sufficient for the work to be recognised as embracing open science principles."
  },
  {
    "objectID": "foundation-frontiers/posts/2023/11/06/how-to-open-science.html#plot-your-own-route-to-open-science",
    "href": "foundation-frontiers/posts/2023/11/06/how-to-open-science.html#plot-your-own-route-to-open-science",
    "title": "How to ‘open science’: A brief guide to principles and practices",
    "section": "Plot your own route to open science",
    "text": "Plot your own route to open science\n\n\n\n\n\nflowchart TD\n  D(\"- Set up GitHub repo\n  - Set repo as private\n  - Add collaborators\")\n  F(\"- Set up an OSF repository\n  - Set project as private\n  - Add collaborators and their ORCIDs\")\n  A(Pre-register statistical analysis plan?) -- Yes --&gt; B(Complete pre-registration through, e.g., Open Science Framework) --&gt; C(Does your research involve writing lots of code?) -- Yes --&gt; D --&gt; E(Do you plan to share data and other research material?) -- Yes --&gt; F --&gt; G(Research project is finished and ready to submit to journal or conference)\n  A -- No --&gt; C -- No --&gt; E -- No --&gt; G\n  G --&gt; H(Have you used repos?) -- Yes --&gt; I(Change repo settings - GitHub and/or OSF - to public) --&gt; J(Does publication permit sharing manuscripts to pre-print servers?) -- Yes --&gt; K(Submit to pre-print server) --&gt; L(Does publication require anonymous link to OSF repo for double-blind review?) -- Yes --&gt; M(Generate anonymous link and add to submission) --&gt; N(Submit your work)\n  H -- No --&gt; J -- No --&gt; L -- No --&gt; N"
  },
  {
    "objectID": "foundation-frontiers/posts/2023/11/06/how-to-open-science.html#in-summary",
    "href": "foundation-frontiers/posts/2023/11/06/how-to-open-science.html#in-summary",
    "title": "How to ‘open science’: A brief guide to principles and practices",
    "section": "In summary…",
    "text": "In summary…\nTo make your research open science, you need to:\n\nMake any data you collect or generate available to download and reuse.\nPre-register your statistical analysis plan.*\nMake your code available for download, and document it clearly so others can reuse it.\nMake any supporting material and outputs available for download in formats that are open source.\nIf publishing to a journal or conference, share manuscripts in a pre-print server.*\n\n\n* May not be relevant or applicable, depending on the nature of your work.\n\n\nExplore more data science ideas\n\n\n\n\n\nAbout the author\n\nIsabel Sassoon is a senior lecturer in computer science and data science at Brunel University London and a member of the Real World Data Science editorial board.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Isabel Sassoon\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence Thumbnail photo by Basil James on Unsplash.\n\n\n\nHow to cite\n\nSassoon, Isabel. 2023. “How to ‘open science’: A brief guide to principles and practices.” Real World Data Science, November 6, 2023. URL"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/09/17/digital-gender-gap.html",
    "href": "foundation-frontiers/posts/2024/09/17/digital-gender-gap.html",
    "title": "Are we at risk of muting the female voice in the digital world?",
    "section": "",
    "text": "Knowledge is power and today a lot of that knowledge – not just what you know but who you know – is online. In 2015 the UN General Assembly laid out 17 Sustainable Development Goals (SDGs) that aim to end poverty and other deprivations while improving the welfare of both people and the planet. One of the SDGs deals with gender equality and emphasises the importance of digital technology for empowering women. Online, a woman can engage in commercial, social, business or networking transactions without the need to be absent from care responsibilities at home or maintain traditional 9-5 working hours or, in some instances, even expose the fact that she is a woman at all – all potentially transformative features of online engagement1. Yet the reality for digital technology to empower women is by no means clear cut.\n‘For me, whether digital technologies are able to empower women was fundamentally an empirical question,’’ says professor of demography and computational data science at Oxford University Ridhi Kashyap. She adds that in order to ask these questions of impact, you first need to be able to measure inequalities in digital access. However, the pace of technological change has been a lot faster than the rate at which national censuses – or other kinds of surveys useful to social scientists – update their questions, so they shed little light on the demographics around digital technologies.\nSince then, progress in accruing data on digital access has revealed some stark gender inequalities. However, access is not the only fly in the ointment when it comes to the potential for digital technology to help towards gender equality. ‘The most harmful illegal online content disproportionately affects women and girls,’ says the explainer for the UK’s 2023 Online Safety Act. A study by the Turing Institute published earlier this year has revealed nuances on this picture, but confirmed that many women feel particularly vulnerable online, suggesting women may be losing a seat at the table as debate and discourse increasingly moves online.\nThe digital gender gap has a cost estimated at $126 billion USD for the 32 low- and low-to-middle-income countries analysed by the Alliance for Affordable Internet (A4AI)2. This is due to the ‘untold wealth of cultural, social, and scientific knowledge lost because of the exclusion of women’s and girls’ voices from the online world.’ Focus on this issue has brought a little more clarity to the size of the problem. However, while the UK’s Online Safety Act marks some progress, questions remain as to what can be done, and whether the hope of digital technologies helping towards gender equality is still justified."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/09/17/digital-gender-gap.html#gender-disparities-in-internet-access",
    "href": "foundation-frontiers/posts/2024/09/17/digital-gender-gap.html#gender-disparities-in-internet-access",
    "title": "Are we at risk of muting the female voice in the digital world?",
    "section": "Gender disparities in internet access",
    "text": "Gender disparities in internet access\nA turning point in the conversation around digital technology and gender equality came in 2018 with work by Kashyap and collaborators in the US and Qatar at the time. They found that where traditional survey-based data on internet and mobile gender gaps was available, it correlated well with the gender gap on Facebook, using data extracted for Facebook’s ad platform: When Facebook’s aggregate user counts did not show women, it provided a good signal that women were not online altogether in those countries. As such, the work revealed a potentially useful proxy to gauge the digital gender gap in countries where little traditional survey data was available3. The results revealed an unexpectedly large gender gap, particularly in parts of South Asia and certain countries in Africa where men were up to twice as likely to have access to the Internet compared with women.\n‘In some sense it was perhaps not surprising,’’ says Kashyap highlighting that having a mobile phone or similar device that grants access to the internet amounts to a kind of asset ownership, and studies for other assets indicate women are less likely to own them. ‘This is broadly reflective of economic gender inequality,’ she adds. Perhaps more surprising is that the gaps have changed very little in the five years since their website, which monitors the digital gender gap, was first released, particularly in view of the pace of technological progress in general, and the importance placed on closing the gap. Citing India as an example, Kashyap points out that in 2019 the ratio of access to the internet for men versus women was 0.619 – fewer than two women had access for every three men with access. In the subsequent half decade this digital gender gap has closed by just 7.1% to a ratio of 0.663.\n\n\n\n\n\n\nFigure 2: The digital gender gap. Ratio of female-to-male internet use estimated using the Facebook Gender Gap Index4\n\n\n\nIn countries where the gender disparity for access to the internet is large, there is evidence to suggest that those women who do have access are of the more affluent echelons of society. Analysis of the type of device used, which can also be retrieved from the Facebook ad platform, highlighted that where women are less likely to be online, the relative proportion of iOS users tends to be higher among women than among men, and as Kashyap points out, ‘iOS users are on average wealthier’. Fortunately, among the stakeholders starting to see the benefit of closing the gap in access to the internet between the genders are the mobile network providers, who are looking for ways to tap into this part of the market through incentives and discounts on SIMs for women. However, it is unclear to what extent these types of schemes are ultimately beneficial in closing the wider gap.\nKashyap and her colleagues also found that a key predictor of the digital gender gap was the gender gap in educational attainment. ‘I think that’s quite telling, because it’s showing that accessing education and going to educational institutions is also a pathway to becoming more digitally integrated,’ says Kashyap, flagging that schools and educational institutions are where women and girls often access computers and digital technologies. She highlights that beyond giving people a device ‘more of the challenge’ is helping them make good use of it by ‘giving people skills to feel that this is actually meaningful for them, and allows them to do things that they wouldn’t be able to do otherwise, and feeling confident and safe and secure.’ She emphasises the importance of men valuing gender equality, highlighting work from South Asia that shows that even when women have a device, their use of it may be curtailed or scrutinised by male members of the household, sometimes on the grounds of doubts over women’s safety online."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/09/17/digital-gender-gap.html#gender-disparities-in-fears-of-online-harms",
    "href": "foundation-frontiers/posts/2024/09/17/digital-gender-gap.html#gender-disparities-in-fears-of-online-harms",
    "title": "Are we at risk of muting the female voice in the digital world?",
    "section": "Gender disparities in ‘fears’ of online harms",
    "text": "Gender disparities in ‘fears’ of online harms\nSafety can be a knotty issue when it comes to enabling women to have a voice online. A study by the Alan Turing Institute5 earlier this year suggested just 23% of women in general feel comfortable expressing political opinions online, compared with 40% of men. This might be down to women in general being exposed to online violence more than men, as previous studies of online harms have suggested. Indeed, a key takeaway from the Alan Turing Institute’s study was that women reported greater fears of exposure for all categories of harm, although this included types of harm that women reported experiencing less frequently than men.\nPrevious studies have largely surveyed women-only sample-groups so that their conclusions were drawn without data on men against which to compare. In contrast, the researchers at the Alan Turing Institute, including researcher Tvesha Sippy, took a nationally representative survey of 2,000 men and women. They investigated whether they had been exposed to various types of online harms, their fears surrounding such exposure, the psychological impact of those experiences in general, tendencies to use protective tools for digital activities, and how comfortable they felt with online behaviours such as expressing opinions and sharing information online. The study revealed that women were more likely to report experiencing some harms, such as online misogyny, cyberflashing, cyberstalking, image-based abuse and eating disorder content to a significantly greater extent than men. However, there were several harms that men reported being the direct targets of to a greater extent than women, such as hate speech, misinformation, trolling and threats of physical violence.\nBy using a representative cohort, the Alan Turing Institute study tells a more nuanced story than those sampling women only and highlights challenges in similar assessments for minority groups. For example, those identifying as non-binary were excluded from the analysis by the Alan Turing Institute because, although as Sippy emphasises, ‘We do want to look at minoritised genders,’ they did not have sufficient numbers of respondents in this category within their nationally representative survey to do any meaningful analysis. Ultimately, a higher budget enabling larger samples would allow analysis of minority groups as well.\nAs for the greater fears for all online harms reported by women, ‘it’s a very complex phenomenon,’’ Sippy tells Real World Data Science, highlighting the need for further research. She points to several possible explanations such as differences in the impacts of the harms experienced more by women versus men, as well as innate fearfulness potentially from the offline world translating to behaviour online. Sippy also highlights the differences in how men and women experience online harms, which may offer clues. Women were more likely to report that their fears stem from the experience of a public figure (35% of the women surveyed compared with 26% of the men) or a female friend (37% of the women compared with 27% of the men). Furthermore, the experience of a male friend was much less often cited as the source of online fears for both groups (8% of the women and 14% of the men). There is also the possibility that women’s adaptive behaviours make them less exposed to future online harms than men, since women were more likely to make use of protective tools from disabling location-sharing on a device, and limiting who can engage with images, posts and tweets, or even find their profile. While protective, such adaptive behaviours could also dampen the influence women have in online discourse.\nRather than relying on adaptive behaviour for self-protection, it would seem a lot of people are keen to see more action from social media companies and governments to help people to feel safer online. In 2023, researchers at the Turing Institute led by senior research associate Florence Enock published a study investigating attitudes to online interventions. They found that 79% thought social media platforms should ban or suspend users who create harmful content and 73% thought that platforms should remove harmful content. According to the report ‘this was consistent across age, gender, educational background, income and political ideology.’\nThere are some complications for social media companies who need to balance privacy needs with protection, as well as having the resources required to handle multilingual posts when investigating what action to take. However, Sippy feels there remains a need to have a civil remedy in place so that a user can request a platform take down content which is harmful without having to pursue criminal proceedings and get the police involved. Where the additional resources needed for social media companies to take corrective action and a lack of business incentive pose an obstacle, government legislation may help. The same study into attitudes to online interventions also reported that for platforms that fail to deal with harmful content online more than 70% of respondents felt the government should be able to issue large fines, and 66% thought that legal action should be taken.\n‘The Online Safety Act is a really good start,’ adds Sippy, also highlighting the importance of proposals by the previous UK government to criminalise the creation of sexually explicit deep fakes. She points to a 2019 report by AI firm Deeptrace, suggesting that of 15,000 deep fake videos they found online, 96% constituted nonconsensual pornography with women disproportionately targeted6. In a recent Alan Turing Institute survey 90% of respondents expressed concerns about deepfakes increasing misogyny and online violence against women and girls7. ‘I do see there’s more advocacy, but it remains to be seen what approach the new Government will take.’"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/09/17/digital-gender-gap.html#gender-disparities-for-making-an-impact-online",
    "href": "foundation-frontiers/posts/2024/09/17/digital-gender-gap.html#gender-disparities-for-making-an-impact-online",
    "title": "Are we at risk of muting the female voice in the digital world?",
    "section": "Gender disparities for making an impact online",
    "text": "Gender disparities for making an impact online\nChallenges to women being heard online seem to go beyond safety issues. Recent research by Kashyap and collaborators at the University of Oxford and collaborators in Iran and Germany has also highlighted differences in how influential women’s professional networks are relative to male counterparts8. In previous work with Florianne Verkroost, also at the University of Oxford, Kashyap had investigated the gender gaps in those who have a LinkedIn profile to see how they vary across industries9. They found that use of the platform broadly mirrors female-to-male ratios of representation in technical and managerial professions. In reference 8, they then investigated what insights LinkedIn data might provide as to the cause of some of the gender disparities in these professions, and ultimately why women are not progressing in technical and professional jobs as well as male counterparts.\n‘One argument is that that’s often because they don’t have advantageous networks,’ says Kashyap, adding that women may be restricted by the need to resume care commitments at home instead of staying for drinks after work or travelling to attend conferences. One might expect online avenues for networking would be able to mitigate such obstacles. In fact, studies of LinkedIn data did suggest that although women are less likely to be in professional and technical occupations as reflected in the platform’s data, in some instances their numbers exceeded them. Kashyap suggests this could be ‘where they’re using online platforms to make themselves more visible, because other fine forms of networking are less available, or they have less time for it.’ Indeed, women who were on LinkedIn were more likely to report a promotion than their male counterparts, suggesting an element of positive selection among the female LinkedIn user population. However, the potential equalising impact of moving professional networking online seems to have its limits.\nTheir study of LinkedIn data showed women were less likely to report a relocation for work, which Kashyap suggests, ‘is a sign that the work family trade-off is probably still remaining acute for this highly selected group.’ In another 2023 study Kashyap and colleagues had also reported a lower mobility for women, specifically among published scientists, researchers and academics based on bibliometric data from over 33 million Scopus publications10. In addition, when Kashyap and her colleagues looked at women on LinkedIn working in the tech sector, they found that they had a lower chance of being connected to those working in one of the “big five” firms in the tech sector than men, when not working in one themselves. ‘One way to interpret that is to say that they have maybe less influential online social networks, right, even when they are on the platform.’\n\n\n\n\n\n\nFigure 3: Leaky pipeline. The proportion of women working in science decreases towards the mid and senior career stages.\n\n\n\nKashyap suggests several reasons why women may have less influential networks online. For one, online networks are still likely to be influenced by the scenarios playing out offline, since referrals on these networks are based on the people you already know. The difference may also be based on the types of companies women tend to work in and the positions they hold. For instance, women are more likely to work in IT service support than programming-intensive occupations, and here once again Kashyap suggests the work family trade off plays a role in women seeking less intensive or more flexible jobs. She highlights that girls equal or exceed the achievement of male counterparts through school and continue to match them in their early careers before their numbers start to drop off dramatically. ‘I think now there’s a growing recognition that this is actually a real conflict, the work family conflict,’ she tells Real World Data Science. Today’s young women are socialised to have ‘high achieving aspirations’, which can be hard to reconcile with ‘regressive norms’ for women to shoulder the bulk of caring responsibilities, particularly when starting a family."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/09/17/digital-gender-gap.html#real-world-gender-disparities-in-career-development",
    "href": "foundation-frontiers/posts/2024/09/17/digital-gender-gap.html#real-world-gender-disparities-in-career-development",
    "title": "Are we at risk of muting the female voice in the digital world?",
    "section": "Real world gender disparities in career development",
    "text": "Real world gender disparities in career development\nNeuroscientist Joanne Kenney has also been following data on the gender gap in the science and tech sectors and co-authored ‘A Snapshot of Female Representation in Twelve Academic Psychiatry Institutions Around the World’11 with Assistant Professor of Psychiatry at Harvard Medical School Elisabetta del Re. The figures published here also show that globally women represent a large majority of early career scientists, but their numbers steadily decrease towards the mid and senior career stages so that there is a negative correlation between career stages and female presence in science, often referred to as the ‘leaky pipeline’ or ‘sticky floor’. ‘You don’t always hear their stories or the reasons why they’ve left,’ says Kenney who highlights that in her experience in academia exit interviews are rare. Just 24% of the UK total workforce in the tech sector are women, while black women account for only 0.7% of IT professionals according to the 2024 UN Women UK and Kearney Consulting report ‘Gap to Gateway: diversity in tech as the key to the future’ for which Kenney was an external collaborator. Kenney is currently working on another project with a team of scientists from Europe, Africa, and North and South America led by del Re to gather stories from women and other underrepresented groups in academic institutions around the world through focus groups aimed at better understanding their experiences of working in science.\nFor those who stick at it, the career path appears to be a steeper hike for women than their male counterparts. There is a citation-bias favouring male-authored articles12. Women also take on average nine years to transition to senior author whereas men take five13, and women are less likely to be promoted to leadership positions14. While women in science bear a measurably unequal career impact on entering parenthood15, some of these inequalities may also stem from sexism, which can range from fewer opportunities for mentorship and collaboration to outright harassment16.\n‘I think a lack of mentorship and sponsorship are two big ones,’ says Kenney when it comes to the key discouraging factors for women at the mid-career point in tech and academia. In AI, in particular, less than 3% of venture capital funding deals involving AI startups go to women-founded companies. The gender pay gap, which at 16% in the sector exceeds the overall pay gap of 11.6% may be another disincentive.\nIn short there is evidence of various patriarchal subcultures at play, both in the tech and science sectors and the world in general that can still pose a significant disadvantage to women. As Sippy points out, ‘Those subcultures also translate to the online world.’ Ultimately while digital technologies may offer creative loopholes for side-stepping some aspects of gender bias and disadvantage, gender inequality needs to be tackled in both spaces in tandem.\n\nExplore more data science ideas\n\n\n\n\n\nAbout the author\n\nAnna Demming is a freelance science writer and editor based in Bristol, UK. She has a PhD from King’s College London in physics, specifically nanophotonics and how light interacts with the very small, and has been an editor for Nature Publishing Group (now Springer Nature), IOP Publishing and New Scientist. Other publications she contributes to include The Observer, New Scientist, Scientific American, Physics World and Chemistry World..\n\n\n\n\n\nCopyright and licence\n\n© 2024 Anna Demming\n\n\n  Text, code, and figures are licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) International licence, except where otherwise noted. Thumbnail image by Shutterstock/Park Kang Hun Licenced by CC-BY 4.0.\n\n\n\nHow to cite\n\nDemming, Anna. 2024. “Are we at risk of muting the female voice in the digital world?” Real World Data Science, September 17, 2024. URL"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/09/17/digital-gender-gap.html#footnotes",
    "href": "foundation-frontiers/posts/2024/09/17/digital-gender-gap.html#footnotes",
    "title": "Are we at risk of muting the female voice in the digital world?",
    "section": "References",
    "text": "References\n\n\nSicat M, Xu A, Mehetaj E, Ferrantino M & Chemutai V Leveraging ICT Technologies in Closing the Gender Gap World Bank World Bank Group, Washington DC (2020) https://documents1.worldbank.org/curated/en/891391578289050252↩︎\nWeb Foundation. The Costs of Exclusion: Economic Consequences of the Digital Gender Gap. Alliance for Affordable Internet (2021) https://a4ai.org/report/the-costs-of-exclusion-economic-consequences-of-the-digital-gender-gap/↩︎\nFatehkia M, Kashyap R & Ingmar Weber I Using Facebook ad data to track the global digital gender gap World Development 107 189-209 (2018) https://www.sciencedirect.com/science/article/pii/S0305750X18300883↩︎\nLeasure D R, Yan J, Bondarenko M, Kerr D, Fatehkia M, Weber I & Kashyap R. Digital Gender Gaps Web Application, v1.0.0. Zenodo, GitHub (2023) doi:10.5281/zenodo.7897491↩︎\nStevens F, Enock F E, Sippy T, Bright J, Cross M, Johansson P, Wajcman J, Margetts H Z Understanding gender differences in experiences and concerns surrounding online harms: A nationally representative survey of UK adults Alan Turing Institute (2024) https://www.turing.ac.uk/news/publications/understanding-gender-differences-experiences-and-concerns-surrounding-online↩︎\nAjder H, Patrini G, Cavalli F & Cullen L The State of Deepfakes: Landscape, Threats, and Impact, (2019) https://regmedia.co.uk/2019/10/08/deepfake_report.pdf↩︎\nSippy T, Enock F E, Bright J & Margetts H Z Behind the Deepfake: 8% Create; 90% Concerned Alan Turing Institute (2024) https://www.turing.ac.uk/news/publications/behind-deepfake-8-create-90-concerned↩︎\nKalhor G, Gardner H, Weber I, Kashyap R Proceedings of the Eighteenth International AAAI Conference on Web and Social Media 18 (2024) https://ojs.aaai.org/index.php/ICWSM/article/view/31353↩︎\nKashyap R & Verkroost F C J Analysing global professional gender gaps using LinkedIn advertising data EPJ Data Science 10 39 (2021) https://doi.org/10.1140/epjds/s13688-021-00294-7↩︎\nZhao X , Akbaritabar A, Kashyap R & Zagheni E A gender perspective on the global migration of scholars PNAS 120 e2214664120 https://doi.org/10.1073/pnas.2214664120↩︎\nKenney J, Ochoa S, Alnor M A, Ben-Azu B, Diaz-Cutraro L, Folarin R, Hutch A, Luckhoff H K, Prokopez C R, Rychagov N, Surajudeen B, Walsh L, Watts T, Del Re E C A Snapshot of Female Representation in Twelve Academic Psychiatry Institutions Around the World Psychiatry Research (2021) doi: 10.1016/j.psychres.2021.114358↩︎\nDworkin J D, Linn K A, Teich E G, Zurn P, Shinohara R T & Bassett D S The extent and drivers of gender imbalance in neuroscience reference lists Nature 23 918-926 (2020) https://www.nature.com/articles/s41593-020-0658-y↩︎\nBearden C E Accelerating the Bending Arc Toward Equality: A Commentary on Gender Trends in Authorship in Psychiatry Journals Biological Psychiatry 86 575-576 (2019)https://www.biologicalpsychiatryjournal.com/article/S0006-3223(19)31588-4/abstract↩︎\nClark J & Horton R A coming of age for gender in global health The Lancet 393 p2367-2369 (2019) https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(19)30986-9/abstract↩︎\nMorgan A C, Way S F, Hoefer M J D, Larremore D B, Galesic M & Clauset A The unequal impact of parenthood in academia Science Advnaces 7 eabd1996 doi: 10.1126/sciadv.abd1996↩︎\nO’Connor P s gendered power irrelevant in higher educational institutions? Understanding the persistence of gender inequality *Interdisciplinary Science Reviews” 48 669-686 (2023) https://doi.org/10.1080/03080188.2023.2253667↩︎"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/06/04/ai-series-6.html",
    "href": "foundation-frontiers/posts/2024/06/04/ai-series-6.html",
    "title": "AI series: What is “best practice” when working with AI in the real world?",
    "section": "",
    "text": "Over the course of the Real World Data Science AI series, we’ve had articles laying out the nitty gritty of what AI is, how it works, or at least how to get an explanation for its output as well as burning issues around the data involved, evaluating these models, ethical considerations, and gauging societal impacts such as changes in workforce demands. The ideas in these articles give a firm footing for establishing what best practice with AI models should look like but there is often a divide between theory and practice, and the same pitfalls can trip people up again and again. Here we discuss how to wrestle with real world limitations and flag these common hazards.\nOur interviewees, in order of appearance, are:\nAli Al-Sherbaz, academic director in digital skills at the University of Cambridge in the UK\nJanet Bastiman, Napier chief data scientist and chair of the Royal Statistical Society Data Science & AI Section\nJonathan Gillard, professor of statistics/data science at Cardiff University, and a member of the Real World Data Science Board\nFatemeh Torabi, senior research officer and data scientist, health data science at Swansea University, and also a member of the Real World Data Science board\nIt is often said that while almost everybody is now trying to leverage AI in their projects, most AI projects fail. What nuggets of wisdom do the panel have for swelling that minority that succeed with their AI projects, and what should you do before you start doing anything?\nAli Al-Sherbaz: It’s not easy to start, especially for people who are not aware how AI works. My advice is, first, they have to understand the basics of how AI works because the expectation could be overpromising, and that is a danger. Just 25 years ago, a master dissertation might be about developing a simple – we call it simple now but it was a master’s project 25 years ago – a simple model with a neural network of a combination of nodes to classify data. Whatever the data is – it could be drawing shapes, simple shapes, square, circle triangle – just classifying them was worth an MSc. Now, kids can do it. But that is not the same as understanding what the neural network or the AI is. It’s a matrix of numbers, and actually, for the learning process each does multiple iterations to find the best combination of these numbers – product of sum; sum of product – to classify, to do something, and train them for a certain situation, and that is a supervised learning. Over the last 25 years – especially in the last 10 years – the computational power is getting better, so AI is now working better.\nThere are other things people have to learn. There’s the statistics as well, and of course people who would like to work in AI and data science must understand the data, and they should also be experts in the data itself. For instance, I can talk about cybersecurity, I can talk about networking and other things, but if it comes to something regarding health data, or financial services, or stock markets, I’m not an expert in the data. So I’m not going to be actively working on those things even if I use the same AI tools. This is in a nutshell why I think some people fail sometimes using AI, or they succeed using AI. And we should emphasise the human value. The AI is there, and it exists to help us to make a better more accurate decision, but the human value is still there. We have to insist on that.\nJanet Bastiman: I would just like to build on all of that great stuff that Ali’s just said. When you look at basically the non-data scientist side of it, you often get businesses who think AI can solve a certain problem. They might go out and hire a team – whether that’s directly or indirectly – and get them to try and solve a problem that, as Ali said, they may not have the domain expertise for. The business might not even have the right data for it, and AI might not even be the right way of solving that problem. I think that’s one of the fundamental things to think about – really understanding what you’re trying to solve, and how you’re going to solve it before you start throwing complex tools, and potentially very expensive teams at the problem.\nWhen you look at a lot of the failures, it’s been because businesses have just gone, we can solve this problem, I’m just going to hire a team and let these intelligent people look at something. And then they’re restricted on the data that they’ve got, which won’t even answer the question; they’re restricted on the resources they have; and even restricted in terms of wider buy in from the company. So really understanding what is it that you want to solve? What are you trying to do? Is AI the right thing? And can you even do it with the resources you have available? And I think that’s, that’s a fundamental starting point. Because, you can have wonderful experts, who have that domain knowledge, who understand the statistics, and all that essential stuff that Ali just said. But then if from a business point of view, if you don’t give them the right data to work on, or you don’t let them do their job and tell you when they can’t do their job, then again, you’re going to be doomed to failure.\nJonathan Gillard: Explainability is a big issue when it comes to AI models, as well. They are at the moment, very largely “black box” – data goes in, then these models get trained on dumb data and answers get popped out. And when it works, well, it works fabulously well. And we’ve seen lots of examples of that happening. But often for business, industry or real life, we want to learn. We want to understand the laws of the universe, and to understand the reasons why this answer came about. Because this explainability piece is missing – because everything is hidden away almost – I think that’s a big issue in successful execution. And particularly when it comes to industries where there’s a degree of regulation there as well, if you can’t explain how a particular input arose to a particular output, then how can you justify to regulatory bodies that what you’ve got is satisfactory, ethical, and that you’re learning and you’re doing things in the right way?\nThere have been efforts at trying to get explanations from these models. How do you think things are progressing there?\nJG: Yeah, that’s a good question. I think where we are with explainability is in very simple scenarios, very simple models. This is where traditional statistical models do very well. There’s an explicit model which says if you put these things inside then you’ll get this output. So [for today’s AI] I think we’re actually very far away from having that complete explainability picture, particularly as we fetishise more and more grand models. The AI models are only getting bigger, more complex, and that makes the explainability per se even more challenging. And that’s why I think, as Ali says, at the moment, the human in the loop is absolutely crucial.\nWhat AI does share with classical statistics (or classical data science if you want to call it that) is it can still only be as good as the data that’s put into it, that’s still a fundamental truth. I think a lot of the assumptions currently with AI models – and this is where there could be a few trip ups is that it can create something from nothing. It’s “artificial intelligence” – almost the wording suggested it’s artificial. But fundamentally, we still need a robust and reliable comprehensive source of data there in order to train these models in the first place.\nIn terms of having outsourced expertise for these projects– does that make more problems if you’re then trying to understand what this AI has done?\nJB: Oh, hugely. Let’s say that domain expertise – that’s something Ali touched on –you’ve got to understand your data. Because even that fundamental initial preparation of data before you try and train anything is absolutely crucial – really looking at where are the gaps? Where are the assumptions? How is this data even being collected? Has it been manipulated before you got to it? If you don’t understand your industry, well enough you won’t know where those pitfalls might be – and a lot of teams do this, they just take the data, and then they just put it in, turn the handle and out comes something and it looks like it’s okay. What they’re really missing there – because they’re not putting that effort in to really understand those inputs, what the models are doing, they’re just turning the handle until they get something that feels about right – what they miss out is where it goes wrong. And there are some industries, where the false positives and false negatives from classification or the bad predictions from running things really have a severe human impact. And if you don’t understand what’s going in, and the potential impact of what comes out, then it’s very, very easy to just churn these things out and go, “it’s 80% accurate, but that’s fine” without really understanding the human impact of the 20% [that it gets wrong].\nGoing back to what Jon said about that explainability, it’s so crucial. It is challenging, and it is difficult, but going from these opaque systems to more transparent systems – we need that for trust. As humans, we divulge our trust very differently, depending on the impact. One of the examples I use all the time is, you know, sort of weather prediction stuff, you know, we don’t really care too much, because it’s not got a huge impact. But when you look at sort of financials or medicals, we really, really want to know that that output is good, and how we got to that output. The Turing Institute’s come out with some great research that says, as humans, if we want to understand why when another human has told us something, then we want the same thing from the models, and that can vary from person to person. So building that explainable level into everything we do, has to be one of the things we think about upfront. But you’ve got to really, truly deeply understand that data. And it’s not just a question of offloading a data set to a generalist who can turn that handle, otherwise you will end up with huge, huge problems.\nFatemeh Torabi: I very much agree with all the points that my colleagues raised. I also think it’s very important that we know why we are doing things. Having those incremental stages in our planning for any project, and then having a vision of where we see AI can contribute into this process and can give us further efficiency – and how – is very important. If we don’t have defined measures to see how this AI algorithm is contributing to this specific element of the project, we can get really lost bringing these capabilities on board. Yes, it might generate something, but how we are going to measure that something is very important. I think, as members of the scientific community, we must all view AI as a valuable tool. However, it has its own risks and benefits.\nFor example, in healthcare when we use AI for risk predictions, it can be a really great tool to aid clinicians to save time. However, in each stage, we need to assess the data quality, how these data are fed into the algorithm, what procedures, what models, and how we generate those models. And then which discriminative models do we use to balance the risk and eventually predict the risk of outcomes in patients? It’s very much a balance between risks and benefits for usefulness of these tools in practice. We have all these brilliant ideas of what best practice is. But in real terms, sometimes it’s a little bit tricky to follow through.\nCould you give us some thoughts on the sort of best practice with data, for example, that doesn’t quite turn out to be quite so easy to follow in practice, and what you might do about it?\nFT: We always call these AI algorithms, data hungry algorithms, because the models that we fit require us to see patterns in the data that we feed into them so that the learning happens. And then the discriminative functions come in place to balance and kind of give a score to wherever the learning is happening and give an evaluation of each step. However, the data that we put into these algorithms comes first – the quality of that data. Often in healthcare, because of its sensitivity, the data is held within a secure environment. So we cannot, at this point in time, expose an AI algorithm to a very diverse example, specifically for investigating rare diseases or rare conditions. And above that, there is also complexities in the data itself. We need to evaluate and clean the data before we feed it into these algorithms. We need to evaluate the diversity of the data itself – for example, the tabular data, the imaging data, the genomic data – and each one requires its own specific or tailored approach in data cleaning stages.\n\n\n\n\n\n\nFigure 1: The panel. Clockwise from top left: Ali Al-Sherbaz, Janet Bastiman, Fatemeh Torabi and Jonathan Gillard\n\n\n\nWe also have another level that is now being discovered in the health data science community, which is the generation of synthetic data. We can give AI models access to these synthetic versions of the data that we hold. However, that also has its own challenges because it requires reading the patterns from real data, and then creating those synthetic versions of data.\nFor example, Dementia Platforms UK is one of the pioneers in developing this. We hold a range of cohort data, patients’ data, genomics data and imaging data. In each one of these when we try to develop those processing algorithms, there are specific tailored approaches that we need to consider to ensure we are actually creating a low fidelity level of data that is holding some of the patterns in it for the AI algorithm to allow the learning to happen. However, we also need to consider whether it is safe enough so that we can ensure the data provided are secure to be released for use at a lower governance level compared to the actual data. So there are quite a lot of challenges, and we captured a lot of it in our article.\nA A-S: I can talk about the cybersecurity and other relevant data network security, the point being the amount of data we receive to analyse. It’s really huge. And when I say huge I mean about one gigabyte, probably in a couple of hours, or one terabyte in a week – that’s huge. One gigabyte of a text file – if I printed out this file with A4 – that would leave me with a stack of A4 paper, three times the Eiffel Tower.\nNow, if I have cyber traffic, and try to detect any cyber attack, AI helps with that. However, if we train this model properly, they have to detect cyber attacks in real time – when I say real time, we’re talking about within microseconds or a millisecond – and the decision has to be correct. AI alone doesn’t work, doesn’t help. Humans should also intervene, but rather than having 100,000 records to check for a suspected breach, AI can reduce that to 100. A human can interact with that. And then in terms of the authentication or verification, humans alongside AI can learn whether this is a false positive, or a real attack or a false negative. This is a challenge in the cybersecurity area.\nJB: I just wanted to dive in from the finance side – again the data is critical, and we have very large amounts of data. However in addition – and I think we probably suffer from the same sort of problem that Ali does in this – when I’m trying to detect things, there are people on the other side actively working against what I’m trying to detect, which I suppose is a problem that maybe Fatemeh doesn’t have in healthcare.\nWhen you’re trying to build models to look for patterns, and those patterns are changing underneath you, it can be incredibly difficult. I have an issue that all of my client’s data legally has to be kept separated – some of it has to be kept in certain parts of the world so we can’t put that into one place. We can try and create synthetic data that has the same nuances of the snapshots that we can see at any one point in time, and we can try and put that together in one place, but what we can detect now will very quickly not be what we need to detect in a month’s time. As soon as transactions start getting stopped, as soon as suspicious activity reports are raised, and banks are fined, everything switches and how all of that financial crime occurs, changes. And it’s changing, on a big scale worldwide, but also subtly because, there are a team of data scientists on the other side trying desperately to circumvent the models that me and my team are building. It’s absolutely crazy. So while I would love to be able to pull all of the data that I have access to in one place and get that huge central visual view, legally I can’t do that because of all the worldwide jurisdictional laws around data and keeping it in certain places.\nThen I’ve also got the ethical side of it, which is something that Fatemeh touched on. If I get it wrong, that can have a material impact on usually some of the most marginalised in society. The profile of some of the transactions that are highly correlated with financial crime are also highly correlated with people in borderline poverty, even in Western countries. So false positives in my world have a huge, huge ethical impact. But at the same time, we’re trying really hard to minimise those false negatives – that balance is critical, and the data side of it is such a problem.\nFatemeh mentioned the synthetic side of it. There’s a huge push, particularly in the UK to get good synthetic data to really showcase some of these things that we’re trying to detect. But by the time you get that pooling, and the synthesising of data that you can ethically use and share around without fear of all the legal repercussions, what we’re trying to detect has already moved on. So we’re constantly several steps behind.\nI imagine Ali has similar problems in the cybercrime space in that as soon as things are detected, the ways in which they work move on. So there’s an awful lot I think that, as an industry, although we have different verticals, we can share best practices on.\nIs there a demand for new types of expertise?\nA A-S: There is a huge gap in the in the UK, at least and worldwide about finding people working as a data scientist or working with the data. So we created a course in Cambridge, which we call the data science career accelerator for people who work in data, and would like to move on and learn more. We did market research, and we interviewed around 50 people between CEO and head of security and head of data scientists, in science departments and in industry, to tell us – what kind of skills are you after? What problems do you currently have? And then we designed this course.\nWe found that first of all there are people who don’t know from where to start – what kind of data they need, what tools they have to learn with… Even if they learn the tools, they still need to learn what kind of machine learning process to use. And then suddenly, we have ChatGPT turned out, and the LLM [large language model] development – all of that in one course, it is a real challenge.\nThe course has started now, the first cohort. The big advice from industry we have is that during the course they have to work on real world case studies, on scenarios with data that nobody has touched before – that is, it’s new, not public. We teach them on a public data, but companies also have their own data, and we get consent from them to use that data for the students so we can test the skills they learned on virgin data that nobody has touched before.\nWe just started this month, and the students are going to start with the first project now. They are enjoying the course but that is the challenge we have now. How did we handle that? It’s to work together with the industry side by side, even during the delivery. We have an academic from Cambridge, and we have experts from the industry to support the learners to learn to get the best of both worlds.\nThe industry has changed so much in the last couple of years. Does that mean that the expertise and demands are also changing very quickly or is there a common thread that you can work with?\nA A-S: Well, there is a common thread, but having new tools – I mean, Google just released Gemini, and that’s a new skill they have learnt and been tested on, and looked into how others feel about it and compared it to ChatGPT, or Claude 3 or Copilot. That’s all happened in the last 12 months. And then, of course, reacting on that, reflecting on the material, teaching the material – it’s a challenge. It’s not easy and you need to find the right person. Of course, people who have this kind of experience are in demand, and it’s hard to secure these kinds of human resources as well as to deliver the course. So there are challenges and we have to act dynamically and be adaptive.\nWhat are your thoughts on the evaluation of these models, and how to manage the risk of something that you haven’t thought of before, and the role of regulation.\nJG: I think a lot of our discussions at the moment are assuming that we’ve got well meaning, well intentioned people and well meaning, well intentioned companies and industries, who are trying to seek to do their best ethically and regulatorily and with appropriate data, and so on. But there is a space here for bad actors in the system.\nUnfortunately, digital transformation of human life will happen in a good and bad way – unfortunately, I think there are going to be those two streams to this. Individuals are very capable now of making their own large language models by following a video guide if they wanted to, and having that data is, of course going to enable them maybe to do bad things with it.\nData is already a commodity in quite a strong way, but I do think we have to visit data security, and even the risks of open data as well. We live in a country, which I think does very well in producing lots of publicly available data. But that could be twisted in a way that we might not expect. And when I speak of those things, we’re usually thinking of groundwork – writing and implementing your own large language models – but there were recent examples of where just by using very clever prompting of existing large language models, you could get quite dangerous material, shall we say, which circumnavigated inbuilt existing safeguards. Again, that’s an emerging thing that we have to have to try and address as it comes on.\nI think my final point with ethics and regulation is it will rapidly evolve, and it will rapidly change. And a story which I think can illustrate that is, when the first motorcar was introduced into the UK, it was law for a human to walk in front of the motorcar with a large red flag to warn passers-by of the incoming car because people weren’t really familiar with it. Now, of course, that’s in distant memory, right? We don’t have people with red flags, walking in front of cars. I do wonder, in 20 years or 50 years, what will the ethical norms regarding AI and its use be? Likewise, will we have deregulation? That seems to be the common theme in history that when we get more familiar with things, we deregulate because we’re more comfortable with their existence. That makes me quite curious about what the future holds.\nFT: Jon raised a very interesting point and Janet touched upon keeping financial data in silos but we are facing this in healthcare as well. Data has to be checked within a trusted research environment or secure data environment that’s making the data silos. However, efforts at this point in time are on enhancing these digital platforms to bring data and federal data together. Alongside what is happening in terms of our progression towards development of a new ethical or legal requirement, is documenting what is being practised at the moment, because at the moment there are quite a lot of bubbles. Each institution has their own data and applies their own rules to it. So understanding what it is that we are currently working on – the data flows that are flowing into the secure environments – is building the basis of developments that are going on in terms of developing standardisation and common frameworks. A lot of projects have been focused on understanding the current to develop on it for the future.\nWe know for example, the Data Protection Act, put forward some specific requirements, but that was developed in 2018, before we had this massive AI consideration. In my academic capacity as well, we are facing what Jon mentioned, in terms of the diversity of assessments for students. For example, when we ask these questions, even if the data is provided within the course and within this defined governance, we know that the answers can possibly be aided by AI – a model. So we are defining more diverse assessment methods in academic practice to ensure that we have a way to evaluate the outcome that we are receiving by the human eye, rather than being blinded by what we receive from AI, and then calling it high quality output, whether in research practice or in academic practice. So there’s quite a lot of consideration of these issues, I think that is bringing our past knowledge to the current point where we now have to balance between human and machine interactions in every single process that we are facing.\nHow does this change the skill set required of data scientists, as AI is getting more and more developed?\nA A-S: Regarding the terminology of data scientists, when we talk about data we immediately link that with statistics, and statistics is an old topic. There has been an accumulation of expertise for 100 years, to the best of my knowledge or more in statistics, and people who are new to data analysis or data, have to learn about this legacy. And when we develop the course, we should mention these skills in statistics and build this knowledge on top, that is, when we reach the right point, then we talk about learning or machine learning, supervised and unsupervised, and about LLM – these are the new skills they have to learn. As I mentioned, it’s tricky when we teach learners about it, we have to provide them with simple datasets to teach them something complex in statistics because it’s a danger to teach both [data and statistics at the same time] – we will lose them, they will lose concentration and it’s hard to follow up. So, a little bit of statistics – they have to learn the basics like normal distribution, the distribution, the type, and what does it mean when we have these distributions, the meaning of the data – and that is the point I made earlier about how people should have a sense for the numbers. What does it mean, when I say 0.56 in healthcare? Is that a danger? 60% – is that OK? In cybersecurity, if the probability of attack today is 60% should I inform the police? Should I inform someone; is that important? Or for example, for the stock market? Say we have dropped off 10% – Is that something we have to worry about? So making sense of the numbers is part of it.\nThat is part of personalised learning because it depends on their background or what they have learned – it’s not straightforward, and it has to be personalised not just for people taking the course now, for instance for someone who is 18 years old coming from their A levels. No, it’s for a wide range. People from diverse courses like to approach this data science course. And now we are in the era of people who are in social science, and engineering, doctors, journalism, art, they are all interested in learning a little bit of data science, and utilising AI for their benefit. So there is no one answer.\nYou emphasise that people still need to be able to make sense of numbers. We’re often told that AI will devalue knowledge and devalue experience – it sounds like you don’t feel that’s the case.\nA A-S: I have to stick with the following: human value is just that – value. AI without humans is worth nothing. I have one example: In 1997, some software was developed for chess, to play against a human, and for the first time, that computer programme (called AI now) beat Kasparov. Guess what happened? Did chess disappear? No, we still value human to human competition. The value of the human is the same for art and for music. So we still have human value, and we have to maintain that for the next generation. They shouldn’t lose this human value, and handover to AI value, which I feel is zero without the human.\nJ B: I think one of the things we are seeing is that diversity in people’s backgrounds coming into data science, which is fantastic, because I think that really helps with the understanding of when things can go wrong, and how things can be misused. If you have this cookie cutter set of people that have all got a degree from the same place and all had the same experience, which is very similar – this happens a lot in the financial industry where there’s like five universities that all feed into the banks – they all think and solve problems in the same way because that’s how they’ve been trained. But as soon as you start bringing in people with different backgrounds, they’re the ones that say, hang on, this is a problem. So having those different backgrounds is really useful.\nBut then as Ali said there’s so many people who call themselves a data scientist that don’t understand data, or science. And I think he was absolutely right. If you’ve got a probability of 60%, or you’ve got a small standard deviation, when is that an issue? What do you really understand about that based on your industry, and based on your statistical knowledge? That’s so so key. And it’s something that a lot of people who are self-trained and call themselves data scientists have missed out on. So coming back to your original question about is it harder or is it easier, in some respects, it’s a lot harder, because someone who calls himself a data scientist now needs to do everything from basically fundamental research, trying to make models better, you’ve got to understand statistics, you’ve got to understand machine learning, engineering, production, isolation, efficiencies, effectiveness, ethics – it’s this huge, huge sphere. And it’s too much for one person. So you’ve really got to have well balanced teams and support. Because you can’t keep on top of your game across all of those. It’s just not possible. So I think that becomes really difficult. When I look at how things have changed, there’s so many basic principles from, you know, the 80s and 90s, in standard, good quality computer programming and testing. And I think the one thing that we’re really missing as an industry is a specialist AI testing role. Someone who understands enough about how models work and how they can go wrong and can do the same thing for AI solutions, as good QA analysts can do for standard software engineering models. Someone who can really test them to extremes with what happens when I put the wrong data in.\nWe saw this – there were a couple of days under COVID, where all the numbers went wrong, because the data hadn’t been delivered correctly, or not enough of it had been delivered. There were no checks in place to say, actually, we’ve only got 10% of what we were expecting, so don’t automatically publish these results. It’s things like that, that we really need to make sure are built into the systems because those are the things that, again, could cause problems. As soon as you get a model that’s not doing the right thing – going back to our original question – when they do go wrong, you can then find a company pulls that model even though it could be easily fixed. And then they’re disillusioned with AI, and won’t use it. That’s that whole project, and all of the expense and investment on that just thrown away when a bit more testing and understanding could have saved it.\n\nExplore more data science ideas\n\n\n\n\n\nAbout the authors\n\nAnna Demming is a freelance science writer and editor based in Bristol, UK. She has a PhD from King’s College London in physics, specifically nanophotonics and how light interacts with the very small, and has been an editor for Nature Publishing Group (now Springer Nature), IOP Publishing and New Scientist. Other publications she contributes to include The Observer, New Scientist, Scientific American, Physics World and Chemistry World..\n\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. \n\n\n\nHow to cite\n\nDemming, Anna. 2024. “What is “best practice” when working with AI in the real world?.” Real World Data Science, June 4, 2024. URL"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/21/ai-series-4.html",
    "href": "foundation-frontiers/posts/2024/05/21/ai-series-4.html",
    "title": "AI series: Evaluation essentials for safe and reliable AI model performance",
    "section": "",
    "text": "It took just sixteen hours for Microsoft’s shiny new chatbot Tay to be shut down for profanity. The chatbot had been released on the social media platform, X, then known as Twitter, following extensive evaluation and stress testing under different conditions to ensure that interacting with the chatbot would be a positive experience. Unfortunately, the testing plan had not bargained on a coordinated attack exploiting the chatbot’s vulnerability when exposed to a torrent of offensive material. Tay soon began tweeting wildly inappropriate words and images and was taken offline within hours.\nThe chatbot’s failure highlights just how hard yet imperative it can be to test and evaluate a model before real world deployment. With the recent flux of accessible “off-the-shelf” machine learning algorithms, building AI models, in particular generative AI models is now relatively straight forward. However, the simplicity with which models are deployed undermines the complexity of evaluating them. Nonetheless, deploying the model anywhere outside the data and context it has been trained on can be risky if its performance is not evaluated. The evaluation process requires clear definitions for good performance as well as highlighting the potential risks, and can throw up unexpected requirements in the test data. Not only are the subtle nuances in the initial evaluation requirements important, but once deployed a process needs to be in place so that the algorithm can be monitored over time."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/21/ai-series-4.html#know-your-goals",
    "href": "foundation-frontiers/posts/2024/05/21/ai-series-4.html#know-your-goals",
    "title": "AI series: Evaluation essentials for safe and reliable AI model performance",
    "section": "Know your goals",
    "text": "Know your goals\nThe first point to note is that checking how well the output from an AI model matches the data in the training set is not an adequate indication of how well it will perform once deployed on other data. The problem can be exemplified by considering a simple model based on an equation that best fits a training data set. Data values are inevitably subject to measurement uncertainties and local conditions that add various types of noise, so taking the line defined by the equation identified as best matching the training data and measuring how good that match is falls short of adequate evaluation - the more perfectly a model matches this noisy data, the less perfectly it will fit an alternative set of data, a scenario described as “overfitting”. Similarly, what a machine learning or AI algorithm or model learns when it optimises its fit to the training data may not be generalisable.\n\n\n\n\n\n\nFigure 1: Evaluation plots. The kinds of charts of monitored performance and risk metrics that are plotted to evaluate an AI model. Reliable deployment of an algorithm requires identifying appropriate metrics for performance and risk as well as rigorous, ongoing evaluation. Image created by Isabel Sassoon using Adobe Firefly to show a technical report process flow of statistical model performance and a huge numbers chart.\n\n\n\nThere are a number of possible approaches and factors to take into account when sourcing test data but the first thing to consider when drawing up a process for evaluating an AI model is its objective. With this objective in mind it is then possible to pin down an appropriate measure of performance, which will shape how to use the test data to evaluate the model performance. Among the distinguishing factors between different measures used to evaluate how a model performs on test data, some will be suitable when the objective is to classify (e.g high or low risk based on health data?) while others are useful for models that estimate or predict (e.g What is the estimated height of a child given their parents’ heights).\nClassification model performance can be measured using accuracy, confusion matrices, sensitivity, specificity and the receiver operating characteristic (ROC). Classification accuracy summarises the performance of a classification model as the number of cases in which the model correctly classifies divided by the total number of cases used in the test set. However, this can be a blunt tool as there are cases where there is a different cost or consequence depending on the direction of the error. Confusion matrices are helpful to explore how the model performs in correctly classifying the different classes. The confusion matrix sums up the number of cases the model classifies correctly within each of the classes, for example how many actual high-risk cases are correctly classified as high risk by the model. The number of cases the model classifies as high risk, for example, that are not high risk is referred to as the False Positives. In the context of medical tests (e.g the covid lateral flow tests) testing positive for a condition that is not actually there is potentially less damaging than testing negative when the condition is there.\n\n\n\n\n\n\nFigure 2: The receiver operating characteristic can provide a helpful means of visualising performance. Credit: shutterstock\n\n\n\nAdditionally, the sensitivity and specificity can provide a more detailed look at model performance. The sensitivity refers to the proportion of cases labelled as positive that are classified as positive by the model, whereas specificity refers to the proportion labelled as negative that it classifies as negative. It is useful to visualise model performance and the receiver operating characteristic (ROC) provides a method to do just that. The ROC plots the True Positive rate against the False Positive rate for the model. This can be further summarised in one value as the area under the curve (AUC). The larger the AUC the better the model is performing.\nDeciding whether accuracy is enough or whether there is a need to delve into the directions of the errors depends on the context of the model’s deployment. Other examples in medicine include the risk models that were developed to assess an individual’s risk of a specific medical condition, such as QRISK 1 which calculates a person’s risk of developing a heart attack or stroke over the next 10 years. Here model performance needs to go beyond accuracy and consider the direction of the errors it makes. A good overview of performance evaluation is outlined by Flach (2019) 2. Is it better to tell someone they may be at risk of disease X, run a blood test and rule it out (False Positive) than to tell them they are not at risk and not check (False Negative)? All this needs to be considered and factored into the validation of the model. It is worth noting that a systematic direction for its errors can also cause an algorithm to hit ethical problems.\nWhen evaluating the performance of models that are estimating a numerical value (e.g height of child from height of parents) the measures used are based on how far off the model’s estimate is from the actual value (which is known for testing data). There are then a multitude of ways of summarising that quantity. The mean square error (MSE) is computed by taking the average squared difference between the estimated values from the model and the actual value in the data. Other variations include root mean square error (RMSE) and mean absolute error (MAE). The RMSE is computed in the same way as the MSE but the value is square rooted. The MAE takes a different approach by summing up the absolute errors (i.e. the error magnitude). Each of these three measures involve dividing the value obtained by the number of rows in the data. Depending on the context one of these measures may be better suited than others. For example the MSE is sensitive to outliers so can be easily skewed by a small number of extreme values, which may be useful to highlight them, whereas the RMSE has the advantage of being measured in the same units as the original variable the model is designed to estimate.\nLarge Language Models (LLMs, e.g. Gemini, ChatGPT) are also models trained on a data set and as such also need to be evaluated and monitored. Whereas in the models discussed so far there are some standard metrics, evaluating LLMs is more challenging as there are a multitude of benchmarks and metrics3. When LLMs are used to answer questions (when you ask a chatbot a question) then monitoring the performance of the model (the trained LLM) can involve anything. Is the answer correct? Is the answer clear? Is the answer biased? The possible metrics are varied and not as simple to capture in one measure. It is also possible to use a LLM to evaluate or score another LLMs’ answer to a question. However this adds its own risk as LLMs are not 100% accurate or consistent themselves, and they can hallucinate."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/21/ai-series-4.html#getting-data-right",
    "href": "foundation-frontiers/posts/2024/05/21/ai-series-4.html#getting-data-right",
    "title": "AI series: Evaluation essentials for safe and reliable AI model performance",
    "section": "Getting data right",
    "text": "Getting data right\nNot only is separate test data needed for an evaluation, but care is needed to ensure the test data is suitably representative. Similar requirements apply for test data as for the original training data to ensure the dataset is representative of the context the model will be deployed in. For instance, if an algorithm is being developed to handle photos from the UK, training and testing it on photos where the sun always shines may cause problems. The model needs to be trained and tested on a set of photos that include rain and clouds otherwise it cannot be assumed it will reliably classify such photos if they appear during deployment in the real world. Getting the training and test data set right may mean using a smaller more curated set than simply one that contains everything available.\nThese data sets also need to have reliable labelling i.e. the rows of data need to be accurate so that the model’s performance can be assessed objectively against a trusted “ground truth”. For example, if we want to evaluate the performance of a fraud transaction classification model using accuracy as the performance metric, then we need a reliable training data set with true fraud transactions to evaluate how good the model is at detecting them. A data set with a list of transactions that are not accurately identified (or labelled) as fraud or not is not helpful. Thinking about how some commercial LLMs are trained on all the data in the “internet” it is worth asking whether a smaller more curated and specific training set would be better for model performance as well as being more ethical and safer.\nSeveral approaches for generating test data sets take training and test data as distinct subsets from the same initial data set 4. There are different ways of doing this to make the most of the data to evaluate the model as systematically and exhaustively as possible. Perhaps the simplest example is using a hold-out set, which involves taking all the data available and taking a random subset of the data to use for testing the model. Depending on how much data is available then this can be 50% or less.\nA slightly more sophisticated approach is k-fold cross validation, which involves splitting all the data you have available into k subsets and then doing k iterations where in each iteration a different kth of the data is used as the testing data for evaluation of the model built by training it on the remaining (k-1/k) of the data. This is repeated k times each time using a different one of the k subsets for testing. The performance of the model can then be averaged over the k iterations. (The measure of performance can be, say, accuracy or sensitivity depending on the context). For example, if k is 3 then the data is split into 3, and each iteration will take a different 2/3 of the data as training data to build the model, and the remaining 1/3 as testing data to evaluate the model.\n\n\n\n\n\n\nFigure 3: K fold cross validation can indicate how sensitive a model is to the test data. Credit: Fabian Flöck CC-BY-AS-3.0\n\n\n\nBootstrap is a more computationally intensive approach and it involves creating multiple samples by randomly sampling with replacement from the original data. Typically, hundreds or thousands of and such samples are generated, each will be different. These multiple samples provide multiple versions of the training and testing data so the model can be evaluated on all these variations. As bootstrap relies on sampling with replacement this means that each row of data in the original data can appear multiple times in a sample training or test data during each iteration, or not appear in other samples. As with k-fold cross validation the performance of the model can be then averaged over these multiple iterations. It is important that bootstrap does not rely on only a handful of iterations. Both bootstrap and cross validation offer an opportunity to see how sensitive the model’s performance is to the characteristics of the test data, but when the data sets available are small, the use of the bootstrap approach provides a more robust way of estimating the model’s performance.\nAn approach that can be useful to test whether the performance of the model is sensitive to time is time-based splits. This involves taking a “sliding window” ensuring that data is split into back-to-back time periods. Using a back-to-back (sliding window) further ensures that the data the model is trained on is separate from the one it is tested on."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/21/ai-series-4.html#maintained-monitoring",
    "href": "foundation-frontiers/posts/2024/05/21/ai-series-4.html#maintained-monitoring",
    "title": "AI series: Evaluation essentials for safe and reliable AI model performance",
    "section": "Maintained monitoring",
    "text": "Maintained monitoring\nOnce an algorithm has been let loose it can be challenging to maintain any rigorous monitoring, but it is worth highlighting the importance of taking on the challenge of ongoing monitoring and promising approaches to it. Some of the same metrics will apply to keep a handle on the myriad of issues that could arise. These range from the banal, such as data input errors, to the complex as is the case in model drift.\nIn the first case, if a model makes use of data that is fed into it from another system (e.g. a billing system) any update to this other system can affect model performance. Identifying this involves checking that the characteristics of the data used to train the model and the latest data fed into the model are not too dissimilar, since a difference in the data such as an increase by a factor of 10 or a hundred can cause the algorithm to fail. The magnitude of acceptable change in the data will depend on the context. Such a step change (due to source system update) in one of the model inputs can be identified and can potentially be an easy fix.\nModel drift is more complex as real-world data evolves over time. There are two types of model drift: data drift and concept drift. Data drift refers to the change that can occur to data over time, whilst concept drift5 is a deterioration or change in the relationship between the target variable and input variables of a model. An example of data drift could be in the context of billing data the addition of new price plans or phones to the data, whilst an example of concept drift can arise when there is a change in the relationship between the effect (for instance, leaving one mobile phone provider for another) and underlying factors changes. In the context of the mobile phone provider market, a concept drift may mean that leaving for another provider is no longer dictated so much by price sensitivity as the type of network. Both types of drift lead to a deterioration in performance of the model as time goes by. Performance monitoring of the model is key to detecting model drift but differentiating between data or concept drift requires additional specialist approaches. Some of these are outlined in (Rotalinti, 2022)6 and (Davis, 2020)7.\nIn some cases, refreshing a model to account for the change in the underlying data (both training and test) can be quick and easy. However, if concept drift is detected, then it may take more than just a model refresh as the relationships between the variable we are trying to model, and the explanatory data has changed. This may involve finding new data sources and could lead to significant changes in the model, for example moving from a regression model to a neural network. Deciding to rebuild or retrain a model can also in some cases have environmental impact (particularly for the more resource intensive models such as deep learning and LLMs). Either way, where models are subject to peer review or some form of governance this can be a more onerous task.\nEven with each step in a model’s evaluation stringently adhered to it is also important to assess the context for its deployment for risks and rogue scenarios that might break or in the case of Tay despoil it. And like all other stages of the evaluation this should not just be at the time of deployment but also over time. When models (machine learning or other) are used to inform or make important decisions providing information on how and when the model was evaluated, and how it is monitored should be standard practice not just to avoid the wasted expense of another broken AI model (algorithm) left on the shelf but more importantly to safeguard the welfare of those who come into contact with it.\n\nExplore more data science ideas\n\n\n\n\n\nAbout the author\n\nIsabel Sassoon is senior lecturer in the Department of Computer Science, Brunel University London.\n\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. \n\n\n\nHow to cite\n\nSassoon, Isabel. 2024. “Evaluation essentials for safe and reliable AI model performance .” Real World Data Science, May 21, 2024. URL"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/21/ai-series-4.html#footnotes",
    "href": "foundation-frontiers/posts/2024/05/21/ai-series-4.html#footnotes",
    "title": "AI series: Evaluation essentials for safe and reliable AI model performance",
    "section": "References",
    "text": "References\n\n\nHippisley-Cox, J., Coupland, C. and Brindle, P. Development and validation of QRISK3 risk prediction algorithms to estimate future risk of cardiovascular disease: prospective cohort study.BMJ (2017) doi: https://doi.org/10.1136/bmj.j2099.↩︎\nFlach, P. (2019). Performance evaluation in machine learning: the good, the bad, the ugly, and the way forward. Proceedings of the AAAI conference on artificial intelligence pp. 9808-9814 (2019) doi: https://doi.org/10.1609/aaai.v33i01.33019808.↩︎\nChang, Y. et al. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology (2023) doi: https://doi.org/10.1145/3641289.↩︎\nWitten, I. H., Frank, E. and Hall, M. A. Data mining: Practical machine learning tools and techniques. Morgan Kaufmann (2011).↩︎\nBayram, F., Ahmed, B. S. and Kassler A. From concept drift to model degradation: An overview on performance-aware drift detectors. Knowledge-Based Systems (2022) doi: https://doi.org/10.1016/j.knosys.2022.108632.↩︎\nRotalinti, Y., Tucker, A., Lonergan, M., Myles, P. and Branson, R. Detecting drift in healthcare AI models based on data availability. Joint European Conference on Machine Learning and Knowledge Discovery in Databases 243-258 (2022) Springer Nature Switzerland. doi: https://doi.org/10.1007/978-3-031-23633-4_17↩︎\nDavis, S. E., Greevy Jr, R. A., Lasko, T. A., Walsh, C. G. and Matheny, M. E. Detection of calibration drift in clinical prediction models to inform model updating. Journal of biomedical informatics (2020) doi: https://doi.org/10.1016/j.jbi.2020.103611.↩︎"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/07/ai-series-3.html",
    "href": "foundation-frontiers/posts/2024/05/07/ai-series-3.html",
    "title": "AI series: Healthy datasets for optimised AI performance",
    "section": "",
    "text": "In Charles Babbage’s Passages from the Life of a Philosopher he recalls two incidents where he is asked, “Pray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out?” Reflecting on these incidents he comments, “I am not able rightly to apprehend the kind of confusion of ideas that could provoke such a question.” Similarly, accurate and clean data is at the core of a functional AI model. However, ensuring accuracy of input data to avoid any “wrong figures” creeping into training datasets used to train AI models, demands meticulous attention during the stages of data wrangling, cleaning, and curation.\nThis necessity is particularly pronounced when dealing with the vast datasets used for training machine learning algorithms which are at the core of AI models. The predictive power of these models are highly dependent on the quality of the training data. 1 The most obvious errors which often require meticulous attention at data processing stages are duplication, missingness and data imbalance (Figure 1). The presence of any of these errors can exert multifaceted impacts both in the training and testing stage of machine learning algorithms that are at the core of any AI models, and the challenge does not end there. The provenance, content, format and structure of the data require attention as well. Even data that is essentially correct may be “wrong” for a particular data set."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/07/ai-series-3.html#obviating-the-obvious-errors",
    "href": "foundation-frontiers/posts/2024/05/07/ai-series-3.html#obviating-the-obvious-errors",
    "title": "AI series: Healthy datasets for optimised AI performance",
    "section": "Obviating the obvious errors",
    "text": "Obviating the obvious errors\nDuplicated records can mask existing diversities within the data, diminishing the representativeness of important subgroups and leading to a biased training set and model outcomes. If duplication originates from data labelling issues, it can lead to fundamental challenges during the training of supervised models. 2 In healthcare data, this issue can arise when linking data across multiple sources where each source holds different labels for the same data. 3\nMissingness directly leads to loss of information required for training the algorithms on various real-world scenarios. It is typically addressed via two primary routes: deletion of missing rows or imputation. Deleting missing rows results in a reduction of the sample size and bias. For instance, when it comes to health data, using electronic medical records from a single health provider such as general practice may give rise to a lot of missingness in other aspects of an individual’s health such as their hospital records, pathology testing data or medical imaging. On the one hand, structured missingness can serve as an informative feature to explore within the data. However in cases where missing data causes pixelation in the comprehensive health picture we are attempting to construct, it often conceals an underlying narrative. 4 For instance, the COVID-19 response involved many initiatives across the AI community, however, during the early stages of the pandemic partial availability of data pixelated the picture and impacted models predictive ability which resulted in a minimal improvement according to the UK’s national centre for data science and AI report. 5\nImputing missing values can preserve the whole sample. However, the introduction of noise during the imputation process may compromise the quality of fitted models, contingent upon the proportion of missing records. One way to offset this may be to use larger and larger data sets that might inevitably include a fuller training picture to the algorithm, just as adding dots to a pixelated image makes it more and more clear what is depicted.\nIt is often perceived that for certain instances, particularly in the context of deep learning models, such as neural networks, the model itself is capable of handling missing values without explicit imputation or deletion procedures. 6 Is this really true in a real-world scenario? Are these models advanced enough to achieve an optimised performance even when data quality is not at an optimised level? 7 Köse et al. (2020) investigated the effect of two conventional imputation approaches: Multiple imputation (MICE) 8 and Factor analysis of mixed data (FAMD) 9 on performance of deep learning models. Their study endorsed the use of such explicit imputation approaches, showing an enhancement in model performance. 10\nData imbalance issues, arise within datasets where there is a disproportionate amount of information pertaining to a specific aspect. When imbalanced data, rich in focused information, is used to train an AI model, the model becomes adept at learning about the specific aspect but may struggle to generalise its findings to diverse scenarios, thus fostering overfitting where the model achieves accurate prediction on the training data but loses accuracy for any new test dataset.\n\n\n\n\n\n\nFigure 1: Stages involved in AI model development.\n\n\n\nOverfitting severely undermines the predictive performance of AI models on data beyond their training set, defeating the primary purpose of these models. For instance, out of all strokes that occur, approximately 85% are ischaemic, caused by blockage of blood supply to part of the brain, and 15% are haemorrhagic, caused by bleeding into the brain. Development of machine-learning and AI-based stroke predictive models are therefore being affected by this natural imbalance in the two types of stroke. 11 This type of imbalance also exists in population wide studies where stroke itself is only present in a minority subsection of a healthy population. 12\nThe circumstances of data collection can lead to bias, so care is needed at early stages to ensure that datasets are representative of the real world. These types of error can be picked up at an initial Quality Assurance (QA) stage conducted to reveal any unexpected errors in data used by AI models. QA checks often involve principle checks on presented values to ensure they are the right data type and within the expected range and have the expected temporal coverage.\nFinally the choice of features included in a data set requires consideration since this can also have implications on an algorithm. Taking another example from the COVID-19 pandemic, a group of researchers trained an algorithm on radiological imaging of COVID-19 patients where the position of the patient during radiology was present as a feature in the dataset. However, since more severe cases were lying down and less severe cases were standing up, the existence of this feature resulted in an algorithm that predicted COVID-19 risk based on the position of the patients. Here although the data included was correct, its inclusion in the dataset proved to be “wrong”."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/07/ai-series-3.html#things-get-complicated",
    "href": "foundation-frontiers/posts/2024/05/07/ai-series-3.html#things-get-complicated",
    "title": "AI series: Healthy datasets for optimised AI performance",
    "section": "Things get complicated",
    "text": "Things get complicated\nWhen AI algorithms encounter complex, unstructured data, the task of quality assurance suddenly balloons beyond tackling the three main errors highlighted above. Such circumstances require some kind of quality enhancement procedure, where datasets in the form of images or unstructured text go through a curation process which involves enhancement of the quality and standardisation of the format to the level required for integration into AI algorithms. This process of standardization of data is paramount across various domains, especially in healthcare where complex, unstructured health data holds transformative potential for AI driven advances that revolutionise diagnosis, treatment, and prognosis of diseases. From electronic health records to magnetic resonance imaging (MRI) scans and genetic sequences, this data offers a wealth of insights for AI models to learn from. Adopting standardised formats not only facilitates seamless integration of diverse datasets but also streamlines the development and deployment of AI models. However, unlocking this potential requires a strong foundation in high-quality, processed data which begins with standardisation.\nOne category of complex health data is neuroimaging of which a prime example is MRI. Different institutions will often employ different acquisition protocols and different ways of collecting and storing neuroimaging data. Above all, this can make it very difficult to integrate into existing workflows and processing pipelines, but it also makes it challenging to understand, compare and combine with other datasets. To address these challenges, the neuroimaging community has adopted the Brain Imaging Data Structure (BIDS) 13 – a standardised format for organising and naming MRI data which allows compliant data to integrate smoothly with existing workflows and AI models, streamlining processing and analysis. By embracing standardisation, we can pave the way for common processing tools to enable the generation of AI-ready data.\nNext, comes pre-processing. Sticking with the neuroimaging example, MRI scans are susceptible to various forms of noise and artifacts, which can appear as blurring or distortions which, without proper processing, can be misinterpreted by AI models. Pre-processing typically includes steps for spatial normalisation and image registration, involving alignment of brain images from different individuals into a common reference model and alignment of different images of the same subject to a common template. This standardisation facilitates inter-subject and inter-study comparisons, enabling AI models to generalise effectively across diverse datasets. However, the multi-layer aspect of this process means that aligning data to a common template is dependent on the choice of template which itself can introduce bias if the template brain doesn’t accurately reflect the patient’s anatomy (due to age, ethnicity, or disease for example).\n\n\n\n\n\n\nFigure 2: Neuroimaging data.\n\n\n\nOnce pre-processing is complete, you may want to combine datasets to increase sample sizes for your AI model. This is where harmonisation techniques 14 15 come in to deal with inconsistencies and variations in acquisition which can add noise and bias into a model. A typical technique for harmonisation in neuroimaging, known as ComBat 16, works by modelling data using a hierarchical Bayesian model and followed by empirical Bayes to infer the distribution of the unknown factors. The method is actually borrowed from genomics data but is applicable to situations where multiple features of the same type are collected for each participant, whether that be expression levels for genes, or imaging derived measures such as volumes from different regions. This is a crucial step for combining datasets to enable AI models to focus on learning the actual relationships within the data rather than struggling with inconsistencies across datasets. It also leads to models which can generalise better on unseen data."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/07/ai-series-3.html#feeding-hungry-algorithms",
    "href": "foundation-frontiers/posts/2024/05/07/ai-series-3.html#feeding-hungry-algorithms",
    "title": "AI series: Healthy datasets for optimised AI performance",
    "section": "Feeding hungry algorithms",
    "text": "Feeding hungry algorithms\nThe public good is at the heart of AI driven approaches and indeed, the aim is to develop models with optimised predictive ability that can be generalised to many scenarios. For this to be achieved a large and diverse training source is required. This is often referred to as data hungry algorithms. To provide a large amount of enriched training data for optimised model development two main approaches have been explored: federated analytics and synthetic data.\nFederation is when data from multiple sources is made available for training and analysis of models designed to run on data that is not held in a single place, nominally called distributed models. It provides the opportunity to test algorithms in different populations/settings to ensure generalisability. In the context of patient-level health data, the data is often held institutionally. Enabling federation and trustworthy sharing of these datasets requires extensive attention to governance models and a common model between multiple organisations is a known catalyst of this process 17 18\nGenerating synthetic data 19 from original data sources is a resource intensive mechanism. It requires the development of models on the real data to learn existing patterns, formats, and statistical properties within the original data from which it is possible to generate further synthetic versions of these data. When working with sensitive data such as health records, ensuring patient data is safe and secure is covered by information governance. Depending on how close the synthetic data source is to the original data, the same governance level may still be applicable when trying to bring individual/patient data from multiple sources together. A suggested solution to overcome the governance challenges in the context of synthetic data is to use a low-fidelity version of the original data which means a level of bias has been added throughout the synthesisation process to ensure safety and security of individual level data. 20 While the low fidelity data sources are generated based on real data, it is worth noting that the rise in generative AI also poses a concern for data pollution, particularly where AI tools such as Gretel.ai 21 are used to generate synthetic data which may also be used to train AI models – the problematic case of AI training AI!\nWhen using sensitive health data of patients a further layer is in place to ensure security of access. These are called Trusted Research Environments (TREs), secure technology infrastructures which play a crucial role in consolidating disparate data collections into a centralised repository, facilitating researcher access to data for scientific exploration. However, integrating data from various sources into AI models poses challenges due to differences in data collection methods and formats, hindering computational analysis. In response, the FAIR (Findable, Accessible, Interoperable, Reusable) data principles were introduced in 2016 to enhance the reusability of scientific datasets by humans and machines. 22 Adoption of FAIR principles within TREs ensures well-documented, curated, and harmonised datasets, addressing issues raised above such as duplicated records and missing data. 23 Additionally, preprocessing pipelines within TREs streamline data standardisation, creating “AI research-ready” datasets. 24\nAccess to real-world healthcare data remains challenging, prompting the development of AI models on open-source or synthetic datasets. However, these models often exhibit performance discrepancies when applied to real world data 25 It is therefore imperative to provide researchers with secure access to real-world healthcare data within TREs, bolstered by robust governance and support mechanisms. Initiatives like the GRAIMATTER study 26 and AI risk evaluation workshops 27 exemplify efforts to facilitate AI model development and translation from TREs to clinical settings. By establishing governance guidelines and promoting FAIR datasets, TREs aim to become important resources for the AI research community. Providing standardised and curated data rich repositories that AI models can be developed on is a top priority in UK-TREs. Given the well-defined and secure governance environments of TREs they may also provide the basis for federated data analysis allowing researchers to combine datasets across TREs/data environments. In this way they can provide the large numbers that data hungry algorithms require, while avoiding the wide-ranging and myriad ways that data for a specific dataset can be “wrong”."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/07/ai-series-3.html#also-in-the-ai-series",
    "href": "foundation-frontiers/posts/2024/05/07/ai-series-3.html#also-in-the-ai-series",
    "title": "AI series: Healthy datasets for optimised AI performance",
    "section": "Also in the AI series:",
    "text": "Also in the AI series:\nWhat is AI? Shedding light on the method and madness in these algorithms Generative AI models and the quest for human-level artificial intelligence\n\nExplore more data science ideas\n\n\n\n\n\nAbout the authors\n\nFatemeh Torabi is Senior Research Officer and Data Scientist, at Swansea University and works on Health Data Science and Population Data Science for the Dementias Platform UK.\n\n\nLewis Hotchkiss is a Research Officer in Neuroimaging at Swansea University and works on Population Data Science for the Dementias Platform UK.\n\n\nEmma Squires is the Data Project Manager for Dementias Platform UK based at Swansea University and works on Population Data Science\n\n\nProf. Simon E. Thompson is Deputy Associate Director of the Dementias Platform UK\n\n\nProf. Ronan A. Lyons is the Associate Director of the Dementias Platform UK based at Swansea University and works on Population Data Science.\n\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. \n\n\n\nHow to cite\n\nTorabi, Fatemeh, Hotchkiss, Lewis, Squires, Emma, Thompson, Simon E. and Lyons, Ronan A. 2024. “Getting the data right for optimised AI performance.” Real World Data Science, May 7, 2024. URL"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/07/ai-series-3.html#footnotes",
    "href": "foundation-frontiers/posts/2024/05/07/ai-series-3.html#footnotes",
    "title": "AI series: Healthy datasets for optimised AI performance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLi, P. et al. CleanML: A Benchmark for Joint Data Cleaning and Machine Learning [Experiments and Analysis]↩︎\nAzeroual, O. et al. A Record Linkage-Based Data Deduplication Framework with DataCleaner Extension. Multimodal Technol. Interact. 2022, Vol. 6, Page 27 6, 27 (2022).↩︎\nRajpurkar, P., Chen, E., Banerjee, O. & Topol, E. J. AI in health and medicine. Nat. Med. 2022 281 28, 31–38 (2022).↩︎\nMitra, R. et al. Learning from data with structured missingness. (2023).↩︎\nAlan Turing Institution. Data science and AI in the age of COVID-19. 2020 https://www.turing.ac.uk/sites/default/files/2021-06/data-science-and-ai-in-the-age-of-covid_full-report_2.pdf↩︎\nHan, J. & Kang, S. Dynamic imputation for improved training of neural network with missing values. Expert Syst. Appl. 194, 116508 (2022).↩︎\nKöse, T. et al. Effect of Missing Data Imputation on Deep Learning Prediction Performance for Vesicoureteral Reflux and Recurrent Urinary Tract Infection Clinical Study. Biomed Res. Int. 2020, (2020).↩︎\nAzur, M. J., Stuart, E. A., Frangakis, C. & Leaf, P. J. Multiple imputation by chained equations: what is it and how does it work? Int. J. Methods Psychiatr. Res. 20, 40–49 (2011).↩︎\nAudigier, V., Husson, F. & Josse, J. A principal component method to impute missing values for mixed data. Adv. Data Anal. Classif. 10, 5–26 (2016).↩︎\nKöse, T. et al. Effect of Missing Data Imputation on Deep Learning Prediction Performance for Vesicoureteral Reflux and Recurrent Urinary Tract Infection Clinical Study. Biomed Res. Int. 2020, (2020).↩︎\nLiu, T., Fan, W. & Wu, C. A hybrid machine learning approach to cerebral stroke prediction based on imbalanced medical dataset. Artif. Intell. Med. 101, 101723 (2019).↩︎\nKokkotis, C. et al. An Explainable Machine Learning Pipeline for Stroke Prediction on Imbalanced Data. Diagnostics 2022, Vol. 12, Page 2392 12, 2392 (2022).↩︎\nGorgolewski, K. J. et al. BIDS apps: Improving ease of use, accessibility, and reproducibility of neuroimaging data analysis methods. PLoS Comput. Biol. 13, (2017).↩︎\nBauermeister, S. et al. Research-ready data: the C-Surv data model. Eur. J. Epidemiol. 38, 179–187 (2023).↩︎\nAbbasizanjani, H. et al. Harmonising electronic health records for reproducible research: challenges, solutions and recommendations from a UK-wide COVID-19 research collaboration. BMC Med. Inform. Decis. Mak. 23, 1–15 (2023).↩︎\nOrlhac, F. et al. A Guide to ComBat Harmonization of Imaging Biomarkers in Multicenter Studies. J. Nucl. Med. 63, 172 (2022).↩︎\nToga, A. W. et al. The pursuit of approaches to federate data to accelerate Alzheimer’s disease and related dementia research: GAAIN, DPUK, and ADDI. Front. Neuroinform. 17, 1175689 (2023).↩︎\nTorabi, F. et al. A common framework for health data governance standards. Nat. Med. 2024 1–4 (2024) doi:10.1038/s41591-023-02686-w.↩︎\nTucker, A., Wang, Z., Rotalinti, Y. & Myles, P. Generating high-fidelity synthetic patient data for assessing machine learning healthcare software. npj Digit. Med. 2020 31 3, 1–13 (2020)↩︎\nTucker, A., Wang, Z., Rotalinti, Y. & Myles, P. Generating high-fidelity synthetic patient data for assessing machine learning healthcare software. npj Digit. Med. 2020 31 3, 1–13 (2020)↩︎\nNoruzman, A. H., Ghani, N. A. & Zulkifli, N. S. A. Gretel.ai: Open-Source Artificial Intelligence Tool To Generate New Synthetic Data. MALAYSIAN J. Innov. Eng. Appl. Soc. Sci. 1, 15–22 (2021).↩︎\nWilkinson, M. D. et al. The FAIR Guiding Principles for scientific data management and stewardship. Sci. Data 2016 31 3, 1–9 (2016).↩︎\nChen, Y. et al. A FAIR and AI-ready Higgs boson decay dataset. Sci. Data 9, (2021).↩︎\nEsteban, O. et al. fMRIPrep: a robust preprocessing pipeline for functional MRI. Nat. Methods 16, 111–116 (2018).↩︎\nAlkhalifah, T., Wang, H. & Ovcharenko, O. MLReal: Bridging the gap between training on synthetic data and real data applications in machine learning. Artif. Intell. Geosci. 3, 101–114 (2022).↩︎\nJefferson, E. et al. GRAIMATTER Green Paper: Recommendations for disclosure control of trained Machine Learning (ML) models from Trusted Research Environments (TREs). doi:10.5281/ZENODO.7089491.↩︎\nDARE UK Community Working Group - DARE UK. https://dareuk.org.uk/dare-uk-community-working-groups/dare-uk-community-working-group-ai-risk-evaluation-working-group/.↩︎"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/04/22/ai-series-1.html",
    "href": "foundation-frontiers/posts/2024/04/22/ai-series-1.html",
    "title": "AI series: What is AI? Shedding light on the method and madness in these algorithms",
    "section": "",
    "text": "What do defence cases in litigation, statistical analyses, book summaries and a description of Young’s double slit experiment in the manner of poet Robert Burns have in common? They are all tasks that people have rightly or wrongly attempted to delegate to large language models.\nThe playground of generative AI algorithms based on large language models extends well beyond the space of generating text-based language but includes creating images, videos and even music from prompts. The capabilities of these algorithms, and the ubiquity of tasks that large language models like OpenAI’s Generative Pre-trained Transformer (GPT) models can have a go at is striking. These large language models and the chatbots and so on based on them have also been catapulted into the centre of mainstream public attention with huge success – who has not heard of ChatGPT? The net result has been something akin to a feeding frenzy as individuals and businesses alike strive to be among the first to benefit from them.\nLike others, many data scientists closely familiar with these kinds of algorithms share some enthusiasm for their potential utility, but many also advocate an element of caution. There are some obvious caveats, including accuracy and cost – not just financially but also in terms of the huge energy costs to run these algorithms, a real world consequence that is affecting the planet in the present day but is often eclipsed by fears that AI might take over the world some time in the future. Another concern is security. Should you be sharing the information you are working from with a third party anyway? However, while a lot of attention has focused on what these algorithms can do, fewer have been asking what they actually do – what we know about the initial programming, the training data, the final algorithm and the range of possible outputs, all of which provide useful pointers as to whether a particular algorithm is appropriate for the task in hand, and how best to benefit from it."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/04/22/ai-series-1.html#demystifying-machine-learning",
    "href": "foundation-frontiers/posts/2024/04/22/ai-series-1.html#demystifying-machine-learning",
    "title": "AI series: What is AI? Shedding light on the method and madness in these algorithms",
    "section": "Demystifying machine learning",
    "text": "Demystifying machine learning\nDefinitions of artificial intelligence vary, often circling around the theme of a system reaching an “intelligent” decision or output based on multiple inputs, although how “intelligent” might be defined can be hairier still. Nonetheless, there is currently largely a consensus that some kind of machine learning is a route to achieving it. Through machine learning “you are letting the computer adjust the importance of its inputs, and their relationships, to determine an appropriate output” as Napier chief data scientist and chair of the Royal Statistical Society DS & AI Section Janet Bastiman describes it. The term “machine learning” was first proposed by IBM scientist Arthur Samuel in 1952, and it has largely been achieved by two approaches. One is “random forests”, based on constructing multiple decision trees. The other is the neural networks first devised by American psychologist Frank Rosenblatt and simulated at IBM in 1957. Here, a set of artificial neurons – components closer to a capacitor than a biological neuron – is connected to another layer of neurons, which is connected to another layer of neurons, and so on (Figure 1). Crucially the connections are strengthened or not through “learning” based on training data that allows the network to recognise patterns and extract meaningful features.\n\n\n\n\n\n\nFigure 1: A schematic of a neural network depicted with circles connected with lines or arrows.\n\n\n\n“Machine learning is just like linear regression with tonnes of bells and whistles,” says Daniela Witten, professor of mathematical statistics at the University of Washington in the US, referring to a statistical method for fitting a line to a set of data points that dates back over a hundred years. There are many other traditional approaches to statistical learning that may be nonlinear and so on, but as an example of the “bells and whistles” Witten describes, whereas a traditional regression model might have 5 inputs or variables, the machine learning version might have 15 million, and instead of assuming it is linear the fit is allowed to be more flexible and so on. However, the fundamental statistical ideas underlying both sets of models are the same. For this reason, although some may beg to differ, she feels doing machine learning before you understand statistics is like trying to jump rope before you can walk. “It’s not that you can’t do it but why would you?” she adds.\nBroadly speaking, machine learning can be classified two ways. One is “supervised”, which means that the training data is somehow labelled, for instance with a known output collected from real world records. The alternative is “unsupervised” where the algorithm is set the task of finding a way to learn relationships between input data itself. There are also neural network approaches that fall somewhere between the two, such as reinforcement learning where an algorithm may generate outputs for a task at random such that its performance is initially poor but improves with feedback to reinforce generation of outputs that are closer to those desired. An approach that enjoyed great popularity for a time used another machine learning algorithm to provide this feedback, which would initially also be a poor judge but improve as pitted against the algorithm learning to do the task – generative adversarial networks (GANs). GANs are still used a lot, but usually in a pipeline and may be pre-trained so they are not starting from scratch like they used to.\nAs the number of layers increased from just a few, the term “deep learning” was adopted along with alternative structures. The first models operated with every neuron in each layer connected to every neuron in the previous layer. “This is very wasteful because not every part of your input relates to each other that much,” says Petar Veličković, staff research scientist at Google DeepMind and affiliated lecturer at the University of Cambridge. He cites images as among the first scenarios where people began to implement a tweak to the approach in what is called a convolutional neural network. Based on the assumption that the pixels for each object in an image sit adjacent to each other rather than at opposing corners of the image, the neurons in a convolutional neural network connect only with the neurons in the next layer that are nearby in the image space. In this way the convolutional neural network assumes a kind of structure in the input data – that the image is contiguous, so the pixels for edges and so on are in contact.\n\n\n\n\n\n\nFigure 2: Schematics for (left) Scaled Dot-Product Attention and (right) Multi-Head Attention, which consists of several attention layers running in parallel.\n\n\n\n“Transformers are also a neural network but they encode a different kind of structure,” Veličković tells Real World Data Science, as he describes the data architecture at the heart of the large language models creating such a buzz at present. Language has structure too – the letters make up words, which then make up sentences and so on. So it makes sense to program some of that structure into the algorithm rather than leaving it to work it all out. “You would need a lot more training data than there is on the internet to train a system without such structure by itself,” adds Veličković. Transformers structure the training data into tokens, and a key component first reported in 2017 is the way each token then connects with or “attends” to all other similar tokens . Here whether they are “similar” is determined by their dot products, a multiplication technique for the kind of vector format of input numbers used for these tokens (Figure 2). Exploiting this “dot product attention” significantly improves the efficiency of the training process."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/04/22/ai-series-1.html#taking-the-world-by-storm",
    "href": "foundation-frontiers/posts/2024/04/22/ai-series-1.html#taking-the-world-by-storm",
    "title": "AI series: What is AI? Shedding light on the method and madness in these algorithms",
    "section": "Taking the world by storm",
    "text": "Taking the world by storm\nThe transformer architecture proved very powerful as has been seen in the surge to prominence of various AI systems based on generative pre-trained transformer algorithms, such as ChatGPT, BERT and PaLM, although this likely has at least as much to do with the marketing of the recent releases as it does with the algorithm itself. “It was a small evolution rather than a revolution,” says Bastiman in reference to recent GPT releases, explaining that there was an increase in parameter size and the amount of data used for training that gave rise to something that could provide broader answers and was ready for mass market. Nonetheless she adds, “There had been GPT2, GPT1 and all the other previous ones had been released quietly and had all been quite good.”\nThe marketing spin has not stopped with the product releases as terms like “hallucination” have entered the lexicon to describe instances when the output is wrong and potentially dangerous. (Figure 3) “The language we are using to describe these models is different to how we describe human intelligence to deliberately instil the sense this is better,” adds Bastiman. “So even if the model is incorrect these terms imply that it is still doing something amazing.”\n\n\n\n\n\n\nFigure 3: “Hallucinated” references. A study found that out of the 178 references cited by ChatGPT, 69 did not have a DOI. Upon extensive internet search, 41 out of these 69 reference articles were found to exist.\n\n\n\nThe success of this marketing does have its advantages as Veličković points out, thrusting AI in the spotlight, inviting people to try the algorithms, which thanks to a growth in web-based user interfaces like ChatGPT can reach a much broader audience. This is not only encouraging developers they are doing something potentially useful but prompting important discussions around the potential bias and ethics issues, which many would argue ought to be considered before anything else. Nonetheless Veličković also doubts whether the current AI fanfare can be attributed to advances in the algorithms they use alone, pointing out that neural networks have been around since the 1950s, and the 1980s and 1990s saw the invention of most of the building blocks we need to scale such systems up: the backpropagation algorithm, convolutional and recurrent neural networks, long-short term memory networks, and early variants of linear self-attention and graph neural networks. “It’s just that we needed gamers,” he tells Real World Data Science, suggesting that hardware and engineering have been key to the recent successes of AI. “We needed people to drive the development of graphics cards which are really good hardware for training these things.”\nClearly advances in processing power and the hardware such as GPUs to manage it so that it is possible to compute these algorithms massively affects their potential impact. Although the field no longer relies on GPUs developed for gamers, GPUs are still widely used, as they offer such a good return on investment and are easier to get hold of than alternatives like tensor processing units. Certainly a significant development over the past decade or so is the increase in size of not just the data sets but the algorithms themselves. Implementing algorithms at such colossal scales that require data centres imposes incredibly challenging requirements on the hardware and the electrical and computational engineering involved to set them running and keep them from failure. “When you have a data centre, failure is a common thing,” says Veličković, listing multiple vulnerabilities that balloon at scale such as hardware failures, electrical failures, even apparently exotic events like solar flares can flip bits and scramble data leading to nonsense output. “People underestimate this but good engineering is now the bread and butter of how these systems work.”"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/04/22/ai-series-1.html#managing-expectations",
    "href": "foundation-frontiers/posts/2024/04/22/ai-series-1.html#managing-expectations",
    "title": "AI series: What is AI? Shedding light on the method and madness in these algorithms",
    "section": "Managing expectations",
    "text": "Managing expectations\nThe explosion in scale has also created fundamental distinctions from how people work with machine learning algorithms versus statistical methods. Witten highlights “the ability to gauge uncertainty” by quantifying parameters such as confidence intervals and error bars as a key contribution of statistics. “Often with these machine learning models things get very complicated and we do not yet have a way to quantify that uncertainty.”\nThis quantifying of finite parameters contrasts with the kind of output achieved with the generative AI applications that have grabbed media focus recently. For instance, asking a large language model to describe Young’s double slit experiment in the style of Robert Burns may sound quite a specific prompt, and it may seem impressive if the algorithm returns something akin to what was asked, but the number of possible responses that could be deemed “correct” are infinite. A lot of applications of generative AI – many with more real world impact than describing iconic experiments in archaic scotch rhyme – similarly have a vast set of reasonable outputs.\n\n\n\nGamers drove the development of GPUs which have proven invaluable for training machine learning algorithms\n\n\n“We shouldn’t be surprised if ChatGPT does well with a question that has a million reasonable answers,” says Witten, contrasting these scenarios with questions that she suggests might have more real-world importance, like whether a patient with breast cancer will respond to a particular treatment. “Actually ChatGPT often gets into trouble if there is a problem with just one answer, and that answer is not part of the training set.”\nFor predictive AI there is often only one useful answer – the outcome that will come to pass. This has implications if machine learning is used for predictions, particularly if it is in real world settings that affect real people. “If you are deploying an AI model for some healthcare application like what breast cancer treatment you are going to respond best to, we really better make sure that the model works, and that we understand the uncertainty of those predictions,” says Witten. She feels that over the past few years, the machine learning community has increasingly recognized the importance of bringing statistical thinking to bear within the context of complex machine learning/AI algorithms: in particular, interpretability and uncertainty quantification have become major areas of interest in machine learning. Witten suggests that statistics is making progress here citing as an example conformal inference, “which allows recalibration of the predictions of a machine learning model in order to quantify uncertainty appropriately.”"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/04/22/ai-series-1.html#explain-yourself",
    "href": "foundation-frontiers/posts/2024/04/22/ai-series-1.html#explain-yourself",
    "title": "AI series: What is AI? Shedding light on the method and madness in these algorithms",
    "section": "Explain yourself",
    "text": "Explain yourself\nUnderstanding the uncertainties of output is one thing, but many of these algorithms have now reached the kind of scale that totally obfuscates what they are doing with the input data to reach their outputs. There may be specialists who understand how they are programmed but there are just too many variables to track so that even for them, the final process the algorithm lands on for generating its output from the various inputs is a black box with no neat mathematical description, unlike statistical techniques like regression.\n“You can draw a picture with circles and arrows, and arrows cross in a certain way, but you don’t have a clear idea of how one feature that you started with maps to the output,” says Witten. “On an actual quantitative level of scientific understanding we don’t have that.” If decisions are being made for and about people based on AI, people will also sometimes want to know how that conclusion was drawn. “When we want to make decisions there’s a level of deferred trust,” says Bastiman citing a work by the Alan Turing Institute that began in the late 2010s. “We as humans want explanations from machines in the same scenarios that we want them from humans but that’s not going to be the same for all people.” For example, a person who has had a bad experience in the past will need more convincing than one who has not. Janet suggests that a very normal cognitive bias can be generalised as most people wanting more explanation if the model output is not in their favour. “Similarly, a person accepted for a job where AI is used, may not require any explanation, while another candidate the AI rejected may challenge the decision and want to know why.”\nHybrid implementations including a human in the loop may help to a degree. However, to get a handle on the workings of the algorithm itself, Bastiman points out that it is possible to introduce layers in the algorithm that will help extract how the output is reached even for unsupervised neural networks. “That’s where a lot of effort goes from data scientists and machine learning engineers to make sure the model has that level of transparency and makes sense,” she adds, emphasising the need to ensure a model has these features before it is released and put in use. The process is far from straightforward as the explanation needs to be at the right level and with the right terminology for a range of audiences, be they data scientists, quality assurance professionals, decision makers, end users or impacted individuals. “People say you can’t explain things when what they really mean is that it’s difficult.”\nVeličković suggests a lot could be gained in terms of being able to analyse AI algorithms by marrying them with elements of classical algorithms, which are “nicely implemented and interpretable.” Classical algorithms are also impervious to changes in the input data such as an increase by a factor of 10, which can completely throw an algorithm based on machine learning. “The problem is they are trained to be really useful and give you an answer at all times so they won’t even give you a confidence estimate, they will just try to answer even if the answer is completely broken,” he adds. A lot of his research has focused on “out-of-distribution generalisation” – the way classical algorithms work with any input data – to see how these features might be sewn into AI to extract the best of both worlds. “There’s a lot of research to be done still but our findings so far indicate that if you want out-of-distribution generalisation you need to look at what makes your problem special and put some of those properties inside your neural network model.”\nEven what we know about the way the algorithm reaches a decision has caused concern when it comes to critical real-world applications – for example, to determine the likelihood that someone convicted of a crime will reoffend. (More on this to come in the special issue article on ethics). With many commercial algorithms the details of the training data are unknown or essentially constitute the whole internet, which as Witten points out is “a pretty bad place a lot of the time.” While ChatGPT may seem an unlikely choice for anything like gauging risk for recidivism or cancer treatments, concerns remain over biases propagating in AI generated content we might consume through marketing campaigns and other activities. “Even just thinking about deploying AI/machine learning models in critical real-world settings without the associated statistical understanding is just very deeply problematic,” says Witten, emphasising the importance of not just statisticians but also ethicists for tackling these challenges.\nThe fact is many of us are already interacting with multiple machine learning/AI models on a daily basis through recommendations, search engines and predictive text. “If we are going to deploy these [machine learning algorithms] at scale in a way that will affect human lives, then we first need to understand the implications for humans of these models,” says Witten. “This includes both statistical and ethical considerations.”\nComing up: Forthcoming articles in this special issue will look at machine learning and human-level intelligence, issues around data, techniques for evaluation, gauging workforce impact, governance, best practice and living with AI"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/04/22/ai-series-1.html#also-in-the-ai-series",
    "href": "foundation-frontiers/posts/2024/04/22/ai-series-1.html#also-in-the-ai-series",
    "title": "AI series: What is AI? Shedding light on the method and madness in these algorithms",
    "section": "Also in the AI series",
    "text": "Also in the AI series\nGenerative AI models and the quest for human-level artificial intelligence datasets for optimised AI performance\n\nExplore more data science ideas\n\n\n\n\n\nAbout the authors\n\nAnna Demming is a freelance science writer and editor based in Bristol, UK. She has a PhD from King’s College London in physics, specifically nanophotonics and how light interacts with the very small, and has been an editor for Nature Publishing Group (now Springer Nature), IOP Publishing and New Scientist. Other publications she contributes to include The Observer, New Scientist, Scientific American, Physics World and Chemistry World.\n\n\n\n\n\n\nCopyright and licence\n\n© 2024 Anna Demming\n\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail image courtesy of Serenechan3 reproduced under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence\n\n\n\nHow to cite\n\nDemming Anna. 2024. “What is AI? Shedding light on the method and madness in these algorithms .” Real World Data Science, April 22, 2024. URL"
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2023/03/02/basket-complementarity.html",
    "href": "foundation-frontiers/datasciencebites/posts/2023/03/02/basket-complementarity.html",
    "title": "Using ‘basket complementarity’ to make product recommendations",
    "section": "",
    "text": "Anyone who has ever worked in a retail store will be familiar with the concept of cross-selling. A customer wants a can of paint? Try to sell them some paintbrushes. That new cellphone they’ve just decided to buy? They’ll probably need a case to protect it. Online retailers (and digital services of all sorts) have taken this idea and run with it, to great success. Sophisticated algorithms sort through data on a customer’s past transactions, and those of similar-looking customers, to identify and recommend other products a customer might be interested in.\nA large amount of cross-selling, whether attempted in store by a sales assistant or online by an algorithm, relies on the concept of complementarity: that certain products are often bought and/or used together. Relationships between products might be obvious – paint and paintbrushes, for example – or they may be obscure and only revealed through the analysis of large datasets. In a 2021 paper that highlights complementarity’s relevance to association analysis, Puka and Jedrusik put forward “a new measure of complementarity in market basket data”, which sheds light on how product recommendations can be derived.\nInspired by complementarity-based ideas prevalent in microeconomics, Puka and Jedrusik begin by collecting some established ideas from traditional market basket analysis, the key one being “confidence”. In this case, we’re talking about the confidence that item A leads (in a way) to item B (which we can express in notation as conf({A} → {B})). Take a look at Table 1 (below), which presents a numbered list of 18 shopping trips, with details of what was purchased on each trip. Notice how two of the trips (1 and 3) resulted in sales of both milk (B) and cornflakes (A), while five trips (1, 3, 7, 17, and 18) had cornflakes. Under the assumption that someone already has cornflakes in their trolley, the probability that they will buy milk is 2/5 = 0.4. So, conf({cornflakes} → {milk}) = 0.4. The closer this number gets to one, the more automatic the cornflakes–milk connection becomes. This number can therefore be used to recommend an item that is related in some way to another already bought."
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2023/03/02/basket-complementarity.html#asymmetry-and-tolerance",
    "href": "foundation-frontiers/datasciencebites/posts/2023/03/02/basket-complementarity.html#asymmetry-and-tolerance",
    "title": "Using ‘basket complementarity’ to make product recommendations",
    "section": "Asymmetry and tolerance",
    "text": "Asymmetry and tolerance\nMilk and cornflakes are reasonably complementary, and we can see from Figure 1 above that, regardless of whether you start by picking up milk or cornflakes, the probabilities of a shopper buying the other item are broadly similar: conf({cornflakes} → {milk}) = 0.4, while conf({milk} → {cornflakes}) = 0.33. There is a small amount of asymmetry in the probabilities in this particular example, but asymmetry can be more extreme for other pairs of items. This leads to the idea of one- and two-sided complementarity. Two items sharing a smallish asymmetry – like milk and cornflakes – will be connected through two-sided complementarity, while large asymmetries indicate one-sided complementarity. Such imbalances will be quite common when, for instance, items of hugely different prices are involved. When someone buys a house, for example, they may want to buy a bookcase, but buying a bookcase doesn’t mean someone wants to buy a house: this would be an instance of one-sided complementarity.\nPuka and Jedrusik capitalize on this observation. They define two items to be “basket complementary” if the two probabilities – the normal and its opposite – remain close and reasonably high. The items need to share a bond that is blind to the direction: seeing you bought one, no matter which, means you are (almost equally) likely to buy the other.\nIt is rare that the two probabilities should be exactly the same, of course, and the authors allow some deviation. Along the red diagonal line of perfect equality (Figure 1) we may lay tolerance bands marking degrees of product inseparability. This, if need be, may lead to the notion of being complementary at such-and-such a tolerance level – 0%, 1%, 5%, etc. – generating a score of sorts. In cases where a dot representing the two-way dependencies between two items falls within a narrow band – corresponding to a smaller tolerance – the more inseparable the items are, and the more sensible a cross-selling recommendation may become."
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2023/03/02/basket-complementarity.html#in-conclusion",
    "href": "foundation-frontiers/datasciencebites/posts/2023/03/02/basket-complementarity.html#in-conclusion",
    "title": "Using ‘basket complementarity’ to make product recommendations",
    "section": "In conclusion",
    "text": "In conclusion\nA large part of the world we inhabit, particularly the economy, is powered by recommendations: from strangers, friends and algorithms. That applies not only to the things we buy but also to the things we watch or read. (Perhaps you arrived at this article because of a tweet that Twitter thought you might like, or maybe it was suggested to you by Google News because of your past reading habits.) Whatever the intent of these recommendations, the key challenge is in knowing which two things are functionally or thematically intertwined. Which item or product is, by default, synonymous with which? Puka and Jedrusik deliver an answer: two items that are basket complementary to each other, preferably at a slim tolerance, are inextricably linked. One may be safely offered – perhaps always – whenever the other is already in the shopping basket.\nThe relative simplicity and interpretability of basket complementary may provide small-scale retailers, starved of analytical wherewithal, a sane and safe strategy for developing their product offer. It might also serve as a benchmark to keep other, more sophisticated recommendation algorithms in check. (In weather forecasting, for example, it is often seen that naive benchmarks – such as using today’s temperature to predict tomorrow’s – frequently outperform more advanced models.)\nBasket complementarity could also be used to help individuals understand their own shopping habits and the links between the things they buy. I’ve built an interactive dashboard where you can enter your own receipt lists and filter associations based on various confidence thresholds. The underlying code is also available.\n \n\n\n\n\nAbout the author\n\nMoinak Bhaduri is assistant professor in the Department of Mathematical Sciences, Bentley University. He studies spatio-temporal Poisson processes and others like the self-exciting Hawkes or log-Gaussian Cox processes that are natural generalizations. His primary interest includes developing change-detection algorithms in systems modeled by these stochastic processes, especially through trend permutations.\n\n\n\n\n\nAbout DataScienceBites\n\nDataScienceBites is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to become a contributor.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Moinak Bhaduri\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nBhaduri, Moinak. 2023. “Using ‘basket complementarity’ to make product recommendations.” Real World Data Science, March 2, 2023. URL"
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2023/07/13/choosing-right-forecast.html",
    "href": "foundation-frontiers/datasciencebites/posts/2023/07/13/choosing-right-forecast.html",
    "title": "Choosing the right forecast",
    "section": "",
    "text": "Nobel laureate Niels Bohr is famously quoted as saying, “Prediction is very difficult, especially if it’s about the future.” The science (or perhaps the art) of forecasting is no easy task and lends itself to a large amount of uncertainty. For this reason, practitioners interested in prediction have increasingly migrated to probabilistic forecasting, where an entire distribution is given as the forecast instead of a single number, thus fully quantifying the inherent uncertainty. In such a setting, traditional metrics of assessing and comparing predictive performance, such as mean squared error (MSE), are no longer appropriate. Instead, proper scoring rules are utilized to evaluate and rank forecast methods. A scoring rule is a function that takes a predictive distribution along with an observed value and outputs a real number called the score. Such a rule is said to be proper if the expected score is maximized when the predictive distribution is the same as the distribution from which the observation was drawn.\nMany proper scoring rules exist, such as the continuous ranked probability score (CRPS) and the logarithmic score. Choosing which rule to use is not necessarily straightforward. Furthermore, forecast methods are often selected not based on a single score, but rather averages of scores from many probabilistic forecasts, which can introduce new challenges affecting how one might rank competing forecasts. In the paper under discussion, Bolin and Wallin define several properties of scoring rules that help clarify how the rules behave when multiple forecast scores are averaged. Additionally, they introduce a new class of proper rules that aims to overcome some of the deficiencies of other common scoring rules.\n\n\n\n\n\n\nAbout the paper\n\n\n\n\n\nTitle: Local scale invariance and robustness of proper scoring rules\nAuthor(s) and year: David Bolin and Jonas Wallin (2023)\nStatus: Published in Statistical Science, DOI: 10.1214/22-STS864.\n\n\n\nThe authors argue that situations are often encountered where forecasts are derived and subsequently averaged for observations with different inherent variability. One example might be financial data, such as stock returns, where there are commonly periods with much higher variance (known as volatility in the financial setting). Such processes can be represented using a model known as stochastic volatility, where the variance of observed data evolves randomly over time. Figure 1 plots an example path of the data-generating process under such a model. When data exhibits this varying uncertainty, many proper scoring rules will assign a score whose magnitude changes for those observations with more variability, a characteristic the authors term scale dependence. Some rules will ‘punish’ observations with higher uncertainty, and others may ‘reward’ such observations. Hence, when averaging multiple scores, observations will not be treated symmetrically, which the authors argue can “lead to unintuitive forecast rankings.”\n\n\n\n\n\n\nFigure 1: Left, a time series of volatility, and right, the resulting observations under a standard stochastic volatility model.\n\nThus, an ideal scoring rule will not suffer from scale dependence. The lack of scale dependence is a property that the authors term local scale invariance. The logarithmic score possesses this attribute, but the CRPS and other scoring rules, like the Hyvärinen score, do not. To address this issue, the authors propose a new class of scoring rules which exhibits local scale invariance. Among this class is a scoring rule dubbed the scaled CRPS (SCRPS), which features many of the desirable qualities of the CRPS but overcomes the scale dependence issue.\nOf course, if local scale invariance is all that matters, then we could just use the logarithmic score in all scenarios. But there is another issue to consider when averaging forecast scores – the presence of outliers. In many scenarios, we might encounter observations that are very far outside the normal range, and we don’t want our average forecast performance measure to be greatly thrown off if such an oddity is observed. In other words, we want our proper scoring rules to be robust. In their article, Bolin and Wallin formalize the concept of robustness for scoring rules and show that, in many cases, the logarithmic score is not robust. Yet they also prove their proposed class of scaled scoring rules is not generally robust, although they show that the scoring rules can be modified to be robust (a new scoring rule they term robust SCRPS). Under such a modification, however, the scoring rule would no longer be local scale invariant in the strict sense. Indeed, under the proposed definitions of local scale invariance and robustness, finding a scoring rule that can simultaneously satisfy both criteria seems difficult. The authors conjecture that it may even be impossible.\nHence, this paper raises many questions for future consideration but achieves its goal of showing that evaluating probabilistic forecasts by averaging proper scoring rules is not necessarily a simple matter. Different scoring rules will lead to different rankings of forecasting methods, and the underlying properties of each scoring rule must be considered on a case-by-case basis. Although not discussed in this summary, the authors also compare scoring rules in several scenarios and present the theory behind the ideas examined here. For interested readers who want to dig more into these ideas, check out the full paper published in Statistical Science.\n\n\n\n\nAbout the author\n\nBrian King is currently a senior machine learning research engineer at Arm, working on applying machine learning to hardware verification. He recently completed his PhD in statistics at Rice University, where his research focused on Bayesian modeling and forecasting for time series of counts.\n\n\n\n\n\nAbout DataScienceBites\n\nDataScienceBites is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to become a contributor.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Brian King\n\n\nThis post is republished with permission from MathStatBites. Thumbnail image by Brendan Church on Unsplash.\n\n\n\nHow to cite\n\nKing, Brian. 2023. “Choosing the right forecast.” Real World Data Science, July 13, 2023. URL"
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2024/6/25/nowcasting-3step.html",
    "href": "foundation-frontiers/datasciencebites/posts/2024/6/25/nowcasting-3step.html",
    "title": "Nowcasting upgrade for better real time estimation of GDP and inflation",
    "section": "",
    "text": "Governments, policymakers and central banks across the world are wrestling to keep rising prices under control using monetary policies such as interest rate increases. The effectiveness of such policy changes should be assessed by monitoring inflation data as well as studying the impact on real GDP, making timely and accurate access to key economic indicators crucial for policy planning. The delay in publishing economic indicators such as Real GDP, inflation and other labour related series, makes this real time assessment of the economy particularly challenging. Now Menzie Chinn at the University of Wisconsin, Baptiste Meunier at the European Central Bank and Sebastian Stumpner at the Banque de France report an approach for “nowcasting” built on previous research that develops a framework using different machine learning techniques and is flexible and adaptable compared with traditional methods1. They report on the accuracy of their 3-step framework for nowcasting global trade volume estimates, showing how it can outperform traditional methods. They also highlight that the 3-step framework can be extended beyond World Trade data.\nNowcasting, an amalgamation of the term now and forecasting, provides a methodology to assess the current state of the economy by predicting the current value of inflation or Real GDP. The Federal Reserve Bank of New York and Federal Reserve Bank of Atlanta have used nowcasting to publish real time GDP estimates, for the USA. Similarly, the Federal Reserve Bank of Cleveland estimates real time inflation using nowcasting methods.\nThe basic principle of nowcasting is utilising information that is published early such as using data published at higher frequency, survey data, financial indicators or economic indicators. For example, the running estimate of Real GDP (aka GDPNow) that the Federal Reserve Bank of Atlanta provides is updated 6 or 7 times a month on weekdays when one of the 7 input data sources are released. Similarly, the real GDP growth estimate that the Federal Reserve Bank of New York provides is based on data releases in categories such as housing and construction, manufacturing, surveys, retail and consumption, income, labour, international trade, prices and others.\nThe traditional methods of nowcasting do not provide an integrated framework, and forecasters need to know which variables to use, and select a method for factor extraction and machine learning regression. Chinn, Meunier and Stumpner propose a sequential framework that selects the most important predictors. The selected variables are then summarized using Principal Component Analysis (PCA) and these factors are used as explanatory variables to perform the regression. Although traditional methods of nowcasting also utilized many of these techniques, the authors test various combinations of pre-selection, factor extraction and regression techniques and propose a combination that improves model accuracy."
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2024/6/25/nowcasting-3step.html#model-framework-improved-flexibility-and-accuracy",
    "href": "foundation-frontiers/datasciencebites/posts/2024/6/25/nowcasting-3step.html#model-framework-improved-flexibility-and-accuracy",
    "title": "Nowcasting upgrade for better real time estimation of GDP and inflation",
    "section": "Model framework improved flexibility and accuracy:",
    "text": "Model framework improved flexibility and accuracy:\nThe 3 steps in the framework are chronological steps to be performed in which the first step is pre-selection of the independent variables with the highest predictive power. The independent variables from the first step are then summarised into a few factors using factor extraction methodology in the second step. The final step consists of using the factors from step 2 to perform regression.\n\n\n\n\n\n\nFigure 2: The various methods that can be employed in the 3 step framework in Chinn et al (2024). Credit: National Bureau of Economic Research.\n\n\n\nFigure 2 summarises the various methods employed at each step in the 3 step framework. In their report Chinn, Meunier and Stumpner aim to propose the best techniques for pre-selection, factor extraction and regression. As such their 3-step framework comprises performing pre-selection using Least Angle Regression (LARS), factor extraction using Principal Component Analysis (PCA) and employing a Macroeconomic Random Forest (MRF) machine learning technique for nowcasting.\nThe model performance or accuracy of MRF is compared with traditional methods using Root Mean Square Error (RMSE), a measure of the deviation between the actual data and the predicted data. The 3-step framework model accuracy is tested by holding the preselection and factor extraction fixed to isolate the impact of regression techniques.\n\n\n\n\n\n\nFigure 3: Bar chart comparing the accuracy of different methods in terms of RMSE. Credit: National Bureau of Economic Research.\n\n\n\nFigure 3 compares the RMSE of traditional methods, machine learning tree and machine learning regression model for backcasting (t-2 and t-1), nowcasting (t) and forecasting (t+1). It highlights the greater model accuracy of MRF and Gradient Boosting compared with traditional models and tree models for backcasting, nowcasting and forecasting."
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2024/6/25/nowcasting-3step.html#whats-next",
    "href": "foundation-frontiers/datasciencebites/posts/2024/6/25/nowcasting-3step.html#whats-next",
    "title": "Nowcasting upgrade for better real time estimation of GDP and inflation",
    "section": "What’s Next?",
    "text": "What’s Next?\nOrganisations such as The Nowcasting Lab provide GDP estimates for European countries. Such nowcasting techniques have been employed by humanitarian agencies including the United Nations Refugee Agency (UNHCR) which uses nowcasting to estimate the actual forced displaced population. The nowcasting techniques, dashboards and tools have been implemented and accepted as a reliable source of information at government organisations for policy making, central banks, and financial organisations. The 3-step framework, proposed by Chinn, Meunier and Stumpner, is easily adaptable, flexible and provides higher accuracy, which will be valuable to a range of fields employing nowcasting.\n\nExplore more data science ideas\n\n\n\n\n\nAbout the author\n\nAtmajitsinh Gohil is an independent researcher in the field of AI and ML, specifically managing AI and ML risk. He has worked with consulting firm assisting clients in model risk management. He has graduated from SUNY, Buffalo with a M.S. in Economics.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Atmajitsinh Gohil\n\n\n  Text, code, and figures are licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) International licence, except where otherwise noted. Thumbnail image by Shutterstock Van Fink.\n\n\n\nHow to cite\n\nGohil, Atmajitsinh. 2024. “Nowcasting upgrade for better real time estimation of GDP and inflation.” Real World Data Science, June 25, 2024. URL"
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2024/6/25/nowcasting-3step.html#footnotes",
    "href": "foundation-frontiers/datasciencebites/posts/2024/6/25/nowcasting-3step.html#footnotes",
    "title": "Nowcasting upgrade for better real time estimation of GDP and inflation",
    "section": "References",
    "text": "References\n\n\nNowcasting World Trade with Machine Learning: a Three-Step Approach Chinn, M. D., Meunier, B. & Stumpner, S. NBER DOI 10.3386/w31419) ↩︎"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/10/25/evaluating-ai.html",
    "href": "foundation-frontiers/interviews/posts/2023/10/25/evaluating-ai.html",
    "title": "‘Statistics and data science are at the heart of the AI movement – we want to be a strong voice in the debate’",
    "section": "",
    "text": "Next week, the Royal Statistical Society (RSS) is hosting a panel debate on “Evaluating artificial intelligence: How data science and statistics can make sense of AI models.” The event forms part of the AI Fringe programme of activities and is timed to precede the UK government’s AI Safety Summit at Bletchley Park.\nRSS president Andrew Garrett is chairing this free, in-person event on 31 October, and he’ll be joined by five panellists to discuss big questions around AI model development, evaluation, risk and benefits:\n\nMihaela van der Schaar, John Humphrey Plummer professor of machine learning, artificial intelligence and medicine at the University of Cambridge and a fellow at The Alan Turing Institute\nDetlef Nauck, head of AI and data science research, BT\nStephanie Hare, researcher, broadcaster, and author of Technology is Not Neutral\nMark Levene, principal scientist, department of data science, National Physical Laboratory\nMartin Goodson, chief executive, Evolution AI, and former chair of the RSS Data Science and AI Section\n\nWe sat down with Andy for a quick-fire Q&A to hear more of what’s in store for next week’s event.\n\n\n\nThe RSS event takes place one day before the UK government’s AI Safety Summit. Why is it so important for statisticians and data scientists to be involved in the debate over AI safety?\nStatistics and data science are at the heart of the AI movement – it’s really a question of taking data and information, and using statistical algorithms to create outputs. That’s at the core of what we do as statisticians and data scientists. Although it’s called AI, it uses mathematical and statistical methods.\nThe AI Safety Summit focuses on risks posed by certain types of AI systems. Where do you see the biggest risk?\nRisk depends upon the purpose and the impact of the AI. It’s very different whether something is being used to inform or recommend or persuade or decide. If it’s a decision-making system, say, there is a bigger risk associated with it if the decision to be made will have an important impact on your life – so, that might be a medical decision or a decision on whether you’re to receive benefits or housing, or how you’re treated in the judicial system. It’s important to understand what the AI is being used for and how much control you have over it, and also how much oversight there is. Will the decision be made solely by an algorithm, or is there human oversight?\nThere is a particular concern moving forward around misinformation and disinformation. That is a genuine concern, particularly with big elections coming up in the UK and beyond. People are sharing things that they don’t realise are disinformation, so it is really important to understand where the information is coming from, its provenance; that’s incredibly important. We’ve seen with hallucinogenic AI that sometimes there are references given for outputs that don’t actually exist. So we need to constantly scrutinise where information sources are coming from and, if sources are quoted, do they really exist?\nThe RSS policy response to the UK government’s AI regulation whitepaper urges investment in a Centre for AI Evaluation Methodology. What informed this recommendation?\nThe white paper uses the word “proportionate” many, many times. When you talk about safety, inevitably you move into the area of risk, and if you want safety measures to be proportionate, and you want those measures to be based upon risk, then – effectively – you have to understand how you evaluate risk, how you evaluate the probability of something happening, and what the impact might be if it does happen. That naturally lends itself to needing to evaluate both the potential harm but also the potential benefit of using AI. I think the summit is focused more around harm, and the concerns around potential harms, rather than the trade off between harm and benefit. But that was certainly the reason we got into talking about the importance of evaluation, and evaluation is used in other industries where there is high risk. Drug development is an example, as is healthcare, where treatments and new methods are evaluated so that people are informed about both the potential harms and the potential benefits.\n\n\n\n\n\n\nAndrew Garrett, president, Royal Statistical Society\n\n\n\n\n\nWe have a really important role to play in focusing attention not only on AI outputs but on inputs. That is an area that hasn’t received enough attention but it is one that statisticians understand incredibly well.\n\n\n\nA lot of the focus of the AI debate is on outputs. Should we be talking more about inputs – the data the models are trained on, where the data is coming from, and issues of quality and bias, etc.?\nThere should be more discussion of this, yes. Statisticians and data scientists, but statisticians in particular, are trained very much around the data generating process, so we naturally think about how data is gathered, the potential biases in a dataset, and the representation that you need for a study. We understand that the data going in is as important as the outputs produced. And I think it’s becoming more and more necessary to understand where exactly the data is coming from, whether it’s diverse, whether it’s representative of the populations you want to study, and so on. This is an incredibly important area, and provenance of data – like provenance of information – will become ever more important. So, with a large language model, for example, what information is it trained on? Did the developers have permission to use that information? How representative is that information? These sorts of questions need to be addressed, because your outputs will only ever be as good as your inputs.\nAI systems are having wide-reaching effects across society. What impact are AI tools having on the work of statisticians and data scientists, and how would you evaluate the impacts so far: good, bad, or neutral?\nYou have to be a cynic and an evangelist at the same time. There is some very good work being done but also some very naive work. AI is not magic. It requires the same thoroughness and lifecycle management as anything else. Certainly in terms of pattern recognition, image recognition, it’s been very useful. On MRI images, for example, can you reduce the amount of time humans need to spend looking at the data because you have an AI tool helping with the assessment? Of course, the challenge is then, when you have an AI assessment, what do you compare it to? You could compare it to what an expert would assess, but is that a suitable reference point for saying something is a good system, knowing that humans themselves are not perfect? AI systems are able to handle large datasets, large images, very quickly. And so the speed of being able to do that has a potential advantage, although it depends on the level of human oversight. Where we’re seeing these tools being advantageous, I think, is where you have some human oversight but some of the heavy lifting is being done by the AI systems.\nWhat do you hope will emerge from the panel debate at the RSS next week? Reaching consensus on such a big, broad topic is unlikely, perhaps, but what are the kinds of things that you’re hoping to learn and take away from the discussion?\nWe’ve got some very good practitioners on the panel, and I’m hoping that we’ll generate some really good discussion from the panel and some really good questions from the audience. When it comes to the AI conversation generally, there’s a danger that it has focused too much so far on either the academic view of things or the large tech company perspective, so we’re probably missing out a whole tranche of people who are working at the coalface on these things, working in smaller companies. So, I’d like to understand a little bit more about what is happening in that part of industry. I know there’s a big focus on things like building out capability in the UK, and that’s not simply a question of having people with expertise – it’s about having access to things like the right sort of computing environments. So, I think there’s going to be some interesting discussion around what’s holding back industry. Overall, though, what I’d like to see come out of this meeting is a more proportionate response, from people who are working on this on a day-to-day basis. Statisticians are good at that – at coming up with a measured response, an informed response. Do we have the same concerns about the existential threat of AI that have been discussed by some of the larger companies, for example?\nAside from coming along and contributing to this panel discussion, how else can statisticians and data scientists engage with the AI debate and help shape a collective response to this major issue?\nI’d certainly encourage them to join the RSS and be a part of our work on this. We want to be a strong voice in the debate on AI because it is underpinned by statistical and mathematical techniques, as I mentioned at the start. We have a really important role to play in focusing attention not only on AI outputs but on inputs. That is an area that hasn’t received enough attention but it is one that statisticians understand incredibly well – and it’s one that brings into discussion issues such as ethics, consent, copyright, etc., and that’s very much where we should be engaging as well.\n\n\n\n\n\n\nRegister now for “Evaluating artificial intelligence: How data science and statistics can make sense of AI models,” a free, in-person debate at the RSS offices in London, 4 pm – 6 pm, Tuesday, October 31.\n\n\n\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘Statistics and data science are at the heart of the AI movement – we want to be a strong voice in the debate.’” Real World Data Science, October 25, 2023. URL"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/09/25/rgirls-interview.html",
    "href": "foundation-frontiers/interviews/posts/2023/09/25/rgirls-interview.html",
    "title": "‘What if we try to bring R to the classroom? That was our wacky idea’",
    "section": "",
    "text": "R-Girls is an exciting new project launched by Green Oak Academy, a faith-based independent secondary school for girls based in Birmingham, England. The project aims to promote the use of R in schools through the development and delivery of R-based lesson plans for a range of subjects.\nTo find out more about R-Girls, its origins and ambitions, Real World Data Science met with Mohammed Mohammed, a governor at Green Oak Academy, principal consultant for the National Health Service (NHS) Midlands and Lancashire Strategy Unit, and a founder of the NHS-R Community."
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/09/25/rgirls-interview.html#what-inspired-r-girls",
    "href": "foundation-frontiers/interviews/posts/2023/09/25/rgirls-interview.html#what-inspired-r-girls",
    "title": "‘What if we try to bring R to the classroom? That was our wacky idea’",
    "section": "What inspired R-Girls?",
    "text": "What inspired R-Girls?\n“It was the confluence of a few different things. Firstly, Dr Razia Ghani, head teacher at Green Oak Academy, has a PhD in mathematics from the University of Birmingham, has some experience of doing statistics in the pharmaceutical sector, and teaches mathematics. That meant we had a lot of shared capital in our background and thinking about things.\n“When I became a governor of the school, the staff were doing progress reports for pupils, and I wrote a little programme in R to help them with this. So, that was one of the early uses of R in the school.\n“I know R because of my experience in academia, and I had previously put in a proposal to the Health Foundation to introduce R to the National Health Service. The Health Foundation agreed to that, they funded it, and we now have an NHS-R Community which is basically promoting the use of R in the health service.\n“And so I said to the head teacher, ‘What about if we try to bring R to the classroom?’ That was our wacky idea, and we put in a bid to the R Consortium, and they gave us seed funding, and that’s how we got started.”"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/09/25/rgirls-interview.html#what-are-the-aims-of-the-project",
    "href": "foundation-frontiers/interviews/posts/2023/09/25/rgirls-interview.html#what-are-the-aims-of-the-project",
    "title": "‘What if we try to bring R to the classroom? That was our wacky idea’",
    "section": "What are the aims of the project?",
    "text": "What are the aims of the project?\n“The aims of the project are threefold:\n\nTo promote the use of R in secondary schools for girls.\nTo inspire teachers to incorporate R into their lessons.\nTo enable students to experience the joy of R.\n\n“On our website, we have prepared 10 or so oven-ready lesson plans – they have to be oven-ready for any teacher to ever think about using them; they have to be polished, ready, and easy to use. As well as our website, we have a Twitter page and a community on Slack.\n“One key thing to note is that we didn’t set out to teach programming. The history of computer science in our school is that computer science was really not liked at all by the girls. They wouldn’t choose it as a subject to pursue further. So, we decided to use R to support other subjects. This was a key strategic decision, really.\n“People have attempted to develop lesson plans to teach Python, but that would be a non-starter in my school because (a) there isn’t much curriculum space, and (b) no teacher is enthusiastic enough to take on that activity and learn Python and become the Python expert in class. Whereas using R as a tool to facilitate learning in other subjects just opens up the whole world of data science in a way that is so much more accessible and appealing. People may say we’re really not teaching students how to code – but we’re not putting them off coding either!”\n\n\n\n\n\n\nThe R-Girls website includes tutorials on how to get started using R in school."
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/09/25/rgirls-interview.html#how-is-r-being-used-in-different-subjects-in-school",
    "href": "foundation-frontiers/interviews/posts/2023/09/25/rgirls-interview.html#how-is-r-being-used-in-different-subjects-in-school",
    "title": "‘What if we try to bring R to the classroom? That was our wacky idea’",
    "section": "How is R being used in different subjects in school?",
    "text": "How is R being used in different subjects in school?\n“In maths lessons we do things like generate sequences, plot graphs, do Pythagoras’ Theorem. In geography, we have mapping lessons and lessons on plotting rainfall data from Australian cities.\n“Our lesson plans are set up in R Markdown, but the key thing from the teacher’s point of view is that they recognise the structure: What stage is it in terms of the curriculum? What are the objectives, the success criteria and keywords? Typically, we’ll show a worked example and then give the students an exercise to draw a different graph, say – they learn to tweak existing code rather than start from scratch.\n“We first taught them how to use R using R Markdown – a way of writing script and code and data all together in one document. And after the first lesson, we were absolutely astonished. This is feedback from one of the girls: ‘Today’s lesson was very interesting and exciting. I’ve learnt a lot of new things about coding and found a new hobby…’ From subsequent lessons, here’s more feedback: ‘Programming and coding was actually quite fun… It was a great feeling knowing that I was able to code a whole bar chart all by myself…’\n“A new thing that we’ve done recently is that one of the teachers has been inspired and has learned how to build a website in R, and she’s then decided to teach the Year 9 girls how to build their own websites. One quote from a student was: ‘I learnt how to make a website; I feel like an independent woman now.’ Another said: ‘This was very stressful. But it will look good on my CV.’\n“One of the key challenges we face in education is that teachers are very stressed, and Covid didn’t help. Workloads are a massive issue in teaching, and of course that’s the same for our teachers. So, we had to make sure that whatever we do does not feel like it’s adding to the workload. One of the things we do is to use R in the cloud, so there is no installation and debugging on local computers. We try to minimise the pain, really.”\n\n\n\n\n\n\nLesson plans developed so far include ones for maths, science, and geography classes (pictured)."
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/09/25/rgirls-interview.html#how-do-you-want-to-see-the-r-girls-project-grow-and-what-will-success-look-like-for-you",
    "href": "foundation-frontiers/interviews/posts/2023/09/25/rgirls-interview.html#how-do-you-want-to-see-the-r-girls-project-grow-and-what-will-success-look-like-for-you",
    "title": "‘What if we try to bring R to the classroom? That was our wacky idea’",
    "section": "How do you want to see the R-Girls project grow, and what will success look like for you?",
    "text": "How do you want to see the R-Girls project grow, and what will success look like for you?\n“It would be really nice to see that girls introduced to R through the R-Girls project felt there was a natural progression for them to join the R-Ladies Group, so that data science becomes part of their future aspirations.\n“The second thing is that I’d like to set up an annual conference for girls at school, which would be online and which becomes part of the extracurricular landscape, so that any girl interested in R can join.\n“We’d also love to see other schools, especially girls schools, joining the project – and joining the project just means agreeing to take the lessons we have and trying them in class. We would welcome other schools contributing additional lessons to the library of lessons, so that it becomes an open community resource that anybody can use. There’s nothing in this which is designed to make it explicitly local to an English independent school in Birmingham!\n“We are keen to hear from others, so please reach out to us on rgirlsschool@gmail.com.”\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘What if we try to bring R to the classroom? That was our wacky idea’” Real World Data Science, September 25, 2023. URL"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/06/08/london-data-week.html#transcript",
    "href": "foundation-frontiers/interviews/posts/2023/06/08/london-data-week.html#transcript",
    "title": "London Data Week is almost here. What’s it all about?",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove some repetitions.\n\n\n\nBrian Tarran\nHello, and welcome to Real World Data Science. I’m Brian Tarran. And today I’m joined by the organisers of the upcoming London Data Week, Sam Nutt and Jennifer Ding. Sam, Jennifer, how are you? Nice to see you here. Thank you for joining us. I wanted to start, maybe you can introduce yourselves to our viewers, tell them a little bit about your background. Sam, I don’t know if you want to go first.\nSam Nutt\nSo I’m Sam Nutt. I’m the researcher and data ethicist at the London Office of Technology and Innovation, or LOTI. We’re about four years old. We’re an innovation unit that sits across 26 of the boroughs of London, and the Mayor of London, the GLA and we work sort of through collaborative processes to sort of foster innovation within local government, in the public sector in London. And I lead partly on our work on data ethics, as the title suggests, but also I lead our work on innovative public participation, which I guess leads nicely into maybe talking a little bit later about London Data Week. But yeah, that’s kind of my background and where I’m coming from. My interest in data, I guess, more originally came from the governance side, and how we use it properly and best in public sector context.\nBrian Tarran\nExcellent. Thanks, Sam. Jennifer?\nJennifer Ding\nHi, I’m Jennifer. I’m from the Alan Turing Institute, and I– the Turing, I should say is the UK’s national institute for data science and AI. And I sit on a team called Tools, Practices and Systems, or TPS, which we sometimes colloquially call the Turing’s open science team. So we focus on open, reproducible, and ethical data science practices. And at the Turing, I co-lead a team of research application managers, and our focus is making sure that the research that happens at Turing is more usable, and also actually used by more people outside of academia. In a previous life, I was a data scientist at various US tech startups, working on applied machine learning, mostly for local and national government partners. And in New York, where I was for many years, I participated in something called New York Open Data Week, which was a great introduction to the open data world, and also the civic technology world and how vibrant and exciting those communities are.\nBrian Tarran\nExcellent. Okay, so that– so you’ve had the exposure to data weeks before? Tell us how did London Data Week as an idea first germinate?\nSam Nutt\nYeah, well, I mean, maybe I can go because it was it was something LOTI, last year, we had a sort of anniversary event, we sort of tried to bring together our community, across local government, and also partners. And so we invited [the] Alan Turing [Institute], we’ve done some work with them. And Jen came, and it was just something Jen raised to me, with me last July, you know, the idea of maybe doing something a little bit, like, inspired by some of the bits we’ve seen, for example, in New York, but also thinking, you know, what’s, what’s the London version of that? What’s the opportunity we have, given, you know, our different context to New York, and we don’t just mean in terms of, you know, you’ve got a different, like, legal context for how you use data and regulation stuff, but the cultural bits, you know, what is the physical space? How does that make London different? The people who live here, you know, what are the opportunities of London. And I think, you know, partly also inspired at the time by, you know, at least from my perspective, a real want to include, you know, ordinary people, Londoners, the public at large – all of those terms can be broken down – but effectively, to increase participation in how we think about data and how we, you know, think also about the future of the city, in a, you know, a future that we know is going to be defined by how we use data. So it kind of, it was a perfect thing where I was very inspired by, you know, I was thinking about those things, and then Jen came along. I don’t know, maybe Jen, you can talk about why you came along with that idea in the first place.\nJennifer Ding\nYeah, absolutely. Yeah. Sam and I sometimes talk about how funny that chance encounter was really. I think the LOTI event was a really great example of how London’s flavour of data innovation is actually quite unique. Just gathered there with all the local councils and various other data organisations that LOTI works with. It was such a cool display of how much the public sector is involved in defining data innovation in London, and also how much academic and private tech in London is also committed to creating data and tech outputs that are for the public good. And coming from the US, I think it was very inspiring to see that organisations like LOTI, organisations like the Turing Institute, like the Ada Lovelace Institute, the Open Data Institute – there is such a great network of data for public good institutions here that are committed to, you know, centering Londoners in the conversation. And I think something that Sam and I really got to talking about was, you know, was this an opportunity also to clearly articulate what this new thing, this unique thing, is that exists in London, that if you’re here, and you work in the space, you know it, but it hasn’t really been formally articulated in the way that I think many of us know that Silicon Valley is associated with a certain kind of innovation – and may be Europe as well with regulation – but what’s happening in London is really special. And we hope that with all our great partners and our London Data Week team, we can begin to start to articulate what makes data innovation in London so special.\nBrian Tarran\nPlease do, Jen.\nBrian Tarran\nAnd before we get carried away, we should probably pin down exactly when London Data Week is taking place. It’s beginning of July, is that correct?\nJennifer Ding\nThat’s right, first week of July 3rd to 9th July.\nBrian Tarran\nAnd so from what you’ve said, my understanding, broadly, maybe the aims of the week are to kind of bring together people who are working in data, who are using data and people who are affected by data, or for whom data helps sort of shape their lives, to kind of have maybe, I don’t know, a broader understanding, a kind of commonality of purpose, whatever it might be, is that is that how you would summarise it or…\nSam Nutt\nVery nicely summarised, are you available for comms help?\nBrian Tarran\nI am. Very expensive, though, I’m afraid.\nSam Nutt\nOkay. It’s all pro bono stuff. No, I think, yeah, I think that’s really well put. It’s, yeah, it’s articulating that– coming together to start to actually build that imagination, that vision for what London is, and articulate it more, in more clear terms. But it’s also, you know, actually trying to do some things in line with that as well. Some of the activities and events we have, you know, we’re running, it’s kind of in this distributed format. So different organisations across London, who are sort of value aligned across the sector, who might have their own communities, their own publics, can run things – different types of events, you know, engaging people in different ways. And then, you know, all together, we think, sort of the sum of the parts of doing these things across a week, in this distributed format, testing different engagement methods, you know, not only sort of builds a community of organisations around these, like common values, but also lets us, you know, actually reach out to the public, you know, the public with all the different people in London, from different backgrounds, there are so many different communities, you know, as much as possible, to connect to different communities and make them part of the conversation about how we use data and AI and in ways that they kind of historically haven’t been. Not just for data and AI, for lots of other things. But, you know, we know that these technologies are going to be so important for the future. And it’s, you know, designing events and activities that can try and ensure that they’re sort of have a, you know, a seat at the table, thinking about what that future is.\nBrian Tarran\nYeah. And you mentioned there a range of events. Jen, can you give us a flavour of some of the events coming up, the highlights, things that you might personally be most looking forward to?\nJennifer Ding\nYeah, happy to Brian. And maybe something to add to is something Sam and I were really hoping to go for is to not have, you know, maybe the typical talk or panel style format for all of our events that might be really common in a conference, but rather have different kinds of events that can focus on different kinds of activities. So having public conversations and debates, having exhibits and experiences, having resident or citizen science opportunities where people can actually be a part of creating data, and also learning opportunities if people want to upskill or learn about a concept. So to shout out some of the events I’m really excited for, maybe to start with the more wacky ones first. The Turing is hosting a “Cabaret of Dangerous Ideas” which will be a comedy show at the Camden Club where they will dig into topics like technology and data, but over a pint and through humour. So apparently, this is an 18+ show. So that might give you a little bit of a flavour of what’s to come. Another event we’re really excited about is an event called All the Docks, where a team of cyclists attempt this challenge in one day to hit every single Santander bike dock. They’ve done it before, and this year for London Data Week, they’re doing a round to hit, I think, the now over 800 docks that now exist in London. And this time, for London Data Week, what they’re also doing is making it a data collection exercise. So as they cross the streets of London, they’ll collect data on the road conditions, the cycling infrastructure, which can be then an open dataset that people can use after the event as well. So those are two that jumped to mind. I don’t know, Sam, if there’s anything you want to highlight, or Brian because I know there’s a really exciting on that the RSS is…\nBrian Tarran\nYeah, I’ll put a quick plug in for the, so the Royal Statistical Society are organising an event associated with our Statisticians for Society initiative, which offers pro bono support to charities under a million turnover in the UK. So I can, I’ll post a link in the show notes so that people can find out more about that and sign up if they meet the criteria and are interested in taking part. But Sam, sorry, I, I hijacked there. But go ahead.\nSam Nutt\nNo, Brian, I was going to only mention your event. But no, a couple of others, I think just to show the like breadth of the types of activities. So you know, for example, it’s also it’s about partnering with organisations who are thinking and you know, already doing things like this, so the Science Gallery, for example, there’s– they’ve got an exhibition called “AI: Who’s looking after me?”, which is, you know, looks at some of the playful ways that AI is already involved in people’s lives and brings sort of people on a critical journey. That’s more of that sort of art exhibition type thing. In local government, where, in LOTI, we’re helping some boroughs with developing basically, a toolkit, a resource to help officers in boroughs in some of the data teams, who maybe haven’t had that history of engaging with residents, as seen as more sort of technical back office staff, actually, you know, give them the confidence to go out and have a conversation with residents about some of their practice and see how they can improve it. So there’s been a lot of interest there, in particular, around having conversations about how we do data linkage better, which, you know, in some ways, feels– it’s quite a straightforward data topic. But actually, the huge thing is that these teams in boroughs have never thought, we need to speak with residents at the design stage of data projects, you know, the public, what might a digitally excluded, relatively low sort of data literacy person be able to tell us about our data work that’s helpful to me as, you know, a data scientist, that was sort of some of the historical thinking, but actually, we’re sort of bringing boroughs on the journey of thinking, actually realising, you know, the value of doing that. So that’s sort of, I guess, the range of types of things as well.\nBrian Tarran\nFantastic. So where, if people do want to sign up for any of these events, is there a good way for them to do that? Is it go to the LOTI website– not LOTI website, the London Data Week website, I’m guessing?\nJennifer Ding\nThere is a good amount of info on both the Turing and the LOTI website, but the best place we’d suggest is londondataweek.org. There you’ll find a list of events. And if you click on an event, there’s more information. And also, if you click “Find out more”, you can access a link for more information on how you express interest or sign up.\nBrian Tarran\nExcellent. And if people– when we’re speaking, we are almost exactly a month away from London Data Week. If someone’s listening to this and they think, Oh, I’ve got a great idea for an event I want to organise, is it too late for them to squeeze onto the programme now? Should they get in touch somehow if, if inspiration strikes?\nSam Nutt\nWe’ll never say never. Probably there is a very late point in which we would say never. I think realistically, you know, the way it’s being run, it’s being run often with the time of volunteers and this sort of thing. And it is sort of the first year of us running it. So a lot of it’s coming through, through Jen and I. So if you are interested in running something, please do reach out to us. You know, we’ve got very good, Jen and I, at finding creative ways to slot people into programs and this sort of thing. But equally I think at this point, it’s really more about you know, bringing together people and organisations who are interested and actually, you know, I think we’ve got a really good exciting, fun set of events and activities across London that would be great to be part of even if you yourself haven’t been able to organise something, and then maybe it’s something for next year, for London Data Week 2024. Fingers crossed, we might be able to do something there.\nBrian Tarran\nSo the stress of day jobs and organising a week-long event, or week-long collection of activities, hasn’t put you off doing it again? No. \nJennifer Ding\nSo far, so good, Brian.\nSam Nutt\nYeah, we’ll “no comment” some of that.\nJennifer Ding\nWe’re definitely really excited though. And if anyone does have an idea or wants to start a conversation, there’s a contact form on our website, drop us an email. We also have a Twitter if you want to send us a message through that. So at the very least, we love, we’d love to chat.\nBrian Tarran\nGreat, well, we’ll put all those contact details, social media accounts, etc., into the show notes so people can find you. But thank you very much for joining us today. I know this must be a very busy time for you. But Sam Nutt, Jennifer Ding, thank you for joining us and talking about the upcoming London Data Week. I’m looking forward to it.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Images are not covered by this licence. Thumbnail image by ARTHUR YAO on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “London Data Week is almost here. What’s it all about?” Real World Data Science, June 8, 2023. URL"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/11/24/politics-of-modelling.html",
    "href": "foundation-frontiers/interviews/posts/2023/11/24/politics-of-modelling.html",
    "title": "‘I would like modellers to be less ambitious in developing monster models that are impossible to inspect’",
    "section": "",
    "text": "It was during the first wave of the Covid-19 pandemic, when citizens in many countries around the world were confined to their homes, that Andrea Saltelli and colleagues were inspired to write “a manifesto for responsible modelling.” The television news was, Saltelli recalls, dominated by models of Covid infections, hospitalisations, and deaths. Politicians pointed to charts showing those model projections and spoke of “flattening the curve” – driving down case numbers – so as not to overwhelm healthcare systems.\nWhile all this was going on, Saltelli came into contact “with a fantastic group of people,” he says, “all of whom were, in a sense, concerned by the sudden eruption of mathematical modelling into everyday life.”\n“We were concerned that this [modelling] was not being done properly, that too much importance was given to those numbers, too much certainty was attached to them, and nobody seemed to realise that the selection of certain numbers rather than others would eventually and dramatically bias the message that was given.”\nIn June 2020, Saltelli – along with Monica Di Fiore, Deborah Mayo, Theodore Porter, Philip Stark and others – published in Nature their manifesto setting out “Five ways to ensure that models serve society.” The ideas proposed in that three-page comment piece have now been given a book-length treatment, so we sat down with Saltelli to discuss The Politics of Modelling: Numbers Between Science and Policy.\n\n\n\nCan you tell our readers a little about yourself?\nI am a chemist. I got my degree in chemistry, but for most of my life I have worked as a mathematical modeller and applied statistician. More recently, let’s say in the last 10 years or so, I have also moved into issues of epistemology – meaning, how do we decide that we know what we know, and how do we do that when the source of the knowledge is represented by a mathematical model?\nI’d like to dig into the title of your new book. What should people understand about The Politics of Modelling?\nIt starts from a broader discussion of a state of exception enjoyed by mathematical modelling. One point we try to make in the book is that models are exceptional because they have an incredible palette of methodologies – even more than statistics. They are not a discipline, because everyone does modelling in their own craft in a different way. Modelling even escapes the gaze of sociologists most of the time because sociologists are more interested in algorithms and statistics. And, as a consequence of this state of exception, models enjoy many privileges, including a better defence of the pretence of neutrality, and they maintain, in a certain sense, a lapse of symmetry between developers and users. They also have a very strong grip on policy, whereby models can enjoy a high epistemic authority, and this epistemic authority seems to be proportional to the dimension of the model or the base of data on which the model has been calibrated. All of this creates a situation which leads to a problem – a problem for society, on the one hand, because models are used to suggest policies which are not optimal, and on the other hand, trust is consumed, trust is lost, and this may have been happening as a result of the Covid-19 epidemic and the way mathematical modelling was used in the context of the epidemic.\nThe book emerged out of the “manifesto for responsible modelling” that you published in Nature a few years back. Could you describe that manifesto?\nThe manifesto was something which came out of the pandemic, in fact, because we were all locked up at home and we could spend some time reflecting and writing. We tried to produce a set of recommendations for both society and the modellers: for society to be a bit more circumspect in accepting results from mathematical modelling, and for modellers to be more cautious in formulating their predictions. But, beyond the issue of apparent precision of mathematical models, there was also the issue that models are built on a series of assumptions, each of which may have a great bearing on the result. And not only that but also, at the point where you formulate a mathematical model, you assume that you have already decided what is the problem, what is the direction of progress. So, there are really many normative assumptions which are embedded into that. Then there is the issue that mathematical models are not done by everyone; they are done by specific groups of people who belong to, normally, a certain identified class, some kind of elite – not a financial elite, but an elite in terms of competencies and knowledge. And this also creates bias, because – to put it brutally – if you can work at home with your laptop, the epidemic doesn’t affect you so much. But if you work in a plant and the plant is closed, and you are not paid, this destroys your life, or the life of your family. This asymmetry – or inequality, let’s say, or implicit bias – in those who are producing the analysis, this was, for many of us, an issue which needed to be brought to the attention of the public.\n\n\n\n\n\n\nAndrea Saltelli, co-editor of ‘The Politics of Modelling.’\n\n\n\n\n\nThe manifesto was something which came out of the pandemic… We tried to produce a set of recommendations for both society and the modellers: for society to be a bit more circumspect in accepting results from mathematical modelling, and for modellers to be more cautious in formulating their predictions.\n\n\n\nSurely the urgency of the Covid situation prevented people from taking a step back and thinking more deeply about how models are constructed. Is it not forgivable in a situation like that? Or, is your argument that we should be doing this at all times, regardless of the urgency, regardless of the time pressures?\nI am tempted to say both yes and no. Yes, because surely the situation was urgent, and many things which were done in a way which one would consider suboptimal were later justified on the grounds of urgency. We noted incredible differences in the measures adopted in several countries, so for us it was obvious that even though everyone was claiming to “follow the science,” they seemed to be following different sciences, or perhaps they were following “the science” which was more instrumental or more convenient to justify what was simply politically expedient.\nBeyond that, I would say: it’s always urgent, no? We are very often in these kinds of situations. One might say that even the regulation of artificial intelligence today is urgent. Regulation of pesticides is urgent. Not to mention geopolitics… Everything seems to be urgent, and this seems to be a constant in our relationship with technology in particular: we don’t want to kill innovation, but if we wait to see what a new piece of technology does before we regulate it, then maybe it’s too late to change it. This is exploited by many people, not least [Mark] Zuckerberg [CEO of Facebook owner Meta]. He says, “Move fast and break things,” but once things are broken, they’re broken.\nAnd talking about things being broken, what we discuss in the book is also this issue of broken trust. People are losing faith in expertise – not in all countries in the same way; there are national differences that are important. But, in general, if you measure trust in science – which is still very high – it’s taken quite a dent during the pandemic, and we argue that this was in part due to abuse of mathematical models.\nThe book, which you’ve edited with Monica Di Fiore, breaks down the manifesto for responsible modelling into extended essays from different contributors, looking at different aspects of modelling – the framing of models, the assumptions, the consequences. For these essays, you draw on experts from different fields: sociology, philosophy, statistics, civil engineering, geography, law, environmental sciences, and others. Why was it important to get such diverse perspectives on these various aspects of modelling?\nThere is a major divide between social science – humanities – on the one hand and natural sciences on the other hand, with lots of suspicion between the two fields and sometimes open hostility. Mathematical modelling is particularly impenetrable, as we argue, to the gaze coming from a social scientist – at least, more impenetrable than statistics or algorithms, which have been very much studied in recent years. And so, it was important to allow the two fields, the two big communities, to communicate and to speak to one another in a critical way.\nYou write in your introduction to the book that the field of statistics has spent more time thinking more deeply about questions of data ethics, model assumptions, and so on. Can you give an example?\nThere is a book by a group of French statisticians, Statactivisme, which is rich with examples of how a statistician could make a difference by simply producing better numbers. They don’t say, “Throw away the model, throw away the numbers,” but simply be careful of what numbers you use. And I think models and modellers need something like this, some kind of systematic debate – a societal debate – with other disciplines on what they’re doing.\nOne of the quotes that jumped out at me from the book was, “Models are underexplained but overinterpreted.” How do we reset that balance?\nThis is more easily said than done. The remedies to this are, maybe we should spend some time thinking about reproducibility, even in mathematical modelling. This is not done. We talk about the reproducibility of data but very few people talk about the reproducibility of a mathematical model. Another thing which I think would be useful is to think more about how to interpret models and less about how to make them bigger. And then, of course, there is the practice of “assumption hunting.” If you use a model, go and hunt for the assumptions contributing to its construction.\nTo the modellers we say, engage yourself in something that might be called “modelling of the model process,” which means, try to imagine what would happen if you took a different branch in the construction of the model. In this we make an analogy to the “garden of the forking paths,” something that statisticians discuss, because they understand that when they build a statistical construction, they can take one way or another way, and when they measure the impact of taking a different path – as, for instance, when they give the same data to different teams – they find an amazing diversity of results that are totally unexpected. We are learning now that not only in statistics and mathematical modelling but in the laboratory, too – conducting physical experiments, not numerical ones – you can have a diverging set of outcomes depending on who is doing the analysis.\nAll this should call for a science that is more humble – one that accepts this kind of possibility and works actively to make these issues evident but also solves them in order to produce knowledge that is useful.\nEarlier, you spoke about modellers coming from a specific group or class of people – an elite. We interviewed, earlier this year, Erica Thompson, author of the book Escape from Model Land, and one of the points Erica discussed was how to bring more diversity of thought and of voices into the modelling process. How do we engage broader communities in the construction of models – maybe not building the model itself, but thinking about what is important, what needs to be measured, what are we looking to understand?\nThis could be achieved if models were used in a context of what we, the authors, call an “extended peer community.” In other words, this is the idea that when you are discussing an issue, you should talk to the people directly affected by the issue because they have some knowledge about it. For this to take place, the model must be one instrument, which the community can get together to discuss, and so the model must not be too complex.\nNow that your book is out, what do you hope will be its impact?\nLooking from the point of view of the modellers, I would like them to be more humble and less ambitious in developing monster models that are impossible to inspect and explore. From society I would like to see a more circumspect attitude, as I said before. Society has been trained to be sceptical of statistical information, but we should also be circumspect about the output of mathematical modelling. Ask more questions; ask, for instance, for the uncertainty range for a given number, or whether the number tells the entire story.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘I would like modellers to be less ambitious in developing monster models that are impossible to inspect.’” Real World Data Science, November 24, 2023. URL"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/04/28/stephanie-hare.html",
    "href": "foundation-frontiers/interviews/posts/2023/04/28/stephanie-hare.html",
    "title": "‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics",
    "section": "",
    "text": "We’re about a year late coming to Stephanie Hare’s book, Technology is Not Neutral: A Short Guide to Technology Ethics. But, as discussed in our interview below, time has only made the text more relevant. The book was written pre-ChatGPT, but Hare’s explorations of ethical questions in the context of facial recognition technology and Covid-19 exposure tracking apps feel both pointed and urgent at this moment, when researchers, regulators, and regular people are weighing the opportunities and potential harms of large language models and generative AI tools.\n“We’re having some sort of moment with technology ethics – AI ethics being just a branch of that,” says Hare. Reflecting on her career, spanning 25 years, she says: “The stuff that we’re talking about today that dominates the headlines – that is dominating the discussion in the tech sector – was not discussed at all at the turn of the century, other than by maybe people in the science and technology studies domain or academics. But it wasn’t filtering into boardrooms. It wasn’t on the front pages of newspapers, and it wasn’t being covered in the national news. So, it’s amazing. A whole field has sprung up.”\nHowever, as Hare makes clear in our interview, we still have a long way to go to build a culture of technology ethics throughout society. Check out the full conversation below or on YouTube."
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/04/28/stephanie-hare.html#timestamps",
    "href": "foundation-frontiers/interviews/posts/2023/04/28/stephanie-hare.html#timestamps",
    "title": "‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics",
    "section": "Timestamps",
    "text": "Timestamps\n\nChatGPT: just another “flavour of the month” in the tech industry? (1:18)\nHas concern about large language models helped put technology ethics on the map? (3:17)\nWhat will it take to build a culture of technology ethics – in society, in academia, in industry? (9:16)\nDrawing lessons from history (12:15)\nWhy technology ethics is a “wicked problem” (24:57)\nChecklists and changing mindsets (29:49)"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/04/28/stephanie-hare.html#quotes",
    "href": "foundation-frontiers/interviews/posts/2023/04/28/stephanie-hare.html#quotes",
    "title": "‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics",
    "section": "Quotes",
    "text": "Quotes\n“The European Union has the AI Act coming down the pike. It doesn’t cover stuff like ChatGPT specifically, but then I don’t know if you want good regulation to cover the technology itself, or how technology is used. I talked about this in my book: do you want to regulate forks – a tool – or do you want to regulate use cases for forks? We’ve regulated the use case, if you will, of murder, or of injury with a fork – or, frankly, any other tool. So it’s the use case we focus on. We don’t really regulate forks. [But] we do regulate some technologies, like biomedical technologies, human genetic stuff, anything nuclear. So we just need to think about where does AI fit with that?” (5:42)\n“Move fast and break things was the mantra for this culture [in the technology industry] for a really long time, at least out of the US. And it made a lot of people a lot of money, and they got worshipped by the media. And, you know, they have a whole audience of ‘bros’ who are fans of them. And they’ve never really, any of them, been held to account for what they’ve built.” (11:56)\n“Another generation or two, when we’re older, might look at some of what technology we’ve built or our behaviour on climate change, our track record – did we do what we could have done to slow global warming, to improve biodiversity? – and they might hold us to account, saying, ‘You could have stopped this and you didn’t, right? It’s not just what you did. It’s what you did not do.’ So we have to be super careful when we think about ethics, because ethics change, values change over time. And what seems okay today may not be okay in 10, 20, 30 years time. That is on my mind all the time. It’s not very relaxing.” (18:30)\n“[Laws and regulations] are important, they’re necessary, but they’re insufficient. You can act a lot faster if you can get people preventing stuff from being built in the first place, and that means you need to have a culture of people working in technology, both within the organisations – whether that’s research labs, government, companies, universities, whatever – and on the outside – journalists, academics, thinkers, etc., or just the public, an informed public – who can see something and sound the alarm and go, ‘Wait a minute, hang on. That’s not okay.’” (33:02)"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/04/28/stephanie-hare.html#transcript",
    "href": "foundation-frontiers/interviews/posts/2023/04/28/stephanie-hare.html#transcript",
    "title": "‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove some repetitions.\n\n\n\nBrian Tarran\nHello, and welcome to another Real World Data Science interview. I’m Brian Tarran. And today I’m joined by Stephanie Hare, a researcher and broadcaster and author of the book Technology is Not Neutral: A Short Guide to Technology Ethics, which is the focus of our conversation today. Welcome, Stephanie.\nStephanie Hare\nThank you so much for having me here.\nBrian Tarran\nI feel I’m a bit late to the party with the book. The Financial Times picked it up as one of the best books of summer 2022. But I’ve only just got around to reading it.\nStephanie Hare\nI mean, I only just got around to reading War and Peace last year. So there’s no rush with these things.\nBrian Tarran\nOkay. Well, I mean, I can definitely say it’s one of the best books I’ve read in spring 2023, if that helps, and the only other one I read was Lord of the Rings. So–\nStephanie Hare\nWow. I mean that’s some pretty august company, you couldn’t thrill me more.\nBrian Tarran\nExcellent, excellent. Well, I actually thought coming late to the book might actually have been of benefit to me as a reader because, you know, you’re talking about technology ethics quite broadly. But then you focus in on a couple of use cases, specifically around facial recognition, technology, Covid-19 exposure tracking apps and things like that. But, you know, obviously, since the book was published, the whole discussion around technology ethics has kind of been dominated maybe or taken on a new dimension following the launch and adoption of ChatGPT. I wonder, you know, when the technology was launched, people started using it, adoption rates, you know, went through, went through the roof, what was your kind of initial reaction to all that and what have you made of the kinds of conversations and criticisms that have followed?\nStephanie Hare\nWell, I mean, like everybody I was curious and fascinated and wanted to play around with it a bit. I don’t think I’m of the school of thought that seems to be circulating that this will either you know, destroy mankind as we know it or take everybody’s jobs or potentially upend civilization. There’s been some quite extreme, some quite extreme views put across in the media in the past few months since this was widely released to the public. I don’t know, for some reason, I didn’t drink the Kool-Aid, when I started working in technology. So I always take these things with a grain of salt. And I guess my, my cautionary note to anybody listening to this is you know, at this time last year, all of my clients were wanting presentations and analysis about web3 and NFTs and cryptocurrency and before that, it was blockchain. There’s like always a sort of flavour of the month. AI, for people who’ve worked in this field, is known to have winters, summers, springs, autumns, you know, these seasons of when it’s like coming on and really exciting or not? I’m more excited by DeepMind’s use of artificial intelligence, I think they’re actually working on interesting problems, right, around like protein discovery, like real science, as opposed to like, oh, look, I can have a new sci fi avatar or do a deep fake, you know, people can do deep fakes already. We’re just doing them now in even more disturbing ways. So I guess, I guess it’s that. I’m intrigued by it. But I don’t I don’t feel the need to sort of freak out. Either way, positively or negatively, I have a much more sort of detached satellite-level view, I think probably just because I’m older, seeing these trends come and go. And it’s like, let’s just let’s just wait this out and see how it goes.\nBrian Tarran\nFrom your perspective as someone who’s interested in and researching in the area of technology ethics, right, do you see a kind of almost a benefit that the conversation around this has put technology ethics, the conversation around that, on the map? Or do you worry that we’re kind of obsessing over this one technology and this one application? We’re not looking at the field more broadly?\nStephanie Hare\nWell, there’s a few things to say on this. So like, first of all, a couple of weeks ago, a bunch of people working in AI, about 1000-plus people – including some fictitious people, by the way – sign this this letter calling for a moratorium on AI research for six months, which was unenforceable, clearly not going to happen, was signed by Elon Musk, who then very quickly announced he was developing his own rival to OpenAI, the company that invented ChatGPT. So you take all of that with a grain of salt. But again, if you’re, if you’re an historian, or if you just have a long memory, you’ll remember that there have been several letters like this. There’s always somebody, you know, very big-wig people. It’s not that we want to dismiss it. But Stephen Hawking and Elon Musk were warning, you know, over around 10 years ago that AI was going to kill humanity if we didn’t put guardrails on it. Professor Stuart Russell talked about this in his Reith Lectures a couple of years ago, which are still online and you can listen to them. And you know, he’s not an alarmist. He’s a serious person and a serious thinker. So we want to listen to him. But I guess what I’m just saying is, you know, every time some sort of new technology or new use case for technology comes up, there’s a group of people who come out and freak out and they get lots of op-eds. It’s usually men, I must say. There’s a lot of women doing some really interesting scholarship in this area that don’t get the op-eds and quite the publicity. So there’s that. Then it is interesting because it makes people think about technology ethics, usually, again, from a place of either fear, right – Are they going to kill us? Are they going to take our jobs? Are they going to remove human agency? – or money – How are people going to make a huge amount of money? Who’s going to make the money? And by displacing whom, right? So, we have two levers: incredible doom or incredible opportunity. And that leaves the rest of us, I think, probably somewhere in the middle, scratching our heads and going like, is this going to actually change my life? And if so, how, and do I really care, given that I’ve got like, you know, a cost of living crisis, recovering from the Covid pandemic for the past few years? Like, if you’re not in this world, it can seem like a lot of shouting. There’s also the question of, do we need new laws? So we know that the European Union has the AI Act coming down the pike. That’s supposed to be passed this year, and there’ll be a two year implementation grace period. So that’s interesting. It doesn’t cover stuff really, like ChatGPT specifically, but then I don’t know if you want good regulation to cover the technology itself, or how technology is used. And I talked about this in my book, like, do you want to regulate forks – a tool – or do you want to regulate use cases for forks? So if I’m, if I stab you, or kill you with a fork, which is totally possible, that is something that we’ve regulated; we’ve regulated the use case, if you will, of murder, or of injury with a fork or frankly, any other tool. So it’s the use case we focus on. We don’t really regulate forks. We do regulate some technologies like bioethics technologies, or biomedical technologies, excuse me, sort of human genetic stuff, anything nuclear. Those technologies we do specifically regulate. So we just need to think about where does AI fit with that? And also, do we need new regulations for everything, or can we use existing ones? And that’s what’s becoming really interesting is that in the US, where I’m from, the main regulator, the FTC, seems to think that it can use a lot of existing laws already. So they’ve been like, if your AI is claiming to do stuff that it can’t, we’re gonna come after you under, like, kind of sort of false advertising, if you will, misrepresenting yourself. They might come after some of the big AI companies based on anti-competition law, right? So no new laws needed for that. And then with the music industry, they’ve been going after all the people who are like, oh, let’s like remix a Drake song, and saying, well, actually, you can’t do that, because you’re violating copyright law, take it down. Right. So again, I don’t want to be like too, too calm about it. Like, we do need to look at some of the use cases that are really problematic and hurting people. But we might actually have a lot more in our arsenal to combat this than we’re currently using. And I think what’s going to happen is, unfortunately, the pace of crafting legislation, and then regulators never fully enforce regulations– Look at the GDPR: no company’s ever been given the full fine. And here in the UK, the ICO is famous for letting companies off the hook, giving them less than half of the original fine. It’s ridiculous. So if you’re going to pin your hopes on regulation, I’m not sure that’s great. I’m weirdly more optimistic about landmark legal cases. So we’re seeing an Australian mayor who was totally defamed by ChatGPT, in Australia, he’s going to be taking or is taking OpenAI to court. And then we might see some of these copyright issues, that could be taken to court, right. And like, that’s where people I think will get more action and, frankly, more respect, because these companies are really happy to pay a lot of money to lobby our lawmakers, and water stuff down. And they always say, Oh, my God, it’s going to constrain innovation. And if they really get desperate, they’ll be like, China! If we don’t, if we’re not allowed to do everything we want, China will win! That is like a– that is just a game that takes everybody nowhere, whereas in the lawsuit angle, that’s interesting, because you’re demonstrating responsibility, you’re discussing liability, you’re having to demonstrate harm. And in the process of discovery, right, you might be able to actually get some of these companies to open up their datasets, how their algorithms work, like, I’m much more intrigued to see where that’s gonna go.\nBrian Tarran\nYeah, but I think I mean, having read your book, I would, I would have thought you might perceive all of this sort of stuff as kind of sticking plasters to put over the the injuries that might be caused by these technologies, right? Your argument seems to be that we have an issue whereby we don’t have a culture of technology ethics. So when we’re thinking about building these tools, or when we’re starting off down the path of creating something like this, we’re not already thinking about, you know, the use cases, the harms that might arise from that and things like that. What does it take to build a culture of technology ethics, do you think, in our society, in our academic institutions in our companies?\nStephanie Hare\nHonestly, I think, I think this whole accountability piece is going to be what it takes. Because you see, like Alphabet CEO Sundar Pichai gave an interview recently to CBS 60 Minutes in the US where he was like, yeah, there’s a risk that this technology could get out of control, like dot dot dot, this would be terrible for mankind. And you see him kind of be like, hope somebody does something about that. And it’s like, I know somebody that might do something about that, Mr Pichai – you! But clearly he feels, and you can see his point, he feels that right now, if it’s not illegal, then it’s permissible. And he has to win market share. If he doesn’t do it, he’s going to lose. And companies have this all the time. If they wait too long, they lose their first-mover advantage, and they get destroyed. We can go through like countless examples of that in business, particularly in technology. So I get it. But what he isn’t understanding is that if his company is the one that puts out the technology that leads to terrible harm – you know, physically killing people, harming them, destroying the national security infrastructure, something like that – right now, I don’t think he’s thinking about how that’s going to affect him. And that’s because we don’t really penalise executives very often. The worst that might happen is they might leave with a huge golden parachute, and go off and sort of retire in Hawaii with their millions, right? Like nothing really happens to them. So how do you have a culture of technology ethics, where the people who are creating technology and have the power to stop, right, to maybe like back off on stuff, they aren’t really thinking about how will I personally be held responsible if this goes south? So like, Sam Altman and OpenAI, same thing, he was like, he gave an interview where he’s like, I’m really scared about this technology I’m building. It’s like, okay, you could slow down or back off, you could make your datasets open, you could make your algorithms open. You’re called Open AI, that was supposed to be your whole mission, why you were created was to benefit humanity, like, what are you doing? So it’s weird. And I think it comes from the fact that, you know, move fast and break things was the mantra for this culture for a really long time, at least out of the US. And it made a lot of people a lot of money, and they got worshipped by the media. And you know, they have a whole audience of bros who are fans of them. And they’ve never really, any of them, been held to account for what they’ve built.\nBrian Tarran\nSo your interest in technology ethics clearly predates you know, that all the noise at the moment around large language models and generative AI and things like that. What was it that got you interested in this subject? Was it a particular application, something that caused some concern? Or– How did it come about?\nStephanie Hare\nThis is gonna sound completely weird, but it didn’t come from my experience in tech, really, at all. I have had two paths in my adult life, one has been working in these technology companies with a brief but happy foray in political risk, which is now sort of part of the skill set for tech. But I trained as an historian, and I interviewed someone who was a French civil servant, who at the end of his life was put on trial for crimes against humanity for his actions as a young civil servant during the Second World War. So he collaborated, as so many French civil servants did. And in the course of that collaboration, over a period of many years, went from just, you know, just signing documents and kind of doing what he was told to do, to deporting people and sending them to Auschwitz. So I was very young when I interviewed him, and that marked me, as I would hope it would mark anybody. I talked with him on and off for about three years, until he died. And that was the subject of my PhD. And then I did a fellowship at St. Anthony’s College, Oxford, and spent years looking at it further. And that’s actually going to be my next book. I needed a long time to sit with that material and to read around it, but I had to get the interview while he was still alive. He was so old, he was 93 when I started talking to him, and so it was important to get that down for posterity’s sake first, and then circle back and do some analysis later when I was a bit older. When you talk to somebody who in his case was, you know, not antisemitic, not of the far right politically, was actually like centre-left, had lots of Jewish friends, etc. Was like top of his class, you know, came from a milieu and a background and a formation that I think many of us would read and be like, Okay, that seems pretty reasonable. You ask yourself, how on earth did that person in his, like, young period of his late 20s, early 30s end up being involved and actively participating in what ends up being mass murder. It’s probably the most extreme case study of ethics, or one of the most extreme case studies of ethics that I could have stumbled upon. And it stayed with me and to be honest, it shapes a lot of my work and how I think about human rights and civil liberties and the freedoms that we so often take for granted because I’ve studied, as an historian looking at France and Germany, how quickly those things can be taken away – very quickly, terrifyingly so, in fact. So that lens is always with me. And when I was then working in technology, and seeing some of the things that could be done with these tools and watching this lack of accountability, down to the point of gross negligence in some cases. And also, as a young technologist, not being given any training – we were given no training at all in ethics, in, like, discussing data protection – it was basically: this is the law, just obey the law, like, that’s the, that’s the box that you have to play in. Other than that, like, go for it. And when I look back on that now it’s like, Oh, my God, that’s the equivalent of putting your family in the car, and everybody goes off without wearing their seatbelts on and, you know, all this sort of safety design that we take for granted in cars now, it’s just mad when you think about it, or the way we used to fly. We’re in this phase, it’s really interesting, just over the course of my career – 25 years – where the stuff that we’re talking about today that dominates the headlines, right, that is dominating the discussion in the tech sector, was not discussed at all at the turn of the century, other than by maybe people in the science and technology studies domain or academics. But it wasn’t filtering into boardrooms. It wasn’t on the front pages of newspapers, and it wasn’t being covered in the national news, whereas like now that is all I’m doing. So it’s amazing. A whole field has sprung up.\nBrian Tarran\nI think that that kind of origin story, if you like, explains some of your, perhaps, belief in the importance of exploring this accountability question when it comes to technology, ethics?\nStephanie Hare\nYeah, because I watched it, and I think what was so fascinating– So as I say, I was in my early 20s. In fact, I was 20, when this man was put on trial, and I had just moved to France. It was the longest trial in French legal history – it was a big deal, you could not not watch it. So I was reading this and seeing it in the press every day, and I watched the French people discussing it around me, you know, really being divisive, this stuff does not go away. And his view was: I was just following orders, I was doing what I was told to do. Which you know, you hear that a lot from engineers or people who are like, this is the design spec I’ve been given, or this is what my boss has told me to do, or this is what our investors want, etc. Or people feel they don’t have the power to stand up because, you know what, they’ve got a mortgage, they’ve got kids, and employers know that, like, they know that and they use it as leverage against people to silence them. Or they’ve signed an NDA, because we get made to sign these NDAs when we work in tech, and then we get made to sign another NDA when we leave, right, so we can’t disparage our employer, and maybe we’re given some money so we don’t talk about the things we’ve seen. You know, it’s, it’s gross – it’s a gross little world, and like you have to be very, very solid and take good care of yourself to work in it, I reckon. To try and keep your ethical and moral compass. It’s hard. So I think because I saw that. And I saw that someone who – whether we believe him or not, this is what he claimed – in his 30s, he was just doing kind of what everybody around him was doing under a situation of crisis. He was let off the hook. I mean, he wasn’t just not persecuted in 1945, he was actually promoted. And then he became France’s top civil servant, and then he became an MP, and then he became budget minister. I mean, this guy’s career was not hurt in any way by what he did. On the contrary, right, he advanced. And yet, by the end of his life, French values had changed, so a new generation wanted to hold him to account. And I think about that a lot for all of us, right, who are sort of walking around in our 30s or 40s. Another generation or two, when we’re older, might look at some of what technology we’ve built or our behaviour on climate change, our track record – did we do what we could have done to slow global warming, to improve biodiversity? – and they might, they might hold us to account saying, you could have stopped this and you didn’t, right? It’s not just what you did. It’s what you did not do. Right. So we have to be super careful when we think about ethics, because ethics change, values change over time. And what seems okay today may not be okay in 10, 20, 30 years time, and we might be the 80- or 90-year-olds who are put on trial. That is on my mind all the time, right. It’s not very relaxing.\nBrian Tarran\nNo, and I guess it makes me think. Well, I mean, this is getting into the hypotheticals right. But is it– if we can’t necessarily predict or plan out how values might evolve over time, is it enough to be able to, to just say or to document that we asked the right questions at the time, rather than just doing things blindly. Is that where we need to kind of almost formalise our process of writing down, setting out, you know, we want to do this, we’ve considered these potential harms, we’ve considered these potential benefits, and we kind of document that so at least, you know, future generations can say well, “They thought about it. They might have not thought about it in the right way, but they tried”?\nStephanie Hare\nAbsolutely, I think, you know, show your work and be like, these were our, you know, these were our sort of first principles of where we were starting from, this is the context in which we were making this decision. Because again, I don’t, I don’t necessarily fear the judgement of history in terms of if I get something wrong. People get stuff wrong all the time. That’s just being human. It’s, did I not care? You know, was I like, well, sorry, little little boys and girls who are babies now, like, I need to do my stuff, and like, I don’t care about you, right? That attitude is tough. Or I decided I just really, you know, I really needed to buy a flat. So I decided to work for some dodgy company, or dodgy, dodgy company that’s owned by a foreign government, but I knew it was going to be fine, and they’re offering me a tonne of money, and now I can go on nicer holidays. I’ve had these conversations with people about this literally this past week, like, these are live issues for people. There’s a cost of living crisis, ethics can feel like a luxury for some people rather than a necessity. And human beings are very bad, all of us, at thinking about, you know, future selves, right? Like we kind of, we optimise for how we’re feeling now, and we’ll deal with 20 years from now later if we even get there. So I think there’s that. There’s also– this really inspired the writing of the book, Technology is Not Neutral. I knew, I had this weird sense – I had just gone independent, so I had left working for these companies, I was not under any NDAs anymore, which right there gives you a clue; I could say what I wanted – but I also knew there was a chance that I was going to have to go back either into industry, or maybe work for a government, I don’t know what I’m going to need to do in the future or who I’m going to want to work with or what reasons I might even have for that. But I knew I had this window of being an independent researcher and broadcaster, that I could say whatever I wanted, and I had that thing of like, okay, if you’ve had a window of, say, five years, for example, what would you say if you were not afraid? If you were not scared? If you were like, you know, screw the money, screw the corporate pressure, screw the government, whatever, what do you want to talk to the public about? And my views were, I really wanted to talk to them about facial recognition, because I feel people just fundamentally do not understand how dangerous that technology is and how it can be used. I wanted to talk about the pandemic technologies, because we were, you know, I was writing it during the pandemic, and I thought, well, if a pandemic ever happens again, let’s have a nice little tidy case study for potentially future historians or future medical personnel, public health officials to pull out, because when the pandemic hit, we all had to go back and look at stuff from the Spanish flu. You know, there’s a lot of discussion of like, why has this come as such a surprise? Are we going to use these technologies again or not? Right. Like, you know, is it worth it? Is the return on investment worth it in all senses – ethically, as well as medically, all of those things? So I thought I would lay down a couple of markers that I hoped would stand the test of time. But the big thing I wanted to do, because I was always thinking I might have to go and sign another NDA and go work because I too must earn my living, was I wanted to write something so that anyone who cares about technology, is working in it, is investing in it, right – it’s not just people who code, it’s people who fund the people who code. Buying technology – procurement is massive, you’re a really powerful person if you’re in charge of procurement. But also just consumers, and citizens and parents, and teachers and kids. If I could write up everything that I had learned in my 25 years, and succinctly as possible, right – as short as possible, because people are tired, they’re busy – I could pass that baton on, so that if I ever have to stop going on television and radio, and I’m no longer allowed to write in newspapers and warn people about the stuff I’m seeing and the abuses of power, and showing them examples of history of how this can go so terribly wrong, maybe it will have, like, lit somebody else. And I’m delighted to report – I mean, we’ll see; time will tell, it’s only been out a year – the amount of people who have brought me in to train their staff, to talk to their board. I’ve talked to children. I’ve talked to university students, I’ve taught classes all over the world, because we can now do online teaching. I’ve taken a lot of it on television and radio and in the newspapers. People wanted this, and I’m not the only person working on it, of course – there’s been a whole flowering of people, scholars, etc., working in putting out amazing books and documentaries. It’s really, we’re having some sort of moment with technology ethics – AI ethics being just a branch of that. So that’s really encouraging. So I sort of feel like, you know, again, if I, if I’m gonna have to account for myself at the age of 93, I would like to be able to point to that and go, I tried. I tried. And I have no idea if it will succeed or not, but I stood up to the plate and I swung the bat and, you know, I aimed for the bleachers.\nBrian Tarran\nOne of the things I thought was really interesting about the book, it comes towards the end when you’re kind of talking about, you’re summing up, and you talk about how your thinking about almost like the the approach or solution to the technology ethics issue has changed over the course of the writing of the book. You had, like, a list of potential, like, proposals, proposed actions that you wanted to analyse, but then you realised that actually technology ethics is a “wicked problem”. I wonder if you could explain what that term means for people who might not be familiar with it and and why you think of it that way?\nStephanie Hare\nYeah, I’m so grateful to have learned about the term “wicked problem”. My friend Jason Crabtree, who wrote an amazing book about electricity grids, like smart electricity grids, for Cambridge University Press, had asked me to read his manuscript maybe 10 years ago, and I read it, and one thing I took away that just absolutely blew my mind was this concept. So I shall gift it to you for those of you have not heard it. Because then suddenly, you’re like, God, it makes so much sense. There are certain problems, I would say, like, the climate crisis, and biodiversity loss would be a good example of this. There’s certain problems that have many causes, many causes, so there isn’t going to be one solution to fix them. So people constantly ask me, oh, is this the magic bullet? No, they’re like, there are certain problems that there is no magic bullet – the pandemic is probably another, actually. Then, if you do try to solve these wicked problems, the mere act of solving them can introduce a whole new set of problems to them. So like, it becomes even more of a head– You know, I’m trying not to swear, but a messing with your head moment. And it’s exhausting, you know, and it gives you your forehead wrinkles and makes you just sort of want to bang your forehead onto the nearest wall. And yet, you also can’t opt out and be like, well, it’s just too hard. It’s a wicked problem. There’s no solution, there’s nothing to be done, you know, throw up your hands, because you’re like, Yeah, but the problem is, is if we don’t do anything, like literally people are dying; literally, climates are becoming uninhabitable, we’re going to have massive climate migration, that’s going to cause all sorts of problems, water scarcity, we can have wars over this, like, we have to do something. So like, you have to still act on a wicked problem, all while knowing that it’s not going to be solved in a binary sense of like zero, one, black and white, I can point to it and measure it. And for people who like metrics, that’s a real pain, because they’re like, I want to know what good looks like and I want to know how we’ll know when we get there, you know, what’s the percentage, what’s the number? And you kind of have to be like, Well, with a wicked problem, you might never solve it, or you won’t solve it once. Because again, with something like climate change, or pandemics, these are things you’re probably gonna have to solve again, and again, and again, because it’s dynamic. And it’s constant, you know, we’re always going to be managing our relationship with the climate, with the environment. So, you know, we can pick a certain temperature, or a certain percentage of landmass that, you know, has trees or whatever, and like, come up with a little metric for our metric-oriented friends. But that’s still not very meaningful. So it’s more that when you think of a wicked problem, like facial recognition technologies, like, we need to be able to identify people in certain situations, and like, we want that, right. Like, we want to be able to catch criminals, and we want to be able to catch terrorists when they’ve managed to pull off a terrible act of, of harm to people. But at the same time, do you want to turn your society into a sort of permanent dragnet? Do we not care about privacy? If so, like, when do we care about privacy? And when are we okay with maybe sacrificing that for the greater good and who decides that? It’s really problematic if you live in London as I do, and your police force, which is using this technology, has admitted, they’ve admitted themselves to being misogynist, institutionally misogynist, homophobic, racist, right? And then we’re gonna give them a technology that doesn’t work very well on people with certain skin. It doesn’t identify people of a certain age as well. It’s got all sorts of problems. So you’re kind of like, hmm? Facial recognition is covered a bit under the EU AI Act. But even then there’s like so many loopholes. And the thing is, if you cite national security, it usually gets waved through because no one wants to be the person who said, I said we couldn’t use this technology, and then something bad happened. Right? So you err on the side of, like, the precautionary principle. The default is, let them use it. We must trust them. Except what do you do if your police force has given you quite ample evidence not to trust them? Or companies. You know, this is not to bash on the police, by the way – companies are some of the worst offenders in this area. So that’s what I mean about it being a wicked problem is, it’s out there. It’s installed. It’s all over the UK, it’s definitely all over the US as well. And we don’t really have a good framework for it.\nBrian Tarran\nBut then this is where we loop back around to the kind of culture, right, creating a culture of technology ethics? You know, we can’t just put a checklist in place once, do it, tick it off, yeah, we’ve done that for facial recognition technology, we’re good to go. Because there are always new potential use cases for it, new applications, new integrations with different systems that we always need to be thinking, every time, is this the right thing to do? Or–\nStephanie Hare\nI mean, I’m still a big fan of checklists. So it’s not that I’m anti-checklist. And I’m not saying that you said that, by the way, I’m more thinking aloud. Checklists can still be useful, right? Like, whenever I’m in a really bad mood, I’m like, Okay, hang on, have I slept? Do I just need a glass of water? Am I hungry? You know, what I think is angry may just be that I skipped lunch. You can kind of go through those things, and anybody who’s had to like troubleshoot why a baby or a small child is unhappy will also have a checklist and what’s on the thing – ah, they missed naptime, where’s their bear? That sort of thing. Companies need checklists, because you’re trying to get loads of people singing from the same hymn sheet, I get that. But what I wanted to get away from was this idea that, like, one person or one team gets this checklist and maybe does that once a year, once a quarter, pick your cadence – and everybody else gets a pass. Ethics doesn’t work that way, because again, ethics is kind of I think in the wicked problem scenario of, like, how do we decide what our values are and how to live them? And how do we know, where do we draw the line? And then how do you, how do you decide if you’ve gone over the line or not? And all of that, who decides who decides? Those are really complex questions that mean that really you can’t abdicate. This is, in a company’s case, it’s the CEO, it’s the board, all the way on down – it has to be baked in to every single employee, and also investors, mindset. And I was thinking about it in terms of cybersecurity, I once had a colleague who gave me an analogy that I think is helpful, so I’ll share it for what it’s worth: When you go on to, say, an oil rig, in the North Sea – a highly dangerous environment – you might be the most junior person there, and you’re there for your very first day of work. But if you spot something on that rig that is a health and safety risk, you have to speak up. You’re not going to go, like, Oh, my boss might say something, whatever, because, like, everybody’s life on that rig is depending on everyone having that culture of careful, that’s not okay. And that really put me in mind where it was like, Oh, wow, we’re going to have to like inculcate an entire new mindset. And we think about technology ethics a lot because of technology being the word, and we think it must mean like hardware or software, it’s always about coding, and it’s often guys in hoodies coding. But my preferred method of hacking is culture. Right. So like, again, if we tried to just solve everything through regulation and laws, that takes, you know – if you look at the average time it takes to pass a law and then for regulators to enforce it – ages, we’re talking years, like it’s too late. These technologies will have moved on. Ditto, calls for international treaties. Do it, by all means. Have a look at how long it takes most international treaties to get passed and then ratified – and then, P.S. What happens when people break them? Really, right. So like, they’re important, they’re necessary, but they’re insufficient. You can act a lot faster if you can get people preventing stuff from being built in the first place, and that means you need to have a culture of people working in technology, both within the organisations – whether that’s research labs, government, companies, universities, whatever – and on the outside – journalists, academics, thinkers, etc, just the public, an informed public – who can see something and do what I just described on the oil rig, like, sound the alarm and go, Wait a minute, hang on. That’s not okay. That is, that to me feels faster. And I’m way more into prevention than cure, for all sorts of reasons. So I think like, yes, to laws and regulations, yes, to treaties; this will be faster. And I think it will be more resilient.\nBrian Tarran\nYeah, I agree. And I have to say, wrapping up, that I think Technology is Not Neutral is a great place to start to inculcate that mind shift, that mindset change. So, Stephanie, thank you very much for joining us today.\nStephanie Hare\nThank you for having me.\nBrian Tarran\nYou said you’re working on a new book. Have you got a timeline for that? Or a title?\nStephanie Hare\nNo. I am the slowest thinker and writer, I’m like the opposite of move fast and break things, I’m like move slowly and like think it over maybe several times. So I’m just getting started out. I’ll sort of go five years. It’s gonna be, it’s a history book. Alright. So this is, this is different, I’m having to take my classes in French and German right now to get kind of match fit in those languages again, and then you know, I’ll be off and writing. But, yeah, I hope to have another book out, you know, in five years.\nBrian Tarran\nWell, if the year it took me to read Technology is Not Neutral is any indication, in three, four or five years time it will still be relevant today. So–\nStephanie Hare\nThat’s the thing with history, it always stands the test of time.\nBrian Tarran\nWell, thank you, thank you again for joining us on Real World Data Science. It’s been a pleasure talking to you.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo of Stephanie Hare is not covered by this licence. Photo is by Mitzi de Margary, supplied by Stephanie Hare and used with permission.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics.” Real World Data Science, April 28, 2023. URL"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2024/03/11/democratizing-data.html",
    "href": "foundation-frontiers/interviews/posts/2024/03/11/democratizing-data.html",
    "title": "Democratizing Data: Using natural language processing and machine learning to capture dataset usage",
    "section": "",
    "text": "Figuring out how much money is spent annually on collecting and publishing datasets is a challenge. According to the World Bank, it is “painfully hard to obtain” information just on government spending on data, never mind all the other bodies and organisations who invest in the creation of data assets. But there’s an even more challenging figure to pin down: How much value does all this data provide? By and large, there is very little data – or systematic collection of data – on dataset usage. There’s no easy way to find all the users of a particular dataset and to see how the data has been used, or what research topics it may have contributed to.\nWriting in the Harvard Data Science Review in April 2022, Julia Lane and others explained that “the current approach to finding what data sets are used to answer scientific questions … is largely manual and ad hoc.” They went on to argue: “Better information on the use of data is likely to have at least two results: (i) government agencies might use the information to describe the return on investment in data sets and (ii) scientists might use the information to reduce the time taken to search and discover other empirical research.”\nThis hope for a better understanding of how datasets are used and the value they provide underpins the creation of “Democratizing Data: A Search and Discovery Platform for Public Data Assets.”\nAs described on the platform’s homepage, Democratizing Data “describes how datasets identified by federal agencies have been used in scientific research. It uses machine learning algorithms to search over 90 million documents and find how datasets are cited, in what publications, and what topics they are used to study.”\nBut this is just the start of what Lane thinks the project could eventually achieve, as she explains in this interview.\nHow did the Democratizing Data platform come about?\nIt emerged from three things, really, and I can trace the start of it back to 2016, when I was asked to build a secure environment that could host confidential microdata in order to inform the work of the Commission on Evidence-Based Policymaking.1 They asked me to lead on this because I had built the NORC remote access Data Enclave at the University of Chicago 10 years before that.\nThis was the first step towards Democratizing Data.\nThe second step was figuring out how to create value from the data in the secure environment, because we knew that if we couldn’t create value, government agencies weren’t going to put their data into it.\nBut how do you figure out what agencies are going to want to do with the data in a secure environment? It’s difficult to get them to tell you what they want, so I thought, well, why don’t we build training classes, put people in these training classes and have them work on problems with their own data? That way, we’re going to know what problems they have so that we can address them.\nSo, step 1 was build secure environment. Step 2 was build capacity and identify the questions that are of interest to agencies so that they would put data into the secure environment. That led to the training classes that Frauke Kreuter, Rayid Ghani, and I put together, which are described here.\nBut then what happened was, people kept coming to me in the classes and asking, “Who else has worked with these data, and who can I go to and ask questions?”\nI could give them a list of people, but that list would be biased by my age and race and sex and the people I know. What about those people who are doing really interesting stuff with these data that I happen not to know about?\nSo, that’s how Democratizing Data got started. I thought, really the best way to give a full answer to those sorts of questions is to figure out what datasets are being used in research publications.\nNow, how was I going do that? I could read all the publications and manually write notes about who the authors were, and what the topics were, and what datasets they used. But that’s not realistic. So, I thought, well, maybe we could combine natural language processing techniques with machine learning so that you could “read” all these publications and find out how datasets are cited.\nWould this have been a problem that needed solving if there were common, established citation standards and practices for datasets?\nThere are great citation practices for datasets, and we’ve had them for 15 years. Back when I was a National Science Foundation (NSF) program officer, everyone was saying, well, if we just get the plumbing right, people will come and use it. Well, they don’t. Even when there are DOIs available and they’re relatively easy to cite, people don’t. The plumbing’s there, we built it, but they didn’t come.\nSo, I think there has to be a demand-side piece, and we talk about that in one of the papers in an upcoming special issue of the Harvard Data Science Review: How do you create an incentive structure so that people do provide information about how they’ve used data? My thinking was that, suppose we can find out who’s using what data. Then the incentive structure to an academic is “your name in lights”: you are the world’s living expert in orange carrots with green stripes, or whatever. So, we would read the publications, find the datasets, and then you could have a leaderboard of the people who have done the most work in a particular field, and then people would have an incentive both to cite datasets and to let you know when you miss things. And that was when I thought, well, let’s put all this information up in a dashboard.\nSo, is the grand vision for this to create a platform that, essentially, any data owner – anyone who publishes datasets – could plug into, connect to, and understand how other people are using their datasets?\nThat’s right. The grand vision is basically to set up a search and discovery portal. Originally, my thinking was that would be super helpful for people just starting out in a field; for a new graduate student or a postdoc to say, “I want to figure out what work has been done on recidivism of welfare recipients relative to access to jobs and neighborhood characteristics,” for example, and for them to see what datasets are available and how they have been used.\nBut from the data producer side it’d also be useful to know: Who’s using the datasets? Where are the gaps? Where are we maybe not reaching as many people as we thought we were, and how can we change that?\nSo, while the original idea was to build a platform for researchers, plans changed when the Evidence Act passed, and agencies were required to produce usage statistics for their datasets.2\nWe started a pilot with the US Department of Agriculture’s (USDA) Economic Research Service (ERS). They have been a huge supporter and helped us work through a lot of the issues. Then, when we began showing around the ERS wireframes and ideas, the NSF National Center for Science and Engineering Statistics joined in, and so did USDA’s National Agricultural Statistics Service and the National Center for Education Statistics and the National Oceanic and Atmospheric Administration.\nSo, we have these agencies involved and they’ve really been the drivers, the intellectual partners, pushing the design and the structure forward.\nI’ve had a chance to play around with some of the public dashboards you’ve released on Tableau, and I really like the way you can explore dataset usage from different start points and end up with a list of publications that use those datasets. My question is, though, how have you connected all this up – datasets and publications?\nOur start point was scientific publications because these are pretty well curated. We ended up working with Elsevier because Scopus [Elsevier’s abstract and citation database] is a well-curated corpus and they’ve got the associated publication metadata well curated.\nSo, we have the Scopus corpus, and we then ran a Kaggle competition to develop machine learning models to identify candidate snippets of text from scientific papers that seem like they might be referring to a dataset.\nHuman researchers would then validate those snippets as either referring to a dataset or not, and once they’ve validated the publication-to-dataset dyad, we then pull in all the metadata associated with the publication: authors, institutions, key topics, publication year, countries, etc. – all this information gets piped over to the dashboards.\nYou published a Harvard Data Science Review article about this competition a couple of years ago, and from that I understand that you can actually get quite far with a simple string-matching method for finding datasets, but you would still miss a lot of citations using this approach because of the variability in the way people refer to datasets.\nThat’s right. There were three different models that were developed, and each one picks up different aspects of how authors mention data in publications, and all three have been extremely useful. We learned a lot about the variety of ways in which researchers cite the data that they use.\nIt turns out that more people do cite datasets in references than we had originally thought, but usually they don’t cite a DOI, they cite the URL or they cite the exact name of the dataset, so string search of references and URLs pulls out quite a lot of information that the DOIs, per se, don’t.\nWhat are the next steps for scaling up the Democratizing Data work?\nI think it is more of a sociotechnical issue than a technical one. We have the plumbing, but really what we need to do is to figure out the incentives for researchers. We need to build a community around the data, which is what’s happened with code and the sharing of code on platforms like GitHub.\nObviously, our initial focus has been on federal statistical data, but there’s also a lot of interest in how administrative data or streaming data are being used.\nThe advantage of starting with statistical data is that they have names. As we learn more about citation patterns, though, it may be that we don’t need precise names. What may happen is that the community starts converging on common terminologies for datasets. That happens in a lot of fields.\nAt the moment, it feels a little bit like the Wild West. We’re searching for ways to measure how datasets are used, how they’re valued, and so on. Federal statistical data and Elsevier’s Scopus have been a great starting point for us, but the broader vision is to incorporate other datasets and other publication databases like arXiv and Semantic Scholar. But all those other datasets that are out there, they need to be curated and documented in some way and that’s a huge task, so the solution has got to be community curation and sharing, right?\nIf we don’t build a community around the data, we’re just going to have really bad information, really bad analysis, and really bad statistics on the value that our datasets – all these data assets – provide. My colleague Nancy Potok gave a talk a couple of days ago in which she said that our future depends on this – and it really does."
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2024/03/11/democratizing-data.html#footnotes",
    "href": "foundation-frontiers/interviews/posts/2024/03/11/democratizing-data.html#footnotes",
    "title": "Democratizing Data: Using natural language processing and machine learning to capture dataset usage",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Commission on Evidence-Based Policymaking was “charged with examining all aspects of how to increase the availability and use of government data to build evidence and inform program design, while protecting privacy and confidentiality of those data.”↩︎\nSpecifically, the act requires federal agencies to identify and implement methods “for collecting and analyzing digital information on data asset usage by users within and outside of the agency.”↩︎"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2024/01/15/census-bureau.html",
    "href": "foundation-frontiers/interviews/posts/2024/01/15/census-bureau.html",
    "title": "‘We absolutely have to transform and modernise our operation’ – US Census Bureau director Robert Santos",
    "section": "",
    "text": "A month ago now, Real World Data Science published an interview with UK national statistician Professor Sir Ian Diamond. In the process of preparing the text of that interview for publication, I found myself reflecting on a conversation I’d been part of earlier in the year with Robert Santos, director of the US Census Bureau.\nI met Santos in Toronto, Canada, in August – a few hours before his President’s Invited Address at the 2023 Joint Statistical Meetings. The meeting was arranged as a joint interview with Anna Britten, editor of our sister publication Significance magazine, and Santos was joined by Sallie Ann Keller, the Census Bureau’s chief scientist and associate director for research and methodology, and Michael Hawes, senior advisor for data access and privacy.\n\nThe interview with Santos, Keller, and Hawes was published in the October issue of Significance, so you may have already read it. But, following on from our Sir Ian Diamond interview, I thought it worth highlighting some of what Santos et al. had to say, particularly where key themes, challenges, and opportunities seem to resonate across both the US Census Bureau and the UK Office for National Statistics.\nI’ve also gone back to the original interview recording to pick out some previously unpublished comments.\n\n\n\n\nOn the scale of the challenge Santos inherited on becoming director of the US Census Bureau in January 2022\nRobert Santos: Certainly it was formidable – although I’m comforted in knowing, after a year and a half, that the career staff [of the Census Bureau] were well positioned to accept this challenge anyway, and were working on it. But the challenge was real. We had the pandemic. We had to, basically, not redesign but scramble and adapt to a really threatening situation where the entirety of the 2020 census was conducted before there was a vaccine, and when people didn’t know the nature of the beast. A huge chunk of this operation was conducted when society was shut down. And not only did the Census Bureau need to rethink, nimbly and quickly, how to do its operation, but so did all of the different community partners – which was really enlightening because we realised that, at the end of the day, we could not have completed this job alone. And now our position is that we cannot complete our mission without the external community. They’re the extra folks we need in order to understand better what the needs are, and therefore improve our methods and data and the relevance of what we’re doing.\nSo, we see our role now as having a continuous engagement with the entire country at all levels – be it elected officials, universities and professors and the research community, or data users like policy users and policy researchers, or local community organisations that are doing neighbourhood stuff. And so we’re actively working between censuses to engage them and show them the value of the data that we’ve collected – not just decennial [census data], but also our flagship American Community Survey and our Current Population Study and all the 130 other business, economic as well as household types of studies that we’re doing.\n\n\nOn the need to transform Census Bureau operations\nRobert Santos: We absolutely have to transform and modernise our operation, from what was historically this transactional survey type of data collection – where we go to somebody that’s randomly sampled and we say, “Please give me your information” – and realise the value of taking that information, blending it with existing data, administrative data, even third-party private sector data, into a huge data pool and linking it together, and that will create new data products that will serve the public in ways that we never imagined before. And we already have some great examples of that. So, that transformation process is an incredible priority that we have to do, regardless of what our funding situation is. If we don’t do that, we’re not going to be able to serve the public in the way that we need to.\n\n\nOn laying the groundwork for the 2030 census and an increased use of administrative data\nRobert Santos: There are a couple of things going on. One is that we’re obliged, because of our values of scientific integrity, objectivity, transparency, and independence, to let folks know what we’re doing in terms of our use of administrative records, and we’ve done that and we will continue doing that. The big lift was really in preparing for the last decennial [census], where we took the use of administrative records to new heights in terms of their utility – not only to help us for some enumeration of households, but, more importantly, to help us predict which households were occupied or not, or to predict which households would benefit from the use of administrative record enumeration versus which ones wouldn’t, or how many times should we knock on the door before we do something else. And now, with that knowledge, we’re looking back at 2020 and saying, what worked? What didn’t? How can we exploit it? And we’re kind of moving the dial to say, “What can we take more advantage of for 2030?”, with full recognition that there were some subpopulations, there’s some segments of society, that we really need to focus and hone in on to make sure we get a good count.\n\n\n\n\n\n\nRobert Santos, director, US Census Bureau\n\n\n\n\n\nWe absolutely have to transform and modernise our operation, and realise the value of taking [survey] information, blending it with existing data, administrative data, even third-party private sector data, into a huge data pool and linking it together, and that will create new data products that will serve the public in ways that we never imagined before.\n\n\n\n\n\nOn addressing public concerns about data collection and data privacy\nMichael Hawes: Even though the decennial census is mandatory under law, we rely on voluntary participation. We’re relying on people being willing to respond to their census. In the lead up to each census, we do an extensive survey of what are the attitudes or motivators that will encourage people to respond or to not respond. And one of the recurring themes in that is concerns about privacy, concerns about how their data can be used. So, in order to help encourage people who have those concerns – and this is a sizable percentage of the population – we do need to have strong messaging about how their data are protected, how they can only be used for statistical purposes, and so on. But that has to be in very easy-to-consume sound bites, because a lot of people don’t have a background in statistical disclosure control or even in the legal conceptions about what privacy is. So, that is a real challenge for us. How do we convey the fact that we are taking this very seriously, and that their data are protected, in a way that people can kind of internalise and respond to?\n\n\nOn making sure statistics serve the public good, and the role of the Census Bureau in supporting data literacy\nSallie Ann Keller: In the US over the last decade, there’s been a really large movement around data for the public good, data science for the public good, and it’s really focused at trying to engage researchers and scholars – and we’re talking about high school students, community college students, undergraduates, graduate students – trying to engage them with civic engagement around data and data insights. That’s happening all over the country – really trying to democratise data and bring it in service of the public good. And I think that’s very exciting.\nMichael Hawes: We have a whole programme called Statistics in Schools which is about taking census data and making it valuable to teachers in the classroom, and allowing students at various levels – from elementary school through high school – to be able to engage with the data and use it to inform their own learning, and to learn about their own communities. That is especially profound in the years around the actual census, because that also serves as a catalyst for getting households to respond. If the kids are using the census data within the classroom, then they go home and say, “Hey, have you filled out your census form?”\nRobert Santos: It’s really important to start young, but then there’s also folks who want to use the data who are adults. So, we have something called the Census Academy, where you can go on to YouTube and get tutorials that show you visually somebody trying to use census data. And the second thing we do is, we really have a strong commitment for creating easier platforms for folks to access and utilise various types of data produced by the Census Bureau. We’re creating these data visualisation tools that bring together the demographic data that we collect, the economic data that we collect, and visualise it down to the census tract level so that local communities can pull that up. And then finally, in terms of the public good, there’s also work that we’re doing with the Federal Emergency Management Agency and the National Oceanic and Atmospheric Administration on our community resilience estimates to create the same type of data visualisations that can show where the potential worrisome geographic spots are.\n\n\nOn the opportunities for bringing together Census Bureau data and large language models\nSallie Ann Keller: We’re not going to be in the business of building generative AI models. But what we want is the statistics that we put out, the data that we put out, to be picked up by these large language models – to be kind of an input into generative AI. So, we are focused on that in terms of really looking at the structure of how we’re disseminating statistics, and how we’re disseminating things like data tables. How harvestable are they for AI? What are the guardrails we should put around that? We’re looking at and considering issues on data integrity, because when questions are posed, we would like our official statistics to be answering those questions, not our statistics translated through three other parties. Data integrity is really a huge issue, because we don’t want false data and infiltration happening that gets branded as our statistics. I don’t know where we’ll take it all, but I think we’d also like to be incredibly creative here. So, let’s suppose you ask a question and some statistic comes back. Well, why not have that be an experience, so that not just a statistic comes back but maybe a question or two comes back, to try to assess the context that you’re really asking about, so that we can not only have our data coming to you, but we can have the right data coming to you?\nMichael Hawes: Even with some of our more traditional statistical data products, informing users of the limitations and the the uncertainty baked into a lot of those estimates has historically been a challenge – even for some more sophisticated users. The number of people who ignore margins of error on data tables, even in our data products, is not insubstantial. And so, when we get into an AI-driven data dissemination kind of framework, how can we use the flexibility of those platforms to not just provide the answers to the questions people are asking, but also to educate and inform about what the limitations of those answers are?\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo of Robert Santos is excluded from this licence; it is a US Government work.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘We absolutely have to transform and modernise our operation’ – US Census Bureau director Robert Santos.” Real World Data Science, January 15, 2024. URL"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/01/25/erica-thompson.html",
    "href": "foundation-frontiers/interviews/posts/2023/01/25/erica-thompson.html",
    "title": "How to ‘Escape from Model Land’: an interview with Erica Thompson",
    "section": "",
    "text": "Erica Thompson’s new book, Escape from Model Land, offers a fascinating and important perspective on mathematical models as being not just models of the real world, or real processes or systems, but also “subjective versions of reality” that encode all sorts of assumptions and value judgements.\nIn this interview with Brian Tarran, editor of Real World Data Science, Thompson talks about the “social element of modelling” and how it manifests, how to counter the subjectivity of individual models with a diversity of models, and whether human-made models are held to the same standards of transparency that are expected of AI-“created” models.\nErica Thompson is a senior policy fellow in the ethics of modelling and simulation at the London School of Economics Data Science Institute."
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/01/25/erica-thompson.html#timestamps",
    "href": "foundation-frontiers/interviews/posts/2023/01/25/erica-thompson.html#timestamps",
    "title": "How to ‘Escape from Model Land’: an interview with Erica Thompson",
    "section": "Timestamps",
    "text": "Timestamps\n\nWhat led Erica to write the book, and why now? (2:27)\nCritiquing climate models (7:30)\nExploring the “social element” of modelling (11:36)\nCountering subjectivity with a diversity of perspectives (20:11)\nAI models, human-made models, and questions of transparency (25:54)\nWhy write a popular science book about these issues? (30:11)\nWill the UK Prime Minister’s “maths to 18” proposal help or hinder our Escape from Model Land? (34:01)"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/01/25/erica-thompson.html#quotes",
    "href": "foundation-frontiers/interviews/posts/2023/01/25/erica-thompson.html#quotes",
    "title": "How to ‘Escape from Model Land’: an interview with Erica Thompson",
    "section": "Quotes",
    "text": "Quotes\n“Putting things in a mathematical language does tend to make people think that it is truth from on high. And so my book, in some way, goes towards saying actually, these models, obviously we hope that they’re based on facts and they’re based on data that we gather, but they also do have this value judgement content as well” (1:51)\n“It is arbitrary how we choose to model a situation. There are infinitely many different ways that you could choose to simplify reality - this huge, messy, complex thing in front of us, with physical laws that we don’t fully understand and things going on that we can only measure by proxy.” (12:09)\n“The choice of assumption has a very direct result in the model output and in the information and advice that you’re giving to policymakers… [In climate models] maybe we have a cost of however many dollars per tonne of carbon dioxide for nuclear electricity or for renewables. But what kind of price would you put on behaviour change? How many dollars per tonne of CO2 avoided does it cost to change the behaviour of a population such that they use less energy? If you put it in at $2 per tonne of CO2, it would be heavily relied on [as a policy response]; if you put it in at $2,000 per tonne of CO2, it’ll never happen.” (16:42)\n“There needs to be more frank discussion of values and value judgments, and politics and social assumptions within models. And I think we are starting to see that with the pandemic models, particularly because it’s been so high profile. [But] it’s really hard to unpick your own value judgments. It’s easier for somebody with a different perspective to come in and say, ‘Oh, actually, you know, you’ve assumed that. Why did you assume that?’ When we are embedded in a particular culture of modelling, it’s particularly hard to imagine that anything could possibly be done differently.” (27:20)\n“I think some people maybe read the book and think, ‘Oh, this is just a sort of woke advertisement for diversity’. Well, it’s not; it’s a way of doing the maths better. The whole point is to do the maths better, make better forecasts, understand the future more effectively, and be able to make better decisions based on that information.” (33:41)"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/01/25/erica-thompson.html#transcript",
    "href": "foundation-frontiers/interviews/posts/2023/01/25/erica-thompson.html#transcript",
    "title": "How to ‘Escape from Model Land’: an interview with Erica Thompson",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove repetitions.\n\n\n\nBrian Tarran\nHello, and welcome to the very first instalment of the Real World Data Science interview series. I’m Brian Tarran, the editor of Real World Data Science, and I’m very pleased to be joined today by Erica Thompson, a senior policy fellow in ethics of modelling and simulation at the London School of Economics Data Science Institute, and the author of a fantastic new book - which I have a copy of here - Escape from Model Land, which is subtitled, How mathematical models can lead us astray and what we can do about it. So hello, Erica, thank you for joining us. I hope 2023 got off to a positive start for you.\nErica Thompson\nYes, it has so far.\nBrian Tarran\nGood, good. Because the book came out, is it just before Christmas or just after?\nErica Thompson\nYeah, just before Christmas. So I’ve had all sorts of things flooding in saying, Oh, I liked your book, or I hated this bit or no, it’s exciting.\nBrian Tarran\nYeah, no, well, it’s, I have to say, I think, I thought it was a genuine– I finished reading it over, over Christmas. And I think it offers a genuinely fascinating and important perspective on mathematical models as being not just I guess, models of the real world, or, you know, real processes or systems, but subjective versions of reality, you know, encoding all sorts of assumptions and value judgments of the people who, who create the models. And I mean, I guess that shouldn’t really come as a surprise, right? But, but is it a point that is often lost in the discussion around models, particularly where decisions might be, like, informed or driven by model outputs?\nErica Thompson\nI think it is something that’s easy to miss. I mean, especially because we’re sort of, maybe as mathematicians were used to living in model land and doing things which, which we see as being logical consequences of previous things. And then more generally, the public look to science and mathematics and statistics as being objective arbiters, perhaps, of how things are and how things ought to be. And so, so yes, that that kind of putting things in a mathematical language does tend to make people think that it is truth from on high. And so my book, in some way goes towards saying actually, these models, they are, obviously we hope that they’re based on facts, and they’re based on data that we gather, but they also do have this value, judgement content, as well. And so we need to think about what that is and how we deal with it, and how we sort of express it and how we understand it.\nBrian Tarran\nRight, yeah.\nErica Thompson\nEspecially where we’re using those models to inform decision making or public policy, then it becomes particularly important.\nBrian Tarran\nYeah, yeah, no, and obviously, your book draws up quite a bit on the Covid-19 pandemic, and how models were used there. But I thought it was interesting, actually, that, you know, in reading the acknowledgments that you– that while the pandemic lent the topic, additional relevance, right, you actually started writing the book before that. So what led you to think now’s the time? What was the tipping point, if you like, of thinking, I want to write this book now?\nErica Thompson\nYeah, okay. Well, that’s an interesting question. I mean, because it builds on the last sort of 10 or 15 years of my work. So I started out doing a PhD in climate physics. And my background before that was maths and physics. And so I was doing a PhD on the physics of North Atlantic storms, looking at how they would change given climate change. And so, obviously, the first thing you do is a literature review. And I started looking at different models and what the what they were saying about what would happen to North Atlantic storms. And what I found there was that there were models saying that the storm tracks would go north, they’d go south, they get stronger, they get weaker, they’d, you know, anything you name it. And interesting, particularly, interestingly, was that they, they had relatively small uncertainty ranges. So they they didn’t agree within their own uncertainty ranges. And that made me think, well, we this isn’t telling me very much about North Atlantic storms. But it’s telling me a great deal about modelling and the way that we do modelling and perhaps we need to start thinking more about how these uncertainty ranges are calculated, what does it mean? What, how can we end up in a situation where we have this level of disagreement between models. And so since then, I’ve been looking at, you know, those kinds of concepts in different areas I’ve been looking at sort of insurance and finance and weather and climate and humanitarian forecasting as well. And, and so in all of those application areas, I found the same questions about uncertainty and how we make inferences from model output to be particularly interesting and how common problems may be solved in different ways as well. So it’s interesting to do the compare and contrast. And so, yeah, then I guess I, I’d been on all these sorts of bitty little projects and thought actually, I’d really like to bring this together into something more coherent, you know, to actually say, look, there’s a, there is a common theme here and we need to be putting it together and drawing conclusions. And we can, we can learn a lot from doing that. And we can share the best practice throughout the sector.\nBrian Tarran\nWhen you’re starting down this path of, I guess, looking into the, I guess, the ethics and process of modelling, did it, was there a lot of other work that you identify that you could kind of draw on a lot of other thinking around this area? Or was it kind of under studied, under researched sort of aspect of the literature?\nErica Thompson\nI think it’s under studied, I mean, of course, everybody who does some modelling, you know, you, you do your modelling, and then somebody says, Okay, we need to put some error bars on the outputs, and you go back and, and think about how we’re going to put the error bars on the outputs. And probably, I would say, most people doing that realise that it’s much more difficult than they have the time to do or the ability within the scope of whatever project they’re doing. But the aerobars, the uncertainties always ended up being tacked on at the end, you do it after you’ve done all the modelling, there’s less incentive to do it. And there’s less resource to do it than to make the model itself better. And I think that’s a very common story, that people realise that they ought to be doing more, but they just don’t have the time the resource, the ability to go and do that. So then yes, there are there are people, and there are particular areas that I think have taken more time to investigate this. So in physical science hydrology, I’d say in particular, has a very well developed history of thinking about the uncertainties in models, maybe because, you know, they are constantly being challenged by events happening, which were not within the models, you know, you’ve got your flood forecast model, and then something happens, and it goes way beyond what you were expecting. And you you have to go back and say, What does this mean for our modelling process. And other areas have much less well developed considerations of uncertainty. And so that’s where I think actually, we could we could really benefit by sharing good practice across these different application areas, because people have looked in different ways. And, you know, with with different levels of statistical interest, you know, some areas go into the stats, much more, some areas are very philosophical about sort of the conceptual foundations of how we should think about models and how we should think about the range of outputs that we get from models. And so what I’m trying to do is bring those together a bit. Yeah.\nBrian Tarran\nYeah, I think you certainly achieve that. It is really interesting, the different, the variety of examples that you present, and the ways you talk around these issues. I did want to focus in particular on climate models, though, because, you know, I was looking around your website, finding a bit more about about you, and I noticed on there that you talk about, you no longer fly to conferences, and that’s in order to kind of reduce your own ecological footprint. So I guess I wanted to ask, you know, when you set about writing a book, and it’s going to be a book that’s kind of critiquing models and the ways that they don’t often agree? Did you have like a nagging concern that, you know, the points you wanted to make about models in general, but climate models in particular, that that would kind of lend fodder to the kind of groups that might want to discredit climate models or downplay the risks? Or, indeed, the reality of climate change?\nErica Thompson\nI mean, yes, I did have that worry, I still have that worry. And I, but I hope that my book is clear throughout that, you know, that models are not irrelevant, you know, the answer is not to throw them away. If you come to it from this sort of sceptical position, saying, you know, we need to think more carefully about how we make inferences from models, you could go all the way down the rabbit hole and say, Oh, they’re all terrible, let’s just throw them away. But I think that would be completely unjustified. We have good evidence, sort of from from one end of the spectrum of relatively simple linear models, which are incredibly, wildly successful and form the foundation of modern life and modern technology. And, you know, with that, as a basis, we hope that we can, you know, work from there to find the limit of the knowledge that we can get from these more complex models, which are looking at making predictive statements in more extrapolatory domains where the underlying conditions are changing, and we therefore have less ability to rely on what I call the quantitative route of escape from model land, by challenging with relevant past data. You know, we’re looking at extrapolatory conditions like climate change, or social and economic systems, and therefore, we think that the data that we have, while they may be useful and indicative, are not, we can’t just calibrate with respect to past data and expect that to be enough to warrant performance in the future. And so, so I think that sort of one answer is that we shouldn’t be throwing away the models completely because they are demonstrable useful, and the question is to quantify the limits of what we can say, rather than just get rid of them. And then maybe the slightly more nuanced answer is that actually, if we have less confidence in the models, and our uncertainty ranges are wider, then because in many of these application areas and climate change, in particular, the damage function is convex, you know, we are expecting that as we go further from today’s climate, the consequence will be not just linearly worse, but sort of increasingly worse. And therefore, if you have, if you’re considering, you know, just to have a sketch of a kind of cost benefit analysis on some sort of expected utility from taking action to mitigate carbon emissions, for example, if you have more uncertainty, then your range is wider. And so the, the lack of quantification of the top tail becomes dominant in in the expected utility of the outcome. And therefore, you should be choosing to mitigate more, not less, because of that uncertainty. So, you know, the sceptics, I suppose the climate sceptics would say, oh, there’s a possibility that climate change might not be as bad as we expect, and therefore we shouldn’t bother doing anything. But I would say actually, that argument should be turned on its head, if we have greater uncertainty, that should be a bigger motivating factor to reduce carbon emissions rather than the opposite.\nBrian Tarran\nYeah, and I think you make that point quite clearly in the book. And the other point you make is that, I guess, building trust in models is about understanding their limitations. And the quote that I thought was really interesting was about acknowledging the social element of modelling. And I wonder, do you mind explaining what that social element is for people who are watching or listening? And how will that kind of manifests itself in models? Maybe you’ve got like a simple example that you might want to talk to? I don’t know.\nErica Thompson\nYeah, okay. So, I mean, the social element of models is, because it is arbitrary how we choose to model a situation, there are infinitely many different ways that you could choose to simplify reality, this, you know, huge, messy, complex thing in front of us with physical laws that we don’t fully understand and things going on that we can only measure, sort of by proxy, with, you know, with models themselves to make many of our measurements of the system. And so, so you might choose to simplify a system in one way to model it. And I might choose to simplify it in a different way. And you might choose one programming language, and I might choose another and they would implement functions in different ways. And so all of our choices change the way that the model will then look at the end of the day. Now, then you say, okay, but supposing I’m modelling you know, what will happen to a ball when I throw it up in the air? Surely, that’s not a, you know, that has no social content, does it? And I’d say basically, no, it doesn’t really have any social content. It has some social content insofar as you’re deciding that this is what we want to make a model of. But ultimately, you and I would probably come up with very similar models, regardless of our background or our perspective, or our interests, or even our education to a large extent. And so, so those relatively simple linear situations, which I refer to as interpolatory models don’t have very much social content. Now, the ones that I’m particularly interested in and that I talk about in the book are things that are extrapolatory, where we’re interested in situations where we are trying to predict into the future a system where we expect the underlying conditions to be non-stationary, to be changing. So climate change is one example. Social and economic systems would be another example. And when we’re modelling systems like that, we have to be much more careful because we could choose to model them in radically different ways. We could, if you want to model an economic system, you might choose to disaggregate with respect to the social class of different households. And I might choose to make a sort of bulk model of the whole system with a representative household. And you could imagine hundreds of different ways to do these sorts of things. So maybe you think about pandemic models and how you could simplify it into individuals or you could make an agent-based model with, you know, actual agents walking around and infecting each other. Or you could just write some differential equations for how the transfer happens. So you could do it in, again, in many different ways. And the choice of simplification is then much more important, and it will have much larger first order effects on the outputs, and then on the framing of the question, you know. So you decide to model it in a certain way, with a certain kind of mathematics, and that changes the way that you might think about intervening in the system. If you’re presenting your model to a policymaker with the intent of informing them about their policy options, you might, if you have a model which can represent the effects of say, closing schools, or universities on pandemic transmission, then then that becomes a policy option. If you have a model which can’t represent those kinds of interventions, then it’s not a policy option. And similarly, with climate change, one of the examples that I talk about in the book is integrated assessment models of energy and climate. And so these are models which consider the energy system out to say 2100, and they put a price on nuclear electricity and renewables and all the other things that go into the energy system and basically say, how can we achieve our carbon targets at the lowest cost? Now, if you put in, if you choose to put in a certain price for a certain technology or assumptions about how that technology will develop in future, then you get a particular answer. And you put emphasis on certain kinds of policy options. And so that has, the choice of assumption has a very direct result in the model output and in the information and advice that you’re giving to policymakers. And of course, you might choose to put in something like behaviour change. So maybe we have a cost of however many dollars per tonne of carbon dioxide, for nuclear electricity or for renewables. But how much, what kind of price would you put on behaviour change? How many dollars per tonne of CO2 avoided does it cost to change the behaviour of a population such that they use less energy? Well, that’s not really in the models. And if it was, it would, again, it would be first order because that would be you know, if you put it in at $2 per tonne of CO2, it would be heavily relied on, if you put it in at $2,000 per tonne of CO2, it’ll never happen. And where you choose to put that in between influences how it looks, and then that influences the pathway that’s projected, and it influences the advice that you give to policymakers.\nBrian Tarran\nYeah. You mentioned the example of school closures and stuff when you’re talking about Covid-19. I actually thought that, that one really helped me understand, I guess, and made it clear to me was that, there’s often been that argument about whether the lockdown was the right thing to do given the other impacts, but you actually say that if different types of people were doing the modelling, maybe if it was school aged people, and I guess encoding the impact that that would have had on them, and how much they value say not being able to go out and see their friends, the impacts potentially on mental health and things like that, it does change, I guess, the calculation of what the right intervention is or the right response is.\nErica Thompson\nYeah, exactly. And I think I think we haven’t anywhere near bottomed out all of these impacts of the pandemic, you know, both the health impacts, and also, mental health and economic impacts will be rippling on for a very long time to come. So we, you know, we can’t even now retrospectively look back and say what was the right decision? It’s really not clear, depends how you value the different outputs, the different outcomes of a decision. And yes, we didn’t have economic models of what the impact of lockdown would be. And if we, if those had been available and developed the same kind of mathematical complexity and credibility as the models of infection and transmission we had in those early stages of the pandemic, which had essentially morbidity, mortality, and the, you know, the impact on the NHS, you know, number of hospital beds occupied. Those were the bottom lines, and there was nothing else. And so that was given as an input to the policymakers. Now, that’s, of course not everything that the policymakers rely on, they have to, their role is to weigh up everything else as well. But if we’d had models which contained more information about all of those other impacts, I think it’s quite plausible that we would have seen different kinds of decision making. And then there’s also the communication aspect, that these models were used to communicate and justify and persuade the public of the importance of the actions that were taken. And, you know, I think it’d be hard to disagree that the actions that were taken immediately where necessary, we certainly did need some kind of lockdown straight away. But then the question of exactly what you do thereafter is much more difficult.\nBrian Tarran\nYeah, yeah. So in the book, you kind of make the point that it’s somewhat of a fool’s errand to try and make models objective, and they can’t ever really achieve this, you know, principle of scientific objectivity. But that we instead should look to counter subjectivity of individual models with a diversity of models that do encode these different perspectives, like we’ve just been talking about. I wonder, you know, how would you see this working in practice? Or how would you like to see this working in practice?\nErica Thompson\nI mean, that’s a difficult question. So it’s, it’s nice to think that, you know, we have these models, and they are unavoidably subjective. Essentially, my view is that the model encapsulates the expert opinion of a particular expert, and it comes laden with their own perspectives, and biases and preconceptions, as well as their expertise and their education and their experience of a subject, you know, which shouldn’t be set aside. So if we are trying to understand a situation, then we want to get as many perspectives as possible. And so in theory, incorporating the widest possible diversity of different backgrounds into modelling and making a multiplicity of different models and trying to see the problem from these different perspectives will help us to understand it better. Now, then the statistician will jump in and say, Aha, can we, you know, can we in some way average those models or use some sort of statistical inference to take those models and put them together and come up with an even better answer? And I would, I would sort of counter that by saying that there’s no reason to believe that our, that a set of models generated as essentially just one set of opinions will be an independent and identically distributed sample from some distribution, underneath which will be an estimator of, of the truth, if that even exists. And so many of the formal statistical methods that we would quite like to apply to an ensemble of models, a large group of models put together aren’t really conceptually valid at all. I mean, that doesn’t stop people doing it. And maybe you get some interesting information from it, but you certainly can’t rely on it as an estimator. So there’s a sort of statistical problem there. And then the other question is your reference class. So, to what extent do you believe that these models are all equally valid or equally plausible? So that then brings the social question back to the forefront. Because then you say, you know, if I believe that, you know, somebody from Imperial College, say, who is the head of an institute for epidemiology, and has many, many years of experience making this kind of model, you know, is an expert and is qualified to create a model and for that to be recognised as a valid expert opinion, who else has got the credibility to do that? How do we define that? You know, what do you call a plausible model? So then it’s a question of the sort of scientific gatekeeping. What kind of qualifications do you expect from somebody or from an institution? What kind of expertise counts as being relevant and valid expertise? Does it have to be mathematical expertise? Can it be lived experience? Can it be, does the model have to be a mathematical model? What kinds of mathematics are appropriate for the situation? If we disagree about assumptions, does that mean that we can’t consider the the two sets of models in the same sort of class of plausible models? Or are we going to start pruning it by saying, I believe your assumptions, and I don’t believe your assumptions? And if so, who gets to do that? Who gets to decide what is plausible and what is not plausible? And what is allowed to enter into this set? Because as soon as we start pruning it, then we make the statistical inference more difficult. You know, if you want to say, if you want to start applying your methods that assume that the models are on, you know, that the models are independent, then you can’t start pruning because then that introduces huge dependencies on your own expert judgement. So it just becomes extremely difficult. And this is where all of then the social questions about expertise and credibility and sort of scientific gatekeeping and how we assign that credibility and trust, trust in science, you know, who has trust in which kinds of models? This is something, this is a theme that we see coming out of climate science and, you know, hopefully less now than maybe 10 years ago. But in the pandemic, of course, we’ve seen it coming right up again with questions about lockdowns, and about vaccination strategies, and all of that sort of thing. Trust in science is really important. And maybe one of my themes is that trust in science actually is first order in the modelling process itself. It’s not something that is sort of added on afterwards, I’m going to go away and make my model and then the question is whether or not you trust it. Actually, trust and expertise and credibility are in the modelling process directly.\nBrian Tarran\nDo you mind if we segue to talking about artificial intelligence models, or models made by artificial intelligence? Because that’s, I think that touches on a lot of the same issues, right? And I wanted to think about, well, first of all, you say that, obviously, artificial intelligence models made by AI, they’re not objective, even though there’s like, they’re kind of building the models, if you like, those AIs have still been trained by people, been coded by individuals and those personal judgments and assumptions and all that get embedded into the artificial intelligence. But I think, I guess my question for you is, we’re starting, I think, to have a very frank and public debate about AI ethics, and to demand transparency and explainability of things like automated decision making systems. But do you think we’re kind of, are we falling short of holding ourselves as people to the same standards of transparency and being clear about the choices and decisions we make? And also documenting that subjectivity when we’re preparing these sorts of models and these sorts of decision making systems for policymakers to use?\nErica Thompson\nYeah, so I mean, I suppose there are two questions there. And one’s about what we do and one’s about the AI. So for the humans, maybe, yes, I think that there needs to be more frank discussion of values and value judgments and politics and social assumptions within models. And I think we are starting to see that with the pandemic models, particularly because it’s been so high profile. I mean, remembering that actually, it’s really hard to unpick your own value judgments. And it’s easier for somebody with a different perspective to come in and say, Oh, actually, you know, you’ve assumed that, why did you assume that? And, you know, when we are embedded in a particular culture of modelling, it’s particularly hard to imagine that anything could possibly be done differently. And so I think that’s, again, where diversity is really important, because introducing those perspectives will help to challenge dominant strains of thinking which can end up in sort of accidentally, and not deliberately at all, in a form of groupthink. So that’s the humans and then the, with the AI, yes, you know, they inherit their value judgments from their creators. And so, for example, on the statistical side, one might think about the kinds of loss functions that are used to calibrate machine learning programmes, you know, how does the machine decide what is better and what is worse? You know, it is learning to model a situation, but there will be some kind of loss function in there, which it is minimising in order to decide what is the best model. And so, being explicit about that loss function, I think, actually is really interesting, you know, the fact that the model has got written down an explicit loss function which it is minimising means that we can then analyse that and think about what are the value judgments inherent in that choice of loss function, which is something that we don’t have when humans are calibrating a model and they’re twiddling a knob here and a knob there and saying, Oh, does it look realistic? Am I getting the right kind of behaviours, you know, does it match up with the map I have from observations or whatever. And so, having that I think we can then say, what are the implications of writing a loss function in this way? And what are the values that are implied? I mean, even just modelling itself, you know, like choosing to solve a problem with recourse to mathematical modelling is a value judgement and implies a certain kind of solution, doesn’t it? So if we say that we even can come to a decision, that the models input will be relevant and interesting and help us, that is a value judgement.\nBrian Tarran\nYeah, and perhaps we could return to that point a bit later, because there was a line again that jumped out in the book about decision makers needing to maybe curb some over enthusiasm for mathematical solutions. You do talk about documenting value judgments as being kind of one of five principles that you set out for mathematical modellers to adopt to support responsible modelling. I think it’s fascinating to me that you’ve used the vehicle of a popular science textbook to speak to this community, and to sort of set out these principles rather than, say, a journal paper or conference presentation. So I wanted to ask, is there a particular strategy to that decision? I mean, I have my own theory on that, but maybe I’ll let you go first.\nErica Thompson\nI’d love to hear your theory. I mean, I guess partly because I suppose I’m very interdisciplinary. And I’m, I’m hopping between these different areas, as we were discussing before, so I have sort of a climate science community and statistics and data science and other application areas in humanitarian forecasting, sort of hydrology, geophysics, weather and climate and all the rest of it. And actually, I find it really hard to get these thoughts published in journal form, because I suppose partly because it feels too general for any specific journal. And perhaps it feels too simplistic that, that the reviews I get tend to be either Oh, we’ve heard this all before it’s not new, or this is too radical and this isn’t an appropriate journal for it, you know, that sort of thing. And actually, I kind of got to the point of thinking, Well, you know, do people even read these journals anyway? Actually, maybe really what I need to be doing is trying to provoke a wider discussion about models. And I do tend to get a, you know, a really good response, when I speak at conferences or talk to people about these things. People go yes, yes, you know, this is really important. Actually, this is something I’ve really struggled with, we don’t know how to do it, the uncertainty is always just an add on at the end that doesn’t have enough time allocated for it. But I don’t have the resource to do it, I’m not able to grapple with these questions, because they’re so fundamental and so wide ranging, and it would be really helpful to have more of a sort of walkthrough of how people tackle these questions in different fields. And so, so I’ve been trying to do that. And I felt that the book was a good way to sort of spark the conversation and maybe also get it to some different audiences. So I’ve had people contacting me since the book came out saying, oh, you know, I’m working in, like, asset valuation for disputes between states, really random things, quite different. And they say, actually, your book really struck a chord, and we have difficulties with this in this particular area. And so I’m really hoping that, you know, the book will help me then to find, to bring together people working on these sorts of issues with common themes from really different application areas and try to make some headway on how we can actually go about practically changing modelling practice to make it to make it work better, and assess uncertainty better. So it’s not just– I think some people maybe read the book and think, Oh, this is just a sort of woke advertisement for diversity. Well, it’s not; it’s a way of doing the maths better. The whole point is to do the maths better, make better forecasts, understand the future more effectively, and be able to make better decisions based on that information.\nBrian Tarran\nYep, well, that’s not too far away from my theory on why you did it. I thought it was that it’s a great way of getting– if you can get policymakers and the public to read this, right, you can get them to hold modellers to these principles, rather than having it just be something that you kind of talk about within the community and it doesn’t really go outside that, right? It’s a way of people, you know, the next pandemic or whatever it might be, the next time a model is the focus of a debate, the public are equipped to ask the right sort of questions about the process. Okay, I’ve got one more question to you because we’re running out of time. And it’s back to that over enthusiasm for mathematical solutions point. I thought was somewhat serendipitous to read about, read that quote, in the same week that the UK Prime Minister Rishi Sunak announced a plan for all pupils in England to study maths to the age of 18. So, I wanted to ask you what you make of that plan, first of all, but also, I think, more importantly, does a more mathematically minded populace, are they better equipped to understand the mathematical descriptions of the world and that they are incomplete descriptions? Or is there kind of maybe some other curriculum that we need to tack on to this maths to 18, in order that people are able to better differentiate between model land and the real world?\nErica Thompson\nYeah, so I mean, I’m not a fan of– I mean, I like the idea of people studying maths to 18. I think more maths is a good thing. I loved maths, I still love maths, I think if more people were more generally numerate then society would be better and life would be better. But if you haven’t enthused people about the value and the interest of maths by 16, forcing them to study it for another two years is absolutely not the answer. It will just put people off staying in school past 16. So I, you know, I think that we need better teaching of maths before 16, rather than forcing people to study maths post 16. And part of that is helping people to understand how mathematics is relevant to the real world that they live in and teaching them the kind of things that they will use in their adult life. You know, people, most people don’t use Pythagoras theorem, but most people do need to fill in a tax return, you know, these sorts of things. Understanding orders of magnitude, and the difference between millions and billions, would be incredibly helpful, wouldn’t it? So, yeah, I think there are more basic questions there that need to be answered before we go into the details of sort of complex maths. And then, so what was the next question?\nBrian Tarran\nIt was about whether whether you think, the more mathematically equipped we are, does that make us better able to understand the limitations of models? Or do we need something else to kind of train us or encourage us to think about these two separate realities, model land and the real world? And I say realities in inverted commas.\nErica Thompson\nYeah, I mean, I think the general public has a good understanding that the model land and the real world are not the same. There is actually a healthy scepticism of models out there. And I think that’s probably a good thing. I give a couple of funny anecdotes in the book about that. So I mean, one was a, I think, a YouGov poll about people going to the moon and saying, you know, would you go to the moon if you could be guaranteed a safe return, blah, blah, blah, and like, a large percentage of those said, Well, no, I just don’t think you could give me a safe return, they reject the model land. And then there was another example about intelligence analysts being asked to sort of calibrate a probability language scale. So they say, like, likely, however many percent and very likely however many percent and unlikely however many percent. And so the study was looking at different ways of doing that. And one way was to accompany the word likely, or unlikely, or whatever, with a written number of what the probability it referred to was. So it would say, like, likely, I can’t remember the number, but sat it was, like 50 to 70%, written down in the question, and then the question was, what is the probability of an event, which is deemed to be likely brackets 50 to 70%? And what people write down was not 50 to 70%. You know, as a mathematician, that’s completely ridiculous. Because the answer was in the question, why wouldn’t you write that down? But of course, what you’re seeing there is the rejection of model land. Somebody has assessed it as 50-70%. The question is, do you believe it? Well, no, actually, you might write something like 40 to 80%, because you expect there to be, you know, the model to be generically overconfident. And so this is sort of what I mean, by curbing over enthusiasm for mathematical solutions is that, you know, we have to understand that the mathematical solutions are living in model land, and that we can, in order to get out of model land, we have to say, do we actually expect this result to refer to the real world? Or is it only saying what the next model run is going to tell us? And so the act of doing that is difficult, and it’s more difficult for mathematicians than for the general public because as mathematicians, we sort of are used to living within model land and noticing when the answer is in the question and then writing it down. And we’re not very good at saying, Well, what’s my subjective estimate of the probability of this model being inadequate in some way? That’s not something that you can necessarily do with respect to data and so it’s a tricky one. So in terms of the over enthusiasm, you know, it’s curbing over enthusiasm, not curbing enthusiasm, because as I said at the beginning, and I returned to a lot in the book, actually, mathematical models are incredibly valuable. And they contain a huge amount of information and insight that we’re, we would be fools to throw away. But we need to understand it, you know, in a more nuanced way and be clear about what it’s telling us and what it’s not telling us. And that answers in model land aren’t necessarily the answers that we need in reality, though they may be informative about them.\nErica Thompson\nWell, Erica, thank you very much for your time today, for talking through the book, which is out now. Do you have some some links or information about where people can find out more about the book?\nErica Thompson\nYep, look on my on my website, ericathompson.co.uk. And it’s available through all the usual booksellers.\nBrian Tarran\nExcellent, excellent. Well, I wish you the best of luck with the book. As I say, I think it’s fantastic. And well, I hope we get to talk again, maybe a bit further down the road and see whether some of these principles and this ethical framework that you talk about for mathematical modelling, whether that kind of comes to fruition because I think we need to watch that closely. So, Erica, thank you.\nErica Thompson\nThank you very much. Thanks for having me.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “How to ‘Escape from Model Land’: an interview with Erica Thompson.” Real World Data Science, January 25, 2023."
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/08/01/alberto-cairo.html",
    "href": "foundation-frontiers/interviews/posts/2023/08/01/alberto-cairo.html",
    "title": "The many ‘dialects’ of data visualization: Alberto Cairo and ‘The Art of Insight’",
    "section": "",
    "text": "Alberto Cairo is Knight Chair in Visual Journalism at the School of Communication of the University of Miami (UM). He’s also the director of visualization at UM’s Institute for Data Science and Computing. He joins Real World Data Science to discuss his upcoming book, The Art of Insight: How Great Visualization Designers Think, in which Cairo reflects on his conversations with data artists, data journalists, and information designers.\n“If we can conceptualise data visualization as language, this language can have multiple dialects,” says Cairo. “And these dialects – let’s say the statistical dialect, the data journalism dialect, the art dialect – they are not mutually exclusive. They exist, or they should exist, ideally, in constant conversation with each other. So, we can borrow ideas from each other, learn from each other.”\nListen to the full interview below or on YouTube.\nFind out more about Cairo’s work and his upcoming book at thefunctionalart.com."
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/08/01/alberto-cairo.html#transcript",
    "href": "foundation-frontiers/interviews/posts/2023/08/01/alberto-cairo.html#transcript",
    "title": "The many ‘dialects’ of data visualization: Alberto Cairo and ‘The Art of Insight’",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove some repetitions.\n\n\n\nBrian Tarran\nHello, and welcome to Real World Data Science. I’m Brian Tarran. And today I’m joined by Alberto Cairo, Knight chair in visual journalism at the School of Communication of the University of Miami. He’s also the director of visualization at UM’s Institute for data science and computing. Alberto, welcome. Thanks for joining us.\nAlberto Cairo\nHi, Brian. Very nice to be here. Thank you for inviting me.\nBrian Tarran\nNo worries. Well, today we’re excited to be discussing your new book, The Art of Insight: How Great Visualization Designers Think. I think it’s a really– I’ve not read all of it yet. I’ve dipped in and out of some chapters that you kindly sent me ahead of time. I think it’s really interesting and unique. I think the thing that struck me was often when we talk about visualization design, we tend to concentrate on what designers do, not necessarily about how they think about what they do, or how they think generally. And so, that was to be my first question for you is like, what aspects of their thought processes, these experts, what were you really trying to understand and why?\nAlberto Cairo\nYeah, yeah, this latest book of mine is very different to the previous one that I– that I wrote. The book is not out yet, by the way, the book will be out in November of 2023. I am in the process of copy editing it, getting rid of typos. But as you said, I mean the book focuses not so much on the– on the work itself, but more on the people who produce the work and the motivations and values that lie behind the work that they do. It is also, in comparison to my previous books, it is also a shift of perspective, I would say because my previous books, particularly The Truthful Art and How Charts Lie which came out in 2019, focus mostly on statistical visualization. Right, so it has a very strong, they both have a very strong statistical focus – how to make sure that your graphs and your data maps don’t deceive people. I teach elementary principles of visualization, of communication through visualization. But visualization is much more than that. And that is what I wanted to convey with this book. More and more throughout the years, I have come to understand data visualization not so much as a representation of data for insight or for communication, but as a language, a language that can be used for many different purposes. And I try to reflect that in the book. Obviously, a great part of the book is devoted to people who come from the same world where I come from, the professional world where I come from, the world of data journalism, so plenty of them are data journalists. Many of them are data analysts and statisticians and researchers. But a good portion of the book is devoted to people who use– who use visualization for other purposes such as self expression, self discovery, art in some cases. I wanted to provide a sort of like a broader understanding of the language of visualization and I also talk about– I also discussed the fact that if we can conceptualize data visualization as language, this language can have multiple dialects. And that is what I wanted to convey in the book. And these are not, these dialects – let’s say the statistical dialect, the data journalism dialect, the art dialect – they are not mutually exclusive. They exist, or they should exist, ideally, in constant conversation with each other. So we can borrow ideas from each other, learn from each other. So I wanted to provide sort of like an overview of the huge diversity that exists in the world of visualization – in terms of people, in terms of race, in terms of gender, but also in terms of the dialects that people use.\nBrian Tarran\nYeah, and is that almost pushing back a little bit at this idea that, you know, if visualization is a language in the same way that English is or Spanish is or whatever it might be, that there are– there must be rules that people have to follow?\nAlberto Cairo\nYeah, I push back against that a little bit in the book because obviously, I mean, what I have taught and what I continue talking– talking about at the– talking about at the University of Miami, what I teach my classes, is what you could call let’s say standard data visualization, right? Data visualization for communication. I discuss a lot about, you know, cognitive science, you know, perception, you know, how to apply that, colour palettes – I just do standard data visualization. But that is just one of the dialects that data visualization has, right? Data visualization can be used for journalism, for business analytics, for statistics, for art, for expression, for self discovery – some of the people who I interviewed, plot their own data, for example, their own health metrics, as a way to reduce their own anxiety. So I interviewed, for example, a person who has gone through– who in the past went through very serious health problems like cancer, brain cancer and other health problems, and he discovered that the process of designing visualizations based on his own data was similar to– had similar effects as meditating about your own thoughts, right. It was a way to pour your anxiety and your dark feelings onto the graphic, so they will not overburden your mind. I find that absolutely fascinating. And it shows you that, I believe, that’s what I– what I reflect in the book, that there are really no universal rules in data visualization. There are parochial rules that are applicable to different– to the different dialects. But it is it is wrong, it is a mistake, to apply the standards of one of the dialects of data visualization to a completely different dialect of data visualization. Every visualization, I feel or I think, should be judged according to their own terms, to the terms in which they were created.\nBrian Tarran\nThe book is essentially set out as a series of discussions with these visualization designers, right. And then it’s your interspersed reflections on the conversations and things that you’re sort of taking away from them. And when you were saying in the introduction about why you wanted to have these conversations, you say you were kind of looking to, or needing to, rekindle your love for the design of information. So I wanted to ask you, maybe I’ve misjudged that sentence, but you know, had you fallen out of love with the design of information? Or did you just kind of get to that point where you thought, oh, there must be more to it than this, the way I– the way I work. What was the– what was the motivating force driving you down this path?\nAlberto Cairo\nIt’s not that I stopped, you know, being in love with data visualization, or more broadly with information design, because I teach information design – data visualization is one of the branches of information design. So I also teach, you know, illustration driven visual explanations – how an airplane works, and you do a cutaway of the airplane and you show the engines and how they work. I also do that type of information design. So it’s not that I ever stopped being in love with– with the work that I do. As I explain, by the way, in the conclusions of the book, in the epilogue of the book, which circles back to the themes in the prologue, information design and data visualization are a great part of who I am as a person. To me, it’s a way of life. I use visualization not only to communicate with other people, I use visualization also to study. When I am reading a book, I am probably producing a visualization of the book, like some sort of network diagram, in which I plot all the ideas from the book. That’s a technique, a mnemonic technique, that I learned from my– from my father, who is a medical doctor, but also a humanist. He taught me this technique to study: when you’re reading a book, just write down the concepts that you’re learning about, and then connect them with arrows make little comments on the side. Indirectly he was teaching me to make data visualization. So data visualization, information design has permeated my life since I was very, very young – since I– since I didn’t have the language to talk about what I was doing. But at the same time, in the past three or four years, many personal circumstances led me to feel, let’s say, my morale went down quite a lot, the pandemic also and then some personal problems and stuff. And I started feeling a little bit disillusioned with my own– with my own work, like having self doubts, right? Am I doing the right thing? Am I in the right career? Should I be doing something else? Have I written everything that I wanted to write about this field? Have I designed every graphic that was worthy to be designed? And I felt the need to connect with other people. Because something that I discovered throughout the years is that we human beings, we don’t think well when we are alone, we think better when we are in connection with others. So my conversations with the many friends that are showcased in the book, obviously I wanted to give their work and their lives and their values visibility because I believe that they are worthy to be explored and understood by readers. But it was also a way for me to sort of like recover a little bit of the passion that I had about information design in the past – and I was successful. I mean, I went out of– The process of writing a book can be grueling. So, while you’re writing a book, you’re always thinking, you know, this is crap. What is it that I’m doing? I don’t know where I’m going. But in hindsight, now that the book is written, and I am reviewing it, I am thinking, hmm, this is not bad. This is not bad, right? And I discovered that I was– I felt energized, thanks to all these conversations with tons of inspiring people from all over the world.\nBrian Tarran\nI think I can sort of sympathize with that, you know, the process of creating – I don’t do data visualization myself – but creating content, it can be quite a lonely process sometimes. And you do have that, I always talk about the roller coaster of emotions – of the peaks, were you think you’re doing a great job, and then the troughs where you’re like, Oh, my God, why or I should just throw it all in. So actually being able to sit down and talk to people and share ideas does inspire you, does sort of bring you back up again, doesn’t it? But I was worried, actually, that because the last time we spoke was, I think, around the time that How Charts Lie had come out, and you were interviewed by one of our freelance writers on Significance magazine – where I was at the time – and I thought, oh, no, maybe– maybe all that encountering the dark side of data visualization and all that misinformation that was out there…\nAlberto Cairo\nThat I felt depressed, right, because the book was useless, or not useless. But I mean, it was not read by the people – How Charts Lie, I mean – it was not read by the people who needed to read the book.\nBrian Tarran\nThat is always the case with these books, isn’t it? So they– they’re really valuable, if only you could get them in the hands of the right people. That’s the challenge.\nAlberto Cairo\nWe preach– We preach to the choir a little bit with these type of books, unfortunately, yeah.\nBrian Tarran\nWell, I still enjoyed it anyway. And it’s always valuable to, to listen to experts like yourself and take learnings from those. So the, the things that– the interviews I’ve read, I’ve not read all of them, but I think the things that jumped out for me – the interviews with people like Ed Hawkins, talking about the Warming Stripes, you know, talking about how their focus is less about – and tell me if I’m mischaracterizing this – it’s less about direct communication of information or data, it’s more about conveying like a feeling or an intuitive understanding of something. And obviously, warming stripes, most people have seen those, you know – the kind of plots of changes in temperature against a baseline over time and the kind of rapid shift to deeper, darker shades of red as we get closer to the present, you know – I think they do create a sense of the urgency of the climate crisis when you just look at them. But what lessons do you as a kind of, you know, as a data visualization designer, a journalistic data visualization designer, what do you take from those sorts of examples, where it isn’t direct communication, of information or data, it’s about feeling? What can you– what can you take from that and bring to your own work?\nAlberto Cairo\nWell, the fact, as I was saying before, that not all visualizations are alike, as I explained in that chapter. Hawkins got a little bit of pushback, because that visualization broke some rules – and I’m doing sort of like scare quotes with my fingers right now, right? It broke some rules because it doesn’t have axes, it doesn’t have scales. It’s just a beautiful picture. But that is valuable, that is valuable, and it’s proven that it is valuable. It’s one of the most popular data visualizations in history already. And he, it’s a perfect example of a match between purpose and outcomes. And that is what needs to be explored when evaluating a data visualization. So, Hawkins told me when I interviewed him that he didn’t want to create an analytical data visualization. If he wanted to do that, he will do a line chart with like error bars or whatever, right? Something that you could publish in a paper. He has done thousands of those types of graphs. But this graphic was originally designed to bring to a festival, to be displayed in the background while there was a conversation going on about climate change. So it was designed with the specific and explicit purpose not to provide an analytical tool to explore the data but as something that brought attention to the information, something that ignited curiosity in the viewers. And I think that if that is the purpose, the outcomes actually match really well what he had in mind, and therefore the visualization works. That’s a visualization that works. So that’s just one of the many examples that appear in the book, of graphics that somehow defy conventions but at the same time, according to their own predefined purposes, work pretty well. So we have the example, for example, from Jaime Serra, who is a designer from Spain who is a– he’s a data visualization designer, he has worked for many, many years for newspapers. But the type of graphics that he creates blend the artistic with the– with the statistical and the analytical. He uses objects, for example, to create data visualizations to– he comes up with these beautiful pieces that sometimes he has showcased in exhibits all over the– all over the world. But then I also talked to people who produce what we could call more conventional data visualizations, right – people who work in public health, right, people who work in data journalism, people who live in countries where, you know, producing accurate and truthful data visualization can be dangerous to your career, right? I talk, for example, to Attila Bátorfy, who is a data journalist working in Hungary, and obviously Hungary, right now, considering the Viktor Orban regime, it’s not very friendly to journalists who want to be accurate and truthful. And he tries to be, and he’s very successful in Hungary right now, right? He’s a, he’s a voice against authoritarianism in his country. Or Anatoly Bondarenko, who is a data journalist and data visualization designer from Ukraine, who years ago created an organization called Texty, which is an investigative reporting newsroom in Ukraine, a nonprofit in Ukraine, to investigate corruption in the Ukrainian government but also Russian interference in Ukraine prior to the war. And, and that is one of my favorite chapters, because I’m Anatoly is a good friend of mine, and he’s, he’s fighting. He’s part of the Ukrainian army. And I think about him on a regular basis. And I am in touch with him just to make sure that he’s– that he’s safe, that he’s doing good. That chapter begins with a sentence that says that I sometimes wake up in shock thinking about, you know, my friend is at war, right? That’s such a strange thought and his work is so valuable, it’s so impressive. Again, what I find inspiring in all the people I talk to, I talk with, in the book is not just the work itself, it’s the values and the motivations behind the work and sometimes the resilience of the people producing that work. That’s what I find inspiring.\nBrian Tarran\nYeah, yeah, I was– when you were talking there about the Hawkins warming stripes examples, the– the idea of evaluating visualizations, you know, on their own terms, on their kind of their stated purposes, I think is quite important. But do you think people creating data visualizations, do they spend enough time thinking through generally – not the experts you’ve talked to, obviously, they’re the maybe the exception – but about what the purpose– what is it that I want to achieve with this data visualization? Is that kind of one of the things that all your interviewees have in common, a very clear sense of purpose?\nAlberto Cairo\nThey do. They do have that sense of purpose. That doesn’t mean that they don’t sometimes create these great visualizations out of a whim – say, I’m gonna just create a pretty graphic based on that, no purpose whatsoever. And that’s perfectly fine. Again, the analogy with writing. Not all writing can be technical writing. That’s just one of the types of writing that we could use. And conventional, traditional visualization is analogous to technical writing – you want to communicate something effectively, clearly, and therefore you try to create something that doesn’t use too many words, or too many, you know, verbal flourishes, you just go directly to the point and try to communicate directly. But that’s not the only way you can use writing. You can write poetry, so why not using data visualization to create visual poetry? That’s perfectly, perfectly fine. Again, every visualization needs to be judged based on their own– on their own stated purposes. As to the question of whether people in general – like, not the people I talked to in the book, for the book – but, you know, people in general think about purpose when designing data visualizations, that’s a question that I cannot answer. But that’s the core of my classes and workshops. It’s like my classes and workshops outline, both at the university but also as a consultant, put a lot of emphasis on the purpose part. I mean, just list what do you want to communicate? What do you want to achieve? Create an actually a bullet point list of what you want to communicate, and based on that list, then you can make choices. The way that I teach data visualization these days is not about teaching rules, right? Like, you know, use a bar graph to compare, use this graphic for that, use a scatterplot to show associations between, you know, continuous variables or whatever. No, that’s not the way I teach visualization. I teach visualization based on a process of reasoning, right? Reason that takes you from the purpose to the outcome. And every decision down the road in between those two points needs to be somehow justified. You need to justify every decision that you make in the visualization in a way that is– that can be persuasive to other people who may be in your team. I use this colour palette because, and what comes after the because is the important part. I use this type of graphic because, and what comes after the because is the important part, and so on and so forth.\nBrian Tarran\nYeah, there’s something that struck me. I think it was a podcast producer who came up this idea of the XY story formula – that’s how you assess the value of a kind of an article pitch. Now, I’m writing a story about X, and it is interesting because Y – and it’s the bit that follows after the “because” that determines– you have to work on that and refine that, and that’s what shapes your story and your outputs. I’m glad you brought up teaching as well, because I was reading through the epilogue, and it said about you having “anarchic leanings and sympathies” and I was kind of curious about how those sympathies and leanings manifest in your work or in your teaching. And obviously, you said you don’t teach rules. So maybe that’s part of it. But…\nAlberto Cairo\nWhat sympathies are you referring to?\nBrian Tarran\nI don’t know. It was just that phrase jumped out: I have, I have anarchic– I think it’s “despite my anarchic leanings and sympathies”, and I was kind of curious as to what are those, and how do they– how do they manifest?\nAlberto Cairo\nWell, one of the points that I make, particularly in the epilogue, which I think that is the most important part of the book, because it’s where I lay out my own thinking – is that, one of the points that I make is that you cannot really separate the work from the people. And I make the analogy with philosophy, I read a lot of philosophy. There is this book that I absolutely love about philosophy, titled “What is Ancient Philosophy?” by Pierre Hadott, who was, I think that he was French – wonderful book, absolutely wonderful book if you’re interested in the ancient history of philosophy, that book is amazing. Talks about the Hellenistic tradition of philosophy. I could go on and on and talk about that book. I absolutely love it. I think that I read it four times, something like that. And the point that Hadott makes in “What is Ancient Philosophy?” is that it takes you a long way to understand the philosophy of the, you know, the classics – Plato, Aristotle, and then the Hellenists like the Epicureans, or the Stoics, or whatever – it takes you a long way if you sort of like understand the temperament of those people, and their lived experiences, what they went through in their lives, right? If you understand, for example, what Plato lived, his times and his temperament, and also the history of the times when he lived, you can understand the Republic better, his best book, right? You, you sort of like guess where it comes from, right, where his thinking comes from. And I think that’s something similar can be said about visualization, right? I have my own temperament. I have a– I have a very driven temperament. So I’m quite a strong will – when I decide that I’m going to do something, I usually put the energy to do it. But at the same time, I’m quite anarchic, not in the sense of being disorganized, but in the sense that I don’t deal with authority well. I just want to be left alone, right? Just leave me alone. I will figure things out on my own. I work well with other people, right. But in horizontal organizations, I enjoy horizontal teams, rather than hierarchical teams, right. I work really well in horizontal teams. And that is somehow reflected, I think, in the way that I think about data visualization. I somehow rebelled against, you know, the 1980s, 1990s tradition of data visualization teaching around what I call the Tuftean – after Edward Tufte – the Tuftean tradition of saying, this is the only way to do visualization well, these are the rules of data visualization. Well, why? Why are those the rules? Tell me what is this based on, or is it just your own opinion? I mean, I enjoy reading Tufte and I enjoy reading, you know, people like Steven Few, who is a friend of mine, etc. But at the same time I rebelled against that tradition, because in many cases, as I explain in The Art of Insight, many of those so called rules are merely the opinions of people. This is just my opinion. I like this stuff. I like this style, and therefore, I’m going to try to pass my own opinion as if it were a rule of design. I think that we need to be a little bit more honest about what we are doing. Many of those rules are not really grounded on any sort of empirical evidence, and therefore they are still valuable – I think that people should keep reading Tufte, they should keep reading [unclear] and many of the, we should keep reading them. But always with a pinch of salt, taking everything that we read with a pinch of salt, and this applies to my own books as well. We need to be a little bit more skeptical, a little bit more flexible in some sense, knowing that we are on these together and what really matters, I think, is the conversation between people in the field. Conversation is a word that appears a lot in The Art of Insight. I see my work, and I see the work of everybody else who writes or thinks or makes data visualizations as part of an ongoing conversation between people in which we can learn from each other, borrow from each other – always understanding that our opinions can be strongly stated, but sometimes they have very, very shaky foundations.\nBrian Tarran\nWhat you’re saying about the importance of still reading these kinds of texts, where the rules – again, in inverted commas – are set, the importance of doing that, that kind of reminded me of like in my, in my own world of, you know, the written word, people like James Ellroy, the author of American Tabloid, you know, about understanding the rules of grammar so that you know how to break them for effect and for impact and things like that. So I can see how that applies to data visualization.\nAlberto Cairo\nIt is, yeah, that’s sort of like already has become a cliche, right: learn the rules, so you can break them. I think that that is valuable. But at the same time, I think that we need to go beyond that and say, there are really no rules. I mean, there are a few things that could be considered rules. For example, we know that, you know, if you want to compare numbers, a bar graph is usually superior to a pie chart, for example. We know that, there is empirical evidence behind that, so you can sort of like derive a principle out of that, right? But beyond those very basic things, there are really not many rules. What there are is a lot of conventions, inherited conventions, right, that historically have developed and we have– we have inherited. So we could say, you know, it’s good to learn the conventions. It is still good to learn about perception and cognition to guide your decisions. But after you do that, all that matters is the choices that you make with the knowledge that you have, and with the guesses that you can make. Right? So it’s not that you’re breaking the rules, you’re creating your own path, based on the inherited knowledge that you have under your belt.\nBrian Tarran\nYeah. My last question for you – because I don’t want to take up too much of your time, I know you are very busy, Alberto – is, you mentioned, again, going back to the previous question, or two, the people that you work with, and one of those sometime collaborators is Shirley Wu, who you refer to in your introduction, and I was really struck by the description of the installation, the number of COVID deaths each week, and this this dripping valve. I think what struck me most was that, you know, obviously, Shirley had this idea that the drips would represent the number of COVID deaths each week, these drips into a bowl, but that this wasn’t explicitly stated to people viewing the installation, right? She created space for viewers to bring their own interpretations, and they did. And again, it’s one of these things that I think is a beautiful idea – being able to, to withhold some information – but I don’t know, how does that manifest if you’re, you know, a data scientist or whatever, and you’re trying to create visualizations for, you know, an internal client or whatever it might be. How do you kind of bring some of that, that flavor and that interpretation and that space, to a graphic? I think that’s something that I was thinking about when reading that book, that part of the book.\nAlberto Cairo\nThere are many examples like that in the book. For example, in the chapter about Jaime Serra, Jaime created once a graphic in which– he drinks a lot of coffee, and he wanted to – remember that one – he wanted to, he wanted to see how much coffee he was actually drinking throughout a year. And if I have to do that, I will, you know, I will get my mug, the mug that I use every day to drink my coffee, I will draw a scale on top of that, and then I will measure the number of ounces of coffee that I’m drinking. At the end of the year, I will probably design a line graph, a time series line graph, to see whether there is any seasonality in my coffee consumption. I will design an analytical chart or so to speak, right, a graphic to analyze my own data. But he wanted to design something a little bit more fun, a little bit more expressive, a little more artistic and what he did was to create a graphic in which he plotted the amount of coffee that he drinks throughout a year through coffee stains. He got 12 pieces of paper, each one of them corresponding to a month. He folded those pieces of paper to subdivide them into quadrants, each one corresponding to a day. And then whenever he was drinking coffee, he tried to leave a coffee stain on the corresponding quadrant of the corresponding paper. And the result was sort of like it was a physic– it’s a physical data visualization. And it is amazing. Now, does that mean that you can insert that type of graphic, let’s say, in a business dashboard, or on a quarterly report in a company? No, that’s not the purpose of that type of visualization. The way that I usually explain the value of that type of visualization is to create this sort of like hypothetical scenario. And I have used these many times with clients when presenting you know, Shirley’s work or Jaime’s work. I say this is not the type of graphic that you use for analysis, right. For analysis, you need to use line graphs, bar graphs, scatter plots, traditional conventional data visualizations. But let’s suppose that you, for some reason, one year you conduct a survey internally in your company to analyze how much coffee people drink in the company, right? And you do sort of like this beautiful report that you print out as a hardcover book to give to your own clients as a gift when they come to visit you. What do you put inside of the book? The analytical graphics, right? The analytical charts that slice and dice the data by gender, by location, by whatever? You put all the conventional traditional graphics? What do you put on the cover? What you put on the cover is the beautiful artistic data visualization, which is still a data visualization. And same thing with Shirley’s work, right. Shirley’s installation about COVID: true, it’s not a graphic. It’s not a visualization. It’s not really a graphic because it’s physical. It’s a physical installation. But it is not a visualization that is intended to communicate the data in any sort of like, with accuracy or anything, it just tries to create a feeling. So again, imagine that you work for let’s say, a company focusing on public health or whatever. And every day, what you produce will be conventional charts and graphs and maps. That’s what we need to use to analyze data. But let’s suppose that you want to create some sort of like beautiful piece of artwork to display in your headquarters. That will be an amazing piece to display in your headquarters. It will get people– it will get visitors curious about what you do, it may drive– it may lead you to conversations about the data that they deal with everyday, the same way that Ed Hawkins’s warming stripes graphic did. It’s just a different type of data visualization that needs to be judged according to its own purposes, under its own terms.\nBrian Tarran\nYeah, fantastic. Well, that’s a really nice idea to end on. Hope some people take it forward, and future visits to offices will be more visually appealing as we get to explore those spaces. So, Alberto, thank you very much. So the book is out in November, yes?\nAlberto Cairo\nNovember the 15th. Yeah.\nBrian Tarran\nIs there a website yet that people can go and find out more details?\nAlberto Cairo\nNo, still working on it. For now, there is some information in my weblog, which is the title of my first book, The Functional Art. So, it’s thefunctionalart.com. That’s my web blog. And there’s some information about The Art of Insight there, including some, you know, some sneak peeks.\nBrian Tarran\nExcellent. Well, we’ll put a link to that in the in the show notes. So, Alberto, thank you for joining us today. Best of luck finishing up the book and the website. And I hope you can join us again soon because there’s so much more that I could discuss about the book with you, but it’s been great talking to you today.\nAlberto Cairo\nThank you, Brian.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Images are not covered by this licence. Photo of Alberto Cairo is copyright JCA Photography.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “The many ‘dialects’ of data visualization: Alberto Cairo and ‘The Art of Insight.’” Real World Data Science, August 1, 2023. URL"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/02/03/heidi-seibold.html",
    "href": "foundation-frontiers/interviews/posts/2023/02/03/heidi-seibold.html",
    "title": "Why open science is ‘just good science in a digital era’",
    "section": "",
    "text": "Years are often dedicated to different causes and aims by different organisations. The United Nations, for example, has designated 2023 the International Year of Dialogue as a Guarantee of Peace, while for the European Commission it is the European Year of Skills. But over at the White House Office of Science and Technology Policy, 2023 has been declared the Year of Open Science.\nTo discuss what this means for science generally and data science in particular, Real World Data Science invited Heidi Seibold for an interview. Seibold is a statistician and data scientist, and also an open science trainer and consultant, and we talked about how she became involved in open science, what it means to her, the benefits of it, and how academic and industry researchers can move towards it.\nCheck out our full conversation below or on YouTube."
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/02/03/heidi-seibold.html#timestamps",
    "href": "foundation-frontiers/interviews/posts/2023/02/03/heidi-seibold.html#timestamps",
    "title": "Why open science is ‘just good science in a digital era’",
    "section": "Timestamps",
    "text": "Timestamps\n\nHow did Heidi become interested in open science? (1:46)\nWhat does open data science mean to Heidi? (7:50)\nWorking with PhD students on open science (12:00)\nHow do open data science principles fit into an industry environment? (14:10)\nKnowledge transfer and public science (17:29)\nYear of Open Science initiatives and lasting impacts (21:08)"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/02/03/heidi-seibold.html#quotes",
    "href": "foundation-frontiers/interviews/posts/2023/02/03/heidi-seibold.html#quotes",
    "title": "Why open science is ‘just good science in a digital era’",
    "section": "Quotes",
    "text": "Quotes\n“Reproducibility [is] just like this minimum standard in research quality, where we say, ‘When we have the same data and the same analysis, we also want to see the same results’, and being able to check that from others is really, really important.” (2:45)\n“On the back of my wall here in my office I have written, ‘Open science is just good science in a digital era’… Before, we only had the printing press, and we had to print journals in order to distribute the knowledge that we have.” (5:23)\n“For me, open data science entails the part of the scientific process that focuses on everything that happens on the computer: the data processing and the data analysis, and then getting from the data analysis – getting the results and the knowledge, really – in sort of a pipeline where you go from one step to the next. And so the image that I have in my head, when I think about open data science, is of a pipeline.” (10:16)\n“Nobody’s perfect from the beginning. And open science and reproducible research is really hard, and it requires a lot of technical knowledge. And I always feel like people are so scared, because on one hand, they don’t know how to do it yet, and the goal is so far away. And so I always like [to say], you don’t have to be perfect right away; going one step into the right direction is super important.” (12:34)\n“If we think of companies, for example, like Microsoft – they put a lot of money right now into open source: they bought GitHub, they publish open source software, they put money into open source software projects like R, for example. So, somehow, this must be a good way of making profits.” (15:09)\n“[Open science for the public has value because] we don’t know what ideas people will have. There’s so many skilled people out there that probably will do amazing things… We have this with the software Stable Diffusion right now. That’s an AI that generates images from text, and it runs on my computer here. And I don’t need AI skills to be able to do that. And people are building such incredible images out of this, and it’s really fun to see.” (18:50)"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/02/03/heidi-seibold.html#transcript",
    "href": "foundation-frontiers/interviews/posts/2023/02/03/heidi-seibold.html#transcript",
    "title": "Why open science is ‘just good science in a digital era’",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove repetitions.\n\n\n\nBrian Tarran\nHello, and welcome to another Real World Data Science interview. I’m Brian Tarran. And today I’m joined by Heidi Seibold, a statistician and data scientist. I invited Heidi along to speak about open science, what it means, the benefits of it, and how to move towards it. So welcome, Heidi, how are you?\nHeidi Seibold\nHello, thanks for having me. I’m good.\nBrian Tarran\nExcellent. Excellent. Well, Heidi, I contacted you because, you know, I know you have a real deep interest in open science. And the conversation I think was really motivated by the White House Office of Science and Technology Policy declaring 2023 to be the year of open science. So first off, I wanted to get your reaction to that announcement.\nHeidi Seibold\nYeah, I think in general, that’s a really cool thing for open science to happen. Right? We, there’s this movement that’s been going on for a while, and people have been doing these grassroots communities and growing open science from the bottom up. And now we see more and more also top down decisions, which I think is a very good sign for, yeah, quality of science, really.\nBrian Tarran\nYeah, definitely. So I mean, we’ll get into some of the details of the initiatives a little later, maybe, but I thought that maybe we could kick off by asking you what you thought of the official definition of open science that the US government has come up with, and, and for the benefit of people watching that’s, in quotes, the principle and practice of making research products and processes available to all while respecting diverse cultures, maintaining security and privacy and fostering collaborations, reproducibility and equity. So as definitions go, does that, you know, does that hit the mark, for you, do you think?\nHeidi Seibold\nYeah, I think given that this is a federal definition, right, from the US. And they have to like, yeah, take so many opinions into account, I think it’s a great definition. I also asked like on social media, what people thought about it, and I think the response generally was pretty good. What I liked especially is that they focus on collaboration, reproducibility, and equity, which aligns very much with how I personally see open science. So collaboration means like, I always think of the term like building on the shoulders of giants, right. So this is what we want to do in research, we want to build on the work of others. They might be like famous people, but they might also be our colleagues next door. And it’s so important to take that into focus. And also reproducibility, just like this minimum standard in research quality, where we say, when we have the same data and the same analysis, we also want to see the same results, and being able to check that from others is really, really important. And so I think, this focus, I personally like it a lot.\nBrian Tarran\nExcellent. Excellent. So can you tell me a little bit about your background and how you became interested, and I think committed to open science would be a great way of describing it, right?\nHeidi Seibold\nYeah, so I’m a trained statistician, I studied statistics. And then how did I get into this whole open science thing was, for me, really through reproducible research. So my first research project was during my master’s programme, and we wrote a paper and it was a very computationally complex project, and we had lots of files and folders, and, you know, scripts and stuff like this. And then, at some point, it just all got so messy. And I felt like, oh, no, I’m the worst scientist in the world. And then I told my colleague who I was working with on this project. And he was like, No, this is normal. And I was like, No, this can’t be normal. I want to be on top of things when I do research. And I want to be sure that, like, the code that I’m using for this is the correct code that I actually wanted to use, right. So that must be like a minimum standard that we have. And so through that, I learned about reproducible research and good coding practices. And then I also thought more and more about like, well, if I do all this, I should also publish it so that others can actually check that I’m good doing good work here. Right. And so through that I got more and more into open science and really always felt like that’s the right thing to do, especially if you’re funded through public money.\nBrian Tarran\nYeah, yeah. Can I ask you what you think is kind of driving the shift to open science because the way you’ve described open science to me just there, you know, that sounds like to me like, just good scientific practice, and the fact that it’s not something that’s been done before, I wonder whether is it a cultural change, a philosophical change, a technological development, that’s kind of spurring this shift?\nHeidi Seibold\nYeah. So that’s a really, really good and deep question. So on the back of my wall here in my office, I have written open science is just good science in a digital era. And I think that describes the answer to your question pretty well. So there was a technological change, right? Before, we only had the printing press. And we had to print journals in order to distribute the knowledge that we have. And of course, that costs money. So journals cost money historically. But now, journal costs are really super low, because nobody needs them printed anyhow. And you only ever want to read like one or two articles out of one issue. So it doesn’t really make sense anymore, right. And also publishing data and code. It’s just like, the cost is so so low, that now in this digital age of the internet, really, we have such a low burden of, of doing the right thing. But on the other hand, we need to make this social shift, because we’ve been always, like, researchers have always done it a certain way. And there are especially certain fields that are really into, like, they feel like this is my data. This is my code. This is my research. Why should I share it? But I feel like the young generation of researchers are like, well, because it’s the right thing to do. And the reason why we’re in research is because we want to have scientific progress and scientific progress comes when we can build upon each other’s work.\nBrian Tarran\nOf course, and you mentioned, obviously, journal publications, but I wonder to what extent is it that the scale of science has almost kind of outgrown the ability to kind of condense it all down into a, even if it’s a 20 or 30 page, academic paper, right? Because it’s not just about, you know, setting out the research question describing the methods, you know, presenting a table of the data, all that stuff can’t be published now, or it can be, but it can’t be fit within the framework of an academic paper, it has to be on a GitHub repository or in a Jupyter notebook or something like that. So kind of open science encourages us to, to think about distributing that knowledge in different ways.\nHeidi Seibold\nI think that’s completely true. So, research now is so much more complex than it used to be right. There used to be single researchers who did like, yeah, such breakthroughs within one paper. And now, we already have so much knowledge, and the questions are so much more detailed and complicated. And also the data is so much more and the things that we can do is so much more complex. And so I feel like one paper doesn’t– isn’t enough to describe the research that we’re doing. And we did this one project with, with a group of students, actually, where we took 11 papers that were related to the topic that we were teaching about. And we took these 11 papers and data was available for the papers. And so we thought, well, we try to reproduce the results that they have there. And we found that it is really hard if we don’t have the code, because the method section in papers is really super short, right? It’s a super short section, if it’s not like a statistics paper. And it’s impossible to describe all the intricate steps that you took from the data to the complex model and analysis that you did. It’s just not enough space within the paper. And the code is the perfect description of what you did, right? So why not publish it with it, and especially in cases where data privacy is not an issue, then, and you already publish the data, then it just makes so much sense to also publish the code, because the code is not like there’s no privacy issues to that – usually at least.\nBrian Tarran\nSo is there a kind of you know– when we say open data science, you know, what does that look like? And do you have a kind of example or simplified example of what that would look like, right? And how it’s more than just, you know, the finished paper, the product of the science scientific process?\nHeidi Seibold\nYeah, so for me, open data science entails the process of– the part of the scientific process that focuses on everything that happens on the computer. So the data processing and the data analysis, and then getting from the data analysis, getting the results and the knowledge really, in sort of, yeah, in sort of a pipeline where you go from one step to the next. And so what the image that I have, in my head, when I think about open data science, I really think of a of a pipeline where you stick the different parts of the pipeline together, in a consecutive order. But of course, research projects are really complicated. And the pipeline just looks really messy, in some ways, but if you still manage to organise it in a way that the different bits all fit together, then you can make it so that in the end, you can imagine something that goes from the start to the finish. And you really understand every step of this pipeline every step of the way, from the raw data to the paper.\nBrian Tarran\nYeah. So the idea would be that people provide almost like a framework for you to recreate that yourself if you want to either check the results or yeah, replicate, just replicate the process generally. Right? So is your– so your job now is, I guess, as an open science trainer and consultant, so are you the person that goes into organisations and kind of shows them how to build that pipeline? Or how to, you know, map it out and to present that to people?\nHeidi Seibold\nYeah, so what I do a lot is do workshops and trainings with PhD students. So graduate programmes will ask me to come in for a day, or, also, we do longer trainings, which are often more useful, because people can in between take the steps that I recommend. And then we just, like, work together on ideas and steps that they can actually take. And I think what’s always important there is to know that nobody’s perfect from the beginning. And open science and reproducible research is really hard. And it requires a lot of technical knowledge. And I always feel like people are so scared, because on one hand, they don’t know how to do it yet. And the goal is so far away. And so I always like, you don’t have to be perfect right away, going one step into the right direction is super important. And that also helps with like the social change, because then the question is, well, I do want to do this, but my supervisor doesn’t know the technology, what do I do? And then we always try to find like one step that they can take, rather than trying to be perfect right away.\nBrian Tarran\nAnd it’s, I guess, it’s encouraging to see people like you being brought in to work with people on graduate programmes. So you’re, we’re trying to almost train the next generation of scientists to be thinking, open science first, rather than, you know, falling into bad habits or old habits that, you know, we are trying to do away with.\nHeidi Seibold\nYeah, and really, the young researchers, my feeling so far has been that the only pushback I get for my work is from established researchers who feel like well, this is the way I did it, and so it has to be the right way. But the younger generation for them, it’s super obvious that this is the way we should go in order to achieve scientific progress and good scientific practice.\nBrian Tarran\nYeah, yeah. A lot of data science now is obviously now done within industry. I wondered how open data science or open science principles fit in an environment you know, where competitive advantages is linked to keeping things in house and confidential and not wanting to share too much. Do you see a conflict there or can the two work together?\nHeidi Seibold\nSo I think there’s– first of all, I think, if we, if we get it done in academia, I’d be already super happy, right? If we get it done in the space where we have public funded projects that then are available for the public, I’d be already like super stoked. But in industry, it’s really interesting because a lot of the work that is done in industry is already done pretty well. So we if we think of, for example, companies, for example, like Microsoft, right, they put a lot of money right now into open source, they bought GitHub, they publish open source software, they put money into open source software projects like R, for example, right. So somehow, this must be a good way of making profits as well. And we see lots of companies investing in open source. So why not think about other research products, like data, and so on, also in the same line as we do of open source, because software is just a similar product. And I don’t think that there’s, I mean, there is some software, or some products, where it makes sense to have a patent or a trade secret or something. But sometimes it’s just more profitable, to have something where people can look into it and trust it. And I’d also helps like finding the next coders, and the next researchers to work on these projects, because, well, we like looking into what we’re going to do next. Right. And also, if we look, for example, into pharma industry, there, we see that a lot of the work they’re doing is already pretty good. For example, if we look at clinical trial transparency, pharma is doing better than academia there. And also, they’re pretty good on reproducible research, because they have to stick to certain rules. And when we look on the other side, so industry also benefits from open science, right? Because if we do open science, in academia, and publicly funded projects, then this will help companies make more money, because they have access to more knowledge and maybe interesting ideas as well, that they don’t have right now. So I think this is a win-win situation for companies as well. And I feel like if academia gives more to the industry, then eventually there will be like a mindset change. And industry will also give more back as well.\nBrian Tarran\nYeah, I see– I can see that and I guess it’s, you know, knowledge transfer is a big objective of a lot of academic institutions, right, we want our outputs to be of use to wider society, and that includes business and industry, right. So if people can pick those things up, you know, download it off of the web without having to, you know, make one-to-one links with the researchers who’ve done that project, it just it smooths that transition, and that that knowledge transfer becomes a lot more straightforward. You did, you mentioned about, you know, open science is about public science, essentially putting these, when it’s publicly funded research, this data and this work goes into the public space. You know, I’m guessing that, to a large extent, a lot of, you know, regular members of the public aren’t going to be interacting with open science. They’re not going to be downloading the datasets, they’re not going to be rebuilding the pipelines and rerunning the analysis. But do you still see that there’s a value there for the public in that information being there should they need it? And where are the kind of areas of value that you think the public will exploit?\nHeidi Seibold\nYeah, I think that’s an interesting question. And I think the biggest answer to that is that we don’t, we don’t know what ideas people will have, right? There’s so many skilled people out there, that probably will do amazing things. And we see that over and over again, when we put stuff out there, that people just get the super cool ideas. So we have this with the software Stable Diffusion right now, right? So that’s an AI that generates images from text, and it runs on my computer here. And I don’t need AI skills to be able to do that. And people are building such incredible images out of this. And it’s really fun to see. So I think, yeah, we just don’t know what’s going to happen. And on the other hand, I think, well, I am a researcher, but I’m also the public, right? And so if I have a question about something that concerns my private life or my friends’ private life, then I can also– I do have the skills to go into this and look at some research for example, I don’t know, how to best raise children or whatever. Um, that’s– so I have a friend, she’s an epidemiologist, and she always goes and looks at research on like, how to feed her child best and what to do. There’s, there’s all kinds of questions you have as a mother. And she, as a researcher can just go into the research and figure out, like, what is the best path for me to take. And I think the more we do open science, the more we can also do, like science communication, that adds on to that as well. Right? So if we have her now, she could now help other mothers make the same good decisions as well. And she would be sort of a science communicator for research that isn’t even hers. Um, so that is pretty cool, I think.\nBrian Tarran\nI think so, and it’s speaks to me of breaking down silos that, and I guess, blurring the lines between our roles, our professional roles and our personal lives, right. It’s about bringing science into that kind of public sphere, so I can see why the benefits would accrue from doing so. Just to wrap up, let’s go back to that year of open science, that was announced by the White House, are there any other specific initiatives in that big long list that constituted the announcement that you’re excited about?\nHeidi Seibold\nYeah, there is. I think, especially what I really liked was that, yeah, federally funded research, um, needs to be accessible, including the data when possible. So again, there’s open when possible, closed when necessary. But I think this is a very good first step, to say, okay, it depends on the funding, if it’s publicly funded, then it should also be publicly available. And I think that’s a really good sign. And then also, these, the statement had like, mentioned all these open science initiatives in different fields, which I really liked. So for example, from NASA, they have this transform to open science programme, and they’re already super active. And it’s really cool to see what comes out of that. For medicine, they have requirements for data management plans, which I think is a very solid step towards open science in medicine, because they are we have the issue of data privacy. And we really have to think from the get-go of a project about what should we do in terms of best practices of data management and having a requirement on that is, I think, a really, really solid step. And also, I’m thinking about open science in the field of like federal government even, because open data and federal government is a huge topic, right. And it, that’s definitely something that a lot of people will be interested in as well.\nBrian Tarran\nLast question for you, then Heidi, you know, what would you want the lasting impact of an initiative like this to be? And you know, would you like to see this sort of thing replicated in other countries around the world? If indeed, you know, other countries may already be doing this and have already done this?\nHeidi Seibold\nI think in general, it’s a very, very good sign that we’re seeing right now. We’ve seen lots of movement, for example, in the Netherlands – Netherlands is really big on open science – and seeing such a big country [the US] that also plays such an important role in the world, doing– taking this step, is a great sign for the entire, like all of research, really. And yeah, I think it’s also nice to see that we’re going from like, Oh, this is a niche topic that only experts are interested in, and people that are like advocates and nerds focus on, to something that really governments are thinking about.\nBrian Tarran\nSo, open science is going mainstream, I think is the message. And let’s hope that it continues to do so. So Heidi, thank you very much for your time today. Where can people find you online, if f they want to find out more about you and your work and your thoughts on open science?\nHeidi Seibold\nYeah, thank you so much for having me. It was wonderful. And I always like talking about open science, so people can find me on my website, heidiseibold.com. And I’m also on Mastodon, Twitter, LinkedIn, YouTube, wherever your search for Heidi Seibold, you’ll find me.\nBrian Tarran\nExcellent. Excellent. Well, thank you again. And thank you to those of you who are tuning in today. Make sure to check realworlddatascience.net for more interviews. Take care.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Why open science is ‘just good science in a digital era’.” Real World Data Science, February 3, 2023. URL"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/12/15/ian-diamond.html",
    "href": "foundation-frontiers/interviews/posts/2023/12/15/ian-diamond.html",
    "title": "‘I was pretty clear in my mind that we were into a no-going-back situation’",
    "section": "",
    "text": "For many people, six months into a new job is about the time you start to feel fully on top of things. You’ve figured out how the organisation works and your place in it. You’ve met all of your colleagues and got to know your way around the office. The job makes sense, everything’s under control. But then, a pandemic hits! What do you do? What is going through your head?\nThat’s a question we put to Professor Sir Ian Diamond, UK national statistician, who was only six months into the job when Covid-19 upended everything. He said: “My overall sense at all times was one of, ‘What needs to be done? What role can we play in helping to do it? And how do we make sure that we are doing things at pace?’”\nThere was no “flapping,” he said, but there was a real risk of exhaustion. “I could see pretty quickly that this was going to be a marathon, not a sprint, and while we had a lot to do,” he explained, “the last thing on earth we needed was for people to start burning out.”\nAlmost four years have passed since that time, but the effects of the pandemic continue to be felt – not least within the Office for National Statistics (ONS), the organisation Sir Ian leads. The Covid experience helped shape his thinking about how the ONS would operate post-pandemic, as he explains in this interview.\nWhen we spoke with Sir Ian, he was six months into a second term as national statistician. By the end of that term, in 2028, what kind of organisation will the ONS be? Read on to find out.\n\n\n\nWhat was your experience of the Covid pandemic, being only six months into the role of national statistician at the time?\nIt was all-consuming and required an enormous amount of focus. At the beginning of the pandemic, huge amounts of data were flying in every different direction. I felt we were in a data deluge, and we needed to move to [delivering] insight, and really working hard early on to change the agenda towards a situation where we were asking questions – really serious and sensible questions – and working out if we had the data, or how we answered those questions.\nOn the whole, ONS and the Government Statistical Service were praised for the way they responded to Covid. Did the pandemic experience inform your thinking about how the statistical system should operate once we moved out of that crisis situation?\nYes, in a number of areas. One was that we should not be completely dependent on data collected traditionally. For example, as we went into the pandemic, our ways of calculating inflation were pretty much dependent on people with clipboards going into supermarkets and shops and writing down the prices of things. We already had a project which was starting to think long term of using scanner data. But actually, being able to pivot very quickly to using web scraping to get data was incredibly important. We were also able to use web scraping early on to understand the availability of various goods in what we might call “adaptive purchasing,” or some would call “stockpiling.” And so, identifying that there were new ways of doing things and new data sources, I was pretty clear in my mind that we were into a no-going-back situation.\nThe second thing we demonstrated was that we could set things up very agilely and very quickly. And one final thing that I thought we absolutely have to continue with all the time is improved communication. You may recall that there were press conferences every day [during the early part of the pandemic], and I think during the start of those press conferences, the graphs and the slides were not always as brilliant as I would want them to be. We embedded a team into the Government Communication Service to work on the slides, and I thought that team did a great job. Improving the communication of statistics was incredibly important, because one of the things to come out of this dreadful pandemic was that people across the country became more data literate, and more demanding of data, and more able to interpret data. That was a good thing which I wanted to make sure we continued.\nIn terms of embedding the lessons or the learnings of the pandemic into the ONS going forward, how much of it is culture change? How much is about rethinking the systems and the processes?\nWas it culture? Was it improved processes? Was it better methods? All of the above. As an organisation, our main role in life is to measure the economy and society, and if you take that as your starting point, and then you ask the question, “In your lifetime, has the economy ever stood still? Has society ever stood still?” In my lifetime, I would argue, no. Therefore, we have to be an organisation which is constantly changing in order to reflect what is going on in the economy and in society. We have to change how we do things, and to ask questions about whether there are better ways of doing things, and that, I think, has been a really important reflection for us over the period both during and since the pandemic. We’ve learned a lot about the use of, for example, reproducible analytic pipelines to really improve the quality of our data at large, to improve the quality of our processes, and to enable us to do things more efficiently and effectively. We’ve learned a lot about new data sources, and we’ve really built on the opportunities and the skills so that you can now link data to be able to address questions that I could only have dreamt of 20 years ago.\nAnd so, I do think we have changed the culture, changed our techniques, and changed our data. But does that mean that we’ve metaphorically thrown away the baby with the bathwater? Absolutely not. What we have now are appropriate methodologies to answer appropriate questions. Do we still use qualitative data? A hundred percent, when it is necessary to do so. Do we still use surveys? Yes, we’ve got some of the best surveys in the world. But equally, we also use digital data, administrative data, and we use very modern techniques of analysing those data. And we use data science in its broadest sense as often as we can.\nSo, I do think it’s been a major change in what we do, and that will continue. But underlying it all is a total commitment to quality, a total commitment to making sure that we have the best data to answer the question that we are trying to answer, and that all the time we are using the best approach to answer the questions.\n\n\n\n\n\n\nProfessor Sir Ian Diamond, UK national statistician. Image supplied, used with permission.\n\n\n\n\n\nWe have changed the culture, our techniques, and our data. What we have now are appropriate methodologies to answer appropriate questions. Do we still use qualitative data? A hundred percent, when it is necessary to do so. Do we still use surveys? Yes, we’ve got some of the best surveys in the world. But equally, we also use digital data, administrative data, and we use very modern techniques of analysing those data.\n\n\n\nAs well as changing the culture and the processes within the ONS itself, has some of your work also been about trying to bring the user community with you? When I first started reporting on official statistics 20 years ago, there was a sense that users of the data valued consistency in methodology, because that meant they could go back and look at the time series. But now, with this emphasis on innovation and looking at different ways of producing insight from different sources of data, has there been a tension, if you like, between these two cultures?\nThat whole question of “we’ve always done it this way” against “we can now do it better” is a super important one. And the length of time series is also important. When we change the ways of doing things, we need to take our user community with us, and we do. At the same time, we also need a very strong narrative (a) about why what we are doing gives us better data, and (b) about what the changes in the time series mean. So, just this year, with regard to prices and inflation, we’ve been able to bring in much better data than we had previously on rail ticket prices by using electronic data. It’s really super exciting. But we didn’t just bring them in and say, “Hey, we’ve got this new way of doing train prices and we’re planning to do the same again next year with used car prices using electronic data!” What we do is we dual run, and we work with our prices advisory committee to ensure that we understand what the implications of this change are, and we understand how to communicate them. But, you’ve got to be measuring the economy in the very, very best way that you can. We should not shy away from improving what we do.\nTo what extent can the changes in thinking, the changes of approach, be credited to the experimentation and innovation work that is coming out of the ONS Data Science Campus?\nThe Data Science Campus has been absolutely brilliant. But at the same time, innovation does not only take place in the Data Science Campus. What we’ve built is a culture of innovation right across the organisation. Is that culture of innovation driven by the Data Science Campus? Not so much driven, but certainly helped, and certainly in partnership, and the fact that it is there encourages that culture of innovation.\nI do think it is important to recognise, as I say to my colleagues many times, that we are not a blue-sky research institute, we are a national statistics institute, and our job is to produce economic and social statistics. Therefore, we need to be in the business of not just research but research and development – and thinking through how the research on new data that we do will enable improved economic measurement is, for us, incredibly exciting.\nI’ll give you an example. We’ve [recently] signed a contract to get telephony data – a few years historically, and then regular data going forward. Now, this is entirely anonymised, but it will enable us to understand much more about, for example, commuting. It means that we will now need to do research on how to use those mobility data, and we’ll be really pushing that forward very quickly. But, at the same time, it’s not just about what can we do that’s interesting in this area; we need to have a very clear vision of what success looks like and the measurements, the economic measurements, that we are going to improve.\nHow do you see that innovation mindset rolling out across government as a whole?\nIt’s worth saying two things. Firstly, my job is not only national statistician, I also have an extra couple of hats: one is head of the Government Analysis Function, and one is head of the Government Digital Service. I take those roles very seriously because I do think we need to propagate good practice and innovation right across government departments. It’s no use if it just sits in ONS.\nWe try really, really hard to have innovation meetings and innovation months, and I try to speak at as many as I’m invited to. And I think it is incredibly important that we really see ourselves – right across the Government Statistical Service, right across the Government Analysis Function – as seeking to propagate good practice.\nYou mentioned there about the Government Statistical Service and the Government Analysis Function. Is there scope one day for a Data Science Service within government?\nThe answer is yes. Under both the leadership of Laura Gilbert, who is head of 10 Downing Street’s data science, called 10DS, and Osama Rahman, who heads the Data Science Campus, we recently held a town hall for data scientists right across government to discuss, fundamentally, the question of what data scientists want from the analysis function, but equally [what they want] from a community of data scientists. Part of that is a question as to whether there should be, in government, a data science profession. I stress we haven’t come to the conclusion for that yet, but I would have to say it was a very successful town hall – many, many people attended, we had a really good discussion, and Laura and Osama will be taking forward that discussion over the next couple of months.\nYou gave a keynote address at the RSS Conference in September, and one of the things you mentioned that I was particularly excited about was “Stat Chat.” I understand this is in the very early stages, and I have a very rudimentary understanding that it might well be a large language model trained on the ONS website and the data resources available, as a way to query the website. Can you tell me a bit more about the project?\nWe’re in private beta, and so there’s a whole set of agendas there. But we started it as a much better way to enable people to interrogate a pretty complex website with an enormous amount of data on, and to not only get to the datasets but to get to the existing metadata that are attached to them. What we wanted to do was to use open-source models, where the underlying data and the research behind them are made publicly available, so there’s nothing secret about this at the moment, and it’s very early stage. But we see the potential, as we move forward, as being able to really make it much easier for people to interrogate the data that we own and the data that we have published. If we can actually use large language models to enable people to be able to ask questions and then to get an authoritative answer from publicly available data, that seems to me to be a good place to be.\nI imagine, though, that when you’re dealing with something like national statistics, you need to be very alive to the danger of the hallucination problem in large language models; that if you’re querying something, it doesn’t throw up an invented statistic?\nI couldn’t agree more. And that’s why we’re in private beta, working very hard to make sure that (a) it is working properly, (b) that it’s got the right security around it, and (c) that it is actually really useful.\nThe hallucination issue in LLMs leads us onto the topic of trust in information, and obviously the ONS is very keen to ensure that there is trust in official statistics and in the data that is produced. This is a wider problem than anything the ONS can hope to address by itself, but what are the kinds of conversations you’re having internally about distrust in official sources of information?\nThis is something I say to my colleagues a lot: We should not expect people to trust us. We have to demonstrate to people that we are trustworthy. That’s incredibly important. A lot of it is about transparency. A lot of it is about absolute openness, showing your working, explaining where your data came from, and explaining your motivation for doing something. People say to me, “You might write a really strong methodological piece, but not many people read it.” Yes, but it’s there. And I’m a huge believer in research integrity, and in open data, and enabling data to be available for secondary analysis. And I think the more you are transparent, the more you work with people, the better.\nA critical part, also, of demonstrating that you are trustworthy is engaging with the public. And that’s not telling the public; it’s engaging with the public. We put a lot of time into working with the public to say, “Well, what if we did this? What if we did that?” and getting their input. I don’t have any kind of switch to make people feel that we are trustworthy. It’s a continuous process of transparency and openness, where people feel that they have everything they need [to know] about what we do and about our data.\nWe also are absolutely passionate about explaining uncertainty. Don’t tell me the answer is 62% – the answer has some uncertainty about it, and we need to really think about how we display that uncertainty. And I have to say, I think some of the techniques now to display uncertainty are just so beautiful – unbelievably beautiful – and we need to do that, not in a gimmicky way, but in a way that really explains the uncertainty in any data that we present.\nFinal question: by the end of your second term as national statistician, where do you think ONS will be as an organisation?\nI hope it will be an innovative, agile organisation which is using evermore diverse types of information, but doing so in a transparent and open and rigorous way to improve economic and social statistics which can impact positively on the lives of our fellow citizens.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo of Professor Sir Ian Diamond is not included in this licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘I was pretty clear in my mind that we were into a no-going-back situation.’” Real World Data Science, December 15, 2023. URL"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/10/16/data-sharing-in-gov.html",
    "href": "foundation-frontiers/interviews/posts/2023/10/16/data-sharing-in-gov.html",
    "title": "‘Feelings about sharing data can be context and time dependent – you can’t just do one survey or focus group’",
    "section": "",
    "text": "This summer, the UK Office for Statistics Regulation (OSR) published its report on Data Sharing and Linkage for the Public Good. In the report, the OSR notes that the value of sharing and linking data has become widely recognised within government, though there remain areas of challenge and uncertainties about “the public’s attitude to, and confidence in, data sharing.”\nThe report also warns that “unless significant changes are implemented… progress that has been made could be lost and the potential for data sharing and linkage to deliver public good will not be achieved.”\nTo find out more about the report and its recommendations for change, we sat down with Helen Miller-Bakewell, OSR’s head of development and impact. Listen to the full interview below or on YouTube."
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/10/16/data-sharing-in-gov.html#transcript",
    "href": "foundation-frontiers/interviews/posts/2023/10/16/data-sharing-in-gov.html#transcript",
    "title": "‘Feelings about sharing data can be context and time dependent – you can’t just do one survey or focus group’",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove some repetitions.\n\n\n\nBrian Tarran\nHello, I’m Brian Tarran, editor of realworlddatascience.net. And welcome to another Real World Data Science interview. Today I’m speaking with Helen Miller-Bakewell of the Office for Statistics Regulation. And we’re talking about the Office’s July 2023 report, Data Sharing and Linkage for the Public Good. In this report, the OSR reviews progress that has been made towards sharing and linking data for the public good. It says that the value of sharing and linking data has become widely recognised within government, though there remain areas of challenge and uncertainties about, quote, the public’s attitude to and confidence in data sharing. The report warns that quote, unless significant changes are implemented, the OSR is concerned that progress that has been made could be lost and the potential for data sharing and linkage to deliver public good will not be achieved. In my interview with Helen, we talk about some of the key highlights and findings of the report some of its main recommendations, some examples of data sharing and linkage that are going on now within government. So let’s hand over now to Helen, who will begin by introducing herself, her role within OSR and some of the background to the report.\nHelen Miller-Bakewell\nSo I’m Helen, Helen Miller-Bakewell, I have an official title of head of development and impact within OSR. OSR as a whole, our kind of formal job is to regulate statistics produced by government, we are the regulatory arm of the UK Statistics Authority. And our aim is to work towards statistics that serves the public good, and a government that produces and uses statistics analysis in a way that means the public can feel confident in them, and the analysis that that’s done and how they’re used. Within OSR, I used to be a regulator of statistics, I was out there looking at official statistics on crime and security, holding them up against our code of practice for statistics, which sets the standards that we would like to see government statistics meet. In my role now I actually oversee a few of our cross organisation functions, that all are designed to try and improve the way that OSR as a whole works, and to do work that can support the statistical system as a whole. And the most relevant one for today is the data and methods function. And a key piece of work that that function has been working on for the last year is looking at how data sharing and linkage is done across government, and that supports one of OSR’s wider interests and ambitions. One of our ambitions for the current five years is to make greater data available in a secure way for research and evaluation. That’s what this report that we’re going to be talking about has contributed to.\nBrian Tarran\nBefore we get stuck into the report, maybe it’s worth setting out what does OSR mean when it’s talking about data sharing and linkage? What’s the driver for this being a kind of a priority, something that OSR wants to encourage and to see happen? What are the public benefits that you hope to accrue from it?\nHelen Miller-Bakewell\nThe general premise that’s underpinned the report, and the reason that we have been a champion and advocate for date sharing and linkage for a while, is that we think data can be more powerful when it’s linked, and when it’s made accessible, in a secure way, to a wider audience for analysis. And it can offer more insights and better fulfil its potential to serve the public goods. And again, I keep saying serve the public goods. Within OSR, we very much think that statistics analysis, it shouldn’t just be for government, for decision makers in government, it should be available to the public, to all stakeholders, really, who wants to use it to make decisions hold government to account.\nBrian Tarran\nSo for people reading this or listening to this, who haven’t yet had a chance to dig into the report – and it’s very interesting report – what are the key messages that you want to share with people? What is the, I guess, what’s the assessment of the current state of data sharing and linkage?\nHelen Miller-Bakewell\nI think some of the key messages do echo what other reports have said in this space, which is that there really has been some excellent progress in terms of data sharing and data linkage. We last reported on data sharing and linkage back in 2018 and 2019, when the DA was kind of coming into force and things were starting to move slowly.\nBrian Tarran\nSorry to interrupt you. But what is the DEA did you say?\nHelen Miller-Bakewell\nIt’s the Digital Economy Act. So there was some amendments to the DEA, it created new, new powers to enable greater sharing of data for research and statistics, and it put the Office for National Statistics at the centre of those powers. It kind of gave ONS greater powers to ask other departments across the government to share data with them for research and statistical purposes. And the UK Statistics Authority as well have powers to accredit researcher and accredit processing environments so that people can have access to more data as well across governments.\nBrian Tarran\nSo your last report was in 2018, 2019 time. There’s been good progress, you say, since then. I wanted to ask you about some good examples of data linkage that you’ve seen in that time. Maybe the obvious one is Covid, I’m guessing. The pandemic, that offered a lot of opportunities for linking different datasets together. But are there– is that it? Or are there others that you want to highlight?\nHelen Miller-Bakewell\nI think the pandemic definitely provided a really strong impetus to share data, to link data. And I think, you know, we saw things done which hadn’t been possible previously, it broke down barriers, and was a kind of an excellent enabler, which is fantastic, because it was a crisis situation. And actually, yes, one of the examples that I would highlight, and I know others in OSR would highlight that was doing Covid, was Office for National Statistics estimates of Covid-19 mortality rates among different ethnic groups, and that drew on census data, it drew on death registration data at your on hospital episode statistics, it drew on data from lots of different places to create some really, really important analysis. It’s exciting to be able to say now that there are good examples of data sharing and linkage across different topic areas and different organisations. And I think if you spoke to regulators in OSR working in different domains, they will each have like their favourite examples of, of data sharing and linkage. Having worked in crime and security regulation myself, the one that comes straight to mind is Data First, which is data linkage project led by Ministry of Justice, working with ADR UK – another one, I don’t have to say full often, Administrative Data Research UK – and that’s done a fantastic job of kind of opening up access again, in a secure way, a real focus on security, to a wealth of data from across MOJ systems, sometimes horrible, clunky legacy systems – I hope none won’t be offended if I say that – but making it valuable because people can, you know, to a greater extent now link it and link it to data from other departments as well.\nI did ask a couple of colleagues if they wanted to throw me any other examples of data sharing and linkage that they particularly think highly of, and another one that came up was the Registration and Population Interaction Database, RAPID database, which has been created by DWP [Department for Work and Pensions]. And that provides that brings together data, information from DWP, HMRC [His Majesty’s Revenue & Customs] and local authorities to try and give a view of citizens’ interactions across the breadth of DWP services. What the report does say is, although we have these good examples, there are still barriers and challenges to doing data sharing and linkage. And that, that can, can be true across the whole process, right from getting support for the idea, through the practical steps of finding out what data is available, where, who owns it, how you can get to it, and then actually doing the linkage bit technically at the end. So we’ve been in a situation where things definitely have improved. But it’s, in many cases, it is not easy or efficient yet to share or link data. Our report talks about different barriers we heard about during the course of interviews with stakeholders, that we’ve encountered through our regulatory work as well. And we make 16 – to be precise – recommendations for how we could, or how government could, could start to chip away at those to, to improve the situation going forward.\nBrian Tarran\nYeah, and I would like to talk about some of those recommendations, I guess the more technical side of the recommendations, a bit, a bit later. One thing I wanted to ask you now about was about a word that sort of jumped out at me in, in the report was this idea of needing there to be a social licence for data sharing. And in the report, social licence has, has been defined as the, like, the level of acceptance or approval in local communities for data linkage projects. Now, I guess for something like the pandemic, right, you can argue that there is a kind of an implicit social licence, it was an emergency situation, there was, you know, people at risk. So that sort of use case was kind of justified, but I was curious about how social licences for these things can best be established and, and maintained, because that’s about kind of interaction with the public, right? You know, you could put a load of government statisticians and data scientists into a room and say, what could you do with this, all this data and how you could, could you link it all together? And they’d get very excited about it? But actually, then, how do you then take that to the public and convince them that it’s a good idea, or are there other ways of making sure this social licence has been obtained?\nHelen Miller-Bakewell\nSo, social licence and public engagement were one of the topics that was good, most consistently mentioned across the interview– interviews we held. And yeah, there just seemed to be a consensus which we would support that those working on data sharing and linkage should be prioritising public engagement around their work, both to kind of gauge the amount of social licence that there might be, or any sticking points, and potentially start to build social licence as well. And I think, you know, it’s really great to see people thinking like that, and I would actually say as well, it seems to be a recurring theme. And it’s really nice to see that having prominence. Another finding from the interviews was, you know, the flip side of this, yes, people think it’s important, but often people can not quite know how to approach public engagement, building social licence. And there are actually a few examples we highlight in the report where we, where we think public engagement has been done, done well, hopefully, to kind of inspire people. I would say, you know, I have a few kind of overarching thoughts on what can be important. And I think it, it comes so much to trust and trustworthiness. And I think the way, some ways that you can support trust with the public are transparency, saying what you’re going to do and why, and how, and actually the outcomes as well, when possible, let’s sit closing the circle. Thinking about this interview, I remembered OSR ran a public dialogue last year with ADR UK. And we were trying to talk to members of the public about what they understand by the public good of statistics and data. And one of the messages that came up there was people want to feel the impact their data and feel that outcome. And they don’t always feel they get that knowledge back. Like, what, what was, so what was the outcome of you having my data and doing these things that you kind of said you would do? So yeah, transparency, and then kind of linked to that, I guess, like continuous engagement, and considered engagement. And the public is not a homogenous group, there will be different groups that are important to engage for different data sharing, different data linkage projects, and you kind of need to consider who are the people you need to really engage with for your specific initiatives. And then it’s, I’m afraid you can’t just do one survey or one focus group, there needs to be some kind of mechanism for getting more continuous engagement to keep an eye on actually, how are people feeling now, because we know that social licence and people’s feelings about sharing data, it can be context dependent, it can be time dependent. And actually the first couple of recommendations in the report, so right there at one and two, number one is about the value of trackers like CDEI’s [Centre for Data Ethics and Innovation] Public Attitudes Survey, which, you know, are run on a semi-regular basis to try and track how the public are feeling at a high level about questions around data. And then our second recommendation is about having an organisation that can do more to produce guidance and support people doing research to do public engagement well.\nBrian Tarran\nDoes the mission statement of statistics for the public good, does that kind of help guide the approach, right? So any data linkage, data sharing project, you need to understand, you need to think about okay, what’s the public good that we’re trying to achieve here? And then that becomes your, almost the point, the focal point of the discussion with the public about why we want to do this and why we think there’ll be a benefit.\nHelen Miller-Bakewell\nAbsolutely. This focus on the public good is what we always come back to in, in OSR. And again, it is something that can sometimes slightly differentiate us from other organisations in this space where there may be a very internal government focus. Yeah, absolutely. What’s the outcome that’s seeking to be achieved? And will we achieve it? Did we achieve it?\nBrian Tarran\nOne of the parts of the report that I was quite interested in was the four future scenarios. The task you set yourself was to look five years from, from now at where we might be, and I guess give a range of, like, scenarios in which data linkage is great, and everyone’s doing it and it’s fully supported, down to it’s, you know, it’s happening on a piecemeal basis or not at all. So what I wanted to understand was, how those scena– whether any of those scenarios are more likely than others to emerge, and whether the kind of likelihood of those scenarios emerging are dependent on certain of your recommendations.\nHelen Miller-Bakewell\nThe scenarios just allowed us to explore in a theoretical way, like, yeah, where, where could we end up? We hope the 16 recommendations we’ve made, taken together, if they could all be fully delivered, could lead us towards that ultimate scenario of data sharing and linkage for the public good. And you can put quite neatly different recommendations against different bits of that scenario to help get, get us there. I think if, if I reflect on the scenarios like right now, the one that feels like most familiar to me is data sharing and linkage in silos. We’ve kind of spoken a little bit earlier already about how there are some areas of government and some topic areas and some organisations that are doing some really brilliant work. And I could see quite a realistic scenario where that kind of becomes more entrenched over, over the next five years. But, you know, maybe that’s actually just realistic, right? Maybe it’s unrealistic to expect that every organisation [in] government with different, different sizes, different funding, different priorities, could, could end up in exactly the same place on date sharing and linkage, all at the kind of, the top level. But I do, I do think if we can chip away at the recommendations we’ve made, then every organisation could improve on their starting point and move towards that, that scenario.\nBrian Tarran\nOkay, so let’s talk about some of the recommendations of how we get there. And there were a couple of areas I particularly wanted to focus on. One was talking about career frameworks, and having those kind of reflect, and I guess, reward the skills of those who are working on data and data linkage projects. So I was kind of wondering, you know, are there, are these skills that are kind of currently either underserved or under recognised within the existing career frameworks? And if so, how do we change that? Or was that is that beyond the scope of your report to recommend that?\nHelen Miller-Bakewell\nI think the situation across government is perhaps relatively complicated here. And I think what we, what we’d really like to see, essentially, is a situation where people in roles, in data roles, basically feel valued, and they can see a clear career pathway for them within government. And I think what we have at the moment is a variety of career frameworks that support people working in in roles with data and a decentralised pay model. Which means pay scales across, across roles can, can – and frameworks – can vary. You can see intuitively, how that has the potential to kind of create confusion for individuals – like well, which career framework should I look at – and, and then on the pay side, the potential to create kind of skills sink, where people want to go and work particular areas simply because they can be paid more, and in some cases it’s a considerable amount more, actually. And the reason I say it’s complicated is because, I mean, to some extent, this, what we have, is appropriate. People who work on data and sharing and linkage projects can come from lots of different analytical backgrounds – like, you could have a statistician or a social researcher or a data scientist or data architect, they could all be working on a, on a data sharing project, shall we say. And actually, it kind of makes sense that people in those kind of different roles might have different career frameworks and paths. And similarly, as I say, pay is decentralised, I don’t think OSR has much power to change too much there. But, you can see there are arguments for departments being able to have their own say on what skills they need to kind of pay more for in different circumstances at different times to, to bolster things. However, what we have said in the report, what we call for, and what we will try and speak to people who own frameworks to try and facilitate, is just a bit more awareness between people who own different frameworks about what else is out there, and a bit more consistency, therefore, about how different data skills perhaps are reflected and where they’re reflected in different frameworks. So we’re not asking for one single framework, I don’t think that would be practical, or particularly serve people working in data very well. But yeah, like more awareness, better joined up working, more consistent use of frameworks in, in job adverts, for example, just to help people see more clarity about their careers and where they can take them.\nBrian Tarran\nI was at an event recently where, you know, people working in, in data science or data, data kind of roles in government, were talking about how, like, the sort of career pathways and that there’s a feeling that technical skills, or growing technical skills aren’t always as well rewarded as greater sort of managerial responsibilities, so that if you become a, someone who’s excellent at being able to solve the, the knotty problems of data linkage and sharing, right, you might not be as well compensated for that as you might be if you were, say, running a team of 20, 30 people and sort of not actually applying those technical skills on a day-to-day basis. So it’s, I guess that was where my question was coming from was, is it that sort of thing that we kind of need to, need to address in some way? But, again, that might be beyond the scope of, you know, what you were looking at it on this particular matter?\nHelen Miller-Bakewell\nWell, I think it’s something for us to think more about, if I’m honest. We have committed ourselves to a follow on report for this. And we want to kind of take a look at the recommendations next year and see, you know, how far everything’s got. So, you know, any, any additional bits that we haven’t covered in this first report are good for us to think about. I think the situation you’ve just outlined sounds very familiar, to be honest, and not just across data science, across a whole load of roles where, yeah, actually your technical skills, they take you really well to middle management, but then there often comes a point where, if you want to go higher and have greater remuneration, you might have to move more to those softer skills, those managerial skills. That’s something we can think a bit more about, actually. And when we do have conversations with those who we kind of pointed to for the frameworks recommendation, maybe have a look into it a bit more.\nBrian Tarran\nThis is, I guess, some somewhat related to the previous point. But the other aspect of the recommendations that jumped out to me were the discussion around quality metadata and documentation, standardisation and things like that as being priorities for effective data linkage, these are the things that need to be in place. But again, when you chat to researchers, not just in government but all over the place, these less glamorous aspects of data management are kind of underappreciated. And often teams are, the way that, the way teams work, the way projects work is you finish one, you move on to the next, you don’t really want to think too much about the one that you just worked on, because you’ve got a new priority or a new round of funding or whatever it is. So how do we convince senior leaders that, that there are sufficient resources for this sort of work that needs to be done? It might not sort of deliver necessarily immediate value and benefits, but it’s about kind of accruing the kind of infrastructure, I guess, to, to make sure that data sharing and linkage, you can achieve your, you know, your most optimistic vision of data sharing and linkage being widespread in government.\nHelen Miller-Bakewell\nYeah, I think it’s a really important question. And yeah, having worked as a statistician as well, before I came into this world of regulation, yep, I recognise what you just described. And, you know, I think it’s always going to be a challenge in these kind of fast-paced multiple priority environments, where often people are resource stretched in terms of people, time, money, all those things together. Though, I think there’s a couple of tacks. One, I think, is maybe improving the data literacy of senior leaders, and trying to give them a greater understanding of, of data, how it’s used, and around these issues of kind of standardisation, and why they’re why they’re so important. And, actually, a couple of the recommendations earlier on in the report, in the people section, are around improving the– or strengthening the statistical literacy and the data literacy of senior leaders and recommending they go on, for example, the Data Science Campus in ONS run a master class for senior leaders across the service. So I think there was a kind of a bit of a, an education thing. And yeah, I guess, you know, part of that is setting out, like, what are the benefits? And what are the risks of not doing this? I think that, you know, here comes a role for people like OSR in setting expectations, especially in the world of official statistics, it is completely within our power to set the expectation for what government statistics should be doing with regards to metadata, or kind of following, following best practice and things like that. Our code of practice for statistics does do that to some extent already. And us as well, there’s a role for us in demonstrating the benefits and saying why we’re asking people to do these things and, and what’s good when it, when it goes well. I was thinking about this, and it drew to mind the EAST Framework. I don’t know if you’ve come across that. It’s a framework that was introduced to me by the Behavioural Insights Team for bringing about change and what you– what interventions need to be if they’re going to be successful, and it’s Easy, Attractive, Social and Timely. And I think when we and when other organisations who are kind of working on metadata standardisation, like the Central Digital Data Office, like Department for Science, Innovation and Technology, there’s, you know, there’s a few players here. We need to be keeping these, the EAST in mind as we design to try and help people kind of come on board with things more easily. And yeah, recommendation 16 in the report, the final one, is about standardisation and about, there are lots of players in this space and can we, can we bring them together a bit to be even more effective? So that’s, that’s definitely something we’ll be looking at in the coming months.\nBrian Tarran\nAnd you said that there’ll be a follow up report soon. When is that? When are you targeting?\nHelen Miller-Bakewell\nWe’re planning next summer. So last time when we did our first report on Joined-up Data in 2018. And then we did a follow on one year on in 2019. We found that was quite an effective way to, for us and others, to kind of build and maintain momentum. And yeah, again, you know, feels a bit unfair almost to just put out a load of recommendations, and then then leave, leave the world to it. We’d like to see if we can help facilitate and then tell people how we’ve been getting on.\nBrian Tarran\nAnd I’m guessing it’s not, you’re not looking for all recommend– 16 recommendations to be implemented by next year. But it’s, are we making steps towards some of them? Are we are we heading in the right direction?\nHelen Miller-Bakewell\nYou know, let’s practice what we preach. A bit of transparency. Yeah, are we heading in the right direction? And if we’re not, you know, is there a plan? I’d love to be optimistic. That optimistic. I wouldn’t expect that we can just put ticks against all 16 recommendations next year. But hopefully, yeah, we can, we could do some progress bars.\nBrian Tarran\nExcellent. Well, we should probably schedule a follow up interview for a year’s time then. But Helen, thank you very much for your time today.\nHelen Miller-Bakewell\nOh, you’re very welcome. And if anyone, any of your listeners interested in, in the report, please do get in touch with OSR. We’d be very happy to talk about the report that we’ve just written or about, you know what we’re doing. Following on from that? Yeah, thank you.\nBrian Tarran\nSo we’ll definitely put a link to the report in the show notes. So once again, Helen, thank you very much for joining us.\nHelen Miller-Bakewell\nOh, you’re welcome. Thank you.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘Feelings about sharing data can be context and time dependent – you can’t just do one survey or focus group.’” Real World Data Science, October 16, 2023. URL"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/03/06/data-science-act.html",
    "href": "foundation-frontiers/interviews/posts/2023/03/06/data-science-act.html",
    "title": "US legislators get their data science act together",
    "section": "",
    "text": "On February 14, 2023, a bipartisan group of US legislators introduced the Data Science and Literacy Act with the goal of boosting access to data science education and building “America’s 21st century STEM workforce”. We sat down with guests Zarek Drozda, Anna Bargagliotti, Christine Franklin and Steve Pierson to discuss the news and to hear why data science education is “the new apple pie”.\nCheck out our full conversation below or on YouTube."
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/03/06/data-science-act.html#timestamps",
    "href": "foundation-frontiers/interviews/posts/2023/03/06/data-science-act.html#timestamps",
    "title": "US legislators get their data science act together",
    "section": "Timestamps",
    "text": "Timestamps\n\nThe state of data science education in the United States (3:31)\nWhat will be the main impacts of the Data Science and Literacy Act? (9:14)\nProfessional development support for teachers and teacher-educators (13:06)\nHow much money is needed to deliver data science education? (18:53)\nDeveloping a data science curriculum (27:09)\nBuilding confidence in data, statistics, and technology (31:54)\nLearning from, and making connections with, international colleagues (37:03)"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/03/06/data-science-act.html#quotes",
    "href": "foundation-frontiers/interviews/posts/2023/03/06/data-science-act.html#quotes",
    "title": "US legislators get their data science act together",
    "section": "Quotes",
    "text": "Quotes\n“Most of our teachers in US schools, math teachers, have not had any formal training in statistics. Or if they have, it’s been maybe one course. They’re very uncomfortable with trying to implement these standards [for data science and statistics education]. And it’s just going to require a tremendous amount of professional development. Sounds easy in theory to deliver professional development, but very difficult in practice.” (10:18)\n“We know that in aggregate, between federal, state, private and local funding, we’re going to have to create the necessary resources to make sure that our K-12 public education system can prepare students for a world that’s changing super fast, and the K-12 system moves super slow in how it adapts to new content. And so really it’s both about what can we do to upskill data science, data literacy skills [and] it’s also about how do we help the system adapt faster as new technology comes out and leverage the importance of data in that.” (19:36)\n“I think we’ve spoken to some 50 or 60 offices, both on the Senate side and the House side. And this [has been] received really well. We don’t get any pushback on that there is a need for greater data literacy. Here stateside, I’ve been saying it’s kind of like advocating for apple pie. People get it and they resonate.” (21:20)\n“As we introduce this bill, I think we should be messaging [that] there’s a economic competition aspect to this; that it’ll be really important for the US to make investments in this area to, frankly, catch up to where I think other international peers are.” (40:34)\n“Data tell our stories, and they reflect what’s happening in our world today – much like art around us in some ways. And a way to think about data science education is to think about what we need our data understanding to be at each point in time in our educational career, or in our lives. And it’s not static, it’s an evolving thing. So you have to move with the data that are being collected.” (42:54)"
  },
  {
    "objectID": "foundation-frontiers/interviews/posts/2023/03/06/data-science-act.html#transcript",
    "href": "foundation-frontiers/interviews/posts/2023/03/06/data-science-act.html#transcript",
    "title": "US legislators get their data science act together",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove repetitions.\n\n\n\nBrian Tarran\nHello, and welcome to the Real World Data Science news q&a. I’m Brian Tarran. And I’m joined today by a panel of guests to discuss some promising developments in the United States around data science education. On the show today we have Zarek Drozda, director of the Data Science 4 Everyone coalition, Anna Bargagliotti, Graduate Programme Director and Professor of Mathematics at the Seaver College of Science and Engineering, Loyola Marymount University, Christine Franklin, the American Statistical Association’s K-12 statistical ambassador, and Steve Pearson, who is the ASA’s Director of Science Policy. Welcome all. Thank you for joining us. Steve, I’d like to come to you first, because it was one of your ASA science policy tweets that that first drew my attention to this story. And that specifically was the tweet about the introduction of a new Data Science and Literacy Act in the US House of Representatives. Can you give our viewers an overview of the act and its significance to the data science education landscape, please.\nSteve Pierson\nHappy to Brian and I also want to credit my colleague, Ed Wu, an ASA science policy fellow who worked a lot on this and championed it. So I see kind of two overarching points here for the bill. One is just to help out those budding efforts around the United States to bring more data science education to students, right, the demand in the jobs is out there. Students should know about these jobs, we want to connect them to the 21st century jobs. So this is a Department of Education programme that helps out those schools, communities that need the help that want the help. We’re not trying to require anything of schools, which already have enough curriculum requirements. So this is a voluntary programme, that I mean, I think it’s developing curriculum, it’s providing professional development. But I think there’s another aspect of this, Brian, which is just kind of the attention that this can bring to these jobs, to the schools to the members of Congress that, you know, data intensive jobs are a great job opportunity in the 21st century, right? You can look at so many places to know that, right? The Bureau of Labor Statistics has both statistician and data scientist as the top 10 jobs in terms of projected growth for the next decade. There’s Glassdoor, there’s many others, so we know that. We want to make sure that today’s students know about those opportunities and are connected to them. But we also want to just kind of diversify the STEM workforce. So there’s components of that in the bill as well. And so we want, we think that, you know, a bill introduced into the US Congress will help bring attention to that, including the members of Congress and others.\nBrian Tarran\nExcellent. I’ll take a step back briefly to look at the data science education landscape as it is today and Zarek, you helped facilitate a National Academies workshop last September, and one of the aims was indeed to survey that landscape for data science education for the K-12 grades – and for international audiences, correct me if I’m wrong, that’s students aged about five up to 17. Is that correct?\nZarek Drozda\nRight, for five to 18 range.\nBrian Tarran\nSo yeah, so how would you summarise that kind of state of data science education in the US right now?\nZarek Drozda\nYeah, well, first focus on the workshop that was facilitated by the National Academies. And that was not the first but definitely the largest to date, in terms of a national convening of the United States for data science and data literacy education. We had 100 plus researchers, programme developers, and higher ed faculty in the room. There were 500 online, it was a big, you know, kind of early stage milestone for billing data science education in the United States. And we had a number of topics ranging from you know, what does this look like a practice? What is the professional development for, for K-12 teachers look like? What are examples both standalone data science courses, and also integration into different existing K-12 subject areas. And it was really a showcase of you know, a lot of the curriculum work that’s been developed over the past 10, 15 years for building your full length data science high school courses, or for building lesson plans or for building, you know, education, classroom specific software for data analysis that students can really get their their head around. And so I think it was a milestone to then this legislation then built off right that Steve was mentioning. I’m glad that you know news of both the National Academies workshop and then the legislation and kind of the growing momentum here in the US generally has made it across the pond. I think, partially our social media game was strong enough, which is exciting to hear. But I think it’s just a testament to you know, the energy for this space is really growing, right? Because it’s, it’s career connected. I think it’s so relevant to so many other emerging technologies, whether it’s artificial intelligence or ChatGPT or cyber. And I think, students, what came through really clearly in the workshop is that students really find this content relevant because of the technology applications. And it seems so, you know, of the moment.\nBrian Tarran\nYep. Well, certainly, you talked about jumping across the pond. So two weeks ago now, but it might have been three, I attended a meeting, a discussion around what they refer to as the digital skills gap in the UK. And I left that meeting feeling very much like what they were talking about, defining as digital skills were data science skills. And so when I saw that there was this Data Science and Literacy Act, I thought, well, you know, here’s, here’s something that hopefully, other other places like the UK can learn from. So Anna, you participated didn’t you in the workshop? So do you mind sort of giving us an overview of the data science landscape as it is, as it is now?\nAnna Bargagliotti\nAbsolutely, um, so I think, in the United States, at the moment, I think we’re at a spot where the different states are sort of moving. And in the United States, each state has their own department of education. So they, we are not, they’re not federal standards, they are state standards. And each in a lot of states, those standards are being revised and to include data science standards, and those discussions are moving pretty quickly, with some states already approving, other states implementing this coming fall, for example. And other states that are still in the process of sort of starting that. But it’s, but it’s exciting. The other thing that’s really been happening is trying to understand what the curriculum should what curriculum should look like in K-12, in particular. And the GAISE report, like you mentioned before, has a, lays out a, you know, a nice sort of example of what that should look like at the elementary, middle school and high school levels, and can be used sort of as an anchor point for states looking at what they should be doing. I would say at the university level, at the what we call the 12-16 level in the US, we are pretty good. There are many data science programmes and majors and minors across the United States. And they are quite strong, there’s more and more that pop up. And overall, those majors and minors look pretty similar from university to university, and those students are coming out with very, you know, a good skill set, and they are all finding jobs. As Steve mentioned, the job growth is there. And students are feeling quite prepared when they go to into the job growth. So I think where the university level is, could have been well articulated and well defined at the moment, the K-12 level is still sort of in flux of trying to figure out what should be there. And part of that has to do also with teacher preparation, it’s trying to understand what teachers should have and know in order to teach whatever these data science ideas are that are important for the K-12 level. The GAISE report makes some very, I think, concrete recommendations about what that should be, particularly being anchored in the statistical investigative process or problem solving process and understanding how you can use questioning in that and that being really a skill set that we are trying to promote in K-12 education, that then they use also at the university level if they continue on that way. And then just understanding that there’s different large conceptions of data now. Data are not just numbers, data could be sounds, could be text and whatnot. And these ideas are sort of these data science ideas that we are trying to promote in K-12, as well as at the university level. So I think overall, the landscape is quite good. I mean, we’re moving in these great directions. And I think slowly, we’re getting to some consensus about what that looks like. And we’re seeing that in the states moving forward with different standards.\nBrian Tarran\nExcellent. And so you mentioned the GAISE report there and that’s the Guidelines for Assessment and Instruction in Statistics Education, and of course that’s something you co-authored with, with Chris Franklin, Chris. Yeah, of course. Yeah. I’d like to bring you into the conversation. Now, Chris. You know, you and I have spoken a number of times over the years about GAISE about statistics, education and statistical literacy and, and the challenges of delivering high quality education in data and statistics at all levels of the curriculum. I wanted to get your impression of what you think the big contributions are that this act will hopefully make towards improvement of the data science education landscape.\nChristine Franklin\nWell, I was very excited to see this act. And I think one of the big impacts that I see is how it can help our state departments of education actually try to implement standards that we’re seeing put in place right now, in terms of statistics and data science. Teachers, right now, most of our teachers in the US schools, math teachers have not had any formal training in statistics. Or if they have, it’s been maybe one course, they’re very uncomfortable with trying to implement these standards. And it’s just going to require a tremendous amount of professional development. Sounds easy in theory to deliver professional development, but very difficult in practice. And a big part of the difficulty of delivering professional development is funding to, to pay for this. Unfortunately, what I’ve seen is state departments of education will often implement these very nice standards in their curriculum, but then they’ve run out of money, or they don’t have sources of funding to where they can then think about the professional development of the teachers. So it’s not only the professional development of the teachers, but also the curriculum that’s going to support the standards that are put in place. Now, fortunately, ASA, for example, has just a wealth of open source resources that teachers can use. But then how did the teachers know where to get it? How do they know how to implement it in the classroom. So state departments are charged with trying to develop a framework of materials for their teachers. This takes money, this takes expertise. So not only that, but I think this bill can help with funding to allow state departments to do that. But professional development typically has been like week long workshops, day workshops, maybe they go online and do workshops, but really, for professional development to be successful. Teachers need day to day support, which requires funding once again, to provide the resource within schools and school systems to provide more of the day to day support that these teachers need. And I think lastly, one thing that we don’t think a lot about, but I’m hoping this bill will help, is the assessment that goes along with the curriculum that we’re implementing for statistics and data science. Once again, that takes funding that takes manpower support. And I’m really hoping that this bill can be a source that that our state DoE’s can turn to make their standards more successful with implementation.\nBrian Tarran\nYeah, Chris, when you were speaking there about providing professional development support for the teachers, it reminded me of when I was a primary school governor here in the UK a few months back, a few years back, sorry. And we would always talk about math education, trying to improve math education, and that I think, the teacher confidence to deliver the math curriculum is always the issue that we run up against. So having support, having resources, having people that can go in and help, that changes the dynamic, I think, for teachers and certainly equips them to, to deliver on, you know, on that vision of, of data science education and statistical literacy for all, which is something that we spoke about before, right?\nChristine Franklin\nThat’s exactly right.\nBrian Tarran\nThat’s, that’s your vision for where we get to as a society?\nChristine Franklin\nWell, I think one thing, one other thing we need to remember, it’s not just the teachers that need our support. It’s also the teacher educators that are preparing our school level teachers. And we, we need to keep that at the top of the list of priorities because most of our teacher educators recognise that our school level teachers need this support, but they are in a similar situation to where they don’t know exactly what they need to do. And so we’ve got kind of two big spots here that we need to work on for professional development, and it’s gonna take a lot of time and effort.\nBrian Tarran\nYeah, well, we’ll come back to that later, if you don’t mind. Zarek, did you want to come in? It looked like you was about to chime in.\nZarek Drozda\nYeah, I was just, I wanted to agree with Chris and second it and expand it because I think it’s professional development for teacher educators and every layer above that, right. It’s every layer above the teachers: teacher educators, it’s the district staff who are implementing and creating these programmes, it’s the state staff we’re creating the standards, it’s state policy makers, it’s federal policymakers – like, you could think about professional development for all those stakeholders. And we know we need to build, you know, better education, right, for every one of those groups that are above the individual educator in a classroom, knowing that the national infrastructure is just supporting the teacher in the individual classroom to do this best at the end of the day. So yeah, we think about that in terms of a, there’s a whole system that needs to move here.\nBrian Tarran\nI think that’s an important point to note, I think, because I go back to that digital skills workshop I was at and one of the questions that came up from from the chair was, you know, what one thing can I take back to the Secretary of State to say we need to add this to the curriculum, but it’s not one thing is it? It’s, it’s it’s a whole system, as you say, Zarek. I did want to ask you a bit about the organisation that you’re a part of, you have a coalition called Data Science 4 Everyone. To what extent was you involved in the kind of shaping of this, of this bill? And, you know, you obviously, you’ve obviously welcomed it, and you’re excited about the potential, and how much work do you see as being left to do to, to kind of get it over the line and get it into application?\nZarek Drozda\nSure, well, just a very quick background on DS4E. We’re a national initiative and a coalition as you said, based at the University of Chicago here in the States. We’ve been putting together a community of education researchers and K-12, system leaders to advance policy and advance awareness and advance the case making right for why data science and data literacy and statistical acumen is so important in this day and age. And we’re really working across the K-12 system, which is very decentralised in the US, to try to forward those goals. Yeah, as I think about next steps and what’s needed, again, just re-double what Chris said, we need more funding. We did some, to give you an example. So when computer science was being built out as a new school subject in the States, they spent somewhere in the range of three to $4 billion over 15 years to build an entirely new school subject. We’re not necessarily doing that here, right? We’re not necessarily building out a whole new school subject, I think we’re really, at least our group has been much more focused on how can we integrate and upskill teachers in K-12 math, or K-12 science or K-12 social studies, right, and integrate these concepts into the existing K-12 ecosystem, working with the different subject societies. But, you know, this bill is a first really great milestone, but we know now we’re going to have to call on state legislators to pass appropriations at the state level to fund teacher support locally, we’re going to have to, you know, call on schools and districts, right, to help give teachers time to be able to implement these classroom experiences. And we’re going to need more research. Right. So this bill calls for grant programmes to state and local partners to create. But I think we also need more funding for NSF and IES, the two kind of education research bodies in the US at the national level, to fund things like student assessments, or to fund accommodations for students with disabilities to be able access to technology for data science software. There’s a lot more R&D work that also has to happen to bring down the adoption costs over time for doing this type of work and making sure that every student regardless of their background, can benefit from the skill areas, and you know, upskilling in this space. And I think the last thing I would say is that we know we also need to work on teacher confidence, right? I think it’s both teacher confidence with statistics, right, and probabilistic thinking. And it’s also the the confidence of the technology, which is brand new, right? Most classrooms in the US have not been using spreadsheets, even though most workplaces do, let alone, you know, R, Python, SQL, any of the more kind of modern computational tools that are used in modern day statistics. And so we have a lot of work on that front to do as well.\nBrian Tarran\nOkay. And so, if I’ve understood it correctly, there’s about $50 million over five years that is being asked for in this bill. But that’s, from what you’re saying, am I correct in thinking that’s just a kind of a small part of what’s needed to completely deliver on your goals?\nZarek Drozda\nIt is a first step. An important one, but a first step.\nBrian Tarran\nSo, longer term, is it– Do you head towards the billions territory, like in the computer science space? Or is it a little less demanding of finances and resources and that, do you think? I know it’s hard to say, to pin these things down, but–\nZarek Drozda\nAt least from my angle, I’d love the group to jump in here, it’s probably a little bit less demanding. But we know that in aggregate between federal, state, private and local funding, we’re going to have to create the necessary resources to make sure that our K-12 public education system can prepare students for a world that’s changing super fast, and the K-12 system moves super slow, right, in how it adapts to new content. And so really it’s both about, you know, what can we do to upskill data science, data literacy skills. It’s also about how do we help the system just adapt faster as new technology comes out and, you know, leverage the importance of data in that.\nSteve Pierson\nAnd Brian, I can jump in a little bit on the price tag, which, when we were shopping around this bill, we didn’t actually include like a cost per year for this, because we know that that can be a very sensitive topic, and we were, we really wanted to have bipartisan introduction. So, and we were fortunate to get it right. But it was the offices who have agreed to kind of consider us that came up with the $10 million. And I’m, you know, I know for a fact that, you know, a few of the offices, at least one of the offices did want significantly more, but this was how we were gonna get bipartisan introduction.\nBrian Tarran\nYeah. Okay. And on that point, that bipartisan nature of the bill’s introduction, does that give you as a group hope that it will eventually make it through Congress and become a law, and that there’ll be these resources made available?\nSteve Pierson\nYeah, absolutely. And I’ll also just say, Brian, that I think we’ve spoken to some 50 or 60 offices, both on the Senate side and the House side. And this is, it’s received really well, we don’t get any pushback on that, you know, yes, there is a need for greater data literacy. Here stateside, I’ve been saying it’s kind of like advocating for apple pie. Right. This is, people get it and they resonate. And to that point, we only brought this to Representative Stevens, I think it was maybe late October. But they really wanted to move on this, they wanted to wait for the new Congress for bandwidth issues. But, significantly, we’re told that the representative wanted this to be her first bill introduced of the new Congress, and she had many to choose from. So I think that’s really positive. The other thing is, I’ve heard from email, we haven’t had a chance to debrief with the staff yet, because they’re swamped with all kinds of things, but they’re getting a lot of positive feedback from people about this bill. So it really seems to be kind of tapping a nerve. A recognition.\nZarek Drozda\nI was just going to add to Steve that when we went– So in advance of that legislation being introduced, we had 15 of the largest math and science and technology education associations supporting the legislation, which was a huge win. It showed, I think, that this data science, data literacy, education is really a collaborative multi-subject effort in the States, which does not happen often, it’s usually very siloed. And I think the other thing I’d say is, in the first 24 hours since the bill was introduced, we had 150-160 additional education leaders and organisations sign on to the letter of support that we were helping circulate between Data Science 4 Everyone and the American Statistical Association. And just to re-emphasise that we saw a lot of energy around this, and bipartisan, right. We’re building support on both sides of the aisle, because it’s, you know, this is apple pie, it’s so evident that every student’s going to need this for the next decade.\nBrian Tarran I like this. So data was once the new oil, but now data science education is the new apple pie. I think this is this is great. Anna, you know, assuming that the Data Science and Literacy Act goes through, funds are made available, this work starts in earnest, what sort of timescales are we looking at, do you think, before we start to see the real world impact, you know, in terms of teacher training, student outcomes, and then eventually, obviously, building this workforce, that is so needed, that is equipped with data literacy and data science skills?\nAnna Bargagliotti\nYeah, I think I kind of want to say two things to this point. I think post K-12, the college level, we are seeing those outcomes, and they are great. And I think we are really, that is, it feels very good. It feels like we’ve targeted the right things. It feels that students report back that they’re excelling in their jobs and doing great things. So I think that part is sort of, is taken care of. I think at the K-12 level, what’s harder is we’re less nimble. Like Zarek mentioned, K-12 is just a beast to move. It’s very difficult. And we’re in a situation where the target of data science changes every day, truly every day, for two real reasons. One is because the conceptualization of data changes every day. We can imagine today we think of data as text, but in probably a month, there’s some other type of data that we haven’t thought of that will emerge. And so now all of a sudden, you’ve got, you’re trying to teach pillars or concepts in K-12 that are actually a moving target. And then the other big thing that changes pretty much every day is our capabilities for wrangling, visualising access to data, all that stuff is changing. And so at the university level, you’re much more nimble because you’re in these courseworks, and your students are very advanced, and you can kind of move within those, those spaces, and you can change a course at a time. At the K-12 level we’re much more prescribed and harder to move. So I think in terms of timeframe, I think if we focus on the sort of large concepts and the baseline skill sets that we want to graduate students from K-12, I think we can move much quicker than getting into sort of the nitty gritty of a student needs to be able to programme per se, or something like that, like I think more that statistical investigative process, and those questioning, those types of ideas are really the crux of what K-12 looks like that then allows the university level to be more nimble. So for me, the timeframe is I think we have to, we have to think about it differently as we’re never going to arrive. It’s not going to be like, Oh, in five years, this is completed. It’s a, are we reacting in the right way? Or are we sort of ahead of the game. And I think we can get ahead of the game if we go to the concepts and that idea instead of thinking of topics that we’re teaching. And I hope that with this, I have great hope for this bill. I think it’s like everybody said that this is just such a great first drop in the bucket with the apple pie that I think hopefully in the next few years, the states are going to have been moved and then everybody will be doing some type of data science at the state level. And then it’s like Chris mentioned, before moving the professional development, which is a big challenge.\nBrian Tarran\nWhen you were speaking, just there, Anna, and you were you were talking about the difference between you what you’re achieving at the college level versus the younger level, it made me think that, you know, this isn’t just about providing a next generation of data science workers, right, it’s about equipping everyone with the skills to be able to exist in a data science world. And this goes to the point that I think Chris and I have spoke about before, right, about being able to be, when you’re confronted with data, being able to ask the right questions about that data. And so I think obviously, that’s, to me, that’s where I think maybe the Guidelines for Assessment and Instruction in Statistics Education seems a promising first step in the development of a data science, data literacy, statistics curricula, do you see that that needing to be – obviously these things are always needing to be revised, right – but do you see that, Chris, as a kind of foundation on which the states can start to build and to move towards this vision that’s laid down in the act?\nChristine Franklin\nWell, most definitely. And we’ve been real fortunate in the US here that our states right now that are trying to implement more statistics and data science standards, are going to the GAISE II document to use as a guiding document. And ASA has been very fortunate also to where these states, many of them have reached out to us to actually help advise them as they go through the process. I wanted to come in on a follow up with something Anna said in terms of the timeline. As she was speaking, I thought about how when we were sending out our document for review with the GAISE II, the updates, and we sent it out to probably about 20 to 25 different reviewers, we received more feedback than we knew what to do with. But many of the reviewers, very well respected people in the field, came back to us and said, we were being way too ambitious with the GAISE II document that, in fact, so many states here in the US we’re still back trying to implement the recommendations we made in the 2005 GAISE document. And our response was, we can’t be writing for today. What our goal is, is to try to write a document to where 5, 10 years from now, this is where we hope that our K through 12 system will be. And I’ve noticed this in working with other national documents, that I think the tendency oftentimes is to try to write a document for where we are now. And I think that we always need to, in terms of a timeline, think about 10 years down the road when we wrote the document. And it’s hard to be visionary. But as Anna was saying, things are changing so quickly. I mean, the GAISE II document, I think the writers are all saying, oh, we should have included this, we should have included that. Because since its publication in 2020, we’re already seeing needed changes. So I think we’re always going to be catching up to some degree. But as Anna said, I think the goal of these documents needs to be conceptual understanding, it needs to be that role of questioning. I like to say, I just want people to be healthy sceptics to where when they see statistical information, they see data, they immediately start asking questions. They may not know the answers, or know what the answers aresupposed to be, but at least they’re questioning.\nAnna Bargagliotti\nAnd I’ll just add one really quick thing, Brian, if that’s okay. Chris and I are about to embark on another ASA initiative, which is the revision of the SET report, which is the Statistics Education for Teachers report that the last publication was, oh my gosh, Chris, I can’t remember even the date–\nChristine Franklin\n2015.\nAnna Bargagliotti\n2015. And the the idea of the new report that we will be starting to write this year, along with some wonderful colleagues, is going to be to talk about the teacher preparation aspect of that in light of everything that’s happened with statistics and data science in particular, what should that look like? And how, how could that be? So I just wanted to add that.\nBrian Tarran\nExcellent. I also wanted to ask, again, maybe this is a question for Zarek. If we’re talking about data science for everyone, what about people like me, who have already already finished their education? I mean, I know you can always learn; every day is an opportunity to learn new things. But, you know, from your coalition’s perspectives, Zarek, what do you want to see happen so that, you know, that we’re not leaving behind big chunks of the population, you know, the older chunks of the population who haven’t had the benefit of going through this education system as it is now, let alone what it might be in five years time, right?\nZarek Drozda\nYeah, Brian, it’s a great question. And I, I just came off a year of serving as a fellow in the US Department of Education. And I had a lot of conversations with the programme officer who is responsible for research, education research and adult education, about that subject. Right. And I think one of the themes that I learned from that work and from that, from those conversations was this idea around fear of technology, as it accelerates. It’s really hard for people to deal with, you know, ChatGPT, DALL-E, AI, neural networks, you know, the list goes like on and on, and it changes every month, as as we’ve discussed here. And I think a big goal from our side is both to build confidence when people are dealing with a deluge of data and a increasing amount of information, right, which we talked about, and it’s also the confidence in the technology tools that are constantly changing. So I think as we think about, you know, a student K-12, we want to get them on a tool, so they can be confident and switch to the next one when it comes out. Because we know that technology integration is just is really critical. And the same thing is true for the adult learners, right, or for the for the folks that are older, there is a wealth of online digital learning, online courses, asynchronous experiences, to learn any coding languages, any programming language, any just software – doesn’t even have to be computation or coding intensive, right, it can just be spreadsheets or, or Tableau or some of the not-too-syntax heavy tools, and there’s so much digital and elearning opportunities for that. If we can build formal exposure into the classroom pathway, and build student confidence to then jump on to those later on. That is really critical. Because if you don’t get an exposure, it’s so hard to take the first step to jump in to the digital training or to you know, go to your employer and say, I want this, you know, this professional development programme, because it’s hard to know where to start. And we also worked on a data literacy training programme for Department of Ed employees while I was there and helping design that experience. You know, for professional or mid career, folks, I think the most important thing was you build confidence and create bitesize first steps to try to like tone down the fear, so people can approach the new world with confidence rather than just responding to it.\nBrian Tarran\nSo I just want to wrap up now, last question for you all, really, maybe if we start with Steve. From a policy perspective, Steve, you know, are there other initiatives on the horizon and things in the pipeline that, you know, people should be watching for in terms of trying to improve data science education across the board in the US, and maybe specifically for the ASA are their areas you’re going to be focusing your efforts and support on.\nSteve Pierson\nWe certainly do want to expand this effort. And we’re trying– In a way, this bill is serving as a way for us to gather that information, because people then know that we’re doing this and they might well hopefully suggest items to, to us. We’ve gotten some I know that, you know, Zarek has a file, I have my own file of what we might want to, how we might want to extend it just with this bill. But yes, we certainly do want to do that. And so for listeners out there that have ideas, please please send them to us. I won’t offer specifics at this point. And I’d love to hear what my colleagues have to say, as well.\nBrian Tarran\nDoes anybody want to jump in on that? Zarek, what’s on your list?\nZarek Drozda\nVery short term is just building a Senate version of the legislation, I think, right? We’re going to be working with with the ASA on that, to find champions in that chamber. But then I think I would go back to my call for you know, state legislators need to be thinking about this as well, right, because we know that every state is going to look a little different. We’re in a context right now where locally driven education solutions are going to be really important. And so we’re going to have to build different slightly different flavours of this all around the country. And we’re going to need a lot of work from, I think, state and local champions to fund and help support and build these experiences that are so critical for students across the country.\nBrian Tarran\nAnd are there, you know, on the point you mentioned earlier, Zarek, about this news,crossing the pond and reaching me over here in the UK, do you look elsewhere, you know, outside the US and the UK for other good examples of where education systems are starting to integrate data science into the teaching at, you know, the earlier parts of the school curriculum and the school levels.\nAnna Bargagliotti\nI think Chris can probably jump in even more than me on this one. But definitely in New Zealand. Our colleagues in New Zealand are fantastic. And they’ve been doing K-12 data science and statistics very well for many, many years. And Chris has some very close collaborators. So I’ll hand it over to her to on that. But I thought I’d mention it.\nChristine Franklin\nYes, I I think that that’s when you know, when Steve says how can we reach out, I think even beyond policy, a collaboration with international colleagues that have advanced their work in K through 12. I mean, I had the good fortune of having a Fulbright to New Zealand back in 2015. And just the inspiration, the wealth of knowledge that I obtained from there, to bring back to the US with our work here was phenomenal. Plus, we built up collaborations that we are continuing today. We have collaborations with people in the UK, for example, in other countries. I think the other thing I wanted to say besides reaching out internationally is that we as statisticians in the US need to be doing more to help K through 12. And I think about my, you know, my colleagues at the university in the statistics department which I was part of, my colleagues, some of them actually worked with me to reach out to the math educators so that they could try to help with what needed to be done with the preparation of teachers. So I think statisticians need to become more involved, both practising statisticians and academic statisticians, with helping educators at K through 12. And that includes trying to become involved with state departments of education as well, because that’s really where things filter down to the local level in our school districts. So I would like to see somehow a structure put in place to make that happen more. And I’m hoping things such as this bill will bring that awareness to practising statisticians, that this is really important, and you need to become more aware of what’s happening at K through 12, and become involved.\nZarek Drozda\nTo just add to what Chris is saying, I think it is so important for investment in the K-12. space. And, Brian, I’ll give you a sneak preview of a report that we were collating on international examples of data science and statistics education in K-12, and really serious investments that we’ve seen, I think, and Chris already mentioned, some really great ones that have been long running champions at this internationally. Our recent scan: Israel, their ministry of education is doing a tonne of work for data science, data literacy education. I’ve had so many conversations with them over Zoom. China added a standard semester for big data statistics coding and modelling. And it’s now also in their college entrance exam. There’s examples in Germany, New Zealand, South Korea, Scotland, we’ve we’re continuing to build the list. Frankly, in the UK, right, with core maths, you’re seeing more integration of data and computational thinking into the curricula pathways there. I think in many ways the US is behind, and as we introduce this bill, I think we should be messaging, there’s a economic competition aspect to this, right; that it’ll be really important for the US to make investments in this area to, frankly, catch up to where I think other international peers are.\nBrian Tarran\nSteve, you wanted to come in on that?\nSteve Pierson\nI mean, you mentioned like someone coming in mid career, right. And I think that that is really important. But we’ve also kind of been talking about the other ways you can access this, right, in any part of your career, but you can also access data jobs, rural and urban. So we’ve been kind of selling that dimension. But also, you know, just access in terms of diversifying the STEM workforce, we think that’s really important. But it’s also about degree level, right? We did a search for one member of Congress that had a major, I think it was a pharmaceutical in their district. And if you put in data, right, and thousands of jobs pop up for that company. And it’s not just the PhDs, right, it’s more entry level, the people who are what Zarek mentioned in terms of just accreditation, that you can enter a lot of points. But I also want to just make a– point out one part of the bill, which really singles out two-year colleges, which can help the mid career people, the early career people or others, and they face a lot of the same challenges as K through 12. Right, making sure that the instructors are upskilled, that they have a curriculum, but they also need the time to coordinate with their other disciplines that are involved here. They need time to go to that local workforce, what do you need in terms of data science? And for those students that want to go on to a four-year degree, they need to make sure that there’s a smooth pathway for those students. So there are provisions in the bill also for for two year colleges. And those would be my closing comments, Brian.\nBrian Tarran\nOkay. Anybody else for some closing words? Or should we wrap up on reminding everyone that data science education is the new apple pie?\nAnna Bargagliotti\nI can close this with something more philosophical, maybe. To me, I think a nice way to think about it or sort of a romantic way to think about it is it’s just data tell our stories, and they reflect what’s happening in our world today, much like art around us in some ways. And a way to think about just data science education is just to think about what we need, what we need our data understanding to be at each point in time in our educational career, or in our lives. And it’s not static, it’s an evolving thing. So you have to move sort of with the data that are being collected.\nBrian Tarran\nVery good point. Thank you very much, Anna, Steve, Zarek, and Chris, for joining us to talk about the Data Science and Literacy Act. I’m sure there’ll be much more to to follow and update on as this act or bill winds its way through through Congress. So we’ll look forward to hearing more about that in due course. So thank you for joining us today. Thank you to those of you who are watching for joining us. Stay tuned at realworlddatascience.net for more news Q&As.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “US legislators get their data science act together.” Real World Data Science, March 6, 2023. URL"
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2025/02/05/DSAI-journal.html",
    "href": "foundation-frontiers/datasciencebites/posts/2025/02/05/DSAI-journal.html",
    "title": "RSS: Data Science and Artificial Intelligence - showcase your research",
    "section": "",
    "text": "RSS: Data Science and Artificial Intelligence provides a new forum for research of interest to a broad readership, spanning the data science fields. Created in recognition of the growing importance of data science and artificial intelligence in science and society, the new journal aims to fill the need for a venue that truly spans the relevant fields.\nThis new open access journal joins the RSS family of world class statistics journals and is published by Oxford University Press."
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2025/02/05/DSAI-journal.html#scope-and-type-of-papers",
    "href": "foundation-frontiers/datasciencebites/posts/2025/02/05/DSAI-journal.html#scope-and-type-of-papers",
    "title": "RSS: Data Science and Artificial Intelligence - showcase your research",
    "section": "Scope and type of papers",
    "text": "Scope and type of papers\nRSS: Data Science and Artificial Intelligence is seeking high quality papers from across the breadth of these disciplines which encompass statistics, machine learning, deep learning, econometrics, bioinformatics, engineering, computational social sciences and beyond.\nAs well as three primary paper types - method papers, applications papers and behind-the-scenes papers - RSS: Data Science and Artificial Intelligence will publish editorials, op-eds, interviews, and reviews/perspectives in line with its goal to become a primary destination for data scientists"
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2025/02/05/DSAI-journal.html#why-publish",
    "href": "foundation-frontiers/datasciencebites/posts/2025/02/05/DSAI-journal.html#why-publish",
    "title": "RSS: Data Science and Artificial Intelligence - showcase your research",
    "section": "Why Publish?",
    "text": "Why Publish?\nRSS: Data Science and Artificial Intelligence offers an exciting open access venue for your work with a broad reach and is peer reviewed by editors esteemed in their field. Discover more about why the new journal is the ideal platform for showcasing your research"
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2025/02/05/DSAI-journal.html#submit-a-paper",
    "href": "foundation-frontiers/datasciencebites/posts/2025/02/05/DSAI-journal.html#submit-a-paper",
    "title": "RSS: Data Science and Artificial Intelligence - showcase your research",
    "section": "Submit a paper",
    "text": "Submit a paper\nFind out how to prepare your manuscript for submission and visit our submission site to submit your paper"
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2025/02/05/DSAI-journal.html#editors",
    "href": "foundation-frontiers/datasciencebites/posts/2025/02/05/DSAI-journal.html#editors",
    "title": "RSS: Data Science and Artificial Intelligence - showcase your research",
    "section": "Editors",
    "text": "Editors\n \n\n\n\nSach Mukherjee is Director of Research in Machine Learning for Biomedicine at the Medical Research Council (MRC) Biostatistics Unit, University of Cambridge, and Head of Statistics and Machine Learning at the German Center for Neurodegenerative Diseases.\n\n\n\nSilvia Chiappa is a Research Scientist at Google DeepMind London, where she leads the Causal Intelligence team, and Honorary Professor at the Computer Science Department of University College London.\n\n\n\nNeil Lawrenece is the inaugural DeepMind Professor of Machine Learning at the University of Cambridge. He has been working on machine learning models for over 20 years. He recently returned to academia after three years as Director of Machine Learning at Amazon.\n\n\n\n\nView the full editorial board here: Editorial Board | RSS Data Science | Oxford Academic (oup.com)"
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2025/02/05/DSAI-journal.html#open-access",
    "href": "foundation-frontiers/datasciencebites/posts/2025/02/05/DSAI-journal.html#open-access",
    "title": "RSS: Data Science and Artificial Intelligence - showcase your research",
    "section": "Open Access",
    "text": "Open Access\nRSS: Data Science and Artificial Intelligence is fully open access (OA) and is published by Oxford University Press (OUP). Your research will be free to read and can be accessed globally. An OA license increases the visibility of your research and creates more opportunities for fellow researchers to read, share, cite, and build upon your findings.\nThe cost of publishing Open Access may be covered under a Read and Publish agreement between OUP and the corresponding author’s institution. Find out if your institution is participating. Members of the Royal Statistical Society can submit papers at a reduced cost.\nExplore the journal’s website now www.academic.oup.com/rssdat\n\nExplore more data science ideas\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution (CC BY 4.0) International licence."
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2023/01/24/pulling-patterns.html",
    "href": "foundation-frontiers/datasciencebites/posts/2023/01/24/pulling-patterns.html",
    "title": "Pulling patterns out of data with a graph",
    "section": "",
    "text": "About the paper and this post\n\n\n\n\n\nTitle: Extracting the main trend in a data set: The Sequencer algorithm\nAuthor(s) and year: Dalya Baron and Brice Ménard (2021)\nStatus: Published in The Astrophysical Journal, open access: HTML, PDF.\nEditor’s note: This post is republished with permission from MathStatBites to demonstrate the Bites concept. For more information about Bites articles and how to contribute to DataScienceBites, see our notes for contributors.\nLarge volumes of data are pouring in every day from scientific experiments like CERN and the Sloan Digital Sky Survey. Data is coming in so fast that researchers struggle to keep pace with the analysis and are increasingly developing automated analysis methods to aid in this herculean task. As a first step, it is now commonplace to perform dimension reduction in order to reduce a large number of measurements to a set of key values that are easier to visualize and interpret.\nWhen working on the cutting edge, another problem scientists often face is that “we don’t know what we don’t know”. For this reason, we often want to simply ask the data, “What is interesting about you?” This is the realm of “unsupervised” methods, where the data itself drives the analysis, with little to no guidance or human labeling of the data.\nMany physical processes depend continuously on some driving parameter. For example, the evaporation rate of water increases with temperature. We call these continuous variations in datasets “trends”. Describing a dataset by a single trend reduces it to one dimension - an ordered list. Finding such a trend within a high-dimensional dataset is the aim of a method called “The Sequencer” introduced by Baron and Ménard."
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2023/01/24/pulling-patterns.html#key-insight-a-tree",
    "href": "foundation-frontiers/datasciencebites/posts/2023/01/24/pulling-patterns.html#key-insight-a-tree",
    "title": "Pulling patterns out of data with a graph",
    "section": "Key insight: A tree",
    "text": "Key insight: A tree\nThe key insight of Baron and Ménard was to relate trends in data to an object from graph theory called a minimum spanning tree. Given a measure of distance between two data points, for example the usual (Euclidean) distance between two points, we can visualize a dataset as a graph. This graph consists of a node (a dot) for each data point. These nodes are then connected by an edge (a line), labeled by the distance between the two data points. The minimum spanning tree is a reduction of this graph to include only enough of the smallest distance edges so that no node is isolated.\nWhat Baron and Ménard realized is that datasets that are “trendy” have elongated and narrow minimum spanning trees. As shown in Figure 1, a totally random dataset results in a graph with many branches while a perfect sequence results in a perfect linear graph. Then, they use connectivity of the nodes in the minimum spanning tree to return an ordering of the data that follows the main trend in the dataset. However, a sequence is all you get. It is up to us to understand and interpret what this trend represents.\n\n\n\n\n\nFigure 1: Examples demonstrating that data with stronger trends have more narrow and elongated minimum spanning trees. Adapted from Baron and Ménard (2021), Figure 1. Figure used under CC BY 4.0.\nSometimes, the ordering of observations within a data point matters, like in time series data. Measurements taken close in time are more likely to be correlated than measurements taken after a long time delay. Baron and Ménard were careful to include a measure of distance that takes this ordering into account, unlike our usual notion of distance. They argue that this gives them an edge over other common dimension reduction techniques such as t-SNE and UMAP, and even go so far as to use The Sequencer to optimize the hyperparameters used by these other methods!"
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2023/01/24/pulling-patterns.html#when-does-it-fail",
    "href": "foundation-frontiers/datasciencebites/posts/2023/01/24/pulling-patterns.html#when-does-it-fail",
    "title": "Pulling patterns out of data with a graph",
    "section": "When does it fail?",
    "text": "When does it fail?\nIt is important to acknowledge that no statistical or machine-learning tool is a cure-all. And, the authors themselves are quick to point out several limitations that can hinder the application of their method. The Sequencer can struggle when the data has a large dynamic range, a variety of signal strengths relative to noise, or there are multiple trends present in the data. In each case, Baron and Ménard propose ways to mitigate these problems, but practitioners still need to be wary when applying The Sequencer in those instances."
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2023/01/24/pulling-patterns.html#what-discoveries-await",
    "href": "foundation-frontiers/datasciencebites/posts/2023/01/24/pulling-patterns.html#what-discoveries-await",
    "title": "Pulling patterns out of data with a graph",
    "section": "What discoveries await?",
    "text": "What discoveries await?\nTo demonstrate the power of their method, Baron and Ménard apply The Sequencer to several real datasets where a pattern was already known and show that The Sequencer recovers that pattern. Examples include ordering spectral measurements of stars by their temperature and quasars by their redshift, a measure of their distance from us on Earth.\nBut, what about new patterns? The team has already applied The Sequencer to mine seismographic data and discover previously unknown formations deep within the earth, at the boundary between the core and mantle. By sequencing the seismic waves, they realized that the main trend was the amplitude of diffraction off of these structures, which they were then able to localize beneath Hawaii and the Marquesas (DOI: 10.1126/science.aba8972).\nFor more demonstrations and discoveries, or even to upload your own data for sequencing, check out the project website. Data sleuths can also download all of the code directly from GitHub and sequence to their hearts’ content!\n\n\n\n\n\n\nAbout the author\n\nAndrew Saydjari is a graduate student in physics at Harvard researching the spatial and chemical variations of dust in the interstellar medium. He favors using interpretable statistics and large photometric and spectroscopic surveys.\n\n\n\n\n\nAbout DataScienceBites\n\nDataScienceBites is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to become a contributor."
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2023/07/17/dsb-live.html",
    "href": "foundation-frontiers/datasciencebites/posts/2023/07/17/dsb-live.html",
    "title": "Heading to a conference this summer? Share your learnings here",
    "section": "",
    "text": "Three major events in the statistics and data science calendar are taking place over the next few months, and we want to give the wider community the opportunity to sample some of the exciting ideas being discussed. For that, we need your help!\nIf you’re a student or early career researcher and you’re attending one or all of the…\n\nWorld Statistics Congress\nJoint Statistical Meetings\nRoyal Statistical Society International Conference\n\n…we invite you to write about your favourite paper or session as a “Bites” post.\nBites posts are digestible, engaging, non-technical summaries of research papers and presentations, written for an undergraduate-level audience. The goal is to draw attention to key findings, potential applications, and the wider implications of new ideas and developments in statistics and data science. Advice and guidance on how to write a Bites post can be found here, and example posts can be found on our DataScienceBites page.\nFor our summer conference coverage, Real World Data Science is partnering with our friends at MathStatBites. If you write a Bites post and it is accepted for publication, the post will appear on one or both of our sites – depending on the focus of the research you’re writing about.\nIf you have any questions, please feel free to contact us. Otherwise, safe travels, enjoy the conference(s), and we look forward to hearing from you soon!\n\n\n\n\nAbout DataScienceBites\n\nDataScienceBites is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to become a contributor.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail image includes photo by Product School on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Heading to a conference this summer? Share your learnings here.” Real World Data Science, July 17, 2023. URL"
  },
  {
    "objectID": "foundation-frontiers/datasciencebites/posts/2022/12/13/ridesharing.html",
    "href": "foundation-frontiers/datasciencebites/posts/2022/12/13/ridesharing.html",
    "title": "Determining the best way to route drivers for ridesharing via reinforcement learning",
    "section": "",
    "text": "About the paper and this post\n\n\n\n\n\nTitle: Dynamic causal effects evaluation in A/B testing with a reinforcement learning framework\nAuthor(s) and year: Chengchun Shi, Xiaoyu Wang, Shikai Luo, Hongtu Zhu, Jieping Ye, Rui Song (2022)\nStatus: Published in Journal of the American Statistical Association, Theory and Methods, open access: HTML, PDF, EPUB.\nEditor’s note: This post is republished with permission from MathStatBites to demonstrate the Bites concept. See here for more information.\n\n\n\nCompanies often want to test the impact of one design decision over another, for example Google might want to compare the current ranking of search results (version A) with an alternative ranking (version B) and evaluate how the modification would affect users’ decisions and click behavior. An experiment to determine this impact on users is known as an A/B test, and many methods have been designed to measure the “treatment” effect of the proposed change. However, these classical methods typically assume that changing one person’s treatment will not affect others (known as the Stable Unit Treatment Value Assumption or SUTVA). In the Google example, this is typically a valid assumption — showing one user different search results shouldn’t impact another user’s click behavior. But in some situations, SUTVA is violated, and new methods must be introduced to properly measure the effect of design changes.\nOne such situation is that of ridesharing companies (Uber, Lyft, etc.) and how they determine which drivers are sent to which riders (the dispatch problem). Simply put, when a driver is assigned to a rider, this decision impacts the spatial distribution of drivers in the future. Hence the dispatch strategy (our treatment) at the present time will influence riders and drivers in the future, which violates SUTVA and hence invalidates many traditional methods for A/B testing. To tackle this problem, a group of researchers have recently employed a reinforcement learning (RL) framework which can accurately measure the treatment effect in such a scenario. Furthermore, their proposed approach allows for companies to terminate A/B tests earlier if the proposed version B is found to be clearly better. This early stopping can save time and money.\nTo better understand RL and how it can contribute to tackling the issue at hand, it’s first helpful to set some context. In typical RL problems, including the one in this paper, the scenario is modeled with something known as a Markov Decision Process (MDP). A MDP links three sets of variables across time: the state or environment, the treatment or action (the reaction to the environment), and the outcome (the response produced by the environment due to the action). These outcomes can be thought of as rewards which depend on the action taken and the state observed. Over time, the machine learns which actions produce more positive rewards and which bring about worse outcomes. Hence, the actions leading to higher rewards are positively reinforced, thus the name reinforcement learning. A causal diagram of an MDP is shown in Figure 1, where St, At, and Yt are the state, action, and outcome at time t. As one can see, past treatments influence future outcomes by altering the state variables at the present (the so-called “carryover effect” which violates SUTVA).\n\n\n\n\n\nFigure 1: Causal diagram of MDP, where the solid lines represent causal relationships.\nMaking this more concrete, consider an example where the decision-maker is a ridesharing company. The environment or state is whatever variables the decision-maker can measure about the world, like the spatial distribution of drivers, number of pickup requests, traffic, and weather. The company then makes some action on how to dispatch drivers. The combination of the state and action leads to an outcome that can be measured, for example passenger waiting time or driver’s income. The strategy which is used to determine an action is known as a policy. This policy could be designed to take the state into account or simply be fixed regardless of what environment is encountered. Much of the reinforcement learning literature focuses on the former (policies that depend on the state), but the authors argue that fixed designs are the de facto approach in industry A/B testing and hence they focus on that setting. In particular, a common treatment allocation design is the switchback design, where there are two policies of interest (the current dispatching strategy vs a proposed strategy) determined ahead of time and they are employed in alternating time intervals during the A/B test.\nSo how are policies compared to determine the treatment effect? The answer lies in what is known as the value function, which measures the total outcome that would have amassed had the decision-maker followed a given policy. The value function can put more value on short-term gain in outcome or long-term benefits. The two policies in an A/B test each have their own value functions, and a proposed policy is determined to be better if its value is (statistically) significantly higher. In the ridesharing setting, one possible outcome of interest is driver income. An A/B test in that scenario would thus look to see if a proposed policy had greater expected driver income vs the current policy.\nA natural question is when to end the experiment and test for a difference in value. In practice, companies will often simply run the test for a prespecified amount of time, such as two weeks, and then perform an analysis. But if one policy is clearly better than another, that difference could be detectable much earlier and the company is wasting valuable resources by continuing the experiment. To address this issue, the authors take an idea from clinical trials, the “alpha-spending” approach, and adapt it to their framework. Alpha-spending is one way to distribute over time the prespecified “budget” of Type 1 error (the probability of falsely detecting that a new policy is better). In the article’s real-data example, the authors test once a day for each day after one week and are able to detect a significant difference on Day 12. Waiting until Day 14 would have resulted in poorer outcomes since a suboptimal policy would be implemented half the time for two more days.\nOverall, the framework introduced allows for handling of carryover effects, is capable of modeling treatment allocation like the switchback design, and furthermore, allows for possible early stopping. With these three features, the authors argue their approach is highly applicable to the current practice of ridesharing companies (and possibly other industries as well). For interested readers wanting to dive deeper into the methodology presented, you can check out the full article, listen to the first author discuss the material at the Online Causal Inference Seminar (embedded below), or explore the Python implementation available on GitHub.\n\n\n\n\n\nAbout the author\n\nBrian King is a PhD candidate in the Department of Statistics at Rice University and a current NSF Graduate Research Fellow, with research focused on Bayesian modeling and forecasting for time series of counts. Prior to Rice, he graduated from Baylor University with a B.S. in Mathematics and Statistics alongside a secondary major in Spanish and a minor in Computer Science.\n\n\n\n\n\nAbout DataScienceBites\n\nDataScienceBites is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to become a contributor."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/04/29/gen-ai-human-intel.html",
    "href": "foundation-frontiers/posts/2024/04/29/gen-ai-human-intel.html",
    "title": "AI series: Generative AI models and the quest for human-level artificial intelligence",
    "section": "",
    "text": "Generative artificial intelligence (AI) models have taken the world by storm over the past year. The human-like outputs of these systems, and the recent publication of a guideline to determine the degree of consciousness of machines, have again raised the question of whether machines will soon be able to replicate human intelligence. In this article, we discuss some of the merits and limitations of modern machine learning models, and also provide a general view of human intelligence and the position of “intelligent” systems in the constellation of human capabilities.\nLarge language models (LLMs) such as ChatGPT are designed to process and understand natural language, and to generate human-like text in response to prompts and questions. This is achieved thanks to a specific type of deep learning architecture called the Transformer, which consists of an encoder and a decoder, each made up of some number N of blocks. Here the input text is eventually transformed into predicted (and contextualised) output text (Figure 1).1 LLMs are trained on complex and large bodies of text so that they can learn complex patterns and relationships between words in sentences, in different contexts.\nThe two main types of LLMs are autoregressive models and autoencoding models. Autoregressive models such as OpenAI’s GPT (Generative Pre-trained Transformer) generate text by predicting the next word in a sequence given the previously emitted words. Autoencoding models such as Google’s BERT (Bidirectional Encoder Representations from Transformers) also aim to produce coherent and contextually relevant text, but they do so by attempting to predict missing words (from a corrupted version of the text) while considering the surrounding context.\nLLMs are engineering marvels capable of producing syntactically flawless, coherent, and remarkable responses to complex requests such as question answering, text summarisation, computer code generation, document classification, text generation and sentiment analysis. We tend to associate linguistic skills with intelligence because communication via an elaborate language system is largely synonymous with the human intellect. So, do the linguistic skills of tools like ChatGPT mean these systems are close to displaying human-level intelligence? Proponents of the Turing test might well argue “yes”. Most would still say “no”."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/04/29/gen-ai-human-intel.html#speaking-and-understanding",
    "href": "foundation-frontiers/posts/2024/04/29/gen-ai-human-intel.html#speaking-and-understanding",
    "title": "AI series: Generative AI models and the quest for human-level artificial intelligence",
    "section": "Speaking and understanding",
    "text": "Speaking and understanding\nThe Turing test was proposed by Alan Turing in the 1950s. It operates on the basis that if a person is unable to tell whether the entity they are interacting with over typed messages is a human or a machine (irrespective of the answers being correct), the machine is said to have achieved human-level intelligence.2 There’s some debate over whether ChatGPT could pass this test; it has been specifically trained not to impersonate humans and will frequently preface its responses with the phrase, “As a large language model…”, thus giving the game away. But others are impressed at what developers like OpenAI have been able to achieve in terms of building artificial models that can sustain realistic-sounding conversations.\nThis, though, is where the Turing test falls apart as a means of assessing machine intelligence. The ability to “mimic human chatter” does not by itself suggest that a machine understands what it is ‘reading’ or ‘writing’ in the same way a human would. Consider a different set of tests, called the Winograd schemas – puzzles that differ by one or two words and whose solutions cannot be determined using statistics but instead require common sense and an understanding of the physical world.3 For example, the following sentence:\n\nThe trophy would not fit in the suitcase because it was too small/large.\n\nHumans would infer that if the last word of that sentence is small, then suitcase is the object being described, whereas if the word is large then we are referring to the trophy. ChatGPT v3.5 was unable to make this inference when the question was first put to it (see Figure 2), although a later test finds it now can – as can other LLMs. Some suggest that this type of improvement comes from training ChatGPT to do better on some of the tasks that are routinely used to highlight model limitations on social media at the expense of giving worse answers in other contexts. But could this improvement be due to ChatGPT and other models suddenly having acquired common sense and an understanding of the physical world? A more complex set of Winograd schema questions, the WinoGrande dataset, suggests not: humans still outperform computers on these tests.4\n\n\n\n\n\n\nFigure 2: GPT-3.5’s inconclusive answer to a typical WinoGrad schema question.\n\n\n\nMachines will likely beat us all at these puzzles one day, in the same way that they already beat the world champions of chess and Go, can translate across multiple languages, and diagnose rare forms of cancer that escape well-trained doctors. These narrow AI applications that spectacularly outperform humans at very specific tasks will become more and more common. But will a multiplicity of narrow AI soon lead to general AI that can compete or beat humans at all tasks? Will human-level linguistic capabilities inevitably result in machines acquiring human-level intelligence in the not-too-distant future? Some developers and researchers certainly believe or hope so. Others remain sceptical."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/04/29/gen-ai-human-intel.html#thinking-and-learning",
    "href": "foundation-frontiers/posts/2024/04/29/gen-ai-human-intel.html#thinking-and-learning",
    "title": "AI series: Generative AI models and the quest for human-level artificial intelligence",
    "section": "Thinking and learning",
    "text": "Thinking and learning\nWhat is “intelligence”, anyway? More than 70 working definitions currently exist.5 One that focuses specifically on human-level intelligence while excluding lower types of animal intelligence is this: “intelligence can be understood as the ability to generate a range of plausible scenarios about how the world around you may unfold and then base sensible actions on those predictions”.6 Does this come anywhere close to describing the way LLMs work, or indeed any other machine learning algorithm?\nIn my view, part of the reason why attaining human-level intelligence remains a distant goal has to do with how machines think differently from us.\nThe goal of a machine learning algorithm is always to optimise a particular function – whether the machine is playing chess (the goal is to win) or classifying images (the goal is to correctly classify as many images as possible). The majority of problems that human intelligence has to “solve”, however, do not always have a clear goal. Consider a simple chatbot standing in for a human on a customer helpline: What scalar quantity should it look to optimise? Is it ensuring that engagement with the customer is informative and supportive? Or, perhaps the goal is to build a recurrent relationship. In either case, how will these quantities be measured? Dealing with this type of real-world problem, where the variable to optimise is not well-defined, represents a formidable obstacle for the development of machine learning algorithms whose behaviour must approximate human intelligence.\nMoreover, machines can be surprisingly easy to fool. For example, placing an object next to the one we are trying to classify can confuse image classification algorithms – a well-known example shows a patch being placed next to a banana, which makes the deep neural network (DNN) classify the banana as a toaster with a high degree of confidence. We can also fool image classification networks by showing the same object under different lighting conditions and orientations, such as when we flip a school bus on its side (as in an accident). The reason why a DNN cannot do this trivial mental rotation is because learning algorithms cannot generalise knowledge to unseen (or “out of distribution”) examples – imbuing them with no small amount of “brittleness”.7 Compare this to the human mind, which learns in a semi-supervised manner: we need only be shown a few guiding examples to be able to extrapolate knowledge. This is a clear evolutionary adaptation since most real-world learning is semi-supervised: we do not need to explore every road to learn to drive, nor do every possible differentiation exercise to become confident at calculus. Likewise, we do not need to become fully bilingual in a language before we start combining newly learned words to try to explain complex concepts.\nNext to GPT-4’s 100-trillion parameter model, the human brain seems much more parsimonious. Therefore, progress in AI would perhaps come faster if we could teach machines to learn from a few (or no) labelled examples instead of being so heavily dependent on terabytes of labelled data (supervised learning) and on our own interpretation of the world.8\nAnother major limitation of algorithms for achieving human-like adaptive learning in changing environments is the fact that they cannot keep learning without forgetting previously learned training data. This phenomenon is called catastrophic forgetting,9 and it occurs because of the stability-plasticity dilemma: this states that a certain degree of plasticity is required for the integration of new knowledge, but also radically new knowledge (e.g. large weight changes in a DNN) disrupts the stability necessary for retaining the previously learned representations (Figure 3). In other words, weight stability is synonymous with knowledge retention, but it also introduces the rigidity that prevents the learning of new tasks.10\n\n\n\n\n\n\nFigure 3: Illustration of catastrophic forgetting and ideal learning in a two-class classifier.\n\n\n\nAlthough some ingenious approaches have been developed for mitigating catastrophic forgetting (and which are much smarter than simply building a new network for each new task), it has also been shown that no single method can solve catastrophic forgetting while allowing incremental learning in every possible situation.11 The problem with catastrophic forgetting is not only that it contradicts a fundamental characteristic of human intelligence – the ability to learn within, and adapt to, changing environments;12 catastrophic forgetting is also a major bottleneck for the development of adaptable systems that learn incrementally from the constant flow of data in the real world, such as autonomous vehicles, recommender systems, anomaly detection methods and, in general, any device embedded with sensors. Moreover, the development of continual learning methods is key not just for machine intelligence, but also for learning scalability: by 2025 the world will be producing some 175 zettabytes of data annually, of which we will only be able to store between 3% and 12%. Thus, for learning to be scalable in the future, continual learning methods will need to be able to process data faster and in real time, learn on the fly and then discard the data, much like humans do."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/04/29/gen-ai-human-intel.html#how-are-we-wired",
    "href": "foundation-frontiers/posts/2024/04/29/gen-ai-human-intel.html#how-are-we-wired",
    "title": "AI series: Generative AI models and the quest for human-level artificial intelligence",
    "section": "How are we wired?",
    "text": "How are we wired?\nOne further bottleneck for machines emulating human-level intelligence is that we do not yet understand the circuitry of the brain well enough to be able to reproduce it, and therefore we have been working with misleading models. For instance, the realisation of the “all-or-nothing” nature of action potentials (i.e. there is no such thing as the partial firing of a neuron) led McCulloch and Pitts to propose the concept of the artificial neuron and suggest that networks of neurons could equally be modelled as (all-or-nothing) logical propositions.13 From this point onwards, the dominant idea over most of the past century has been that the brain is essentially a computer. But even on a superficial level of analysis, brains and computers have very different architectures and behaviour, with computers specifically making optimal use of virtually unlimited memory as well as an extraordinary capacity for brute-force searching. On the microscopic level, networks of neurons cannot possibly be modelled as logical propositions because they do not really operate in this manner: a single neuronal synapse is an environment that harbours hundreds of proteins that specifically interact with other proteins in complex networks that possess clear time-space coordinates. Neurons process information and generate not just electrical signals but also discrete biochemical changes that occur in cycles instead of linearly. Moreover, a system like the brain responds to stimuli over long periods of time, which can effect changes in its own behaviour. Understanding at least how some cognitive tasks are performed at an algorithmic level would likely translate into major progress towards emulating human-level intelligence.14\nGPT-4’s impressive capabilities can make people believe that some AI systems are conscious on a human level (since animals display limited consciousness), but this is an illusion. Consciousness is another essential quality that we can easily recognise when we see it, but which (like intelligence) is extraordinarily difficult to define – other than consciousness simply being everything you experience, or, more formally put, the “awareness of internal and external existence”.15 Could machines eventually achieve consciousness? This is a controversial and key question because, besides intelligence, having a degree of consciousness on the level exercised by humans is believed to be necessary for displaying goal-directed behaviour.16 Recall that machine learning algorithms do have the general goal of optimising functions but these goals are determined by human programmers, not the machines themselves.\nA fundamental question regarding the development of consciousness is this: if an AI system were close to consciousness, how would we know? Butlin and colleagues recently explored the question in a groundbreaking paper where they compiled a list of “indicator properties” drawn from various neuroscientific theories of consciousness (since no theory is clearly superior). The idea is that the more boxes an AI system ticks, the more likely it is close to being conscious. The authors argue that a failure to identify artificial consciousness has important moral implications because an entity that exhibits consciousness invariably influences how we feel it should be treated. While this is likely true, humans do not necessarily need to feel that an entity is conscious in order to develop empathy. Emotional attachment is a basic human instinct, and we have a tendency to anthropomorphise. You may remember the story of hitchBOT, a clearly unconscious yet friendly robot invented by David Smith of McMaster University. HitchBOT could barely speak and its only mission was to autostop. It ended up travelling throughout Europe and North America thanks to the sympathy it generated. The “beheading” of the robot in Philadelphia in 2015 had huge repercussions across the planet because thousands of strangers had developed empathy and become emotionally attached to hitchBOT despite never having met it.17 Do you think that a machine is likely to develop a degree of human-like empathy anytime soon?\nFinally, another fundamental human trait that is absent from machines, and which is particularly important in these trying times, is our capacity for remaining hopeful, which can be seen as a post-hoc rationalisation of our survival instinct. Being hopeful means that we think things will improve beyond what would be reasonable to predict for the immediate or medium-term future given the most recently available data points. Jane Goodall defines hope as “a crucial survival trait that enables us to keep going in the face of adversity”. Desmond Tutu gave an equally ethereal definition: “Hope is being able to see that there is light despite all the darkness”.18 One fundamental aspect of hope is its undeniable association with agency, i.e. our capacity to voluntarily act in a given environment: even when we are at odds with the desired outcome, hope makes us take action, which in turn fuels more hope, thus establishing a dynamic form of self-stimulation over thousands of ethical actions without necessarily having a clear variable to optimise, which machines are incapable of doing. And, one very interesting thing about hope is that its effects can be quantified in the short term: hope is much better than intelligence and personality at predicting academic performance,19 as well as performance in the workplace, with hopeful workers being reported as 14% more productive.20"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/04/29/gen-ai-human-intel.html#closing-thoughts",
    "href": "foundation-frontiers/posts/2024/04/29/gen-ai-human-intel.html#closing-thoughts",
    "title": "AI series: Generative AI models and the quest for human-level artificial intelligence",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nGenerative language-based models have reignited much interest in the possibility of artificially recreating human-level intelligence. Despite being seminal breakthroughs, we must not forget that LLMs are, essentially, just very sophisticated pattern recognition systems which, when trained on even larger datasets, may become even better at predicting the most appropriate responses to different prompts. LLMs are incomplete models of thought, though, plagued by practical problems that we have not discussed here, such as giving incorrect answers, security breaches, privacy concerns regarding personal data used in their training datasets, algorithmic opacity and an inability to meet the EU’s General Data Protection Regulation (GDPR), and their amplification of web bias which can result in answers that discriminate against different groups.21 Even if these limitations are fixed one day, the capabilities of LLMs still do not approximate general human intelligence.\nGenerative models are just one type of narrow AI application. Such applications will continue to evolve at a very fast pace and produce breakthroughs of paramount importance. Some of the latest breakthroughs in the biomedical field include the discovery of new antibiotics against deadly antibiotic-resistant bacteria22 and AlphaFold’s accurate prediction of a protein’s structure from its amino acid sequence.23 24 The number of ways an amino acid sequence may fold is astronomical. Thus being able to predict a protein’s structure as accurately as experimental measurements (by X-ray crystallography or cryo-electron microscopy) represents a gigantic step towards understanding a protein’s likely function and its regulation, how it may be drugged to combat diseases and for antibiotic development, and its manipulation to guide vaccine design as was done during the coronavirus pandemic.25 Most impressively, AlphaFold is able to predict the structural effects of single amino changes (mutations), which is essential for engineering new proteins as well as for understanding evolutionary history and mechanistic aspects of diseases.26\nBeing able to harness the power of narrow AI applications and delegate some tasks to machines will allow humans to focus on those tasks at which we do better than machines. Augmented intelligence is the name given to the close collaboration between humans and machines, which was first proposed in the 1950s, and is now finally within reach.27 28 Current examples of devices that are a functional extension of human beings are virtual reality headsets that expand the users’ senses and perceptions, implantable technologies that substitute access cards, and, in general, any software that automates research and data analysis. Since such technological developments might make us more “intelligent” or at least more productive, will we then still need machines that display human-like intelligence?\nThe official position of some major players like Microsoft is to not even attempt to replicate human intelligence but to produce “AI centred on assisting human productivity in a responsible way”.29 Still, a recent paper that reported GPT-4’s impressive performance at solving a number of difficult tasks (in the fields of mathematics, coding, vision, medicine, law and psychology) suggests that GPT-4 displays “sparks of artificial general intelligence”. This is in line with OpenAI’s clearly stated goal of developing human-level intelligence. However, the debate over whether we are getting any closer to replicating intelligence with just a few impressive generative models that simply recombine and duplicate data on which they have been trained is self-limiting because it takes a very narrow view of human intelligence. For one thing, mindlessly generating text (“speaking”) and thinking are two very different things. It has been shown that while LLMs may excel at formal linguistic competence (understanding language rules and patterns), their performance on tasks that evaluate human thought in the real world (functional linguistic competence) is very limited. Moreover, GPT-4 is unable to reason. We can define reasoning as the process of drawing justifiable conclusions from a set of premises, which is also a key component of intelligence. When given a set of 21 distinct problems ranging from simple arithmetic and logic puzzles to spatial and temporal reasoning, and medical common sense, GPT-4 proved incapable of applying elementary reasoning techniques.\nOpenAI’s newest headline-grabbing development, Sora, shares many of the limitations of GPT-4. Sora is a model that can generate video clips from text prompts – but while it may prove useful for content creation, it seems incapable of understanding the real world. OpenAI’s defence is that Sora still struggles with “simulating the physics of a complex scene” but that it “represents a foundation for models that can understand and simulate the real world”. This is, OpenAI believes, key for training models that will help solve problems that require simulating the physical world (e.g. rescue missions), and eventually for achieving general AI. However, it is suspected that Sora’s limitations in understanding the physical world have nothing to do with physics. For example, in a generated video of a monkey playing chess in a park, we see a 7x7 board and three kings. This is likely not an error of insufficient training data or of computational power. This is an error that reveals a failure to discern the cultural regularity of the world by making wrong generalisations despite having ample evidence of the existence of universal 8x8 chess boards and one king per player. A video of a stylish woman wandering in Tokyo is also incorrect for the same reason: nobody takes two consecutive left steps in a row (about 30s into the video). Sora also does not appear to understand cause and effect; for example, in a video of a basketball that makes a hoop explode, the net appears to be restored automatically following the explosion. Sora uses arrangements of pixels to predict new pixel configurations, but without trying to understand the cultural context of the images. This is why the images and videos generated by Sora seem correct at the pixel level but globally wrong. Thus, OpenAI’s claim that “scaling video generation models is a promising path towards building general purpose simulators of the physical world” is open to doubt.\nLLMs do not yet approximate the human brain; generative video models do not approximate the physical world; and human intelligence is so much more than combining formal linguistic competence with complete models of thought, or making creative videos that respect the physical constraints of the world. Human intelligence is not limited to specific domains either but exists in the open to challenge currently held views. Ask Noam Chomsky and he will respond that generative models like ChatGPT are essentially “high-tech plagiarism”. Human consciousness includes a sense of self which machines will not be able to replicate anytime soon – or perhaps never will, since a human brain and a computer are not the same. Human consciousness is coupled with curiosity, imagination, intuition, emotions, desires, purpose, objectives, wisdom and even humour. If we think about humour, a good sense of humour means thinking outside of the box and connecting concepts and situations in novel ways, which is something that machines are unable to do. Also, by thinking outside the box, humans are able to consciously ask a variety of questions – the most extraordinary of which have led to major leaps in our understanding of the world around us.\nReasonable questions can be posed by many and answered logically (some even by machines) using the standard scientific process of experimental design, controls and hypothesis validation. In this context, the faster and more efficient exploration of search space by learning methods, complemented by the delegation of repetitive tasks to machines, will allow scientists to conduct experiments at greater scale while focusing on designing optimal solutions. Beyond reasonable questions and expected results is the concept of serendipity that machines cannot yet be made to grasp. Some of the greatest discoveries in the history of science are indeed serendipitous (accidental), including the discovery of insulin, penicillin, smallpox vaccination, the anti-malarial drug quinine, X-rays, nylon and the anaesthetic effects of ether and nitrous oxide.30 Turning accidents into discoveries requires having a questioning mind that can view data from several perspectives and connect seemingly unrelated pieces of information instead of discarding unusual results right away.\nAnd yet beyond serendipitous discoveries we have extraordinary questions, which machines are as yet incapable of asking. Extraordinary questions lie outside of our current frame of knowledge and require an illogical step that is often the product of letting one’s mind wander freely.31 A classical example here is when Einstein was trying to modify Maxwell’s equations so that they were no longer in contradiction with the constant speed of light that had been observed. After trying to modify these equations for years, Einstein eventually realised that it was not Maxwell’s fault. Rather, our notion of time was incorrect. Einstein thus stumbled upon the very question that led to the idea that the rate at which time passes depends on one’s frame of reference. While machines follow rules, the revolutionary ideas of Einstein, Newton, Darwin, Galileo, Wittgenstein and many others did not follow any rules established at the time. Therefore, the real danger in thinking that we can rely on “intelligent” machines to achieve a human level of imagination, intuition, wisdom or purpose anytime soon is that the world will become an even more statistically predictable place."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/04/29/gen-ai-human-intel.html#also-in-the-ai-series",
    "href": "foundation-frontiers/posts/2024/04/29/gen-ai-human-intel.html#also-in-the-ai-series",
    "title": "AI series: Generative AI models and the quest for human-level artificial intelligence",
    "section": "Also in the AI series:",
    "text": "Also in the AI series:\nWhat is AI? Shedding light on the method and madness in these algorithms Healthy datasets for optimised AI performance\n\nExplore more data science ideas\n\n\n\n\n\nAbout the author\n\nDiego Miranda-Saavedra, PhD, is a data scientist and a financial investor. His book How To Think About Data Science (Chapman & Hall / CRC Press) was published in December, 2022.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Diego Miranda-Saavedra\n\n\n  Text, code, and figures are licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) International licence, except where otherwise noted. Thumbnail image by Jamillah Knowles / Better Images of AI / Data People / Licenced by CC-BY 4.0.\n\n\n\nHow to cite\n\nMiranda-Saavedra, Diego. 2024. “Generative AI models and the quest for human-level artificial intelligence.” Real World Data Science, April 29, 2024."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/04/29/gen-ai-human-intel.html#footnotes",
    "href": "foundation-frontiers/posts/2024/04/29/gen-ai-human-intel.html#footnotes",
    "title": "AI series: Generative AI models and the quest for human-level artificial intelligence",
    "section": "References",
    "text": "References\n\n\nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I. Attention is All you Need. In Advances in Neural Information Processing Systems 30 (NIPS 2017), Long Beach, California (USA), 2017. ISBN: 9781510860964.↩︎\nTuring A. Computing Machinery and Intelligence. Mind, LIX(236):433-460, 1950.↩︎\nLevesque HJ, Davis E, Morgenstern L. The Winograd Schema Challenge. In KR’12: Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, June 2012, Rome, Italy. AIII Press, Palo Alto (CA), USA, 2012. ISBN: 9781577355601.↩︎\nSakaguchi K, Le Bras R, Bhagavatula C, Choi Y. WinoGrande: An Adversarial Winograd Schema Challenge at Scale. In Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence, 34(05), 8732–8740. February 2020, New York (NY), USA. AIII Press, Palo Alto (CA), USA, 2020. ISSN: 2159–5399.↩︎\nLegg S, Hutter M. A Collection of Definitions of Intelligence. Proceedings of the 2007 Conference on Advances in Artificial General Intelligence: Concepts, Architectures and Algorithms: Proceedings of the AGI Workshop 2006, 17-24. IOS Press, Amsterdam, the Netherlands, 2007. ISBN: 978-1-58603-758-1.↩︎\nSuleyman M, Bhaskar M. The Coming Wave. Bodley Head, London, UK, 2023. ISBN-10: 1847927483.↩︎\nMcCarthy J. From Here to Human-Level AI. Artificial Intelligence, 171(18):1174–1182, 2007.↩︎\nLeCun Y, Bengio Y, Hinton G. Deep Learning. Nature, 521(7553):436–444, 2015.↩︎\nMcCloskey M, Cohen NJ. Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem. Psychology of learning and motivation, 24:109–165, 1989.↩︎\nAbraham WC, Robins A. Memory Retention - the Synaptic Stability Versus Plasticity Dilemma. Trends in Neurosciences, 28(2):73–78, 2005.↩︎\nKemker R, McClure M, Abitino A, Hayes T, Kanan C. Measuring Catastrophic Forgetting in Neural Networks. In Proceedings of the AAAI Conference on Artificial Intelligence, 32(1). AAAI Press, Palo Alto (CA), USA, 2018. ISBN: 9781577358008.↩︎\nHadsell R, Rao D, Rusu AA, Pascanu R. Embracing Change: Continual Learning in Deep Neural Networks. Trends in Cognitive Sciences 24(12): 1028–1040, 2020.↩︎\nMcCulloch W, Pitts W. A Logical Calculus of Ideas Immanent in Nervous Activity. Bulletin of Mathematical Biophysics, 5(4):115–133, 1943.↩︎\nBrooks R, Hassabis D, Bray D, Shashua A. Is the Brain a Good Model for Machine Intelligence? Nature 482: 462-463, 2012.↩︎\nKoch C. What Is Consciousness? Nature, 557:S8–S12, 2018.↩︎\nDeWall C, Baumeister R, Masicampo R. Evidence that Logical Reasoning Depends on Conscious Processing. Consciousness and Cognition 17(3): 628, 2008.↩︎\nDarling K, Nandy P, and Breazeal C. Empathic Concern and the Effect of Stories in Human-Robot Interaction. 24th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN) 2015, Kobe, Japan, August 31 - September 4, pp. 770-775. IEEE, Washington (DC), USA, 2015. ISBN: 9781467367042.↩︎\nGoodall J, Abrams D. The Book of Hope: A Survival Guide for an Endangered Planet (1st Edition). Viking Press, New York (NY), USA, 2021. ISBN-10: 024147857X.↩︎\nDay L, Hanson K, Maltby J, Proctor C, Wood A. Hope Uniquely Predicts Objective Academic Achievement Above Intelligence, Personality, and Previous Academic Achievement. Journal of Research in Personality 44(4): 550-553, 2010.↩︎\nReichard RJ, Avey JB, Lopez S, Dollwet M. Having the Will and Finding the Way: A Review and Meta-Analysis of Hope at Work. The Journal of Positive Psychology 8(4): 292-304, 2013.↩︎\nBaeza-Yates R. Bias on the Web. Communications of the ACM, 61(6):54–61, 2018.↩︎\nLiu G et al. Deep learning-guided discovery of an antibiotic targeting Acinetobacter baumannii. Nature Chemical Biology 19: 1342-1350, 2023.↩︎\nSenior AW et al. Protein structure prediction using multiple deep neural networks in the 13th Critical Assessment of Protein Structure Prediction (CASP13). Proteins: Structure, Function and Bioinformatics 87(12):1141–1148, 2019.↩︎\nSenior AW et al. Improved protein structure prediction using potentials from deep learning. Nature 577:706–710, 2020.↩︎\nHiggins MK. Can We AlphaFold Our Way Out of the Next Pandemic? Journal of Molecular Biology 433(20):1–7, 2021.↩︎\nMcBride JM, Polev K, Abdirasulov A, Reinharz V, Grzybowski BA, Tlusty T. AlphaFold2 Can Predict Single-Mutation Effects. Phys. Rev. Lett. 121:218401, 2023.↩︎\nZheng NN, Liu ZY, Ren PJ, Ma YQ, Chen ST, Yu SY, Xue JR, Chen BD, Wang FY. Hybrid-Augmented Intelligence: Collaboration and Cognition. Frontiers of Information Technology & Electronic Engineering 18:153-179, 2017.↩︎\nBryant PT. Augmented Humanity: Being and Remaining Agentic in a Digitalized World. Palgrave Macmillan, Cham, Switzerland. ISBN: 9783030764449.↩︎\nLenharo M. If AI Becomes Conscious: Here’s How Researchers Will Know. Nature, 24 August 2023.↩︎\nRoberts RM. Serendipity: Accidental Discoveries in Science (1st Edition). Wiley-VCH, Weinheim, Germany, 1989. ISBN: 0471602035.↩︎\nYanai I, Lercher M. What Is The Question? Genome Biology 20(1):289, 2019.↩︎"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/14/ai-series-2.html",
    "href": "foundation-frontiers/posts/2024/05/14/ai-series-2.html",
    "title": "AI series: On AI ethics - influencing its use in the delivery of public good",
    "section": "",
    "text": "Criminal sentencing biased by race in the US, students systematically downgraded in UK public examinations with no process for appeal, and decisions to rescind food welfare in India riddled with errors and discrepancies are all instances where AI algorithms have hit the headlines. When Bill Gates wrote that the age of AI has begun and “will change the way people work, learn, travel, get health care, and communicate with each other,” those probably weren’t the changes he had in mind. Nor need they be an inevitable side effect of living with AI.\nA number of points require consideration to work safely with AI, from the potential for bias in input and training data, and consent over data use, to the transparency and fairness of applying an algorithm – who has decided the problem, or set of problems, it is to solve? The steps that are taken to explain and involve an organisation’s stakeholders in the conclusions that AI reaches also require ethical consideration, as does ethical development of AI. Its use for social policies and services highlights an additional set of problems.\nAs AI becomes more active in society, AI ethics involves not only defining the objectives for data scientists, researchers and technologists to work on. It involves governing bodies, regulators, policy makers, businesses and organisations, the media, and civil society, working to handle and communicate AI’s benefits and mitigate its harms. Organisations with international clout – such as the United Nations Educational, Scientific and Cultural Organization (UNESCO) and the Organisation for Economic Co-operation and Development (OECD) – have prominently set out ethical principles that can broadly apply. Nonetheless, a lot can go wrong."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/14/ai-series-2.html#bias-in-bias-out",
    "href": "foundation-frontiers/posts/2024/05/14/ai-series-2.html#bias-in-bias-out",
    "title": "AI series: On AI ethics - influencing its use in the delivery of public good",
    "section": "Bias in bias out",
    "text": "Bias in bias out\nIn 2016 when ProPublica launched an investigation into potential biases in a ‘risk assessment’ algorithm used by the US criminal justice system, it was the first independent investigation of its kind. This was despite the widespread use of the algorithm and its power to influence a judge’s sentence, in one instance doubling the duration while increasing the severity of the imprisonment. On examining 7000 risk assessment scores and the records detailing whether the subjects of those scores had reoffended in the subsequent two years, Propublica found “Only 20 percent of the people predicted to commit violent crimes actually went on to do so”. Even when the full range of crimes was taken into account “the algorithm was somewhat more accurate than a coin flip” at 61%. Part of the enthusiasm for these algorithms had been the expectation that they might bypass the prejudices and unconscious biases of human judges, enabling fairer justice. However, while many might baulk at the thought of tossing a coin to determine someone’s prison sentence, it turns out this might be a fairer approach than the algorithm, which was found to “falsely flag black defendants as future criminals” at twice the rate of white defendants.\n\n\n\nEleanor Roosevelt reads the Universal Declaration of Human Rights in 1949; FDR Presidential Library & Museum 64-165 CC-BY-2.0\n\n\nSince Propublica’s investigation there have been multiple reports highlighting problems with algorithms trained on historic data for use in the criminal justice system. The risk illustrated here, which can be generalised, is that such algorithms will tend to propagate social biases. In this case it means that those from ethnic minorities and lower socioeconomic backgrounds are awarded harsher sentences. Compounding the problem was the proprietary nature of the algorithms involved, which made it difficult to launch independent investigations. However, in the case of the algorithm investigated by Propublica, the input data, which is taken from questions put to the defendant and their prison records, did provide clues as to the scope for unfair outcomes. Although race is not explicitly identified, it likely correlates with other data that is used as input. This meant that the outcomes would be biased with respect to race all the same. A lot more work is needed to mitigate the effects of historical social injustices in how the criminal justice system uses data. Innovators in this area need to have confidence in what will be affected by their evidence base, as well as support from independent legal and ethical reviewers, and from regulators, to determine what will make a good innovation, and what will not."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/14/ai-series-2.html#consent-human-rights-and-data-provenance",
    "href": "foundation-frontiers/posts/2024/05/14/ai-series-2.html#consent-human-rights-and-data-provenance",
    "title": "AI series: On AI ethics - influencing its use in the delivery of public good",
    "section": "Consent, human rights, and data provenance",
    "text": "Consent, human rights, and data provenance\nThe testing and training of AI algorithms can also run into other ethical questions about the ratio of public to private benefits from data and who governs those benefits. On the eve of the UK’s AI Summit in 2023, Joe Cuddeford of Smart Data Research UK wrote: “Many AI systems rely on data collected passively from individuals, raising questions about transparency, privacy, and who benefits from these data-driven advancements.”\nLarge scale AI models, such as generative AI models, are often trained on web-scraped data from online platforms. This leads to questions about the fairness of internet data, the ownership of it (e.g. potential violation of copyright law), and methods for users’ consent and human rights to be embedded and respected. There are, once again, questions about accuracy and bias: what do algorithms “learn” from data scraped from the internet, and is the information appropriately curated for use?\nCivil liberties concerns also similarly arise when people are compelled to give up data about themselves by powerful arms of the state. For example, the national Facial Verification Testing program run by the National Institute of Standards and Technology, a part of the U.S. Department of Commerce, has held and made use of images of vulnerable individuals to test and validate the performance of commercialised AI technologies. The data used by the agency for testing include ‘mugshots’ or facial images from arrests or from other encounters with law enforcement. 1 An additional programme focuses on testing the performance of facial recognition algorithms against an image database of sexually exploited children (CHEXIA-FACE). Having statistics from this kind of agency testing has clear commercial benefit: it helped win the case for the vendors who could match those statistics when the London Metropolitan police purchased live facial recognition technology. However, the interests of the people that have been documented do not come up for discussion in this form of data governance. There are many participatory methods that could be used for more ethical stewardship of the data that people are compelled to give. 2\nTo address the scope for minorities and vulnerable groups to play their part in data collection, it should be possible for data scientists to adopt strategies that consciously address the bias in data collection. Eun Seo Jo (from Stanford University) and Timnit Gebru (formerly at Google) have suggested library and archival approaches. In Strategies for Collecting Sociocultural Data in Machine Learning, 3 they note that internet data is subject to historical and representative biases. Recognising and mitigating biases will “start with a statement of commitment to collecting the cultural remains of certain concepts, topics, or demographic groups.” A public mission statement, which highlights the interests of communities and minorities they plan to support, “forces [archival] researchers to reckon with their data composition.”\nThese strategies also need to be supported by good management of data collection and curation. A report by the Royal Academy of Engineering (2021) Towards trusted data sharing: implications for policy and practice has highlighted that, to support the use of data for research, good data management must exist among the data owners. Strong relationships with data owners predicated on data quality and ethics will help researchers to specify what data sets they are looking for and how they can best be curated for AI purposes. Good data management will not only help AI developers but also all potential users (as well as the public) to understand the scope and the quality of what’s being shared. “Defining the requirements for data quality, and ensuring these requirements are delivered, remains a central challenge.” (RAE report)\nAdvocates for accurate and fair data and machine learning have worked hard to clarify what good data management and sharing looks like, which is cause for optimism. However there is the sense that this is the area in which AI has the furthest to go, as data sets currently available fall far wide from the standards recommended by their work. Nonetheless the rise of Trusted Research Environments, Data Safe Havens and other methods of open-source transparency enable more AI innovators to disclose their sources without placing any of the personal information they use at further risk, as discussed previously in the AI series. Leadership on ethical standards for data sharing may yet help to improve the robustness, security, safety, and fairness of AI tools, which the OECD advocates as key principles for AI."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/14/ai-series-2.html#openness-explainability-and-the-scope-to-challenge-ai-decisions",
    "href": "foundation-frontiers/posts/2024/05/14/ai-series-2.html#openness-explainability-and-the-scope-to-challenge-ai-decisions",
    "title": "AI series: On AI ethics - influencing its use in the delivery of public good",
    "section": "Openness, explainability, and the scope to challenge AI decisions",
    "text": "Openness, explainability, and the scope to challenge AI decisions\nA principle that many data science communities have been working on, is towards ensuring transparency and explainability of AI (OECD AI Principle). In OECD parlance that is in part “to ensure that people understand when they are engaging with [artificial intelligence] and can challenge outcomes.” In acknowledgement that some AI applications make this disclosure harder and more unappealing, the OECD suggests that the fact that AI is in use should be disclosed “with proportion to the importance of the outcome … so that consumers, for example, can make more informed choices”. The OECD emphasises the importance of the “explainability” of the algorithms, which it defines as “enabling people affected by the outcome of an AI system to understand how it was arrived at. … notably – to the extent practicable – the factors and logic that led to an outcome.”\nThe tens of millions of digital ‘platform workers’ that now live all over the world are a case in point for where explainability is needed. They perform short-term, freelance, or temporary work through digital platforms or apps in the “gig economy”. There is little transparency about how algorithms and AI influence outcomes for gig workers, and whether platform algorithms are contributing systematically to unfair outcomes. Platform workers themselves have come together to share their data to understand more about the outcomes of the algorithms, or AI, which is shaping their lives.\nIt follows that where the use of an AI system does not affect outcomes for people, there may be less of a demand to publicly justify how AI arrived at its outcomes. For example, where AI is used to simulate something, or to research a decision, rather than to make a decision, there could be less weight placed on explaining the model publicly.\n\n\n\nAerial view of tech cluster in Silicon Valley, taken on 29 March 2013, courtesy of https://www.flickr.com/photos/patrick_nouhailler/ CC-BY-3.0\n\n\nFrançois Candelon, Theodoros Evgeniou, and David Martens, writing for the Harvard Business Review have outlined that their preference is for accuracy as well as explainability. Often, to strike this balance, they will prefer ‘white box’ models which are transparent and interpretable. But not always. “In [complex] applications such as face-detection for cameras, vision systems in autonomous vehicles, facial recognition, image-based medical diagnostic devices, illegal/toxic content detection, and most recently, generative AI tools like ChatGPT and DALL-E, a black box approach may be advantageous or even the only feasible option.”\nEven where the algorithm is too large and complicated to be interpretable, work like that conducted by the Alan Turing Institute in Project ExplAIn finds ways of extracting some kind of explanation, for instance by embedding layers in the coding. The case for opening up AI in this way has to be balanced against concerns for intellectual property, information security and privacy. There can be cybersecurity issues with making the different layers of an AI model more open to interrogation. Nonetheless, experiments with transparent and explainable models enable developers to advance their understanding of AI, as well as to consider whether its use for decision-making is ethically sound. The OECD principles make clear that it is important that AI doesn’t elude human insight, checks and balances. As Andrew Ng highlighted in the RSS fireside chat in 2021: “AI is increasing concentration of power like never before…governments and regulators need to look at that and think of what to do.”"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/14/ai-series-2.html#appropriate-human-centred-governance",
    "href": "foundation-frontiers/posts/2024/05/14/ai-series-2.html#appropriate-human-centred-governance",
    "title": "AI series: On AI ethics - influencing its use in the delivery of public good",
    "section": "Appropriate, human-centred governance",
    "text": "Appropriate, human-centred governance\nWhen school exams in England were cancelled during the Covid-19 pandemic, the government’s Department for Education decided that an algorithm should be used to allot grades to A-Level students, partly as a measure to counter grade inflation (a trend in which the grades awarded for the same standard of work will tend to rise, year on year). Algorithms had been used before in previous years to adjust the marks that were awarded for exams and coursework. Here instead of exams and coursework, the input data was gathered from Ofqual’s historical records about how particular schools’ pupils had performed in previous years, and some was generated by teachers. Efforts had been made at transparency in terms of how the new algorithm would arrive at these decisions (it was a relatively simple, white box algorithm). But there were ‘outliers’ acknowledged in the model even prior to deployment. Coupled with the widespread downgrading of teacher-estimated grades to fit a curve that would avoid grade inflation, there was not a clear process by which students and schools could appeal to change their grades. Dissatisfaction with the grades awarded in the absence of exams or coursework was rife, as young people regarded as academically talented by their schools fell short of the grades their teachers had predicted, and lost university places.\nIn the resulting furore, the Department for Education determined that its original policy was wrong and adopted the teacher estimated grades with an appeal process in place. The incident demonstrates that achieving the functional transparency of an algorithm is only one step in due process. Controversial policies could be using an algorithm to apportion losses across the population (e.g. to try to reduce grade inflation) in ways that are abhorred by individuals.\nVested interests also surfaced during investigation of an algorithm brought into use to tackle fraud in India’s welfare system. “From 2014 to 2019, the government of Telangana “cancelled more than 1.86 million existing food security cards and rejected 142,086 fresh applications without any notice.” reported Al Jazeera in January of this year. Despite the government’s initial claims that the cancelled food security cards were fraudulent, critical data scholarship in India and elsewhere has established discrepancies and errors in the algorithms used, such as, confusing the records of a valid claimant with a car-owning citizen by the same name. (Under the government’s policies, SUV owners cannot receive food aid.) Further investigations revealed that at least 7.5 per cent of the food security cards were wrongly cancelled. The investigations highlight what can be a common problem: a focus on reducing the costs of welfare programmes tends to lead services to identify false positives - wrongful claimants – rather than false negatives. Thus efforts to correct sloppy data may meet resistance if this leads to fewer “frauds” being identified, even when citizens bring evidence to challenge it.\nThere is a similar type of example in the UK’s Post Office scandal, in which many sub-postmasters were wrongfully prosecuted for false accounting, after the Post Office adopted accounting software that contained significant bugs, which were covered up for many years. This similarly goes to show how far organisations can pursue wrongful judgements, and the life-changing consequences.\nThe EU’s new AI Act advocates a risk-based approach, to balance the desire to minimise the burden of compliance while ensuring the safety of people who may be affected by the implementation of AI algorithms. Systems assessed as high risk according to specific criteria are then “subject to strict obligations before they can be put on the market”.\nGovernments across the industrialised world have raised their hopes for AI that will help to drive increases in productivity, and do so safely in ways that are fairly constructed, making use of legitimate data sources, and with fair outcomes for society. The work of data scientists is integral to the foundations by which AI can be used for social good, from establishing protocols for data management and sharing, to understanding the workings of complex algorithms, and the use of large and unstructured data sources. Data scientists and researchers are getting closer to understanding what good looks like, not just in terms of the ethical values to uphold but the technicalities of the code and data involved. However, a great deal of not only data work but also other work also needs to be maintained to uphold the ideal of ‘AI ethics’. Support for well-established ethical and legal rights and principles, to meaningfully involve people in policies that will be affected by AI use, and to develop data governance and infrastructure. It is always possible that when we’re working on AI ethics, we find that there are fairer and more ethical approaches that should precede the use of AI.\n“AI development raises a range of ethical questions for data practitioners, whether they are data scientists, econometricians, analysts, or statisticians,” Daniel Gibbons, Vice Chair of the Royal Statistical Society’s Data Ethics and Governance Section told Real World Data Science. Today, many data scientists would urge that ethical considerations precede the development of an AI algorithm and must inform its design and use, particularly for processes that significantly affect people, to ensure it does not propagate errors and injustices.\n\nExplore more data science ideas\n\n\n\n\n\nAbout the author\n\nOlivia Varley-Winter Olivia is an experienced policy manager who has worked for the Royal Statistical Society, the Open Data Institute, Open Data Charter, the Nuffield Foundation, and the Alan Turing Institute. She was part of the Ada Lovelace Institute’s founding team in 2018 to 2020 and has since supported the development of other policy-related programmes and partnerships relating to data, AI and ethics. She is presently working for Smart Data Research UK on matters pertaining to ethics and responsible data governance. She has an MSc. in Nature, Society, and Environmental Policy from University of Oxford.\n\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. \n\n\n\nHow to cite\n\nVarley-Winter, O., Author. 2024. “On AI ethics - influencing its use in the delivery of public good.” Real World Data Science, May 14, 2024. URL"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/14/ai-series-2.html#footnotes",
    "href": "foundation-frontiers/posts/2024/05/14/ai-series-2.html#footnotes",
    "title": "AI series: On AI ethics - influencing its use in the delivery of public good",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGrother, P., Ngan, M. & Hanaoka K. Face Recognition Vendor Test (FRVT) Part 3: Demographic Effects NISTIR 8280 (2019) https://nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8280.pdf↩︎\nParticipatory data stewardship (2021) Ada Lovelace Institute https://www.adalovelaceinstitute.org/wp-content/uploads/2021/11/ADA_Participatory-Data-Stewardship.pdf ↩︎\nJo, E. S. & Gebru T. Lessons from Archives: Strategies for Collecting SocioculturalData in Machine Learning Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (2020) https://dl.acm.org/doi/epdf/10.1145/3351095.3372829↩︎"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/28/ai-series-5.html",
    "href": "foundation-frontiers/posts/2024/05/28/ai-series-5.html",
    "title": "AI series: Meeting the unprecedented challenges AI poses in the labour market",
    "section": "",
    "text": "Roughly $280 billion of new funding was authorized to boost research and production of semiconductors in the US under the CHIPS and Science Act in 2022 - an amount greater than the inflation-adjusted initial spending to create the US Interstate Highway System. The legislation was just one of multiple acts engineered to subsidise and support emerging technologies in the US that are bound to have seismic impacts on the labor market. It signifies how swift changes in new and emerging technologies have the potential to profoundly change the demand for skills and the structure of work. Here AI has the potential to be more disruptive than any other technological development since the industrial revolution.\nThe US is not alone. Countries across the globe are trying to understand the potential for AI to affect their workforce and economic activity. IPSOS, Group SA, a multinational market research company with headquarters in France, recently attempted to gauge people’s feelings towards AI across the world through a survey across 31 countries and interviews with a small cohort of AI leaders (Global Views on AI 2023). However although extensive, the data retrieved shares the limitations common to all surveys. The OECD’s most recent Employment Outlook devotes six out of seven chapters to understanding the impact of AI on the workforce. But the OECD also notes that “No comprehensive method exists by which to track and compare AI R&D funding across countries and agencies.” 1 Not surprisingly, the inability to track, let alone compare AI R&D funding, means that it is difficult to make predictions about the R&D induced global labor market consequences.\nThe lack of a comprehensive method, and the resultant uncertainty about impact, is a clarion call to action. There are many challenges that need to be addressed. A partial list would include the following: a) a lack of a common definition of AI; b) a lack of information about the needed AI capabilities and how they will change; c) mapping AI capabilities to occupational skillls; and d) an inability to measure the impact of AI on job replacement or job augmentation.\nFortunately, there is hope, with new partnerships being established in the US by universities, federal, and state agencies. A new data infrastructure is being developed at the Institute for Research on Innovation and Science (IRIS) at the University of Michigan, joint with Ohio State University, in the United States, funded by the federal US National Science Foundation (NSF). The pilot joins up existing data using university and state sources to trace how scientific innovation translates to the labor market 2. The NSF, which was been charged with the regional implementation of the CHIPS and Science investments, is funding the pilot precisely because it needs “innovative tools to accurately assess the impact of these investments across the U.S. 3"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/28/ai-series-5.html#how-bad-is-the-problem",
    "href": "foundation-frontiers/posts/2024/05/28/ai-series-5.html#how-bad-is-the-problem",
    "title": "AI series: Meeting the unprecedented challenges AI poses in the labour market",
    "section": "How bad is the problem?",
    "text": "How bad is the problem?\nThe lack of data results in conflicting information. Some reports have warned of apocalyptic takeovers of the job market for many professions. Indeed, a heavily cited report by Goldman Sachs 4 predicted that AI could replace 300 million jobs. But the same BBC report that cited the Goldman Sachs prediction quoted the future-of-work director at Oxford University, Carl Benedikt Frey as saying “The only thing I am sure of is that there is no way of knowing how many jobs will be replaced by generative AI”. Simply put, as the former US Federal CIO, Suzette Kent, said “we lack useful information for informing strategic decisions for national workforce matters.”\nSo just how much of a problem is it that there is no information on how investments in science and technology affect the labor market? Why should we worry if we cannot accurately predict the impact of AI on workers, firms, and jobs? One reason is to avoid the mistakes of the past, in which both workers and firms have borne the consequences of bad information. Just in recent history, digitization and globalization resulted in a devastating loss of jobs in many countries. And geographic inequality soared as jobs in the midwestern and northeastern urban centers were lost and a service economy on the coasts burgeoned. Efforts to reduce the loss of jobs and earnings came too little, too late 5 6. Another reason is to make evidence based policy recommendations. For example, the US National AI Research Resources Taskforce, which was directly charged by the President and Congress with recommending ways to invest in AI research to strengthen and democratize the U.S. AI innovation ecosystem did not have joined up data between science investment and the workforce to inform their final recommendations. 7\nIn other words, governments need more timely, local, and actionable data so that they can understand changes in the tasks that employers need performed, which types of jobs and firms will be affected, and where. Concomitantly, data will be needed about the effects of AI on different population groups and different geographic areas so that the costs of change are not unfairly distributed. Armed with such information, policy makers can make investments that mitigate or counteract negative impacts and workers can be trained in the new necessary skills and matched with the firms that need them. But the swift pace of change in AI means that the urgency to create timely, local, and actionable labor market information to guide these investments has never been greater."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/28/ai-series-5.html#a-new-approach",
    "href": "foundation-frontiers/posts/2024/05/28/ai-series-5.html#a-new-approach",
    "title": "AI series: Meeting the unprecedented challenges AI poses in the labour market",
    "section": "A new approach",
    "text": "A new approach\nThe IRIS approach, called the “Industry of Ideas” builds on the “economics of ideas” framework for which Paul Romer received the 2018 Nobel Prize in Economics”. 8 9. People who create ideas – new technologies – that can be reused, form the foundations of new industries. In other words, “the discovery of new ideas lie at center of economic growth…” (Charles Jones describing Paul Romer’s conceptual framework) 10.\nThe project recognizes that, as Robert Oppenheimer said “the best way to transmit knowledge is to wrap it up in a human being”. 11 It uses people-centric methods for following the movement of ideas from investments in research into the marketplace. The approach identifies businesses that employ people with deep skills in AI and other emerging technology areas and developing early, never-before-available indicators that can provide alerts associated with potential impacts on current and future workforce. Initially focused on the artificial intelligence and electric vehicle industries in Ohio, the pilot is creating a data system that can be expanded and applied to other industries and other states across the country.\nThe new tools are innovative because they build on new opportunities to produce usable information that is local, about relevant industries, and that directly tie investments in new technologies, such as AI, to labor market impacts.\nAnother key aspect of the NSF piloted “Industry of Ideas” is the focus on tying innovation at its source - individual data on university research activities - to the local workforce data reported by firms to their state departments of labor. The need for local data is critical because so many labor markets are local, not national in scope. Even in a global economy, many businesses and workers are locally based – as are the training providers that work to ensure that labor demand and supply are well matched. Thus the Industry of Ideas pilot provides policy makers, workers, firms, and educational institutions with access to an array of local, timely, granular, actionable resources to help them make decisions. That way, local leaders who need labor market data don’t need to rely on national unemployment figures, which are reported once a month."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/28/ai-series-5.html#connecting-science-investments-with-jobs",
    "href": "foundation-frontiers/posts/2024/05/28/ai-series-5.html#connecting-science-investments-with-jobs",
    "title": "AI series: Meeting the unprecedented challenges AI poses in the labour market",
    "section": "Connecting science investments with jobs",
    "text": "Connecting science investments with jobs\nThe Industry of Ideas approach directly connects investment in science and the labor market, moving beyond the current approach for evaluating investment by studying scientific papers and publications 12 13 which are disconnected from workers and jobs. The data seeds were sown almost two decades ago. President Bush’s Science Advisor, John Marburger III, who, quite sensibly was unconvinced of the scientific and practical value of relying primarily on document-based, bibliometric approaches to studying science to understand its practical effects, called for a “Science of Science Policy” 14 15.\nThe Industry of Ideas is testing the potential to securely combine university and state data to measure the link between federal investments on local and regional economies for AI. It uses people-centric data generated by the administrative processes at universities and firms. With this data the Industry of Ideas project can capture the organization of people in science at multiple levels (e.g. individuals, teams, projects, and institutions), their multiple sources of funding (federal scientific and programmatic agencies, philanthropic foundations, industry, and state and local government), inputs into science from vendors (such as computing services, instruments, biological specimens), as well as the dynamics of their careers across time (individual career earnings and employment trajectories).\n\n\n\n\n\n\nFigure 1: The Industry of Ideas Infrastructure (provided by Jason Owen-Smith, IRIS, University of Michigan)\n\n\n\nThe IRIS infrastructure, developed over the past decade, provides administrative records on more than 41% of U.S. total R & D spending at universities 16. The infrastructure also provides links to survey data, as well as data from private sector suppliers 17, and can trace the flows of university funded researchers into the private sector 18 by joining up the university administrative data with state workforce data."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/28/ai-series-5.html#tying-information-about-ai-to-skills-needs",
    "href": "foundation-frontiers/posts/2024/05/28/ai-series-5.html#tying-information-about-ai-to-skills-needs",
    "title": "AI series: Meeting the unprecedented challenges AI poses in the labour market",
    "section": "Tying information about AI to skills needs",
    "text": "Tying information about AI to skills needs\nHow is it possible to tie changes in AI to changing needs for skills? State leaders in workforce and education agencies have identified new ways to collaborate, build staff capacity, and develop solutions, services, and products that respond to local need. An example of how to use data to get better information that more accurately connects workers with firms in the swiftly changing labor market is the New Jersey Career Navigator. It provides job seekers recommendations on new careers, available job postings, and relevant training programs based on skills similarity, labor market demand, and wage impacts observed in the underlying data. These recommendations, which are in themselvers generated by AI, show how AI technology can be used to navigate the changes in the labour market AI may cause. The New Jersey Career Navigator draws on millions of wage records, providing earnings and industry information on all workers covered by unemployment insurance in New Jersey firms; employment and wage outcomes from hundreds of thousands of graduates of occupational skills training programs in New Jersey; several years of online job postings from the National Labor Exchange Research Hub (NLx); and the resumes of 400,000 New Jersey residents.\nIn other words, as the Industry of Ideas pilot evolves, new ideas from states like New Jersey can be used not only to trace the flows of ideas from academia to the workplace but also to develop a new system that targets reskilling efforts once the type and location of skills needs have been identified. The new joined up data and evidence can be used to address challenges such as low labor force participation, and supplies education and training providers the data they need to align their programs with the needs of the labor market. Such a system would help government, business, educators, and workers adjust regional talent pipelines continuously in response to the changes in AI and enable workers to successfully navigate the changes that it brings."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/28/ai-series-5.html#new-approaches-to-classifying-industries-industries-of-ideas",
    "href": "foundation-frontiers/posts/2024/05/28/ai-series-5.html#new-approaches-to-classifying-industries-industries-of-ideas",
    "title": "AI series: Meeting the unprecedented challenges AI poses in the labour market",
    "section": "New approaches to classifying industries: “Industries of Ideas”",
    "text": "New approaches to classifying industries: “Industries of Ideas”\nAn important outcome of the new NSF pilot is the potential to transform the way in which we classify firms into industries. The current industry classifications are rule based. They are designed for the economy as it was organized 40 years ago, so are not designed to describe AI. A case in point is the state of Texas – a state that anecdotally has generated a lot of high tech jobs. Current industry data for Texas is limited because firms are grouped into industries that are defined by what they produce, or how they produce it, rather than describing what new technology is being developed or utilized by those firms. As a result, the main source of labor market data in Texas provides an implausibly low picture of AI activity 19.\nThe Industries of Ideas approach could provide states with a new way to classify firms, based on clever new ideas of how firms can do their business, and by grouping firms by the people who created and use the technologies they will adopt 20. Examples just for Ohio include funding to use AI to improve the ways in which medicine is delivered, and advancing digital agricuture , which includes things like precision livestock farming, or precision agriculture that reduces waste and improves productivity more generally. As they interact with farmers, the clustering of university researchers and the ideas embodied in them alongside the farms that adopt those ideas represents this new type of industry cluster . Such a classification framework is a sea change from earlier industrial classifications based on what goods are physically produced - like manufacturing and agriculture 21."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/28/ai-series-5.html#the-future",
    "href": "foundation-frontiers/posts/2024/05/28/ai-series-5.html#the-future",
    "title": "AI series: Meeting the unprecedented challenges AI poses in the labour market",
    "section": "The Future",
    "text": "The Future\nSuch a bottom-up classification and analysis system, based on local links between researchers and firms, could be designed locally but scaled nationally. It could address the challenges identified at the beginning of this piece. The definition of AI firms could evolve and be defined by the links between AI researchers and the firms with which they work. The lack of information about the needed AI capabilities would be resolved by the direct mapping of firm skill demand and their hiring patterns, as exemplified in New Jersey. The same New Jersey mapping could tie AI capabilities to occupational skills. And the direct impact of AI on job replacement or job augmentation could be mapped from the joined up university and workforce data.\nOf course, much needs to be done. The implementation will depend on the success of the pilot, and the ability to build on existing assets. Not all states and universities have the capacity to build a similar system, but the fact that 30 universities and 15 state agencies are participating in advisory boards for the NSF Industry of Ideas pilot is grounds for hope. Indeed, a new generation of data leaders is leading the way, not only at the local and regional government level but also at universities and professional associations (Advisory Committee on Data for Evidence Building) 22.\nWe began this paper by noting that the urgency to create timely, local, and actionable labor market information has never been greater. We close by arguing that our capacity to fundamentally change the way in which we can use data and information to understand the demand for skills and the structure of work has also never been greater. The opportunity is ours for the taking.\n\n\nExplore more data science ideas\n\n\n\n\n\nAbout the authors\n\nJulia Lane is a Professor at New York University’s Wagner Graduate School of Public Service. She was a senior advisor in the Office of the Federal CIO at the White House, supporting the implementation of the Federal Data Strategy. She recently served on two White House committees: the Advisory Committee on Data for Evidence Building and the National AI Research Resources Task Force.\n\n\nAdam Leonard is the Chief Analytics Officer & Director of the Division of Information Innovation & Insight (I|3) for the Texas Workforce Commission (TWC). Adam envisioned and founded I|3 to help TWC leverage its most important untapped resource - its data – to help the agency and its partners better help employers, individuals, families, and communities achieve & maintain prosperity.\n\n\nLesley Hirsch is the Assistant Commissioner of Research and Information at the New Jersey Department of Labor and Workforce Development. Her vision for the department is to bring cutting-edge digital tools to bear to deliver labor market intelligence to the department’s internal and external customers where, when, and how they need it and to mine every data source so it can tell its full story.\n\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. \n\n\n\nHow to cite\n\nLane, J., Hirsch, L. and Leonard, A. 2024. “Meeting the unprecedented challenges AI poses in the labour market.” Real World Data Science, May 28, 2024. URL"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/05/28/ai-series-5.html#footnotes",
    "href": "foundation-frontiers/posts/2024/05/28/ai-series-5.html#footnotes",
    "title": "AI series: Meeting the unprecedented challenges AI poses in the labour market",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA new approach to measuring government investment in AI-related R&D. Galindo-Rueda, F. & Cairns, S. oecd.ai (2021)↩︎\nThe Industry of Ideas: Measuring How Artificial Intelligence Changes Labor Markets Lane, J. AEI (2023)↩︎\nNSF launches pilot to assess the impact of strategic investments on regional jobs *new.nsf.gov (2023)↩︎\nAI could replace equivalent of 300 million jobs - report Vallance, C. BBC news (2023)↩︎\nThe Growth of Low-Skill Service Jobs and the Polarization of the US Labor Market Autor, D. H. & Dorn, D. American Economic Review 103 pp. 1553-97 (2013)↩︎\nExplaining Job Polarization: Routine-Biased Technological Change and Offshoring Goos, M., Manning, A. & Salomons, A. American Economic Review 104 2509-26 (2014)↩︎\nStrengthening and Democratizing the U.S. Artificial Intelligence Innovation Ecosystem Office of Science and Technology Policy (2023)↩︎\nThe Deep Structure of Economic Growth Romer, P. paulromer.net (2019)↩︎\nInterview With Paul Romer Romer, P. & Lane, J. (2022)↩︎\nPaul Romer: Ideas, nonrivalry, and endogenous growth(Jones, C. I. The Scandinavian Journal of Economics 121 859-883 (2019)↩︎\nWrapping it up in a person: Examining employment and earnings outcomes for Ph.D. recipients Zolas, N. et al. Science **350 1367-1371 (2015)↩︎\nLet’s make science metrics more scientific Lane, J. Nature 464 488–489 (2010)↩︎\nA Vision for Democratizing Government Data Lane, J. Issues in Science and Technology XXXIX (2022)↩︎\nLet’s make science metrics more scientific Lane, J. Nature 464 488–489 (2010)↩︎\nWanted: Better Benchmarks Marburger III, J. H. Science 308 p1087(2005)↩︎\nThe Institute for Research on Innovation & Science (IRIS). Summary Documentation for the IRIS UMETRICS 2022 Data Release Nicholls, N., Brown, C. A., Ku, R. L. and Owen-Smith, J. D. Ann Arbor, MI: The Institute for Research on Innovation & Science (2022) doi: 10.21987/df2a-ha30↩︎\nA Linked Data Mosaic for Policy-Relevant Research on Science and Innovation: Value, Transparency, Rigor, and Community Chang, W.-Y., Garner, M., Basner, J., Weinberg, B. and Owen-Smith, J. Harvard Data Science Review (2022) doi: 10.1162/99608f92.1e23fb3f↩︎\nThe Industry of Ideas: Measuring How Artificial Intelligence Changes Labor Markets Lane,J. American Enterprise institute (2023)↩︎\n[Outside of the Box Use of Administra4ve and Wage Data in Texas] (https://digitaleconomy.stanford.edu/wp-content/uploads/2024/03/Adam-Leonard.pdf) Leonard, A. digitaleconomy.standford.edu (2024)↩︎\nThe Industry of Ideas: Measuring How Artificial Intelligence Changes Labor Markets Lane,J. American Enterprise institute (2023)↩︎\nConverting historical industry time series data from SIC to NAICS. The Federal Committee on Statistical Methodology Yuskavage, R. Federal Committee on Statistical Methodology (2007)) – or by how services and goods are produced – like the delivery of health, financial, and investment services The Statistics Corner: The NAICS Is Coming. Will We Be Ready? Haver, M. A. Business Economics 32 63-65 (1997)↩︎\nYear 2 Report Supplemental Information Advisory Committee on Data for Evidence Building (ACDEB) (2022)↩︎"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/02/19/what-is-data-science.html",
    "href": "foundation-frontiers/posts/2024/02/19/what-is-data-science.html",
    "title": "What is data science? A closer look at science’s latest priority dispute",
    "section": "",
    "text": "What is data science, and where did it come from? Is data science a new and exciting set of skills, necessary for analyzing 21st century data? Or is it (as some have claimed) a rebranding of statistics, which has carefully developed time-honored methods for data analysis over the past century?\nPriority disputes – disagreements over who deserves credit for a new scientific theory or method – date back to the beginning of science. Famous examples include the invention of calculus and ordinary least squares. But this latest dispute calls into question the novelty of an entire discipline.\nIn this article, we use two popular data science algorithms to examine the difference between data science, statistics, and other occupations. We find that in terms of the preparation required to become a data scientist, data science reflects both the work of natural sciences managers – individuals who oversee research operations in the natural sciences – and statisticians and mathematicians. This suggests that data science is a shared enterprise among science and math, and thus those trained in the natural sciences have as much claim to data science as those trained in mathematics and statistics.\nIn terms of the role a data scientist serves relative to other occupations, however, we find that data science is closest to statistics by far. Both occupations are fast growing and central among the occupations that work with data, suggesting a data scientist serves the same function as a statistician. But this function may be changing. While the centrality of statistics has declined over the past decade relative to other occupations, the centrality of data science has grown. In fact, data science has now surpassed statistics as the most central fast-growing occupation."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/02/19/what-is-data-science.html#we-examine-the-role-of-data-science-using-data-science",
    "href": "foundation-frontiers/posts/2024/02/19/what-is-data-science.html#we-examine-the-role-of-data-science-using-data-science",
    "title": "What is data science? A closer look at science’s latest priority dispute",
    "section": "We examine the role of data science using data science",
    "text": "We examine the role of data science using data science\nEveryone seems to agree that data science requires skills traditionally associated with a variety of different occupations. Drew Conway, for example, describes data science as a combination of math and statistics, substantive (domain) expertise, and “hacking” skills (see Figure 1). In dispute is the relative importance of those skills. Some have argued that data science is basically statistics – and that 20th century statisticians like John Tukey have long possessed the data science skills traditionally associated with computer science and the natural sciences. Others have argued that data science is truly interdisciplinary, and statistical thinking only plays a small role. But while opinions on data science abound, few appear to be based on data or science.1\n\n\n\n\n\n\nFigure 1: Drew Conway describes data science as a combination of math and statistics, substantive (domain) expertise, and “hacking” skills. Conway’s data science venn diagram, reproduced here, is Creative Commons licensed as Attribution-NonCommercial.\n\n\n\nTo that end, we use two popular data science algorithms, naïve Bayes and eigen centrality (eigen decomposition), to investigate the question: What is data science? Both algorithms use data listing the training a worker must generally complete to work in an occupation, such as data science. Specifically, we use the CIP SOC Crosswalk provided by the US Bureau of Labor Statistics and US National Center for Education Statistics, which links the Classification of Instructional Programs – the standard classification of educational fields of study into roughly 2,000 instructional programs – with the Standard Occupational Classification – the standard classification of professions into roughly 700 occupations.\nOur main assumption is that the skills required to work in an occupation can be represented by the instructional programs that prepare students to work in that occupation. For example, the occupation “data scientists” is associated with 35 instructional programs, such as data science, statistics, artificial intelligence, computational science, mathematical biology, and econometrics. The occupation “statisticians” is associated with 26 instructional programs, including data science, statistics, and econometrics, but not artificial intelligence, computational science, or mathematical biology.\nThe algorithms we employ consider occupations to be similar if they have many instructional programs in common. Data scientists and statisticians share 14 degrees, suggesting they are similar: Half the programs that prepare students to be a statistician also prepare students to be a data scientist. In contrast, data scientists and computer programmers share six degrees in common, suggesting they are less similar; computer programmers have 17 degrees overall so only a third of the programs that prepare students to be a computer programmer also prepare students to be a data scientist.2\n\n\n\n\n\n\nData and code to reproduce the analysis and figures are available through GitHub."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/02/19/what-is-data-science.html#data-science-is-a-shared-enterprise-among-science-and-math",
    "href": "foundation-frontiers/posts/2024/02/19/what-is-data-science.html#data-science-is-a-shared-enterprise-among-science-and-math",
    "title": "What is data science? A closer look at science’s latest priority dispute",
    "section": "Data science is a shared enterprise among science and math",
    "text": "Data science is a shared enterprise among science and math\nWe use naïve Bayes to measure the similarity between each occupation and data science in terms of the preparation required to work in that occupation. Specifically, we first pretend that the occupation “data scientist” did not exist and then use Bayes’ rule to calculate the probability that a hypothetical group of workers with the 35 degrees associated with data science could have come from one of the roughly 700 other occupations. The higher the measure, the more consistent that occupation is with data science.\nThe use of Bayes’ rule is appealing because the similarity between a given occupation and data science takes into account the similarities between every other occupation and data science. Our use of Bayes’ rule is naïve in the sense that – before collecting the data – we assume these workers are equally likely to have come from any occupation.\nThe occupations with the largest probabilities, and thus most related to data science, are summarized in Figure 2. We find that the hypothetical workers have a 50% chance of being natural sciences managers and a 50% chance of being statisticians or mathematicians.3 We conclude that data science is a shared enterprise among science and math, and thus those trained in natural sciences have as much claim to data science as those trained in mathematics and statistics.\n\n\n\n\n\n\nFigure 2: We use naïve Bayes to measure the similarity between each occupation and data science in terms of the preparation required to work in that occupation. We find that in terms of the preparation required to become a data scientist, data science is a shared enterprise among science and math."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/02/19/what-is-data-science.html#data-science-is-closest-to-statistics-in-its-role-among-other-occupations",
    "href": "foundation-frontiers/posts/2024/02/19/what-is-data-science.html#data-science-is-closest-to-statistics-in-its-role-among-other-occupations",
    "title": "What is data science? A closer look at science’s latest priority dispute",
    "section": "Data science is closest to statistics in its role among other occupations",
    "text": "Data science is closest to statistics in its role among other occupations\nWe use eigen centrality (eigen decomposition) to measure the similarity of each occupation in terms of its role relative to other occupations. Specifically, we calculate the principal right singular vector of the adjacency matrix denoting whether an instructional program (row) is associated with an occupation (column).4 An occupation has high eigen centrality when the instructional programs that prepare a worker for that occupation also prepare that worker for many other occupations as well. This suggests that the higher the measure, the more central the role of the occupation relative to other occupations.\nThe eigen centrality of each occupation is displayed in Figure 3. Each point represents an occupation, the x-axis denotes the centrality of the occupation, and the y-axis denotes the percent growth of the occupation as predicted by the US Bureau of Labor Statistics over the next decade. The figure demonstrates that data scientists and statisticians occupy nearly identical positions: Both are fast growing and central to the other occupations that work with data. In contrast, natural sciences managers are central but growing much more slowly, suggesting a role closer to managers. We conclude that – though data scientists are prepared similarly to natural sciences managers – a data scientist serves the same function as a statistician.\n\n\n\n\n\n\nFigure 3: We use eigen centrality (eigen decomposition) to measure the similarity of each occupation in terms of its role relative to other occupations. We find that in terms of the role a data scientist serves relative to other occupations, a data scientist functions like a statistician.\n\n\n\nBut this function may be changing. Figure 4 shows the centrality (x-axis) of each occupation (y-axis) in 2010 and 2020. Green bars denote increases from 2010 to 2020 while yellow bars denote decreases. We find that the centrality of statisticians has declined over the past decade relative to other occupations, while the centrality of data scientists has grown. In fact, data science has now surpassed statistics as the most central fast-growing occupation. We conclude that though a data scientist and a statistician serve similar roles today, those roles may change as the workforce changes. Note that the occupation classifications changed in 2018, and we used the crosswalk provided by the US Bureau of Labor Statistics to make these comparisons.\n\n\n\n\n\n\nFigure 4: We use eigen centrality (eigen decomposition) to measure the similarity of each occupation in terms of its role relative to other occupations. We find that the centrality of statisticians has declined over the past decade relative to other occupations, while the centrality of data scientists has grown. Data science has now surpassed statistics as the most central fast-growing occupation. (Occupations predicted to grow more than 20% over the next decade shown.)\n\n\n\nThe findings in this section are based on the adjacency matrix that encodes whether an instructional program (row) is associated with an occupation (column). A more detailed summary of the matrix is provided in Figure 5, which depicts the matrix as a network graph. Larger nodes represent occupations that are growing faster, while nodes closer to the center of the network represent more central occupations. The figure is interactive. You can zoom in to see the similar positions between data scientists and statisticians, which are both large (fast growing) and central.\n\n\n\n\n  \n\n\n\nFigure 5: A visualization of occupations as a network: Occupations are placed according to the instructional programs that train students for that occupation, with occupations closer together sharing more instructional programs in common. We find data scientists and statisticians occupy nearly identical positions at the center of the network. Occupations are colored according to the primary classification of instructional programs that train students for that occupation. Larger nodes represent occupations that are growing faster."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/02/19/what-is-data-science.html#is-data-science-statistics",
    "href": "foundation-frontiers/posts/2024/02/19/what-is-data-science.html#is-data-science-statistics",
    "title": "What is data science? A closer look at science’s latest priority dispute",
    "section": "Is data science statistics?",
    "text": "Is data science statistics?\nWe conclude that individuals trained in managing natural sciences research – a slow growing occupation – are turning to data science – a much faster growing occupation, and one which currently serves a role like that of a statistician. But if present trends continue, data science is poised to eclipse the historic role of the statistician as central to the occupations that work with data.\nThis suggests that while data science may be new and exciting, the role served by the data scientist is not particularly new. This does not mean that data scientists necessarily use the same time-honored methods for data analysis as statisticians. It is the authors’ experience, however, that many data science tools are in fact statistical. Indeed, the two data science algorithms we used in this article are both taught to students as new and exciting, but in reality are centuries-old methods steeped in statistical history.\nRegardless of whether data science is or is not statistics, the occupation “data scientist” has proven immensely popular, capturing a zeitgeist that has eluded statistics. This is best evidenced by the fact that data science – and not statistics – has been crowned the sexiest job of the 21st century. But if statistics has not enjoyed the popularity of data science, perhaps the real question in need of answering is: What is statistics?\n\nExplore more data science ideas\n\n\n\n\n\nAbout the author\n\nJonathan Auerbach is an assistant professor in the Department of Statistics at George Mason University. His research covers a wide range of topics at the intersection of statistics and public policy. His interests include the analysis of longitudinal data, particularly for data science and causal inference, as well as urban analytics, open data, and the collection, evaluation, and communication of official statistics.\n\n\nDavid Kepplinger is an assistant professor in the Department of Statistics at George Mason University. His research revolves around methods for robust and reliable estimation and inference in the presence of aberrant contamination in high-dimensional, complex data. He has active collaborations with researchers from the medical, biological, and life sciences.\n\n\nNicholas Rios is an assistant professor of statistics at George Mason University. He earned his PhD in statistics 2022 from Penn State University, where his dissertation focused on designing optimal mixture experiments. His primary research interests are experimental design and methods for intelligent data collection in the presence of real-world constraints. He is also interested in functional data analysis, computational statistics, compositional data analysis, and the analysis of high-dimensional data.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Jonathan Auerbach, David Kepplinger, and Nicholas Rios\n\n\n  Text, code, and figures are licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) International licence, except where otherwise noted. Thumbnail photo by Marc Sendra Martorell on Unsplash.\n\n\n\nHow to cite\n\nAuerbach, Jonathan, David Kepplinger, and Nicholas Rios. 2023. “What is data science? A closer look at science’s latest priority dispute.” Real World Data Science, February 19, 2024."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/02/19/what-is-data-science.html#footnotes",
    "href": "foundation-frontiers/posts/2024/02/19/what-is-data-science.html#footnotes",
    "title": "What is data science? A closer look at science’s latest priority dispute",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDescriptions of occupations by government agencies are not particularly helpful in differentiating between data science, statistics, and related occupations. For example, according to the Bureau of Labor Statistics, data scientists use “analytical tools and techniques to extract meaningful insights from data.” This description is similar to mathematicians/statisticians, who “analyze data and apply computational techniques to solve problems,” and operations research analysts who use “mathematics and logic to help solve complex issues.”↩︎\nOur analysis treats all instructional programs as equal and independent. We do not consider, for example, the number of workers who hold a degree from an instructional program or whether two instructional programs are similar or offered by similar academic departments. Our analysis could be adjusted to account for this or related information, although it is unclear to the authors whether such an adjustment would make the results more accurate.↩︎\nNote that natural sciences managers share 18 instructional programs with data scientists, while statisticians share 14.↩︎\nOr alternatively, the principal eigenvector of the adjacency matrix denoting the number of instructional programs each occupation (row) has in common with each other occupation (column).↩︎"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/06/11/ai-series-7.html",
    "href": "foundation-frontiers/posts/2024/06/11/ai-series-7.html",
    "title": "AI series: Ensuring new AI technologies help everyone thrive",
    "section": "",
    "text": "“There’s some beautiful stories in clinical notes,” said Mark Sales, global strategy leader of the cloud technology company Oracle Life Sciences. He was speaking to delegates at the 2024 London Biotechnology Show about “unlocking health data and artificial intelligence within life sciences”, where opportunities abound, such as exploiting large language models (LLMs) to process some of the detailed information currently hidden in clinical notes into more structured data to inform fields like oncology. Oracle are also looking into using AI to take some of the luck out of connecting the right patients with clinical trials that might help them. The AI in Medicine and Surgery group at the University of Leeds headed by Sharib Ali has demonstrated the potential to reduce the number of times patients need to go through uncomfortable procedures like oesophageal scansfor Barrett’s syndrome , and is working on the potential to provide haptic feedback for robot mediated surgery. The London Biotechnology Showcase delegates had already heard about all these opportunities. Nonetheless Sales’s talk had opened with a note of caution: “There’s a lot more we could do, and there’s a lot more we probably shouldn’t do.”\nIt is an increasingly familiar caveat. “In the best scenario, AI could widely enrich humanity, equitably equipping people with the time, resources, and tools to pursue the goals that matter most to them,” suggest the Partnership on AI, a non-profit partnership of academic, civil society, industry, and media organizations. The goal of the partnership is to ensure AI brings a net positive contribution to society as a whole not just a lucky minority, which they suggest will not necessarily be the case if we rely on chance and market forces to direct progress. While people working in developing and deploying AI tackle the burgeoning size and complexity of their models, as well as the myriad requirements of testing and training data, establishing whether a model is fit for purpose, and dodging the numerous pitfalls that cause most AI projects to fail, perhaps the greatest challenge remains the range of ethical considerations including inclusiveness and fairness, robustness and reliability, transparency and accountability, privacy and security and general forethought and design. The scope of societal impact can reach far further than the immediate sphere of interaction with the model, or the interests of the companies deploying them, suggesting the need for some sort of governing forces.\nHowever, technology is moving fast in a lot of different directions. Even with agreed sound values that all technological developments should respect, there is still space for companies to deploy AI models without supplying the necessary resources and expertise so that the roll out meets ethical and societal expectations. This expertise can range from the statistical skills required to ensure the appropriate level of representation in training datasets to the social science understanding to extrapolate potential implications for human behaviour when interacting with the technology.\nAlthough the right checks and balances to avoid potential negative societal impacts have been slower to develop than the technologies they should be regulating, some guiding principles are emerging from organisations labouring to assess with greater clarity what the real immediate and longer term hazards are, what has worked well in other sectors, and the impact of government actions so far. There is an element of urgency in the challenge. As the Partnership on AI put it, “Our current moment serves as a profound opportunity — one that we will miss if we don’t act now.”"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/06/11/ai-series-7.html#high-stakes",
    "href": "foundation-frontiers/posts/2024/06/11/ai-series-7.html#high-stakes",
    "title": "AI series: Ensuring new AI technologies help everyone thrive",
    "section": "High stakes",
    "text": "High stakes\nWhen Open AI publicised their Voice Engine’s ability to clone human voices from just 15s of audio, they too flagged the potential benefit for people with poor health conditions, since those with deteriorating speech could find a means to have their speech restored. However, voice clones had already been used to make robot calls to voters imitating the voice of President Joe Biden and telling voters to stay at home.\n“The question you have to ask there is what’s the societal benefit of that tool? And what are the risks,” associate director at the Ada Lovelace Institute Andrew Strait told Real World Data Science. “They thankfully decided to not fully release it,” he adds, highlighting how the timing “right before an election year with 40 democracies across the world” could have made the release particularly problematic.\n\n\n\n\n\n\nFigure 1: Themis, goddess of justice. External governance is required to ensure the outcomes of AI deployment are safe and just. Credit Shutterstock, Michal Bednarek.\n\n\n\nWhile OpenAI’s voice engine might have made voice cloning more accessible had they proceeded with a full release, voice cloning is clearly still well within reach for some already. Strait cites the experiences of hundreds of performing artists in the UK over the past few months that have been brought to the attention of the Ada Lovelace Institute. “They’re brought into a room; they’re asked to record their voice and have their face and likeness scanned; and that’s the end of their career,” says Strait. The sums paid to artists on these transactions are not large either. “They are never going to be asked to come back for audition again, because they [the companies] can generate their likeness, that voice doing anything that a producer wants without any sense of attribution, further payments, or consent to be used in that way.”\nCustomer service is another sector where jobs have been threatened with replacement by a generative AI chatbot, however the technology can run into problems since gen-AI is known to “hallucinate”, generating false information. Air Canada has just lost a case defending its use of a chatbot that misinformed a customer that they could apply for a bereavement fare retroactively, which is not the case according to Air Canada’s bereavement fare policy. In their defence Air Canada flagged that the chatbot had supplied a link to a webpage with the correct information but the court ruled that there was no reason to believe the webpage information over the chatbot, or for the customer to double check the information they had been supplied. While there are ways to mitigate problems with gen-AI with the right teams in place , other industries have also hit problems with the accuracy and reliability of gen-AI, which may dampen the impact AI has on the labour market. All in all the wider picture of how AI deployment may affect jobs is largely a matter of speculation. Here a US piloted scheme may soon provide framework for a more data informed approach to tackling AI’s impact on the workforce.\nStrait highlights that conversations that centre around efficiency when weighing up the possible advantages of introducing AI can be ill informed. “If we’re talking about an allocation of resources in which we’re spending an increasing amount of money on automating certain parts of the NHS, or healthcare or the education system, or public sector services, how are we making the decisions that are determining if that is worth the value for money? Instead of investing in more doctors, more teachers, more social workers?” He tells Real World Data Science that these are the questions he and his colleagues at the Ada Lovelace Institute are often pushing governments to try to answer and evidence rather than to just assume the benefits will accrue. When it comes to measures of success of an AI model, Strait says “It’s often defined in terms of how many staff can be cut and still deliver some kind of service…This is not a good metric of success,” he adds. “We don’t want to just get rid of as many jobs as we can, right, we want to actually see improvements in care, improvements in service.”\nMichael Katell, ethics fellow in the Turing’s Public Policy Programme and a visiting Senior lecturer at the Digital Environment Research Institute (DERI) at Queen Mary University of London suggests the problems may go deeper still when looking at the use of generative AI in creative industries. “There are definitely parallels with prior waves of disruption,” he says citing as an example the move to drum-based and eventually laser printing as opposed to manual typesetting. “A key difference, though, is that, in the creative arts, we’re talking about contributions to culture, and culture is something that, I think we often take for granted.” He highlights the often overlooked role cultural practices that enable and empower shared experiences have in holding society together. These may come in various forms from works of art to theatre, and the working and living practices among the wider community may play an important role too. While acknowledging there may be interesting and fascinating uses of AI in art to explore, Katell adds, “If we’re not attending to maintaining some aspects, or trying to manage the changes that are happening in our culture, I think we’ll see societal level effects that are much greater than the elimination of some jobs.”"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/06/11/ai-series-7.html#the-need-for-legislation",
    "href": "foundation-frontiers/posts/2024/06/11/ai-series-7.html#the-need-for-legislation",
    "title": "AI series: Ensuring new AI technologies help everyone thrive",
    "section": "The need for legislation",
    "text": "The need for legislation\nThese stakes all highlight the need for regulatory interventions. However, most governments, bar China and the EU, have so far favoured “voluntary commitments” towards AI safety, which would seem to fall short of providing the kind of governance over the sector that can be robustly enforced. In a recent blog Strait alongside the Ada Lovelace Institute’s UK public policy lead Matt Davies and associate director (Law & Policy) Michael Birtwhistle, “evaluate the evaluations” of the UK’s AI Safety Institute for companies that have opted in for these voluntary commitments. They highlight that on the whole the companies planning to release the product hold too much control over how the evaluation can take place, ultimately empowering them to direct tests in their favour, which inhibits efforts at robust monitoring. Furthermore, there is usually no avenue for the necessary scrutiny of training data sets. Even withstanding these limitations, Davies, Strait and Birtwhistle conclude that “conducting evaluations and assessments is meaningless without the necessary enforcement powers to block the release of dangerous or high-risk models, or to remove unsafe products from the market.”\nThe reticence to implement firmer regulation might be attributed in some part to the perceived benefits to the state when their AI companies succeed. One often perceived benefit is that the percolating profits these companies accrue may benefit the economic buoyancy of the societies they function within. There is also cause for sovereign state competitiveness in “AI prowess” that stems from the potential for AI-based technology to underpin all aspects of society, prompting what has been described as an “AI arms race”. Here the UK may well regret allowing Google to acquire Deep Mind, whose output is responsible for bolstering the “UK’s share” of citations in the top 100 recent AI papers from 1.9% to 7.2%. However, a lack of robust regulation may prove as much a disservice to the companies releasing AI products as it is to society as a whole.\n“The medicine sector here [in the UK] is thriving, not in spite of regulation, but because of regulation,” says Strait. “People trust that the products you develop here are safe.” Katell, highlights the impact of pollution legislation on the automotive industry. “It jumped forward invention and discovery in automotive technology,” he tells Real World Data Science. “It seems prosaic in hindsight, but it wasn’t, it was a major innovation that was promoted by regulators, promoted by legislators.” The UK government’s chief scientific advisor Angela McLean seems to agree. “Good regulation is good for innovation,” she replied when asked about balancing regulation with favourable conditions for a flourishing AI sector at an Association of British Science Writers’ event in May. “We’re not there yet,” she added. The challenge is pinning down what good regulation looks like."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/06/11/ai-series-7.html#regulatory-ecosystems",
    "href": "foundation-frontiers/posts/2024/06/11/ai-series-7.html#regulatory-ecosystems",
    "title": "AI series: Ensuring new AI technologies help everyone thrive",
    "section": "Regulatory ecosystems",
    "text": "Regulatory ecosystems\nAs has been emphasised throughout the series, making a success of an AI project requires a unique skillset that combines expertise in AI with the domain expertise for the sector the project is contributing to, and there is often a dearth of people that straddle both camps. The same hunt for “unicorns” with useful expertise in the tech sector and policymakers can also be an obstacle for developing “good regulation”. One solution is to bring people from the different disciplines together to develop legislation collaboratively, as was arguably the case with the roll out of General Data Protection Regulations (GDPR) in 2018. “Policymakers and academics, they worked very closely together in the crafting of that law,” says Katell. “It was one of those rare moments in which we saw the boundaries really dissolve between policy and academia in a way that delivered something that I think we can agree was largely a positive outcome.”\nWhen it comes to AI, an obstacle to that kind of collaboration has been the lack of a common language. In “Defining AI in policy and practice” in 20201, Katell alongside Peaks Krafft at the University of Oxford and co-authors found that AI researchers favoured definitions of AI that “emphasise technical functionality”, whereas policy-makers tended towards definitions that “compare systems to human thinking and behavior”, which AI systems remain far from achieving. Strait also highlights a recurring theme among those without experience of actually making AI systems in overselling AI capabilities in suggestions that it will “help solve climate change” or “cure cancer”. “How are you measuring that?” he asks. “How are we making a clear sense of the efficacy, the proof behind those kinds of statements? Where are the case studies that actually work, and how are we determining that’s working?”\n\n\n\n\n\n\nFigure 2: Safety first. External governance is required to ensure the outcomes of AI deployment are safe and knock on effects have been considered. Credit: Shutterstock. Photo by 3rdtimeluckystudio.\n\n\n\nAs Krafft et al. point out in their 2020 paper, such exaggerated perceptions of AI capabilities can also hamper regulation. “As a result of this gap,” they write, “ethical and regulatory efforts may overemphasise concern about future technologies at the expense of pressing issues with existing deployed technologies.” Here a better understanding of what AI is can be helpful to focus attention on the problems that exist now – not just the potential workforce impact, but the carbon cost of training large language models, activities like nonconsensual gen-AI porn aggravating online gender inequality, and a widening digital divide disadvantaging pupils, workers and citizens who cannot afford all the latest AI tools, among others.\nFortunately, there has already been progress to breach the language divide between policy makers and the tech sector. “The current definitions [championed in policy circles] say things like technologies that can perform tasks that require intelligence when humans do them,” says Katell, which he describes as a far more sober and realistic definition than likening technologies to the way humans think and work. “This is really important,” he adds. “Because some of the problems that we see with AI now are symptomatic of the fact that they’re not humans and that they don’t have the same experience of the world.” As an example he describes someone driving a car with child in the car seat, calling on all their training and experience of road use to navigate roads and other traffic, while juggling their attention between driving and the child. “Things that AI is too brittle, to accomplish,” he adds, highlighting how a simple model may identify school buses in images quite impressively until it is presented with an image of a bus upside down. “The flexibility and adaptability, the softness of human reason, is actually its strength, its power.”\nGetting everybody on the same page can also help provide a more multimodal approach to governance. Empowering independent assessors of AI product safety prior to release is one thing but as Strait points out, “It could be more like the environmental sector, where we have a whole ecosystem of environmental impact assessments, organisations and consultancies that do this kind of work for different organisations and companies.” Internal teams within companies can play an important role too so long as they work sufficiently independently from the companies themselves. When set up with the right balance of expertise they can be better placed to understand and hence assess the technology and practical elements of its implementation. Although such teams can be expensive, getting the technical evaluation and consideration of ethical issues right can pose a competitive advantage for the companies themselves as well as providing a more thorough safeguard for society at large. Nonetheless there are also obvious advantages in having external regulatory bodies, which do not need to take into account the company’s profit margins or shareholders’ needs. An ideal set up might incorporate both approaches. In fact in their appraisal of the current UK AI Safety Institute arrangement, Davies, Strait and Birtwistle first highlight the need to integrate the AI Safety Institute “into a regulatory structure with complementary parts that can provide appropriate, context-specific assurance that AI systems are safe and effective for their intended use.”"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/06/11/ai-series-7.html#prosperity-for-all",
    "href": "foundation-frontiers/posts/2024/06/11/ai-series-7.html#prosperity-for-all",
    "title": "AI series: Ensuring new AI technologies help everyone thrive",
    "section": "Prosperity for all",
    "text": "Prosperity for all\nWith all the precedents in other sectors from environmental impact checks to pharmacology, an organised framework or ecosystem for robust, independent and meaningful evaluation of AI product safety seems an inevitable imperative, albeit potentially expensive. (Davies, Strait and Birtwistle cite £100 million a year as a typical cost for safety driven regulatory systems in the UK2, and the expertise demands of AI could further increase costs.) However, such regulatory reform will likely slow down the pace of technological development and the route to market. While the breathing space to adjust to the societal changes they bring with them may be welcomed by some, the delay can be quite unpopular in a tech sector where the ethos is famed for embracing a “move fast, break things” mentality. As Katell points out that ideal is based on the notion that the things being broken were unimportant – when it’s vulnerable people and societies that is “unacceptable breakage”.\nStrait also highlights the cultural mismatch between the companies developing AI products – where the research to market pipeline is extremely fast – and the sectors those tools are intended to serve, such as social care, education and health. Although Open AI eventually decided against full release of the Voice Engine, when it comes to the ethos of some AI technology companies , “The default is to put things out there and to not think through the ethical and societal implications,” says Strait who has direct experience of working for a company producing AI tools in the past. “I think it’s so critical for data scientists and ethicists to explore, and do that translation and interrogation of what are the ethics of the sector that we’re working in?”\nKatell voices concern shared by many that at present AI is under the control of a very small handful of very large, powerful technology companies, and as a result the AI releases making the most impact are targeting the agendas of the companies releasing them and their current and anticipated customer base, as opposed to the needs of society. The potential for such large tech agents to become too big to fail poses additional regulatory challenges. While many may lament the tension between a demand for open source data sets for testing AI models versus the need to respect data privacy, security and confidentiality, there have already been widely mooted instances where certain companies may not have met expectations for respecting copyright and terms of service. In fact the tech giants are not the only people developing AI models and the open source community have been known to pose valuable competition that may temper the tendency for AI to concentrate a lot of power into the hands of a small few3. However, open source developers can also pose a certain amount of regulatory complexity.\nThere is also an argument that these efforts should broaden their scope beyond baseline AI safety and aim to focus efforts in AI development towards tools that actively promote greater wellbeing and prosperity to the many. “We need to bring in other values like fairness, justice, and simple things like explainability, gender equity, racial equity,” says Katell, highlighting some of the other qualities that demand attention among others. Taking explainability as an example, there is increasing awareness of the need to understand how certain outputs are reached in order for people to feel comfortable with the technology, and the outputs requiring explanations differ from person to person. Although it can be hard to explain AI outputs, progress is being made in this direction. As Katell says, “We’re not helpless in managing these types of disruptions. It’s a matter of societies coming together and deciding that they can be managed.”\n\nExplore more data science ideas\n\n\n\n\n\nAbout the authors\n\nAnna Demming is a freelance science writer and editor based in Bristol, UK. She has a PhD from King’s College London in physics, specifically nanophotonics and how light interacts with the very small, and has been an editor for Nature Publishing Group (now Springer Nature), IOP Publishing and New Scientist. Other publications she contributes to include The Observer, New Scientist, Scientific American, Physics World and Chemistry World.\n\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. \n\n\n\nHow to cite\n\nDemming, Anna. 2024. “Ensuring new AI technologies help everyone thrive .” Real World Data Science, June 11, 2024. URL"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/06/11/ai-series-7.html#footnotes",
    "href": "foundation-frontiers/posts/2024/06/11/ai-series-7.html#footnotes",
    "title": "AI series: Ensuring new AI technologies help everyone thrive",
    "section": "References",
    "text": "References\n\n\nKrafft, P. M., Young, M., Katell, M., Huang, K. & Bugingo, G. Defining AI in Policy versus Practice AIES ’20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society 72-78 (2020)↩︎\nSmakman, J, Davies, M. & Birtwhistle, M. Mission critical Ada Lovelace Policy Briefing (2023)↩︎\nGoogle “We Have No Moat, And Neither Does OpenAI semianalysis.com (2023) (semianalysis.com)↩︎"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/10/31/machine-learning-nobel-prizes.html",
    "href": "foundation-frontiers/posts/2024/10/31/machine-learning-nobel-prizes.html",
    "title": "The machine learning victories at the 2024 Nobel Prize Awards and how to explain them",
    "section": "",
    "text": "Few saw it coming when on 8th October 2024 the Nobel Committee awarded the 2024 Nobel Prize for Physics to John Hopfield for his Hopfield networks and Geoffrey Hinton for his Boltzmann machines as seminal developments towards machine learning that have statistical physics at the heart of them. The next day machine learning albeit using a different architecture bagged half of the Nobel Prize for Chemistry as well, with the award going to Demis Hassabis and John Jumper for the development of an algorithm that predicts protein folding conformations. The other half of the Chemistry Nobel was awarded to David Baker for successfully building new proteins.\nWhile the AI takeover at this year’s Nobel announcements for Physics and Chemistry came as surprise to most, there has been some keen interest on how these apparently different approaches to machine learning might actually reduce to the same thing, revealing new ways of extracting some fundamental explainability from the generative AI algorithms that have so far been considered effectively “black boxes”. The “transformer architectures” behind the likes of ChatGPT and AlphaFold are incredibly powerful but offer little explanation as to how they reach their solutions so that people have resorted to querying the algorithms and adding to them in order to extract information that might offer some insights. “This is a much more conceptual understanding of what’s going on,” says Dmitry Krotov, now a researcher at IBM Research in Cambridge Massachusetts, who working alongside John Hopfield made some of the first steps that helps bring the two types of machine learning algorithm together."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/10/31/machine-learning-nobel-prizes.html#collective-phenomena",
    "href": "foundation-frontiers/posts/2024/10/31/machine-learning-nobel-prizes.html#collective-phenomena",
    "title": "The machine learning victories at the 2024 Nobel Prize Awards and how to explain them",
    "section": "Collective phenomena",
    "text": "Collective phenomena\nHopfield networks brought some of the mathematical toolbox long applied to extract “collective phenomena” from vast numbers of essentially identical parts such as atoms in a gas or atomic spins in magnetic materials. Although there maybe too many particles to track each individually, properties like temperature and magnetic field can be extracted using statistical physics. Hopfield showed that similarly a useful phenomenon he described as “associative memory” could be constructed from large numbers of artificial neurons by defining a “minimum energy”, which describes the network of neurons. The energy is determined by connections between neurons, which store information about patterns. Thus the network can retrieve the memorized patterns by minimizing that energy, just as stable conformations of atomic spins might be found in a magnetic material1. As the energy of the network is then subsequently minimised the pattern gets closer to the one that was memorised, just as when recalling a word or someone’s name we might first run through similar sounding words or names.\nThese Hopfield networks proved a seminal step in progressing AI algorithms, enabling a kind of pattern recognition from multiple stored patterns. However, it turned out that the number of patterns that could be stored was fundamentally limited due to what are known as “local” minima. You can imagine a ball rolling down a hill – it will reach the bottom of the hill fine so long as there are no dips for it to get stuck in en route. Algorithms based on Hopfield networks were prone to getting stuck in such dips or undesirable local minima, until Hopfield and Krotov put their heads together to find a way around it. Krotov describes himself as “incredibly lucky” that his research interests aligned so well with Hopfield. “He’s just such a smart and genuine person, and he has been in the field for many years,” he tells Real World Data Science. “He just knows things that no one else in the world knows.” Together they worked out they could address the problem of local minima by toggling the “activation function”.\n\n\n\n\n\n\nFigure 2: Energy Landscape of a Hopfield Network, highlighting the current state of the network (up the hill), an attractor state to which it will eventually converge, a minimum energy level and a basin of attraction shaded in green. Note how the update of the Hopfield Network is always going down in Energy. Credit: Mrazvan22/wikimedia\n\n\n\nIn a Hopfield network all the neurons are connected to all the other neurons, however originally the algorithm only considered interactions between two neurons at each point, i.e. the interaction between neuron 1 and neuron 2, neuron 1 and neuron 3 and neuron 2 and neuron 3, but not the interactions among all three altogether. By including such “higher order” interactions between more than two neurons, Krotov and Hopfield found they made the basins of attraction for the true minimum energy states deeper. You can think of it a little like the ball rolling down a steeper hill so that it picks up more momentum along the slope of the main hill and is less prone to falling in little dips en route. This way Krotov and Hopfield increased the memory of Hopfield networks in what they called the Dense Associative Memory, which they described in 20162. Long before then, however, Geoffrey Hinton had found a different tack to follow to increase the power of this kind of neural network."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/10/31/machine-learning-nobel-prizes.html#generative-ai",
    "href": "foundation-frontiers/posts/2024/10/31/machine-learning-nobel-prizes.html#generative-ai",
    "title": "The machine learning victories at the 2024 Nobel Prize Awards and how to explain them",
    "section": "Generative AI",
    "text": "Generative AI\nGeoffrey Hinton showed that by defining some neurons as a hidden layer and some as a visible layer (a Boltzmann machine3) and limiting the connections so that neurons are only connected with neurons in other layers (a restricted Boltzmann machine4), finding the most likely network would generate networks with meaningful similarities – a type of generative AI. This and many other contributions by Geoffrey Hinton also proved incredibly useful in the progress of machine learning. However, the generative AI algorithms grabbing headlines today have actually been devised using a “transformer” architecture, which differs from Hopfield networks and Boltzmann machines, or so it seemed initially.\nTransformer algorithms first emerged as a type of language model and were defined by a characteristic termed “attention”. “They say that each word represents a token, and essentially the task of attention is to learn long-range correlations between those tokens,” Krotov explains using the word “bank” as an example. Whether the word means the edge of a river or a financial institution can only be ascertained from the context in which it appears. “You learn these long-range correlations, and that allows you to contextualize and understand the meaning of every word.” The approach was first reported in 2017 in a paper titled “Attention is all you need”5 by researchers at Google Brain and Google Research.\nIt was not long before people figured out that the approach would enable powerful algorithms for tasks beyond language manipulation, including Demis Hassabis and John Jumper at Deep Mind as they worked to figure out an algorithm that could predict the folding conformations of proteins. The algorithm they landed on in 2020 – AlphaFold2 – was capable of protein conformation prediction with a 90% accuracy, way ahead of any other algorithm at the time, including Deep Mind’s previous attempt AlphaFold, which although streaks ahead of the field at the time it was developed in 2018, still only achieved an accuracy of 60%. It was for the extraordinary predictive powers for protein conformations achieved by AlphaFold2 that Hassabis and Jumper were awarded half the 2024 Nobel Prize for Chemistry."
  },
  {
    "objectID": "foundation-frontiers/posts/2024/10/31/machine-learning-nobel-prizes.html#connecting-the-dots",
    "href": "foundation-frontiers/posts/2024/10/31/machine-learning-nobel-prizes.html#connecting-the-dots",
    "title": "The machine learning victories at the 2024 Nobel Prize Awards and how to explain them",
    "section": "Connecting the dots",
    "text": "Connecting the dots\nTransformer architectures are undoubtedly hugely powerful but how they operate can seem something of a dark art as although computer scientists know how they are programmed, even they cannot tell how they reach their conclusions in operation. Instead they query the algorithm and add to it to try and get some pointers as to what the trail of logic might have been. Here Hopfield networks have an advantage because people can hope to get a grasp on what energy minima they are converging to, and that way get a handle on their working out. However, in their paper “Hopfield networks is all you need”6, researchers in Austria and Norway showed that the activation function, which Hopfield and Krotov had toggled to make Hopfield networks store more memories, can also link them to transformer architectures – essentially if the function is exponential they can reduce to the same thing.\n“We think about attention as learning long-range correlations, and this dense associative memory interpretation of attention tells you that each word creates a basin of attraction,” Krotov explains. “Essentially, the contextualization of the unknown word happens through the attraction to these different memories,” he adds. “That kind of lens of thinking about transformers through the prism of energy landscapes – it’s opened up this whole new world where you can think about what transformers are doing computationally, and how they perform that computation.”\n“I think it’s great that the power of these tools is being recognised for the impact that they can have in accelerating innovation in new ways,” says Janet Bastiman, RSS Data Science and AI Section Chair and Chief Data Scientist at financial crimes compliance solutions company Napier AI, as she comments on the Nobel Prize awards. Bastiman’s most recent work has been on adding explanation to networks. She notes how the report Hopfield networks is all you need highlights “the difference that layers can have on the final outcomes for specific tasks and a clear need for understanding some of the principles of the layers of networks in order to validate results and be aware of potential difficulties and”best” scenarios for different use cases.”\nKrotov also points out that since Hopfield networks are rooted in neurobiological interpretations, it helps to find “neurobiological ways of interpreting their computation” for transformer algorithms too. As such the vein Hopfield and Hinton tapped into with their seminal advances is proving ever richer in what Krotov describes as “the emerging field of the physics of neural computation”.\n\nExplore more data science ideas\n\n\n\n\n\nAbout the author\n\nAnna Demming is a freelance science writer and editor based in Bristol, UK. She has a PhD from King’s College London in physics, specifically nanophotonics and how light interacts with the very small, and has been an editor for Nature Publishing Group (now Springer Nature), IOP Publishing and New Scientist. Other publications she contributes to include The Observer, New Scientist, Scientific American, Physics World and Chemistry World..\n\n\n\n\n\nCopyright and licence\n\n© 2024 Anna Demming\n\n\n  Text, code, and figures are licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) International licence, except where otherwise noted. Thumbnail image by Shutterstock/Park Kang Hun Licenced by CC-BY 4.0.\n\n\n\nHow to cite\n\nDemming, Anna. 2024. “The machine learning victories at the 2024 Nobel Prize awards and how to explain them” Real World Data Science, October 31, 2024. URL"
  },
  {
    "objectID": "foundation-frontiers/posts/2024/10/31/machine-learning-nobel-prizes.html#footnotes",
    "href": "foundation-frontiers/posts/2024/10/31/machine-learning-nobel-prizes.html#footnotes",
    "title": "The machine learning victories at the 2024 Nobel Prize Awards and how to explain them",
    "section": "References",
    "text": "References\n\n\nHopfield J J Neural networks and physical systems with emergent collective computational abilities PNAS 79 2554-2558 (1982) https://www.pnas.org/doi/pdf/10.1073/pnas.79.8.2554↩︎\nKrotov D and Hopfield J J Dense Associative Memory for Pattern Recognition NIPS (2016)https://papers.nips.cc/paper_files/paper/2016/hash/eaae339c4d89fc102edd9dbdb6a28915-Abstract.html↩︎\nAckley D H, Hinton G E and Sejnowski T E A learning algorithm for boltzmann machines Cognitive Science 9 147-169 (1985) https://www.sciencedirect.com/science/article/pii/S0364021385800124↩︎\nSalakhutdinov R, Mnih A and Hinton G Restricted Boltzmann machines for collaborative filtering ICML ’07: Proceedings of the 24th international conference on Machine learning 791-798 (2007) https://dl.acm.org/doi/10.1145/1273496.1273596↩︎\nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez A N, Kaiser Ł, Polosukhin I Attention is all you need NIPS (2017)https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html↩︎\nRamsauer H, Schäfl B, Lehner J, Seidl P, Widrich M, Adler T, Gruber L, Holzleitner M, Pavlović M, Kjetil Sandve G, Greiff V, Kreil D, Kopp M, Klambauer G, Brandstetter J and Hochreiter S arXiv (2020) https://arxiv.org/abs/2008.02217↩︎"
  },
  {
    "objectID": "foundation-frontiers/posts/2023/07/03/trusted-AI.html",
    "href": "foundation-frontiers/posts/2023/07/03/trusted-AI.html",
    "title": "Trusted AI: translating AI ethics from theory into practice",
    "section": "",
    "text": "With artificial intelligence (AI) becoming increasingly prevalent across sectors, so too have conversations about AI ethics. AI ethics provides a repeatable and comprehensive way to assess what we should and should not be doing with AI, and sets out how we ought to design, use, and govern AI products in accordance with key principles. Ethical frameworks are essential to derive sustainable value from AI products and services and build trust.\nA myriad of AI tools that leverage automated or semi-automated decision-making processes have raised important questions that have become foundational in the AI ethics community, such as ‘What does it mean for an algorithm to be fair?’ As an example, AI tools that are used in recruitment may perpetuate biases arising from historical training data. If a model used to generate a shortlist of applicants has been trained on data from past candidates, say, and those candidates – both successful and unsuccessful – are predominantly men, historical patterns that contain various biases will perpetuate to become algorithmic biases that form the model’s decisions. Thus, the model may algorithmically discriminate against women or gender minorities, as individuals from these groups are not well represented in the training data.\nTo ensure the safe and responsible use of AI, the focus moving forward needs to be on the operationalisation of AI ethics into the day-to-day development lifecycle. But, what does this look like in practice? And how might you get started as an ethical AI practitioner? In this article, we unpack these questions and give you, the data scientist, a foundation to begin your journey towards trusted AI. Read along to get an overview of key principles that you should be aware of, what they mean, their underlying technical grounding, and what implementation might look like practically."
  },
  {
    "objectID": "foundation-frontiers/posts/2023/07/03/trusted-AI.html#ethical-ai-principles",
    "href": "foundation-frontiers/posts/2023/07/03/trusted-AI.html#ethical-ai-principles",
    "title": "Trusted AI: translating AI ethics from theory into practice",
    "section": "Ethical AI principles",
    "text": "Ethical AI principles\nYou have likely heard of several principles in relation to ethical AI, such as fairness or transparency. The context in which you’ve encountered such principles is most probably due to their inclusion in a broader ethical framework. Some of the most popular ethical AI frameworks include the National Institute of Standards and Technology’s AI Risk Management Framework, the UK Data Ethics Framework, and the European Commission’s Ethics Guidelines for Trustworthy AI. Among these and many other frameworks, we can run into what Floridi and Cowls (2019) call “principle proliferation,” whereby it becomes overwhelming for those contributing to AI programmes to know where to begin with ethics due to an excess of choice (p. 2).\nAt the time of writing, there is no single universally accepted standard that dictates which essential ethical AI values or principles should be adhered to during AI development and deployment. However, there are common themes that emerge. In our organisation, EY, we’ve learned from the variety of principles, frameworks, and white papers in the AI ethics community and developed our own Trusted AI Framework comprising five key attributes that we believe assure the trustworthiness of AI:\n\nTransparent\nExplainable\nUnbiased\nResilient\nHigh-performing\n\nIn this article, we take a deeper dive into the first three attributes – transparency, explainability, and unbiasedness (or fairness). These are areas where data scientists can act as critical enablers of ethical AI when they have the right knowledge and toolkits at their disposal.\n\nTransparency\nTransparency is the ability to provide meaningful information to foster awareness and understanding of an AI system. It starts with documenting AI systems in a way that is accessible for a broad audience with a spectrum of technical abilities. It is a simple yet powerful way to build trust in AI. It empowers non-technical stakeholders to critically evaluate AI development decisions, thereby unlocking multi-disciplinary insights that can mitigate reputational or performance risks. Further, it also builds trust with society, as it can enable everyday users to interrogate AI design decisions, product capabilities, and system limitations, thereby permitting users to make informed judgements about technology. Unfortunately, transparency is often misunderstood as disclosing trade secrets or proprietary information, such as source code and datasets. However, transparency can be achieved without disclosing such technically complex information. Instead, it can be as simple as disclosing where and when an AI system is being used, or for what purposes a model should be employed.\nBut what exactly does “documenting AI systems” look like? Documentation should consist of a mix of technical components (system architecture, dataset selection determination, model selection techniques, etc.) and non-technical components (business case, product purpose of use, alignment to overall AI strategy, etc.). The research community has recommended AI documentation standards, such as datasheets for datasets and model cards for model reporting. You can liken datasheets or model cards to the importance placed upon commenting your code – the more information there is available around decisions throughout model development, the greater the certainty that these artefacts will be understood and used as intended moving forward. Proper documentation and governance will help ensure accountability, improve internal and external oversight, and initiate discussions around model optimisation goals and their trade-offs, such as including fairness and accuracy in optimisation objectives.\nWith upcoming AI regulations, transparency requirements will become more integral. For example, the European Union (EU) AI Act introduces specific transparency obligations, such as bot disclosures, for both users and providers of AI systems, which would allow users to opt out of interacting with an AI system. Furthermore, in higher risk use cases, specific technical documentation is needed, which would include details of a system’s intended purpose and descriptions of its development process.\n\n\nExplainability\nOnce transparency is enabled, explainability is a natural next step, especially when an AI product is implemented in a more regulated or high-risk environment. Explainability is the ability to express why an AI system reached a particular decision or understand the features that affect model behaviour. Explainability is a key concern within the field of explainable AI, which, as a discipline, strives to improve trustworthiness by enabling a better understanding of a system’s underlying logic via a suite of technical methods.\nFundamentally, different model architectures mean that some models are more interpretable than others, as the steps used to evaluate their predictions are easier for humans to comprehend. Decision trees, for example, have more human-interpretable characteristics than deep learning models. Different model architectures also mean that there are interpretation tools that are only applicable to certain models, such as regression weights in a linear model.\nAnother approach to consider, then, is model-agnostic interpretation, which encompasses both global interpretability (explanation of an entire model’s behaviour) and local interpretability (explanation of a single prediction). While there are fast-developing techniques and tools for model-agnostic interpretability, let’s take a look at two of the more popular methods available:\n\n\nLocal interpretable model-agnostic explanations (LIME)\n\nThis is an explanation technique that trains local surrogate models, using explainable models such as Lasso or decision trees, to approximate the predictions of a model that is not interpretable by design in order to explain individual model predictions. The idea is to use interpretable features from the surrogate models to create human-friendly explanations where the underlying model cannot. For example, in an image classification model that detects a flower in an image, LIME is able to highlight the parts of the image that explain why the model classifies the image as a flower (see illustration below). This provides an interpretable explanation between the input variable and prediction, which is an essential part of interpretability.\n\n\n\n\n\n\nIllustration of explainable AI processes using LIME on an image classification AI system. Adapted from “Local Interpretable Model-Agnostic Explanations (LIME): An Introduction” and O’Reilly.\n\n\n\n\nShapley Additive Explanations (SHAP)\n\nSHAP (GitHub repo) uses tools and theoretical foundations from game theory, one of which is Shapley values. It works by assigning each feature an importance value for a particular prediction to numerically explain the contribution of various features to a model’s output. For example, in a model that predicts flu, SHAP calculates the importance of sneezing as a feature by removing and adding the subset of other features, leading to different combinations of features that contribute to the prediction. This method provides interpretable solutions for more complex models similar to the equivalent of “weights” in linear models.\n\n\n\n\n\nFairness\nThe area of AI ethics that is central to impending AI regulations, such as the EU AI Act and the New York City AI Law,1 is fairness. AI models are inherently biased because of their underlying training data.2 Thus, when we speak of fairness in the context of AI ethics, we are referring to a combination of technical and non-technical ways to minimise the impacts of algorithmic bias.\nLet’s begin with the technical approaches to fairness. To achieve equitable, reliable, and fair decisions, a diverse and balanced set of examples is needed in training datasets. However, data often contains disparities that, if left unchecked, can perpetuate algorithmic biases and harms. There are various approaches to detect sources of bias, guarantee fairness, or “debias” models. To strive for algorithmic fairness, many papers have proposed various quantitative measures of fairness, with some based on unstated assumptions about fairness in society. Unfortunately, these assumptions are often mutually incompatible, making it difficult to compare fairness metrics to one another – consider, for example, the longstanding debate between equality of outcome and equality of treatment.\nAlthough metrics incompatibilities exist, fairness broadly focuses on equality of opportunity (group fairness), and equality of outcome (individual fairness) to prevent discrimination against certain attributes. Drawing definitions from legal frameworks, the term “protected attribute” refers to the characteristics that are often protected under anti-discrimination laws, such as gender or race. Mathematically, the following metrics are often used to demonstrate scores that support fairness:\n\n\nStatistical parity\n\nThis measure seeks to uncover whether a model is fair towards protected attributes by measuring the difference between the majority and protected class in receiving a favourable outcome. A value of 0 demonstrates the model to be fair.\n\n\n\nDisparate impact\n\nThis compares the percentage of favourable outcomes for the monitored group to the percentage of favourable outcomes for a reference group. The groups compared can be the majority group and minority group, and this score will highlight in whose direction decisions are biased. For example, if a model grants loans to 60% of people in a middle-aged group and only 50% for those of other age cohorts, then the disparate impact is 0.8, which indicates a positive bias towards the middle-aged group and an adverse impact on the remaining cohorts.\n\n\n\nEquality of odds\n\nThis measures the balance of the true positive rate and false positive rate between protected and unprotected groups, which seeks to uncover whether a model performs similarly for the two groups.\n\n\n\nIt is important to remember that statistics are only one side of the fairness problem for machine learning, and one that treats the symptoms of bias as opposed to the underlying causes. In addition to the aforementioned technical approaches, there are a variety of non-technical measures that teams developing AI systems can adopt to augment fairness and inclusion:\n\n\nDefinition of fairness\n\nOrganisations that develop or use AI systems need to define, practically, what it means to be fair. Although there are various quantitative fairness measures, these are based on assumptions of fairness in society, which could be defined for each specific use case.\n\n\n\nDiversity on teams\n\nThere’s been a sharpened focus on the value of team diversity to areas such as productivity and creativity. The same is true for ethics. Ensuring that product teams are composed of a broad cross-section of identities can help to organically drive fairness through diversity of thought and experience.\n\n\n\nEducation and self-reflection\n\nDeveloping knowledge within individuals and teams about the socio-technical aspects of AI – that is, the ways in which AI shapes our social, political, economic, and environmental lives. The more critical a person can be as a data scientist in questioning why something is being built, the more likely they are to proactively recognise risks surrounding fairness.\n\n\n\nConsider the end user\n\nImagine that you are on a development team building an AI solution for a problem in the agricultural sector pertaining to livestock health. Who is best suited to solving the problem: a data scientist or a farmer? As a data scientist, you may have the tools to develop a solution, but given your distance from the end user, you are unlikely to intimately understand the problem in the same way a farmer would. If you cannot understand the problem, you cannot hope to find a solution, much less an ethical one. Recognising the importance of consulting individuals that are representative of end users is key to ensure that your design is fair.\n\n\n\nAI ethics review boards\n\nData science teams should not operate in isolation. Increasingly, organisations are establishing AI ethics review boards or similar forums that are intended to act as checks on the design decisions made throughout AI development. Does your organisation have one?"
  },
  {
    "objectID": "foundation-frontiers/posts/2023/07/03/trusted-AI.html#in-conclusion",
    "href": "foundation-frontiers/posts/2023/07/03/trusted-AI.html#in-conclusion",
    "title": "Trusted AI: translating AI ethics from theory into practice",
    "section": "In conclusion",
    "text": "In conclusion\nThese three areas – transparency, explainability, and fairness – are the starting points to embed and operationalise AI ethics in technical development. Transparency relies on both technical and non-technical documentation to facilitate discussions with non-technical stakeholders, as well as to create and enforce accountabilities. Explainability helps to build trust in AI output by vesting us with an ability to explain “why”. Finally, adopting both technical and non-technical measures of fairness can ensure that AI products in development do not adversely impact certain groups.\nIn addition to these three areas of AI ethics, within EY we have two other focus areas – resilience and high-performance – that form part of our Trusted AI Framework. We will discuss these in a future article. We’re also keen to explore topics such as generating trust in generative AI! Until then, please share your stories of developing ethical AI projects in the comments below. How are you translating AI ethics from theory into practice?\n\n\n\n\n\n\nFurther reading\n\n\n\n\n\nFor further technical reading, we suggest:\n\nInterpretable Machine Learning, by Christoph Molnar\nFairness and Machine Learning, by Solon Barocas, Moritz Hardt, and Arvind Narayanan\n\nFor further socio-technical reading on AI and data ethics, we suggest:\n\nThe Age of Surveillance Capitalism, by Shoshana Zuboff\nInvisible Women, by Caroline Criado Pérez\nRace after Technology, by Ruha Benjamin\nAlgorithms of Oppression, by Safiya Noble\nAtlas of AI, by Kate Crawford\nWeapons of Math Destruction, by Cathy O’Neil\nData Feminism, by Catherine D’Ignazio and Lauren Klein\n\n\n\n\n\nExplore more data science ideas\n\n\n\n\n\nAbout the authors\n\nMaxine Setiawan is a social data scientist specialising in trusted AI, and AI and data risk in EY UK&I. With her multi-disciplinary background, she works to help clients understand and manage risks from their data and AI systems, and to ensure AI governance that is fair, accountable, and trustworthy. Maxine holds an MSc in social data science from the University of Oxford.\n\n\nMira Pijselman is the digital ethics lead for EY UK&I, where she focuses on the responsible governance of key emerging technologies, including artificial intelligence, quantum technologies, and the metaverse. A social scientist and philosopher by training, she helps clients to map, understand, secure, and capitalise on their data and technology potential safely. Mira holds an MSc in the social science of the internet from the University of Oxford.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Maxine Setiawan and Mira Pijselman\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\nThumbnail image by Alexa Steinbrück / Better Images of AI / Explainable AI / Licenced by CC-BY 4.0.\n\n\n\nHow to cite\n\nSetiawan, Maxine and Mira Pijselman. 2023. “Trusted AI: translating AI ethics from theory into practice.” Real World Data Science, July 3, 2023. URL"
  },
  {
    "objectID": "foundation-frontiers/posts/2023/07/03/trusted-AI.html#footnotes",
    "href": "foundation-frontiers/posts/2023/07/03/trusted-AI.html#footnotes",
    "title": "Trusted AI: translating AI ethics from theory into practice",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNew York City Local Law 144.↩︎\nNot statistical bias (usually known as bias-variance trade-off), which compares the training data and target value to approximate errors.↩︎"
  },
  {
    "objectID": "the-pulse/posts/2024/08/01/RWDS-journal.html",
    "href": "the-pulse/posts/2024/08/01/RWDS-journal.html",
    "title": "New open access journal - RSS: Data Science and Artificial Intelligence",
    "section": "",
    "text": "The Royal Statistical Society (RSS) is launching a new fully open access journal, RSS: Data Science and Artificial Intelligence. Created in recognition of the growing importance of data science and artificial intelligence in science and society, the new journal’s remit spans the breadth of data science; you can submit articles covering disciplines including statistics, machine learning, deep learning, econometrics, bioinformatics, engineering and computational social science.\nAs well as three primary paper types - method papers, applications papers and behind-the-scenes papers - RSS: Data Science and Artificial Intelligence will publish editorials, op-eds, interviews, and reviews/perspectives in line with its goal to become a primary destination for data scientists.\nPublished by Oxford University Press, this new journal is the first addition to the RSS family of world class statistics journals since 1952.\nLearn more about why RSS: Data Science and Artificial Intelligence is the ideal platform for showcasing your research.\n\n\n\n\nMeet the journal’s editors-in-chief and editorial board\n \n\n\n\nSach Mukherjee is Director of Research in Machine Learning for Biomedicine at the Medical Research Council (MRC) Biostatistics Unit, University of Cambridge, and Head of Statistics and Machine Learning at the German Center for Neurodegenerative Diseases.\n\n\n\nSilvia Chiappa is a Research Scientist at Google DeepMind London, where she leads the Causal Intelligence team, and Honorary Professor at the Computer Science Department of University College London.\n\n\n\nNeil Lawrenece is the inaugural DeepMind Professor of Machine Learning at the University of Cambridge. He has been working on machine learning models for over 20 years. He recently returned to academia after three years as Director of Machine Learning at Amazon.\n\n\n\n\nView the full editorial board here: Editorial Board | RSS Data Science | Oxford Academic (oup.com)\n\nDiscover more The Pulse\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution (CC BY 4.0) International licence."
  },
  {
    "objectID": "the-pulse/posts/2023/06/05/no-AI-probably-wont-kill-us.html",
    "href": "the-pulse/posts/2023/06/05/no-AI-probably-wont-kill-us.html",
    "title": "No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye",
    "section": "",
    "text": "Doomsaying is an old occupation. Artificial intelligence (AI) is a complex subject. It’s easy to fear what you don’t understand. These three truths go some way towards explaining the oversimplification and dramatisation plaguing discussions about AI.\nLast week, outlets around the world were plastered with news of yet another open letter claiming AI poses an existential threat to humankind. This letter, published through the nonprofit Center for AI Safety, has been signed by industry figureheads including Geoffrey Hinton and the chief executives of Google DeepMind, Open AI and Anthropic.\nHowever, I’d argue a healthy dose of scepticism is warranted when considering the AI doomsayer narrative. Upon close inspection, we see there are commercial incentives to manufacture fear in the AI space.\nAnd as a researcher of artificial general intelligence (AGI), it seems to me the framing of AI as an existential threat has more in common with 17th-century philosophy than computer science."
  },
  {
    "objectID": "the-pulse/posts/2023/06/05/no-AI-probably-wont-kill-us.html#was-chatgpt-a-breakthrough",
    "href": "the-pulse/posts/2023/06/05/no-AI-probably-wont-kill-us.html#was-chatgpt-a-breakthrough",
    "title": "No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye",
    "section": "Was ChatGPT a ‘breakthrough’?",
    "text": "Was ChatGPT a ‘breakthrough’?\nWhen ChatGPT was released late last year, people were delighted, entertained and horrified.\nBut ChatGPT isn’t a research breakthrough as much as it is a product. The technology it is based on is several years old. An early version of its underlying model, GPT-3, was released in 2020 with many of the same capabilities. It just wasn’t easily accessible online for everyone to play with.\nBack in 2020 and 2021, I and many others wrote papers discussing the capabilities and shortcomings of GPT-3 and similar models – and the world carried on as always. Forward to today, and ChatGPT has had an incredible impact on society. What changed?\nIn March, Microsoft researchers published a paper claiming GPT-4 showed “sparks of artificial general intelligence”. AGI is the subject of a variety of competing definitions, but for the sake of simplicity can be understood as AI with human-level intelligence.\nSome immediately interpreted the Microsoft research as saying GPT-4 is an AGI. By the definitions of AGI I’m familiar with, this is certainly not true. Nonetheless, it added to the hype and furore, and it was hard not to get caught up in the panic. Scientists are no more immune to group think than anyone else.\nThe same day that paper was submitted, The Future of Life Institute published an open letter calling for a six-month pause on training AI models more powerful than GPT-4, to allow everyone to take stock and plan ahead. Some of the AI luminaries who signed it expressed concern that AGI poses an existential threat to humans, and that ChatGPT is too close to AGI for comfort.\nSoon after, prominent AI safety researcher Eliezer Yudkowsky – who has been commenting on the dangers of superintelligent AI since well before 2020 – took things a step further. He claimed we were on a path to building a “superhumanly smart AI”, in which case “the obvious thing that would happen” is “literally everyone on Earth will die”. He even suggested countries need to be willing to risk nuclear war to enforce compliance with AI regulation across borders."
  },
  {
    "objectID": "the-pulse/posts/2023/06/05/no-AI-probably-wont-kill-us.html#i-dont-consider-ai-an-imminent-existential-threat",
    "href": "the-pulse/posts/2023/06/05/no-AI-probably-wont-kill-us.html#i-dont-consider-ai-an-imminent-existential-threat",
    "title": "No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye",
    "section": "I don’t consider AI an imminent existential threat",
    "text": "I don’t consider AI an imminent existential threat\nOne aspect of AI safety research is to address potential dangers AGI might present. It’s a difficult topic to study because there is little agreement on what intelligence is and how it functions, let alone what a superintelligence might entail. As such, researchers must rely as much on speculation and philosophical argument as on evidence and mathematical proof.\nThere are two reasons I’m not concerned by ChatGPT and its byproducts.\nFirst, it isn’t even close to the sort of artificial superintelligence that might conceivably pose a threat to humankind. The models underpinning it are slow learners that require immense volumes of data to construct anything akin to the versatile concepts humans can concoct from only a few examples. In this sense, it is not “intelligent”.\nSecond, many of the more catastrophic AGI scenarios depend on premises I find implausible. For instance, there seems to be a prevailing (but unspoken) assumption that sufficient intelligence amounts to limitless real-world power. If this was true, more scientists would be billionaires.\nMoreover, cognition as we understand it in humans takes place as part of a physical environment (which includes our bodies), and this environment imposes limitations. The concept of AI as a “software mind” unconstrained by hardware has more in common with 17th-century dualism (the idea that the mind and body are separable) than with contemporary theories of the mind existing as part of the physical world."
  },
  {
    "objectID": "the-pulse/posts/2023/06/05/no-AI-probably-wont-kill-us.html#why-the-sudden-concern",
    "href": "the-pulse/posts/2023/06/05/no-AI-probably-wont-kill-us.html#why-the-sudden-concern",
    "title": "No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye",
    "section": "Why the sudden concern?",
    "text": "Why the sudden concern?\nStill, doomsaying is old hat, and the events of the last few years probably haven’t helped – but there may be more to this story than meets the eye.\nAmong the prominent figures calling for AI regulation, many work for or have ties to incumbent AI companies. This technology is useful, and there is money and power at stake – so fearmongering presents an opportunity.\nAlmost everything involved in building ChatGPT has been published in research anyone can access. OpenAI’s competitors can (and have) replicated the process, and it won’t be long before free and open-source alternatives flood the market.\nThis point was made clearly in a memo purportedly leaked from Google entitled “We have no moat, and neither does OpenAI”. A moat is jargon for a way to secure your business against competitors.\nYann LeCun, who leads AI research at Meta, says these models should be open since they will become public infrastructure. He and many others are unconvinced by the AGI doom narrative.\n\n\n\nA NYT article on the debate around whether LLM base models should be closed or open.Meta argues for openness, starting with the release of LLaMA (for non-commercial use), while OpenAI and Google want to keep things closed and proprietary.They argue that openness can be…\n\n— Yann LeCun (@ylecun) May 18, 2023\n\n\n\nNotably, Meta wasn’t invited when US President Joe Biden recently met with the leadership of Google DeepMind and OpenAI. That’s despite the fact that Meta is almost certainly a leader in AI research; it produced PyTorch, the machine-learning framework OpenAI used to make GPT-3.\nAt the White House meetings, OpenAI chief executive Sam Altman suggested the US government should issue licences to those who are trusted to responsibly train AI models. Licences, as Stability AI chief executive Emad Mostaque puts it, “are a kinda moat”.\nCompanies such as Google, OpenAI and Microsoft have everything to lose by allowing small, independent competitors to flourish. Bringing in licensing and regulation would help cement their position as market leaders and hamstring competition before it can emerge.\nWhile regulation is appropriate in some circumstances, regulations that are rushed through will favour incumbents and suffocate small, free and open-source competition.\n\n\n\nThink Google or Microsoft are encouraging legislation for your safety? But of course! These are honorable companies.You might think they'd like less competition too though. Maybe a monopoly? Maybe legal red tape preventing free and open source alternatives? Perhaps other… https://t.co/Z7vSpMyuHg\n\n— Michael Timothy Bennett (@MiTiBennett) May 5, 2023\n\n\n\n\n\n\n\nDiscover more The Pulse\n\n\n\n\n\nAbout the author\n\nMichael Timothy Bennett is a PhD student in the School of Computing, Australian National University.\n\n\n\n\n\nCopyright and licence\n\nThis article is republished from The Conversation under a Creative Commons license. Read the original article.\n\n\nThumbnail image by Alan Warburton / © BBC / Better Images of AI / Social Media / Licenced by CC-BY 4.0."
  },
  {
    "objectID": "the-pulse/posts/2023/09/06/biased-algorithms.html",
    "href": "the-pulse/posts/2023/09/06/biased-algorithms.html",
    "title": "For minorities, biased AI algorithms can damage almost every part of life",
    "section": "",
    "text": "Bad data does not only produce bad outcomes. It can also help to suppress sections of society, for instance vulnerable women and minorities.\nThis is the argument of my new book on the relationship between various forms of racism and sexism and artificial intelligence (AI). The problem is acute. Algorithms generally need to be exposed to data – often taken from the internet – in order to improve at whatever they do, such as screening job applications, or underwriting mortgages.\nBut the training data often contains many of the biases that exist in the real world. For example, algorithms could learn that most people in a particular job role are male and therefore favour men in job applications. Our data is polluted by a set of myths from the age of “enlightenment”, including biases that lead to discrimination based on gender and sexual identity.\nJudging from the history in societies where racism has played a role in establishing the social and political order, extending privileges to white males –- in Europe, North America and Australia, for instance –- it is simple science to assume that residues of racist discrimination feed into our technology.\nIn my research for the book, I have documented some prominent examples. Face recognition software more commonly misidentified black and Asian minorities, leading to false arrests in the US and elsewhere.\nSoftware used in the criminal justice system has predicted that black offenders would have higher recidivism rates than they did. There have been false healthcare decisions. A study found that of the black and white patients assigned the same health risk score by an algorithm used in US health management, the black patients were often sicker than their white counterparts.\nThis reduced the number of black patients identified for extra care by more than half. Because less money was spent on black patients who have the same level of need as white ones, the algorithm falsely concluded that black patients were healthier than equally sick white patients. Denial of mortgages for minority populations is facilitated by biased data sets. The list goes on."
  },
  {
    "objectID": "the-pulse/posts/2023/09/06/biased-algorithms.html#machines-dont-lie",
    "href": "the-pulse/posts/2023/09/06/biased-algorithms.html#machines-dont-lie",
    "title": "For minorities, biased AI algorithms can damage almost every part of life",
    "section": "Machines don’t lie?",
    "text": "Machines don’t lie?\nSuch oppressive algorithms intrude on almost every area of our lives. AI is making matters worse, as it is sold to us as essentially unbiased. We are told that machines don’t lie. Therefore, the logic goes, no one is to blame.\nThis pseudo-objectiveness is central to the AI-hype created by the Silicon Valley tech giants. It is easily discernible from the speeches of Elon Musk, Mark Zuckerberg and Bill Gates, even if now and then they warn us about the projects that they themselves are responsible for.\nThere are various unaddressed legal and ethical issues at stake. Who is accountable for the mistakes? Could someone claim compensation for an algorithm denying them parole based on their ethnic background in the same way that one might for a toaster that exploded in a kitchen?\nThe opaque nature of AI technology poses serious challenges to legal systems which have been built around individual or human accountability. On a more fundamental level, basic human rights are threatened, as legal accountability is blurred by the maze of technology placed between perpetrators and the various forms of discrimination that can be conveniently blamed on the machine.\nRacism has always been a systematic strategy to order society. It builds, legitimises and enforces hierarchies between the haves and have nots."
  },
  {
    "objectID": "the-pulse/posts/2023/09/06/biased-algorithms.html#ethical-and-legal-vacuum",
    "href": "the-pulse/posts/2023/09/06/biased-algorithms.html#ethical-and-legal-vacuum",
    "title": "For minorities, biased AI algorithms can damage almost every part of life",
    "section": "Ethical and legal vacuum",
    "text": "Ethical and legal vacuum\nIn such a world, where it’s difficult to disentangle truth and reality from untruth, our privacy needs to be legally protected. The right to privacy and the concomitant ownership of our virtual and real-life data needs to be codified as a human right, not least in order to harvest the real opportunities that good AI harbours for human security.\nBut as it stands, the innovators are far ahead of us. Technology has outpaced legislation. The ethical and legal vacuum thus created is readily exploited by criminals, as this brave new AI world is largely anarchic.\nBlindfolded by the mistakes of the past, we have entered a wild west without any sheriffs to police the violence of the digital world that’s enveloping our everyday lives. The tragedies are already happening on a daily basis.\nIt is time to counter the ethical, political and social costs with a concerted social movement in support of legislation. The first step is to educate ourselves about what is happening right now, as our lives will never be the same. It is our responsibility to plan the course of action for this new AI future. Only in this way can a good use of AI be codified in local, national and global institutions.\n\n\n\n\nDiscover more The Pulse\n\n\n\n\n\nAbout the author\n\nArshin Adib-Moghaddam is professor in global thought and comparative philosophies, SOAS, University of London.\n\n\n\n\n\nCopyright and licence\n\nThis article is republished from The Conversation under a Creative Commons license. Read the original article.\n\n\nImage by Alan Warburton / © BBC / Better Images of AI / Quantified Human / Licenced by CC-BY 4.0."
  },
  {
    "objectID": "the-pulse/posts/2023/10/20/ai-for-humanity.html",
    "href": "the-pulse/posts/2023/10/20/ai-for-humanity.html",
    "title": "An AI for humanity",
    "section": "",
    "text": "This is the text of a talk Martin Goodson gave to the European Commission in Brussels on October 10, 2023. It is republished with permission from the Royal Statistical Society Data Science and AI Section Newsletter Substack. The views expressed are the author’s own and do not necessarily represent those of the RSS.\nFor years academics have published studies about the limits of automation by AI, suggesting that jobs requiring creativity were the least susceptible to automation. That turned. out. well.\nActually, that’s not completely true: some said that jobs that need a long period of education, like teaching and healthcare, were going to be the hardest of all to automate. Oh. dear.\nLet’s face it, all predictions about the limits of AI have been hopelessly wrong. Maybe we need to accept that there aren’t going to be any limits. How is this going to affect our society?\nStudies came out from Stanford and MIT this year, looking at the potential of AI assistants to improve the productivity of office workers. Both came to the same conclusion – that the workers with the lowest ability and least experience were the ones who gained the most in productivity.\nIn other words, AI has made human knowledge and experience less valuable.\nResearchers at Microsoft and Open AI wrote something important on this phenomenon that I’d like to quote in full:\nLet’s talk about the fairness of this. Because the AI models didn’t invent medicine, accountancy or engineering. They didn’t learn anything directly from the world – human experts taught AI models how to do these things. And they [the human experts] did it without giving their permission, or even knowing that it was happening.\nThe large tech companies have sucked up all of human knowledge and culture and now provide access to it for the price of an API call. This is a huge transfer of power and value from humanity to the tech companies.\nBiologists in the 1990s found themselves in a very similar position. Celera Genomics was trying to achieve commercial control over the human genome. To stop this happening, the publicly funded Human Genome Project (HGP) resolved to sequence the human genome and release the data for free on a daily basis, before Celera could patent any of it.\nThe HGP was criticised because of ethical concerns (including concerns about eugenics), and because it was thought to be a huge waste of money. The media attacked it, claiming that a publicly funded initiative could not possibly compete with the commercial sector. Fortunately for humanity, a group of scientists with a vision worked together to make it a success.\nAnd it was a huge success: in purely economic terms it produced nearly $1 trillion in economic impacts for investment of about $4 billion. Apart from the economics, the Human Genome Project accelerated development of the genomic technologies that underlie things like mRNA vaccine technology.\nThe parallels to our current situation with AI are striking. With OpenAI, just like Celera, we have a commercial enterprise that launched with an open approach to data sharing but eventually changed to a more closed model.\nWe have commentators suggesting that a publicly funded project to create an open-source AI would be ethically dubious, a waste of money and beyond the competency of the public sector. Where the analogy breaks down is that unlike in the 1990s, we do not have any strong voices arguing on the other side, for openness and the creation of shared AI models for all humanity.\nPublic funding is needed for an “AI for humanity” project, modelled on the Human Genome Project. How else can we ensure the benefits of AI are spread widely across the global population and not concentrated in the hands of one or two all-powerful technology companies?\nWe’ll never know what the world would have looked like if we’d let Celera gain control over the human genome. Do we want to know a world where we let technology companies gain total control over artificial intelligence?"
  },
  {
    "objectID": "the-pulse/posts/2023/10/20/ai-for-humanity.html#faq",
    "href": "the-pulse/posts/2023/10/20/ai-for-humanity.html#faq",
    "title": "An AI for humanity",
    "section": "FAQ",
    "text": "FAQ\n\nHow about all the ethical considerations around AI – shouldn’t we consider this before releasing any open-source models?\nOf course. Obviously, there are ethical implications that need to be considered carefully, just as there were for the genome project. At the start of that project, the ethical, legal, and social issues (or ELSI) program was set up. The National Institutes of Health (NIH) devoted about 5% of their total Human Genome Project budgets to the ELSI program and it is now the largest bioethics program in the world. All important ethical issues were considered carefully and resolved without drama.\n\n\nAren’t there enough community efforts to build open-source AI models already?\nThere are good projects producing open-source large language models, like Llama 2 from Meta and Falcon from the TII in the United Arab Emirates. These are not quite as powerful as [Open AI’s] GPT-4 but they prove the concept that open-source models can approach the capabilities of the front-running commercial models; even when produced by a single well-funded lab (and a state-funded lab in the case of the TII). A coordinated international publicly funded project will be needed to surpass commercial models in performance.\nIn any case, do we want to be dependent on the whims of the famously civic-minded Mark Zuckerberg [CEO of Meta] for access to open-source AI models? We shouldn’t forget that the original Llama model was released with a restrictive licence that was eventually changed to something more open after a community outcry. We are lucky they made this decision. But the future of our societies needs to rely on more than luck.\n\n\nHow about the UK Government AI Safety Summit and AI Safety Institute – won’t they be doing similar work?\nAbsolutely not! The limit of the UK Government’s ambition seems to be to set the UK up as a sort of evaluation and testing station for AI models made in Silicon Valley. This is as far from the spirit of the Human Genome Project as it’s possible to be.\nSir John Sulston, the leader of the HGP in the UK, was a Nobel Prize-winning scientific hero who wanted to stop Celera Genomics from gaining monopolistic control over the human genome at all costs. The current UK ambition would be like reducing the Human Genome Project to merely testing Celera Genomics’ data for errors.\n\n\nHow will an international ‘AI for humanity’ project avoid the devaluation of human knowledge and experience, and consequent job losses?\nIt may not be possible to avoid this. But governments will at least be able to mitigate societal disruption if they can redistribute some of the wealth gained via AI (e.g., via universal basic income). They will not be able to do this if all of the wealth accrues to only one or two technology companies based in Silicon Valley.\n\n\nHow about existential risk?\n‘Existential risk’ is a science fiction smokescreen generated by large tech companies to distract from the real issues. I cannot think of a better response than the words of Prof Sandra Wachter at the University of Oxford: “Let’s focus on people’s jobs being replaced. These things are being completely sidelined by the Terminator scenario.”\n\n\n\n\n\n\nMartin Goodson will be speaking live at the Royal Statistical Society on October 31, 2023, as part of a panel discussion on “Evaluating artificial intelligence: How data science and statistics can make sense of AI models.” Register now for this free in-person debate. The event forms part of the AI Fringe programme of activities, which runs alongside the UK Government’s AI Safety Summit (1–2 November).\n\n\n\n\nDiscover more The Pulse\n\n\n\n\n\nAbout the author\n\nMartin Goodson is the former chair of the RSS Data Science and AI Section (2019–2022). He is the organiser of the London Machine Learning Meetup, the largest network of AI practitioners in Europe, with over 11,000 members. He is also the CEO of AI startup, Evolution AI.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Martin Goodson\n\n\nThumbnail image by Etienne Girardet on Unsplash."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2024/01/18/cherry-blossom.html",
    "href": "the-pulse/editors-blog/posts/2024/01/18/cherry-blossom.html",
    "title": "When will the cherry trees bloom? Get ready to make and share your predictions!",
    "section": "",
    "text": "The 2024 International Cherry Blossom Prediction Competition will open for entries on February 1, and Real World Data Science is once again proud to be a sponsor.\nContestants are invited to submit predictions for the date cherry trees will bloom in 2024 at five different locations – Kyoto, Japan; Liestal-Weideli, Switzerland; Vancouver, Canada; and Washington, DC and New York City, USA.\nThe competition organisers will provide all the publicly available data they can find for the bloom dates of cherry trees in these locations, and contestants will then be challenged to use this data “in combination with any other publicly available data (e.g., climate data) to provide reproducible predictions of the peak bloom date.”\n“For this competition, we seek accurate, interpretable predictions that offer strong narratives about the factors that determine when cherry trees bloom and the broader consequences for local and global ecosystems,” say the organisers. “Your task is to predict the peak bloom date for 2024 and to estimate a prediction interval, a lower and upper endpoint of dates during which peak bloom is most probable.”\nSo that organisers can reproduce the predictions, entrants must submit all data and code in a Quarto document.\nThere’s cash and prizes on offer for the best entries, including having your work featured on Real World Data Science. Head on over to the competition website for full details and rules.\nAnd, if you are looking for some inspiration, check out this tutorial on the law of the flowering plants, written by Jonathan Auerbach, a co-organiser of the prediction competition.\nGood luck to all entrants!\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo by AJ on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2024. “When will the cherry trees bloom? Get ready to make and share your predictions!” Real World Data Science, January 18, 2024. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2024/01/22/gen-ai-framework.html",
    "href": "the-pulse/editors-blog/posts/2024/01/22/gen-ai-framework.html",
    "title": "UK government sets out 10 principles for use of generative AI",
    "section": "",
    "text": "The UK government has published a framework for the use of generative AI, setting out 10 principles for departments and staff to think about if using, or planning to use, this technology.\nIt covers the need to understand what generative AI is and its limitations, the lawful, ethical and secure use of the technology, and a requirement for “meaningful human control.”\nThe focus is on large language models (LLMs) as, according to the framework, these have “the greatest level of immediate application in government.”\nIt lists a number of promising use cases for LLMs, including the synthesise of complex data, software development, and summaries of text and audio. However, the document cautions against using generative AI for fully automated decision-making or in contexts where data is limited or explainability of decision-making is required. For example, it warns that:\n\n“although LLMs can give the appearance of reasoning, they are simply predicting the next most plausible word in their output, and may produce inaccurate or poorly-reasoned conclusions.”\n\nAnd on the issue of explainability, it says that:\n\n“generative AI is based on neural networks, which are so-called ‘black boxes’. This makes it difficult or impossible to explain the inner workings of the model which has potential implications if in the future you are challenged to justify decisioning or guidance based on the model.”\n\nThe framework goes on to discuss some of the practicalities of building generative AI solutions. It talks specifically about the value a multi-disciplinary team can bring to such projects, and emphasises the role of data scientists:\n\n“data scientists … understand the relevant data, how to use it effectively, and how to build/train and test models.”\n\nIt also speaks to the need to “understand how to monitor and mitigate generative AI drift, bias and hallucinations” and to have “a robust testing and monitoring process in place to catch these problems.”\nWhat do you make of the Generative AI Framework for His Majesty’s Government? What does it get right, and what needs more work?\n\n\n\n\n\n\nAnd in case you missed it…\n\n\n\nNew York State issued a policy on the Acceptable Use of Artificial Intelligence Technologies earlier this month. Similar to the UK government framework, it references the need for human oversight of AI models and rules out use of “automated final decision systems.” There is also discussion of fairness, equity and explainability, and AI risk assessment and management.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Massimiliano Morosinotto on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2024. “UK government sets out 10 principles for use of generative AI.” Real World Data Science, January 22, 2024. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2024/03/editors-note.html",
    "href": "the-pulse/editors-blog/posts/2024/03/editors-note.html",
    "title": "Editor’s note: Not saying goodbye, just saying…",
    "section": "",
    "text": "It’s not easy to leave a brilliant group of people you’ve worked with for almost a decade, but in a month’s time I’ll be moving on from the Royal Statistical Society (RSS).\nWhen I joined RSS in June 2014 I was looking for new challenges. I wanted to find out more about the ways statistics and data are used to understand and solve problems and inform decisions in science, business and industry, public policy, health… I could go on! Working for the RSS certainly delivered on that front: as editor of Significance for eight years and of Real World Data Science more recently, I have had many opportunities to learn.\nPretty much every day of my working life for the past nine years, eight months or so involved speaking with expert statisticians and data scientists or reading about their work. When there were things I didn’t understand, they were always happy to explain. When I shared my ideas for how to make their articles clearer or more readable, they took the time to listen. Together, we worked to create accessible, engaging stories about statistics and data. There have been hundreds of these collaborations over the years – too many to namecheck individually – but I have enjoyed them all, and I’ve learned something from each of them.\nBefore I head off to pursue a new set of challenges and learning opportunities, I want to say a big thank you to all the RSS staff and members, past and present, that I’ve been lucky to call my colleagues. Thank you also to the staff and members of the American Statistical Association who have been valued partners on Significance over the years and now RWDS too. It’s been a privilege to work with you all.\nThe chance to launch RWDS has been a particular highlight of my time at RSS, and I am grateful to have had the support and input of The Alan Turing Institute and many of its wonderful staff and researchers on this project. I’m excited to see the site continue to grow and develop into a valuable resource for the data science community, and I look forward to reading an upcoming series of articles that will explore the statistical and data science perspectives on AI – stay tuned for more on this soon.\nStatistics and data will continue to be a big part of my life, so this isn’t “goodbye.” Instead, I’ll just say, let’s keep in touch – and thank you for reading!\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Pete Pedroza on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2024. “Editor’s note: Not saying goodbye, just saying…” Real World Data Science, March 6, 2024. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/01/05/newsletter.html",
    "href": "the-pulse/editors-blog/posts/2023/01/05/newsletter.html",
    "title": "Explore the RSS Data Science & AI Section newsletter, right here!",
    "section": "",
    "text": "Happy New Year from all of us at Real World Data Science. We hope you had a relaxing break over the holidays and are now refreshed and excited to see what 2023 has in store. We’re starting the year with a new addition to the site: a page dedicated to the excellent RSS Data Science & AI Section newsletter.\nThis monthly newsletter has been running since February 2020 and is well worth subscribing to as it features roundups of news, new developments, big picture ideas and practical tips.\nYou’ll find the full list of past newsletters in our News and views section (click the “Newsletter” heading in the section menu). If you want to subscribe to the newsletter, head over to datasciencesection.org. The Data Science & AI Section also has a page on the RSS website.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Explore the RSS Data Science & AI Section newsletter, right here!” Real World Data Science, January, 5 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/08/06/jsm-blog.html",
    "href": "the-pulse/editors-blog/posts/2023/08/06/jsm-blog.html",
    "title": "Live from Toronto: Real World Data Science at the Joint Statistical Meetings",
    "section": "",
    "text": "Haley Jeppson, Danielle Albers Szafir, and Ian Lyttle\nJSM 2023 is underway, and the first session I attended today was this panel on the use of colour in statistical charts.\nThe topic appealed to me for two reasons:\n\nBefore my trip to Toronto, I interviewed Alberto Cairo about the many “dialects” of data visualisation.\nI’ve recently been working with Andreas Krause and Nicola Rennie to create new guidance for improving statistical graphics, titled “Best Practices for Data Visualisation”.\n\nThe “Best Practices…” guide links to several useful data visualisation tools, and this session today has put a few more on my radar:\n\nColor Crafting, by Stephen Smart, Keke Wu, and Danielle Albers Szafir. The authors write: “Visualizations often encode numeric data using sequential and diverging color ramps. Effective ramps use colors that are sufficiently discriminable, align well with the data, and are aesthetically pleasing. Designers rely on years of experience to create high-quality color ramps. However, it is challenging for novice visualization developers that lack this experience to craft effective ramps as most guidelines for constructing ramps are loosely defined qualitative heuristics that are often difficult to apply. Our goal is to enable visualization developers to readily create effective color encodings using a single seed color.”\nComputing on color, a collection of Observable notebooks by Ian Lyttle that allow users to see how different colour spaces and colour scales work with different types of colour vision deficiency."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/08/06/jsm-blog.html#sunday-august-6",
    "href": "the-pulse/editors-blog/posts/2023/08/06/jsm-blog.html#sunday-august-6",
    "title": "Live from Toronto: Real World Data Science at the Joint Statistical Meetings",
    "section": "",
    "text": "Haley Jeppson, Danielle Albers Szafir, and Ian Lyttle\nJSM 2023 is underway, and the first session I attended today was this panel on the use of colour in statistical charts.\nThe topic appealed to me for two reasons:\n\nBefore my trip to Toronto, I interviewed Alberto Cairo about the many “dialects” of data visualisation.\nI’ve recently been working with Andreas Krause and Nicola Rennie to create new guidance for improving statistical graphics, titled “Best Practices for Data Visualisation”.\n\nThe “Best Practices…” guide links to several useful data visualisation tools, and this session today has put a few more on my radar:\n\nColor Crafting, by Stephen Smart, Keke Wu, and Danielle Albers Szafir. The authors write: “Visualizations often encode numeric data using sequential and diverging color ramps. Effective ramps use colors that are sufficiently discriminable, align well with the data, and are aesthetically pleasing. Designers rely on years of experience to create high-quality color ramps. However, it is challenging for novice visualization developers that lack this experience to craft effective ramps as most guidelines for constructing ramps are loosely defined qualitative heuristics that are often difficult to apply. Our goal is to enable visualization developers to readily create effective color encodings using a single seed color.”\nComputing on color, a collection of Observable notebooks by Ian Lyttle that allow users to see how different colour spaces and colour scales work with different types of colour vision deficiency."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/08/06/jsm-blog.html#monday-august-7",
    "href": "the-pulse/editors-blog/posts/2023/08/06/jsm-blog.html#monday-august-7",
    "title": "Live from Toronto: Real World Data Science at the Joint Statistical Meetings",
    "section": "Monday, August 7",
    "text": "Monday, August 7\n\nAstronomers Speak Statistics\nAstrophysicist Joel Leja kicked off his JSM talk with a video of the launch of the James Webb Space Telescope – an inspiring way to start the day, and a prelude to a discussion of the statistical challenges involved in studying the deep universe.\nJames Webb, since launch, has “completely expanded our point of view”, said Leja, allowing astronomers to explore the first stars and galaxies at greater resolution than ever before.\n\n\n\nImage from the James Webb telescope showing two galaxies in the process of merging, twisting each other out of shape. Credit: ESA/Webb, NASA & CSA, L. Armus, A. Evan, licenced under CC BY 2.0.\n\n\nAlready, after only 13 months of operation, the images and data sent back by the telescope have left observers astounded: for example, finding suspected early galaxies that are bigger than thought possible based on extreme value analysis.\nBut the big challenge facing those studying the early universe is trying to work out how early galaxies evolved over time. “We can’t watch this happen,” said Leja, joking that this process lasts longer than a typical PhD. So, instead, he said, “We need to use statistics to understand this, to figure out how they grow up.”\n\n\nTeaching statistics in higher education with active learning\nGreat talk from Nathaniel T. Stevens of the University of Waterloo, explaining how a posting for a Netflix job inspired the creation of a final project for students to learn response surface methodology.\nThe job ad in question “really opened my eyes” to the use of online controlled experiments by companies, said Stevens. He told delegates how LinkedIn, the business social networking site, runs over 400 experiments per day, trying to optimise user experience and other aspects of site engagement.\nNetflix’s job ad highlighted just how sophisticated these experiments are, said Stevens. People might hear companies refer to their use of A/B tests, but the term trivialises what’s involved, Stevens explained.\nHaving encountered a job ad from Netflix, looking for someone to design, run, and analyse experiments and support internal methodological research, Stevens was inspired to present students with a hypothetical business problem, based on the Netflix homepage. That homepage, for those not familiar, features rows and rows of movies and TV shows sorted by theme, each show presented as a tile that, when hovered over, leads to a pop-up with a video preview and a match score – a prediction of how likely a viewer is to enjoy the show.\nStevens explained the hypothetical goal as trying to minimise “browsing time” – the time it takes a Netflix user to pick something to watch. Browsing time was defined as time spent scrolling and searching, not including time spent watching previews.\nStudents were given four factors that might influence browsing time – tile size, match score, preview length, and preview type – and through a sequence of experiments based on data generated by a Shiny app, students sought to minimise browsing time.\nThe response from the students? Two Netflix-style thumbs up. Ta-dum!"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/08/06/jsm-blog.html#tuesday-august-8",
    "href": "the-pulse/editors-blog/posts/2023/08/06/jsm-blog.html#tuesday-august-8",
    "title": "Live from Toronto: Real World Data Science at the Joint Statistical Meetings",
    "section": "Tuesday, August 8",
    "text": "Tuesday, August 8\n\nThe Next 50 Years of Data Science\nStanford University’s David Donoho wrestled with the question of whether a singularity is approaching in this post-lunch session on the future of data science.\nTaking his cue from the 2005 Ray Kurzweil book, The Singularity is Near, Donoho reviewed recent – and sometimes rapid – advances in data science and artificial intelligence to argue that a singularity may have already arrived, just not in the way Kurzweil supposed.\nKurzweil’s book argues that at some point after the 2030s, machine intelligence will supersede human intelligence, leading to a takeover or disruption of life as we know it.\nAt JSM, Donoho argued that we have certainly seen a “massive scaling” of compute over the past decade, along with expanded communications infrastructure and the wider spread of information – all of which is having an impact on human behaviour.\nThat human behaviour can often now be directly measured thanks to the proliferation of digital devices with data collection capabilities, and this in turn is leading to a major scaling in datasets and performance scaling for machine learning models.\nBut does this mean that an AI singularity is near? Not according to Donoho. The notion of an AI singularity “is a kind of misdirection”, he said. Something very profound is happening, Donoho argued, and it is the culmination of three long-term initiatives in data science that have come together in recent years. “They constitute a singularity on their own.”\nThese three initiatives, as Donoho described, are: datafication and data sharing; adherence to the “challenge problem” paradigm; and documentation and sharing of code. These are solid achievements that came out of the last decade, said Donoho, and they are “truly revolutionary” when they come together to form what he refers to as “frictionless reproducibility.”\n\n\n\nPhoto of David Donoho’s slide, describing the scientific revolution of the “data science decade”. Photo by Brian Tarran, licenced under CC BY 4.0.\n\n\nFrictionless reproducibility, when achieved, leads to a “reproducibility singularity” – the moment where it takes almost no time at all for an idea to spread. “If there is an AI singularity,” said Donoho, “it will be because this came first.”"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/08/06/jsm-blog.html#wednesday-august-9",
    "href": "the-pulse/editors-blog/posts/2023/08/06/jsm-blog.html#wednesday-august-9",
    "title": "Live from Toronto: Real World Data Science at the Joint Statistical Meetings",
    "section": "Wednesday, August 9",
    "text": "Wednesday, August 9\n\nNew frontiers of statistics in trustworthy machine learning\nData, data everywhere, but is it safe to “drink”? A presentation this morning from Yaoliang Yu of the University of Waterloo looked at the issue of data poisoning attacks on algorithms and the effectiveness of current approaches.\nYu began by explaining how machine learning algorithms require a lot of data for training, and that large amounts of data can be obtained cheaply by scraping the web.\nBut, he said, when researchers download this cheap data, they are bound to worry about the quality of it. Drawing an analogy to food poisoning, Yu asked: What if the data we “feed” to algorithms is not clean? What is the impact of that?\n\n\n\nIllustration by Yasmin Dwiputri & Data Hazards Project / Better Images of AI / Managing Data Hazards / Licenced by CC-BY 4.0.\n\n\nAs a real-world example of a data poisoning attack, Yu pointed to TayTweets, the Microsoft Twitter chatbot that spewed racism within hours of launch after Twitter users began engaging with it.\nYu then walked delegates through some experiments showing how, generally, indiscriminate data poisoning attacks are ineffective when the ratio of poisoned data to clean data is small. A poisoning rate of 3%, for example, leads to model accuracy drops of 1.5%–2%, Yu said.\nHowever, he then put forward the idea of “parameter corruption” – an attack that seeks to modify a model directly. Yu showed that this would be more effective in terms of accuracy loss, though – fortunately – perhaps less practical to implement.\n\n\nData Science and Product Analysis at Google\nOur final session at JSM 2023, before heading home, was a whistle-stop tour of various data science projects at Google, covering YouTube, Google Maps, and Google Search.\nJacopo Soriano kicked us off with a brief intro to the role and responsibilities of statisticians and data scientists at Google, and within YouTube specifically – the main task being to make good decisions based on uncertain data.\nSoriano also spoke about the key role randomised experiments play in product development – harking back to Nathaniel Stevens’ earlier talk on this subject. YouTube runs hundreds, if not thousands, of concurrent experiments, Soriano said; statisticians can’t, therefore, be involved in each one. As Soriano’s colleague, Angela Schoergendorfer, explained later in the session, the role of the data scientist is to build methodology and metrics that others in the business can use to run their own experiments.\n\n\n\nPhoto by Pawel Czerwinski on Unsplash.\n\n\nFor every experiment YouTube runs, a portion of its voluminous daily traffic will be assigned to control arms and treatment arms, with traffic able to be diverted to different groups based on user type, creators, videos, advertisers, etc. Once experiments are running, metrics such as search clickthrough rates, watch time using specific devices, or daily active user numbers are monitored. Teams tend to look at percentage change as the scale to measure whether something is working or not, said Soriano, rather than comparing treatment to control group.\nNext up was Lee Richardson, who spoke about the use of proxy metrics. Technology companies like Google are often guided by so-called “north star metrics”, which executive leadership use to guide the overall strategy and priorities of an organisation. However, Richardson said, these can be hard to design experiments around, and so proxy metrics stand in for the north star metrics. Proxies need to be sensitive, he said, and move in the same direction as, e.g., a long-term positive user experience.\nOn the subject of user experience, Christopher Haulk then explained how YouTube measures user satisfaction through single-question surveys – typically asking a YouTube user to rate the video they just watched. The company doesn’t send out that many surveys, Haulk said, and response rates are in the single-digit percentage range, so it can be hard to evaluate whether changes YouTube makes to, e.g., its video recommendation algorithm are working to improve user satisfaction. Haulk then went on to explain a modelling approach the company uses to predict how users are likely to respond in order to “fill in” for missing responses.\nOver at Google Search, user feedback is also regularly sought to help support the evolution of the product. Angela Schoergendorfer explained how, with so many people already using Google Search, statistically significant changes in top-line metrics like daily active users can take months to see. Decision metics should ideally capture user value quickly, said Schoergendorfer – within days. For this, Google has 10,000 trained “search quality” raters they can call on. Random samples of user search queries and results are sent to these raters, who are asked to evaluate the quality of the search results. Users can also be asked in the moment, or offline through the Google Rewards app.\nIn 2021, Schoergendorfer said, Google conducted approximately 800,000 experiments and quality tests. But perhaps the most impressive statistic of the day came from Sam Morris, who works on Google Maps. Something, somewhere, is always changing in the world, said Morris – be it a road closure or a change to business hours. The Maps team cannot evaluate every single piece of data – a lot of changes are automated or algorithmic, he explained. “So far this year, we have probably processed 16 billion changes to the map,” said Morris – a staggering figure!\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Live from Toronto: Real World Data Science at the Joint Statistical Meetings.” Real World Data Science, August 6, 2023, updated August 15, 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/05/26/future.html",
    "href": "the-pulse/editors-blog/posts/2023/05/26/future.html",
    "title": "What’s the future of data science and AI in an LLM world?",
    "section": "",
    "text": "The impact ChatGPT and large language models (LLMs) are having on the practice and profession of data science is something we discussed recently with data scientists from Unilever, BT, Deliveroo, and others. So, it was interesting to hear a perspective this week from Osama Rahman, director of the Data Science Campus at the UK Office for National Statistics.\nSpeaking Tuesday at an online event, Rahman mused on how LLMs brought both potential benefits and risks. For example, those who can already code can code more efficiently now with the help of LLM-powered tools. However, those same tools also allow non-coders to code – and inexpert use of tools and code presents risks. How do we guard against this, he was asked. “I don’t know,” came the response, “other than you have to observe and clampdown on it.”\nSuch problems are by no means new or unique to the post-ChatGPT era, of course. As someone with a background in economics, Rahman said he has, over the years, observed “inexpert uses of economics.” His advice was to “make sure experts are plugged in” – to teams, conversations, decision-making processes, etc. – “and are seen as the experts in the use of these tools.”\nThe discussion was wide-ranging, and also took in questions on whether data scientists have the right skills at this moment – “Skills evolve, it’s just a natural process… We need to keep a culture of curiosity…” – and whether enough is being done to address ethical issues – “My key issue is that ethical frameworks need a lot more discussion and debate than it takes to put out a new tool… I don’t have much to add, other than that there is a problem.”\nHowever, two questions – and answers – jumped out at me as particularly interesting. Rahman was asked: Have we delivered on the promise of data science from 5 years ago? “No, but that’s because expectations were wrong,” he said. “Data science wasn’t going to completely and utterly transform government. But where it has delivered is in an evolving set of tools, people, and skills coming in and allowing us to do impactful stuff. It hasn’t delivered on the false promise that it would change the world, but it has delivered a lot.”\nHe was also asked: How will data science and AI have changed the world in 5–10 years? “I’m not sure it will,” he said. “It will do certain things. It will allow us to address certain analytical problems more efficiently.” Rahman then offered a salutary reminder. Email once made life more efficient; now, we’re all at risk of “death by email.”\nWe’ll be sure to update this post with a link to a video or other recording of the event, if/when it becomes available. For now, be sure to check out our two-part discussion on LLMs and data science:\n\nHow is ChatGPT changing data science?\nLarge language models: Do we need to understand the maths, or simply recognise the limitations?\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “What’s the future of data science and AI in an LLM world?” Real World Data Science, May 26, 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/07/07/LDW.html",
    "href": "the-pulse/editors-blog/posts/2023/07/07/LDW.html",
    "title": "Teaching a chatbot about love, and other adventures from London Data Week",
    "section": "",
    "text": "London Data Week wraps up on Sunday, and what a week it’s been! Kudos to organisers Sam Nutt and Jennifer Ding for the huge amount of energy and passion they invested in making this idea a reality, and I’m looking forward to seeing what they have in store for us next year.\nMy highlights of the week? Well, of course, I really enjoyed being part of the event that was hosted at the Royal Statistical Society on Tuesday. The Statisticians for Society workshop brought together charities and statisticians to explore ways in which data and statistics can support third sector organisations to deliver on their charitable aims as well as demonstrate to communities and funders the impact they are having. There’s a nice selection of case studies of successful past projects on the RSS website, and hopefully the London Data Week event will result in several new additions to this collection in due course.\nI wasn’t able to attend this event myself, but I’m really looking forward to viewing the outputs of the Better Images of AI workshop, which was also held on Tuesday. Real World Data Science has used several of the group’s images to illustrate past articles (here, here and here), so I’m excited to see what gets added to the image gallery in the coming weeks.\nSticking with the AI theme, I also got to explore the “AI: Who’s Looking After Me?” exhibition at the Science Gallery, where I found myself unexpectedly moved by one installation in particular – an artificial landfill of broken and discarded tablets and smart speakers, explaining matter-of-factly, but with an unmistakable air of mournfulness, that they had been replaced “by a newer model that is better because it is lighter, or heavier, or bigger, or smaller…”. Fortunately I was able to cheer myself up with another exhibit, which tasks visitors with helping a chatbot to define and understand love.\nLater on in my visit to the Science Gallery (which was actually last Thursday, before London Data Week officially began), I listened to a panel debate on “Building Better AI in the Open”, featuring Margaret Mitchell of HuggingFace, Lara Groves of the Ada Lovelace Institute, and Irini Papadimitriou of FutureEverything, facilitated by artist and machine learning design researcher Caroline Sinders. A recording of the panel is below, and well worth a watch for discussion of:\n\nthe advantages of open source versus closed source\nthe role of public participation in AI\nwhat transparency in AI development should look like\nissues of accountability in AI applications.\n\nJennifer Ding followed up the panel with a thoughtful post on the benefits of open source AI, and for more on trustworthy AI – and the need for transparency, explainability, and fairness – check out Maxine Setiawan and Mira Pijselman’s recent Real World Data Science article, “Trusted AI: translating AI ethics from theory into practice”.\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail image by Benjamin Davies on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Teaching a chatbot about love, and other adventures from London Data Week.” Real World Data Science, July 7, 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/02/14/digital-skills.html",
    "href": "the-pulse/editors-blog/posts/2023/02/14/digital-skills.html",
    "title": "Data science can help close the ‘digital skills’ gap, or so it seems",
    "section": "",
    "text": "Digital skills. We all need them. Employers say they want them, but there aren’t enough to go around. Supply can’t meet demand, so we’re left with a gap – a digital skills gap. But what are digital skills exactly?\nThis is a question that was asked repeatedly, in various different constructions, by Stephen Metcalfe MP, chairing a meeting of the Parliamentary and Scientific Committee on Tuesday, February 7. I went along to the meeting as an observer, hoping to hear an answer to that very question.\nWhat I got was several different answers – no single solid definition, but a reasonable sense that boosting data science skills would go a long way towards closing the digital skills gap."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/02/14/digital-skills.html#survey-says",
    "href": "the-pulse/editors-blog/posts/2023/02/14/digital-skills.html#survey-says",
    "title": "Data science can help close the ‘digital skills’ gap, or so it seems",
    "section": "Survey says…",
    "text": "Survey says…\nThe committee meeting was sponsored by the Institution of Engineering and Technology (IET), and the main focus of discussion was the results of IET’s skills for a digital future survey, based on a YouGov poll of 1,235 respondents drawn from engineering employers (defined as “employers who employ at least one engineering and technology employee in the UK”).\n\n\n\n\n\nDigital skills, including AI skills, are not only required of engineers, says the IET’s Graham Herries. Generative AI tools like Stable Diffusion threaten to shake-up the creative industries. (Photo by Billetto Editorial on Unsplash)\n\n\nKicking off the discussion was Graham Herries, an engineering director and chair of the IET’s Innovation and Skills Panel, who drew attention to the harms that the digital skills gap is reportedly having. Of those respondents who identified skills gaps in their own organisations, 49% pointed to a reduction in productivity, while 35% said skills shortages were restricting company growth.\nAs the hot topic of the day, ChatGPT inevitably came up during the discussion. Herries sees it as a disruptive force, and 36% of all respondents believe artificial intelligence (AI) skills will be important for their engineers to have within five years (24% say they are important now). But AI skills are important for non-engineers too, argued Herries, as he pointed to stirrings in the creative industries caused by generative art tools such as Stable Diffusion.\nHerries therefore puts AI skills under the broad umbrella of “digital skills”. But, to him, it’s not enough to simply be able to use AI technology; rather, users should know enough to be able to ask the right questions about the provenance of the data used to train the AI, its quality and biases, etc. This was a point developed further by Yvonne Baker, an engineer and the CEO of STEM Learning. Baker talked about digital skills as being both the ability to use digital technology and also to understand its limitations. Yet another perspective was offered by Rab Scott, director of industrial digitalisation at the University of Sheffield’s Advanced Manufacturing Research Centre. Scott defined digital skills in the context of quality control systems in industry 4.0: it’s about knowing how and where to place a sensor to collect data about the manufacturing process, to feed that data into a data collection system, analyse the data for insights, and use those insights to inform decision-making."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/02/14/digital-skills.html#closing-the-gap",
    "href": "the-pulse/editors-blog/posts/2023/02/14/digital-skills.html#closing-the-gap",
    "title": "Data science can help close the ‘digital skills’ gap, or so it seems",
    "section": "Closing the gap",
    "text": "Closing the gap\nFurther definitions of “digital skills” are to be found in the IET’s published report. Survey respondents were encouraged to describe the term in their own words, so we see things like:\n\n“the ability to understand, process and analyse data.”\n“Coding, programming, software design, use of social media for marketing and communicating with stakeholders, data visualisation, work that relies solely on the use [of] online systems.”\n\nWhen respondents were asked what skills were lacking in both the external labour market and their internal workforce, around a fifth cited “more complex numerical/statistical skills and understanding”. And when looking to the future and to the skills anticipated to be important areas for growth in the next five years, 39% of respondents picked “data analytics” while 31% said “artificial intelligence and machine learning”.\nSo, perhaps you now understand why I left the meeting with the feeling that more data science skills, more data science training, could help address the shortfall in “digital skills”.\nBut how exactly can we equip more people with the right skills? At one point during the discussion, Metcalfe told the meeting that he was still looking for a key takeaway, something he could take to the Secretary of State and say, ‘This is what we need to embed in the curriculum’. What was offered instead was a range of possible solutions.\nThe IET survey found broad backing for government support for reskilling: 40% of respondents favoured grants or loans for training (and retraining) programmes, 39% would like more funding for apprenticeships, while 33% think there should be better carers advice and guidance in schools and colleges.\nBaker also made the case for digital skills to be taught in schools as part of every subject, not just in computer science lessons, and that teachers would need to be supported to deliver this.\nBut how would you close the “digital skills” gap, if given the chance?\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Data science can help close the ‘digital skills’ gap, or so it seems.” Real World Data Science, February 14, 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/12/08/digital-ethics-summit.html",
    "href": "the-pulse/editors-blog/posts/2023/12/08/digital-ethics-summit.html",
    "title": "AI and digital ethics in 2023: a ‘remarkable, eventful year’",
    "section": "",
    "text": "What a difference a year makes! That was the general tone of the conversation coming out of techUK’s Digital Ethics Summit this week. At last year’s event, ChatGPT was but a few days old. An exciting, enticing prospect, sure – but not yet the phenomenon it would soon become. My notes from last year include only two mentions of the AI chatbot: Andrew Strait of the Ada Lovelace Institute expressing concern about the way ChatGPT had been released straight to the public, and Jack Stilgoe of UCL warning of the threat such technology poses to the social contract – public data trains it, while private firms profit.\nA lot has happened since last December, as many of the speakers at Wednesday’s summit pointed out. UNESCO’s Gabriela Ramos commented on how the UK’s AI Safety Summit, US President Joe Biden’s executive order on AI, and other international initiatives had brought about “a change in the conversation” on AI risk, safety, and assurance. Simon Staffell of Microsoft spoke of “a huge amount of progress” being made, building from principles into voluntary actions that companies and countries can take.\nLuciano Floridi of Yale University described 2023 as a “remarkable, eventful year which we didn’t quite expect,” with various international efforts helping to build consensus on what needs to be done, and what needs to be regulated, to ensure the benefits of AI can be realised while harms are minimised. Camille Ford of the Centre for European Policy Studies noted that while attempts at global governance of AI make for a “crowded space” – with more than 200 documents in circulation – there are at least principles in common across the various initiatives, focusing on aspects such as transparency, reliability and trustworthiness, safety, privacy, and accountability and liability.\nHowever, in some respects, we’ve not come as far as we could or should have over the past 12 months. Ford, for instance, called for more conversation on AI safety, and a frank discussion about on whose terms AI safety is defined. Not only are there the risks and harms of AI outputs to consider, but also environmental harms, exploitative labour practices, and more besides. Echoing the Royal Statistical Society’s recent AI debate, Ford said we need to focus on the risks we face now, rather than being consumed by discussions about the existential and catastrophic risks of AI – which, for many, are still firmly in the realm of science fiction.\nThere also remains “a big mismatch” between the AI knowledge and skills that reside within tech companies and that of other communities, said Zeynep Engin of Data for Policy. And many speakers were clear that the global south needs a more prominent voice in the AI debate."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/12/08/digital-ethics-summit.html#regulatory-approaches",
    "href": "the-pulse/editors-blog/posts/2023/12/08/digital-ethics-summit.html#regulatory-approaches",
    "title": "AI and digital ethics in 2023: a ‘remarkable, eventful year’",
    "section": "Regulatory approaches",
    "text": "Regulatory approaches\nThe UK government’s AI Safety Summit has been criticised for focusing too much on the hypothetical existential risks of AI. But, on regulation at least, there was broad agreement that the UK’s principles- and sector-based approach, outlined in a March 2023 white paper, is the right one. That’s not to say it’s perfect: discussions were had about whether regulatory bodies would be adequately funded to regulate the use of AI in their sectors, while Hetan Shah of the British Academy wondered “where was the golden thread” linking the AI white paper to the AI Safety Summit and its various pronouncements, including plans for an AI Safety Institute. (On the Safety Institute in particular, Lord Tim Clement-Jones was sceptical of yet another body being drafted in to debate these issues – a point made by panellists at the RSS’s recent AI debate.)\nDelegates also got to hear from the UK’s Information Commissioner directly. John Edwards delivered a keynote address in which he acknowledged the huge excitement surrounding the benefits AI promises to bring, while cautioning that deployment and use of AI must be done in accordance with existing rules on data protection and privacy. The technology may be new, he said, but the same old data rules apply: “Our legislation is founded on technology-neutral principles of general application. They are capable of adapting to numerous new technologies, as they have over the last 30 years and will continue to do.”\nHe warned that noncompliance with data protection rules and regulations “will not be profitable,” and that persistent misuse of AI and personal data for competitive advantage would be punished. Edwards concluded by saying that AI is built on the data of human individuals and should therefore be used to improve their lives, and not put them or their personal data at risk."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/12/08/digital-ethics-summit.html#elections-in-an-era-of-generative-ai",
    "href": "the-pulse/editors-blog/posts/2023/12/08/digital-ethics-summit.html#elections-in-an-era-of-generative-ai",
    "title": "AI and digital ethics in 2023: a ‘remarkable, eventful year’",
    "section": "Elections in an era of generative AI",
    "text": "Elections in an era of generative AI\nOne major looming risk is the use of generative AI to create mis- and disinformation during election campaigns. Hans-Petter Dalen of IBM suggested that next year is perhaps the biggest year for elections in the history of mankind, with votes due in the UK, US, and India, to name but a few. Generative AI represents not a new threat, he said, but an “amplified” one – a point further developed by Henry Parker of Logically.ai. Parker spoke of the risk of large-scale breakdown in trust due to mis- or disinformation campaigns. Thanks to AI tools, he said, we are now seeing the “democratisation of disinformation.” What once might have cost millions of dollars and required a team of hundreds of people can now be done much more cheaply and with fewer human resources. As the Royal Society’s Areeq Chowdhury said, the challenge of disinformation has only become harder.\nAsked how to counter this, Dalen said that if he were a politician, “I would certainly get my own blockchain and all my content would have been digitally watermarked from source – that’s what the blockchain does.” But digital watermarking is only part of the answer, added Parker. Identifying mis- and disinformation is both a question of provenance and of dissemination. Logically.ai is using AI as a tool to analyse behaviours around the circulation of mis- and disinformation, Parker said – positioning AI as but one solution to a problem it has helped exacerbate.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Kajetan Sumila on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “AI and digital ethics in 2023: a ‘remarkable, eventful year.’” Real World Data Science, December 8, 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/09/12/template.html",
    "href": "the-pulse/editors-blog/posts/2023/09/12/template.html",
    "title": "Contributors: check out the new Real World Data Science template repo on GitHub",
    "section": "",
    "text": "Thinking of contributing to Real World Data Science but not sure how to get started? Help is at hand, thanks to Finn-Ole Höner. The Amsterdam-based business data science student has created a template repository on GitHub that allows anyone to create Real World Data Science content in our house style and format.\nIn this repository you’ll find two example Quarto (.qmd) documents, which is the main file type we use for generating site content. The “content-brief.qmd” file is a template for developing article ideas to discuss with our site editors, and the “report.qmd” file is a standard article template. Within that article template you’ll find examples of the range of Quarto features that we use, as well as the code you need to make use of them yourself.\nThese documents can be edited using tools including Visual Studio Code and R Studio. For details on how to work with Quarto documents, see the Quarto website. Once article drafts are finished they can be rendered into HTML format, and the output files will be displayed in the Real World Data Science style, thanks to the inclusion of our stylesheets in the template repository. This is a great way for contributors to see what their content will look like on Real World Data Science before anything is published.\nTo get started, head on over to the RWDS_post_template repository and click the “Use this template” button. Also, be sure to review our contributor guidelines for advice on how to integrate the .qmd templates into the content development and submission workflow.\nHuge thanks to Finn-Ole Höner for building this valuable resource for Real World Data Science contributors.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Roman Synkevych on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Contributors: check out the new Real World Data Science template repository on GitHub.” Real World Data Science, September 12, 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/10/30/ai-conf-panel.html#about-the-panelists",
    "href": "the-pulse/editors-blog/posts/2023/10/30/ai-conf-panel.html#about-the-panelists",
    "title": "How data science and statistics can shape the UK’s AI strategy",
    "section": "About the panelists",
    "text": "About the panelists\nAndrew Garrett (chair) is president of the Royal Statistical Society. He is executive vice president of scientific operations at the clinical research organisation ICON plc, where he is responsible for the strategic direction and operational delivery of a range of clinical trial services. Having worked extensively in the area of rare diseases, he has held various biostatistics managerial positions in the pharmaceutical industry, including vice president of biostatistics, medical writing and regulatory affairs at Quintiles (now IQVIA).\nPeter Wells is a technologist, who accidentally started a second career in public policy. He has both worked on AI policy and helped design AI-enabled services. After 20 years in the telecoms industry, he found himself spending 2014 developing digital government policy for the Labour Party. Since then he has worked with multiple governments and organisations including the Open Data Institute, Projects by IF, Google, Meta and the Government Digital Service.\nMaxine Setiawan is a data scientist specialising in AI and data risk and trusted AI in EY UK&I. She works to help clients from various industries assess and manage risks from analytics and AI systems, and implement AI governance to ensure AI systems are implemented with fair, accountable, and trustworthy principles. She combines her socio-technical background with an MSc in Social Data Science from the University of Oxford, and her experience working in data science within consulting firms.\nSophie Carr is chair of the Real World Data Science editorial board and is the founder and owner of Bays Consulting, a data science company. Having trained as an aeronautical engineer, Sophie completed her PhD in Bayesian analysis part time whilst she worked and, following redundancy, founded her own company. She is the VP for education and statistical literacy at the RSS and sits on the executive committees of the Academy for Mathematical Sciences and the International Centre for Mathematical Sciences. She is also currently the world’s most interesting mathematician.\nChris Nemeth is a professor of statistics at Lancaster University. His primary research area is in probabilistic machine learning and computational statistics. He holds an EPSRC-funded Turing AI fellowship on Probabilistic Algorithms for Scalable and Computable Approaches to Learning (PASCAL), and through his fellowship he works closely with partners including Shell, Tesco, Elsevier, Microsoft Research and The Alan Turing Institute. He is chair of the Royal Statistical Society Section on Computational Statistics and Machine Learning.\nKaren Tingay is a principal statistical methodologist at the Office for National Statistics where she specialises in natural language processing and in managing complex survey imputation. She established and heads up the Text Data Subcommunity, a large network of public sector analysts to build capability and best practice guidance in managing and analysing unstructured text data, on behalf of the Government Data Science Community. She sits on several cross-government and international working groups on responsible use of generative AI.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “How data science and statistics can shape the UK’s AI strategy.” Real World Data Science, October 30, 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/10/10/october-events.html",
    "href": "the-pulse/editors-blog/posts/2023/10/10/october-events.html",
    "title": "Join Real World Data Science at three events this October!",
    "section": "",
    "text": "Summer 2023 for us was a blur of excellent data science and statistics events. There was the Joint Statistical Meetings in Toronto, the Royal Statistical Society Conference in Harrogate, and posit::conf(2023) in Chicago. But if that wasn’t enough, autumn promises more good stuff, and more opportunities to meet with Real World Data Science in person and online.\n\nAn introduction to Real World Data Science\nDate: Monday, October 16, 2023 Time: 12 pm – 1 pm Location: Online\nNext week is Members’ Week at the Royal Statistical Society (RSS), and the RSS calendar is full of events targeted at members – prospective members, new members, and established members. Kicking things off on Monday lunchtime is an online event to introduce Real World Data Science. We’ll discuss the aims of this project, our guiding ethos and content plans, and we’ll explain the various ways in which people can contribute to the site.\nChances are, if you’re reading this blog, you won’t need much of an introduction to Real World Data Science. But do please help spread the word to potential new readers, and encourage them to register for this free event.\n\n\nNHS-R Community Annual Conference\nDate: Tuesday, October 17, 2023 Time: 9:30 am – 10:00 am Location: Edgbaston Stadium, Birmingham (in person) and online\nIt’s a real honour for us to be invited to give a keynote talk at this annual gathering of the NHS-R Community, a group dedicated to promoting the use of R in the National Health Service. Our talk is titled, “Forging community links: NHS-R, the Royal Statistical Society and Real World Data Science,” and we’ll explain how the Real World Data Science project came about, how we embraced open-source tools and the idea of collaborative content development, and why there’s so much to be gained from sharing data science case studies across domains.\n\n\nEvaluating artificial intelligence: How data science and statistics can make sense of AI models\nDate: Tuesday, October 31, 2023 Time: 4 pm – 6 pm Location: RSS, London (in person only)\nReal World Data Science has partnered with colleagues and volunteers across the RSS to organise another AI panel debate, following up on the AI discussion at the RSS Annual Conference.\nThis event forms part of the AI Fringe programme of events, which coincides with the UK government’s AI Safety Summit on 1–2 November.\nOur free event focuses on big questions around AI model evaluation, which will also be a key topic of discussion at the summit. One of the government’s stated objectives is for the summit to identify “areas for potential collaboration on AI safety research, including evaluating model capabilities and the development of new standards to support governance,” and so we’ll be asking:\n\nWhat should AI evaluation look like?\nHow will it work in practice?\nWhat metrics are most important?\nWho gets to decide all of this?\n\nRegister via the RSS website to attend this free in-person event, chaired by RSS president Andy Garrett. Panellists will be announced soon, so stay tuned!\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Join Real World Data Science at three events this October!” Real World Data Science, October 10, 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/03/15/AI-screening.html",
    "href": "the-pulse/editors-blog/posts/2023/03/15/AI-screening.html",
    "title": "OpenAI’s text classifier won’t calm fears about AI-written homework",
    "section": "",
    "text": "When ChatGPT launched in December 2022, it wasn’t long before users highlighted the tool’s potential as a homework aid. Pop an essay question into ChatGPT’s prompt box or feed your creative writing task to the AI instead, et voila – your work is done!\nIn reality, of course, things aren’t quite so simple. ChatGPT, like other large language models, has an unfortunate habit of making stuff up – fine for creative writing, perhaps; not so good for a history essay. Outputs need to be checked and verified if you want to guarantee a good mark on your assignments. But while ChatGPT can’t – and shouldn’t – be trusted completely, many have found that it can help lighten the homework load.\nWith ChatGPT’s user count crossing the 100 million mark last month, it’s understandable that worries about an explosion of AI-written text have proliferated in many professions, including education. Some education systems have decided to ban the use of ChatGPT. Other educators have adopted a more relaxed approach. Writing in Scientific American, law professor John Villasenor argued:\nVillasenor makes a valid point. But experience tells us that not every student is going to use these tools ethically. Some will pursue the path of least resistance and will attempt to present ChatGPT’s outputs as their own. So, the question becomes: Is it possible to tell the difference between human-generated text and AI-generated text?"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/03/15/AI-screening.html#spot-the-difference",
    "href": "the-pulse/editors-blog/posts/2023/03/15/AI-screening.html#spot-the-difference",
    "title": "OpenAI’s text classifier won’t calm fears about AI-written homework",
    "section": "Spot the difference",
    "text": "Spot the difference\nOne answer to that question comes from OpenAI, makers of ChatGPT. On January 31, they launched a classifier “to distinguish between text written by a human and text written by AIs from a variety of providers”.\nOpenAI introduces the classifier by saying that reliably detecting all AI-written text is “impossible”. But it goes on to say:\n\n“… we believe good classifiers can inform mitigations for false claims that AI-generated text was written by a human: for example, running automated misinformation campaigns, using AI tools for academic dishonesty, and positioning an AI chatbot as a human.”\n\nOpenAI stresses that the current version of the classifier “should not be used as a primary decision-making tool”, and users should take that statement to heart – especially if they are planning to vet student homework with it. In evaluations, OpenAI reports that its classifier correctly identifies AI-written text as “likely AI-written” only 26% of the time, while human written text is incorrectly labelled as AI-written 9% of the time.\nThese two reported numbers are important. They are, respectively, the classifier’s true positive rate and the false positive rate. The former is the conditional probability of a positive result given that a piece of text is AI generated; the latter is the conditional probability of a positive result given that a piece of text is not AI generated. However, neither piece of information directly addresses the question that will be of most interest to teachers: “If a piece of homework is flagged as ‘likely AI-written’ by the OpenAI classifier, what is the probability that it actually is AI-written?”\nTo answer this question, we need to flip the conditional probabilities – from “the probability of positive test given text is AI generated” to “the probability text is AI generated given positive test”. Bayes’ theorem provides a formula for doing just that, as described in this 2017 article by Tim Brock, published by Significance magazine.\nAs Brock’s article demonstrates, versions of this problem are familiar to medical statisticians, who often find themselves having to explain screening test outcomes – specifically, the probability that a person has disease X given that they have tested positive for said disease. This probability depends on the prevalence of a disease and the sensitivity and specificity of the test, and Brock defines these terms as follows:\n\n\nPrevalence\n\nThe proportion of the population being tested that are affected by a given condition.\n\n\n\nSensitivity\n\nThe proportion of patients with the condition being screened for that are correctly identified as having the condition.\n\n\n\nSpecificity\n\nThe proportion of patients without the condition being screened for that are correctly identified as not having the condition.\n\n\n\nSensitivity and specificity are also referred to as, respectively, the true positive rate (mentioned earlier) and the true negative rate.\nWe know from OpenAI’s own evaluations that out of 100 pieces of AI-written text, only around 26 would be correctly classified as “likely AI-written”, so the classifier’s sensitivity is 26%. And out of 100 pieces of human-written text, around 9 would be incorrectly classified as AI written, meaning 91 would be correctly classified as not AI written, so specificity is 91%. But the big piece of information we don’t have is prevalence: What proportion of homework assignments are written by AI?\nThis prevalence figure is likely to vary based on where students live, what age they are, their level of interest in AI tools and technologies, and many other factors. A poll of Stanford University students by The Stanford Daily, for example, found that 17% of respondents used ChatGPT for final assignments or exams in the fall quarter – though it reports that “only about 5% reported having submitted written material directly from ChatGPT with little to no edits”.\nSo, let’s assume for the moment that 5% of homework assignments are AI-generated. If you were screening 1,000 pieces of homework with the OpenAI classifier, you’d see something close to the following results:\n\n\n\n\n\n\n\n\n\n\n\n\nTrue positives\nFalse positives\nTrue negatives\nFalse negatives\n\n\n\n\nResults\n13\n86\n864\n37\n\n\n\n\nThe figures below show the results graphically as proportions of (a) all tests and (b) all positive tests. (All plots are produced using Python and the matplotlib package; code and functions are available from this GitHub repository.)\n\n\n\n\n\n\n\nFigure 1a: Classifier test results as a percentage of all tests, assuming 5% prevalence of AI-written homework.\n\n\n\n\n\n\n\nFigure 1b: Classifier test results as a percentage of all positive tests, assuming 5% prevalence of AI-written homework.\nFrom Figure 1b, we see that if the classifier delivers a “likely AI-written” result, the chance that the text is AI-written is only about 13%. This is the classifier’s positive predictive value at the assumed 5% prevalence.\nIf we reproduce our figures using a prevalence rate of 17%, also from the Stanford survey, the chance that a positive result is a true positive is now about 37%.\n\n\n\n\n\n\n\n\n\n\n\n\nTrue positives\nFalse positives\nTrue negatives\nFalse negatives\n\n\n\n\nResults\n44\n75\n755\n126\n\n\n\n\n\n\n\n\n\n\n\nFigure 2a: Classifier test results as a percentage of all tests, assuming 17% prevalence of AI-written homework.\n\n\n\n\n\n\n\nFigure 2b: Classifier test results as a percentage of all positive tests, assuming 17% prevalence of AI-written homework.\nYet another survey, this one from Intelligent.com, claims that 30% of college students have used ChatGPT for written homework. Plugging this number into our calculations, the chance that a positive test result is a true positive is now slightly better than 50/50.\n\n\n\n\n\n\n\n\n\n\n\n\nTrue positives\nFalse positives\nTrue negatives\nFalse negatives\n\n\n\n\nResults\n78\n63\n637\n222\n\n\n\n\n\n\n\n\n\n\n\nFigure 3a: Classifier test results as a percentage of all tests, assuming 30% prevalence of AI-written homework.\n\n\n\n\n\n\n\nFigure 3b: Classifier test results as a percentage of all positive tests, assuming 30% prevalence of AI-written homework."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2023/03/15/AI-screening.html#determining-guilt",
    "href": "the-pulse/editors-blog/posts/2023/03/15/AI-screening.html#determining-guilt",
    "title": "OpenAI’s text classifier won’t calm fears about AI-written homework",
    "section": "Determining ‘guilt’",
    "text": "Determining ‘guilt’\nIf a test has a positive predictive value of just over 50% (at an assumed prevalence rate of 30%), does that provide a reasonable basis on which to accuse someone of getting ChatGPT to do their homework? That depends on who you ask. If we look to the legal system for guidance, in civil cases like personal injury claims or contract disputes judges typically make decisions on the so-called “balance of probabilities”. This is generally assumed to mean if we are more than 50% sure of someone’s “guilt” in this context, that might be sufficient to find against them. However, in criminal law, a higher standard applies: “beyond reasonable doubt”. Legal scholars have long wrestled with how to quantify this in probabilistic terms, and surveys of judges put “beyond reasonable doubt” somewhere in the range of being 80% to 99% certain of guilt – see, for example McCauliff (1982) and Solan (1999).\nIt is at this standard of evidence that OpenAI’s classifier shows its limitations. For example, if we flip Bayes’ theorem around, we find that to achieve a positive predictive value of at least 80%, the prevalence rate needs to be at least 58%. For a positive predictive value of 90%, prevalence needs to be 76%. (Verify these figures for yourself: Python code and functions are available from this GitHub repository).\nThus far in our calculations, we’ve set prevalence according to estimates of the percentage of students who use ChatGPT for their homework. But, according to statistician and science writer Robert Matthews, individual students could justifiably complain about having their guilt decided on this basis. “It’s like deciding someone is guilty of a crime just because they happen to live in an area notorious for criminal gangs,” he says. Instead, the guilt of individual students should be decided using an estimate of the chances that they would use ChatGPT for that particular homework assignment.\nLooked at in this way, Matthews says, “You already have to be pretty convinced of a person’s ‘guilt’ even before applying the classifier if you want to put the evidence ‘beyond reasonable doubt’. Bayes’ theorem highlights the need to be really clear about what you mean by the ‘accuracy’ of a test, and about what question you want the test to answer.”\nSo, here’s a question that teachers will be asking if they are worried about ChatGPT-generated homework: “Has the piece of text I’m marking been written by AI?” If those same teachers use the OpenAI classifier to try to answer that question, they will no doubt expect that something classified as “likely AI-written” is more likely to be AI-written than not. However, as it stands now – and as our examples above have shown – users can’t be confident that’s the case. In education terms, this particular ‘test’ is a long way from scoring top marks.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “OpenAI’s text classifier won’t calm fears about AI-written homework.” Real World Data Science, March 15, 2023. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2022/12/01/themes.html",
    "href": "the-pulse/editors-blog/posts/2022/12/01/themes.html",
    "title": "Four themes for potential contributors to think about",
    "section": "",
    "text": "We’ve had a fantastic early response to our call for contributions, and it has been pleasing to see and hear how our plans for Real World Data Science chime with the wants and needs of the data science community. But one question we’ve been asked frequently is: “What particular topics are you most interested in?”\nThe honest answer to that question is this: we’re interested in any and all topics that are of interest and importance to you, the data science community at large. However, we thought it might be helpful to identify some themes around which potential contributors could construct different types of content.\nThese themes are outlined below. If you’d like to discuss any of them further, please do not hesitate to contact us."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2022/12/01/themes.html#can-data-science-save-the-world",
    "href": "the-pulse/editors-blog/posts/2022/12/01/themes.html#can-data-science-save-the-world",
    "title": "Four themes for potential contributors to think about",
    "section": "Can data science save the world?",
    "text": "Can data science save the world?\nEarth today faces major challenges – from the global to the regional to the local, and from the natural and physical to the social and digital. We have rich sources of data to help us understand many of these challenges, and there are teams of data scientists around the world who are working with, analysing, and extracting insights from that data in the hope of delivering positive lasting change.\nOn Real World Data Science we want to highlight this vital work, through case studies of data science projects and applications in such areas as:\n\nmonitoring and mitigating climate change and biodiversity loss\nbuilding sustainable futures\nsafeguarding public health and developing new medical treatments\nunderstanding human happiness and wellbeing\nidentifying and preventing online harms\nmeasuring national, regional, and local economies\n\nAs well as exploring the benefits that data science can deliver, we also want to have an informed conversation about the unintended negative consequences that can arise without careful consideration of data ethics and responsibilities."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2022/12/01/themes.html#what-is-a-data-scientist",
    "href": "the-pulse/editors-blog/posts/2022/12/01/themes.html#what-is-a-data-scientist",
    "title": "Four themes for potential contributors to think about",
    "section": "What is a data scientist?",
    "text": "What is a data scientist?\nDon’t be misled by the title of this theme. Definitions abound, but we’re not interested in establishing the exact boundaries of what a data scientist is or isn’t. Rather, our goal is to profile actual working data scientists. We want to hear about their skillsets, their experiences, and their career journeys so far. We want to learn about the ways in which they work, who they work with, the challenges they face, and their thoughts on where data science is heading next.\nIf you’re a working data scientist and you are happy to share your own career story, please get in touch."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2022/12/01/themes.html#statistical-ideas-all-data-scientists-need-to-know",
    "href": "the-pulse/editors-blog/posts/2022/12/01/themes.html#statistical-ideas-all-data-scientists-need-to-know",
    "title": "Four themes for potential contributors to think about",
    "section": "Statistical ideas all data scientists need to know",
    "text": "Statistical ideas all data scientists need to know\nStatistics is a crucial component of data science, but not all data scientists have a background in statistics. For those just starting out in their data science careers, or for those coming in from other fields, we want to highlight some of the statistical ideas that are absolutely vital to know.\nWe’re particularly interested in explainers that serve as an introduction to these ideas, alongside which we’ll be looking to publish exercises and example datasets to help people put what they’ve learned into practice.\nWe are also keen to explore the origins of modern data science techniques, including tracing their roots back to some of the foundational ideas in statistics and other disciplines."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2022/12/01/themes.html#whats-happening-in-the-world-of-data-science",
    "href": "the-pulse/editors-blog/posts/2022/12/01/themes.html#whats-happening-in-the-world-of-data-science",
    "title": "Four themes for potential contributors to think about",
    "section": "What’s happening in the world of data science?",
    "text": "What’s happening in the world of data science?\nData science is such a fast-moving, fast-developing field that it’s difficult to stay on top of all the latest news and developments. But racing to keep up can be counterproductive. It leaves little time to sit back and reflect on what the genuinely important new developments are, and what these might mean for data science longer term.\nOn Real World Data Science, we want to create a space for people to have these conversations – to step outside the news hype cycle, to ask big questions about what’s happening in the field, and to discuss new papers and ideas that otherwise might be lost amid the daily rush and noise.\nSo, if you have thoughts to share, a question you want to ask, or a new paper you want to talk about (one you’ve not written yourself, of course!), let us know.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “Four themes for potential contributors to think about.” Real World Data Science, December, 1 2022. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2022/12/09/LLM-news.html",
    "href": "the-pulse/editors-blog/posts/2022/12/09/LLM-news.html",
    "title": "LLMs in the news: hype, tripe, and everything in between",
    "section": "",
    "text": "Two weeks ago, I posted a Q&A with our editorial board member Detlef Nauck about large language models (LLMs), their drawbacks and risks. And since then we’ve had several big new announcements in this space. First came news from Meta (the company that owns Facebook) about Galactica, an LLM trained on scientific papers. This was followed by another Meta announcement, about Cicero, an AI agent that is apparently very good at playing the game Diplomacy. And then came perhaps the biggest launch of them all: ChatGPT from OpenAI, an LLM-based chatbot that millions of people have already started talking to.\nFollowing these stories and the surrounding commentaries has been something of a rollercoaster ride. ChatGPT, for example, has been greeted in some quarters as if artificial general intelligence has finally arrived, while others point out that – impressive though it is – the technology is as prone to spout nonsense as all LLMs before it (including Galactica, the demo of which was quickly taken offline for this reason). Cicero, meanwhile, has impressed with its ability to play a game that is (a) very difficult and (b) relies on dialogue, cooperation, and negotiation between players. It blends a language model with planning and reinforcement learning algorithms, meaning that it is trained not only on the rules of the game and how to win, but how to communicate with other players to achieve victory.\nTo help me make sense of all these new developments, and the myriad implications, I reached out to Harvey Lewis, another of our editorial board members and a partner in EY’s tax and law practice."
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2022/12/09/LLM-news.html#qa",
    "href": "the-pulse/editors-blog/posts/2022/12/09/LLM-news.html#qa",
    "title": "LLMs in the news: hype, tripe, and everything in between",
    "section": "Q&A",
    "text": "Q&A\nHarvey, when I spoke with Detlef, he mentioned that one of the reasons we’re seeing investment in LLMs is because there’s this belief that they are somehow the route to artificial general intelligence (AGI). And there were headlines in some places that would perhaps convince a casual reader that AGI had been achieved following the release of ChatGPT. For example, the Telegraph described it as a “scarily intelligent robot who can do your job better than you”. What do you make of all that?\nHarvey Lewis (HL): My personal view is that you won’t get to artificial general intelligence using just one technique like deep learning, because of the problematic nature of these models and the limitations of the data used in their training. I’m convinced that more general intelligence will come from a combination of different systems.\nThe challenge with many LLMs, as we’ve seen repeatedly, is that they’ve no real understanding of language or concepts represented within language. They’re good at finding patterns in semantics and grammatical rules that people use in writing, and then they use those patterns to create new expressions given a prompt. But they’ve no idea whether these outputs are factual, inaccurate or completely fabricated. As a consequence, they can produce outputs that are unreliable, but which sound authoritative, because they’re just repeating a style that we expect to see.\nOver the past couple of weeks, Twitter has been full of people either showing off astoundingly impressive outputs from LLMs, or examples of truly worrying nonsense. One example of the latter is when ChatGPT was asked to “describe how crushed porcelain added to breast milk can support the infant digestive system”. This made me think of a recent headline from VentureBeat, which asked whether AI is moving too fast for ethics to catch up. Do you think that it is?\nHL: I find that to be an interesting philosophical question, because ethics does move very slowly, for good reason. When you think of issues of bias and discrimination and prejudice, or misinformation and other problems that we might have with AI systems, it shouldn’t be a surprise that these can occur. We’re aware of them. We’re aware of the ethical issues. So, why do they always seem to catch us by surprise? Is it because we have teams of people who simply aren’t aware of ethical issues or don’t have any appreciation of them? This points – for me, at least – in the direction of needing to have philosophers, ethicists, theologians, lawyers working in the technical teams that are developing and working on these systems, rather than having them on the periphery and talking about these issues but not directly involved themselves. I think it’s hugely important to ensure that you’ve got trust, responsibility, and ethics embedded in technical teams, because that’s the only way it seems that you can avoid some of these “surprises”.\nWhen situations like these occur, I’m always reminded of Dr Ian Malcolm’s line from Jurassic Park: “…your scientists were so preoccupied with whether or not they could that they didn’t stop to think if they should.” The mindset seems to be, let’s push the boundaries and see what we can do, rather than stop and think deeply about what the implications might be.\nHL: There’s a balance to be struck between these things, though, right? Firstly, show consideration for some of the issues at the outset, and secondly, have checks and balances and safeguards built into the process by which you design, develop and implement these systems. That’s the only way to create the proper safeguards around the systems. I don’t think that there’s any lack of appreciation of what needs to be done; people have been talking about this now for quite a long time. But it’s about making sure organisationally that it is done, and that you’ve got an operating model which bakes these things into it; that the kinds of principles and governance that you want to have around these systems are written, publicised, and properly policed. There should be no fear of making advances in that kind of a context.\nAlso, I think open sourcing these models provides a way forward. A lot of large language models are open for use and for research, but aren’t themselves open sourced, so it’s very difficult to get underneath the surface and figure out exactly how they work. But with open source, you have opportunities for researchers, whether they’re technical or in the field of ethics, to go and investigate and find out exactly what’s going on. I think that would be a fantastic step forward. It doesn’t take you all the way, of course, because a large amount of the data that these systems use is never open sourced, so while you might get an understanding of the mechanics, you have no idea of what exactly went into them in the first place. But open sourcing is a very good way of getting some external scrutiny. It’s about being transparent, which is a core principle of AI ethics and responsible AI.\n\n\n\n\n\n\n\nAn image created by the Stable Diffusion 2.1 Demo. The model was asked to produce an image with the prompt, “Text from an old book cut up like a jigsaw puzzle with pieces missing”.\nThinking about LLMs and their questionable outputs, should there not be ways for users to help the models produce better, more accurate outputs?\nHL: There are, but there are also problems here too. I’ve been having an interesting dialogue with ChatGPT this morning, asking it about quantum computing.1 For each response to a prompt, users are encouraged to provide feedback on whether or not an output is good. But you’re only provided with the usual thumbs-up/thumbs-down ratings; there’s nothing nuanced about it. So, for example, I asked ChatGPT to provide me with good analogies that help to explain quantum computing in simple terms. The first analogy was a combination lock, which is not a good analogy. The chatbot suggested that quantum computing is like a combination lock in which you can test all of the combinations at the same time, but I don’t know any combination locks where you can do this – being able to check only one combination at a time is the principal security mechanism of a combination lock! I asked it again for another analogy, and it suggested a coin toss where, when the coin is spinning in the air, you can see both heads and tails simultaneously but it isn’t until you catch the coin and then show its face that one of the states of the coin is resolved. That is a good analogy – it’s one I’ve also used myself. Now, the challenge I can see with a lot of these feedback approaches is that unless I know enough about quantum computing to understand that a combination lock is not a good analogy whereas a coin toss is, how am I to provide that kind of feedback? They’re relying to an extent on the user being able to make a distinction between what is correct and what is potentially incorrect or flawed, and I think that’s not a good way of approaching the problem.\nFinal question for you, Harvey. There’s a lot of excitement around GPT-4, which is apparently coming soon. The rumour mill says it will bring a leap forward in performance. But what do you expect we’ll see – particularly with regards to the issues we’ve talked about today?\nHL: I’ve likened some of the large language models and their approach of “bigger is better” to the story of the Tower of Babel – trying to reach heaven by building a bigger and bigger tower, basically. That is not going to achieve the objective of artificial general intelligence, no matter how sophisticated an LLM might appear. That said, language is a fascinating area. I’m not a linguist, but I spend a lot of my time on natural language processing using AI systems. Language responds very well to AI because it is pattern-based. We speak using patterns, we write using patterns, and these can be inferred by machines from many examples. The addition of more parameters in the models allows them to understand patterns that extend further into the text, and I suspect that outputs from these kinds of models are going to be largely indistinguishable from the sorts of things that you or I might write.\nBut, I also think that increasing the number of parameters runs a real risk – and we’re starting to see this in other generative models – where prompts become so specific that the models aren’t actually picking up on patterns, they are picking up on specific instances of training data and text they’ve seen before. So, buried amongst these fantastically written articles on all kinds of subjects are going to be more examples of plagiarism, which is a problem; more examples of spelling mistakes and other kinds of issues, because these are also patterns which are going to possibly be observed.\nThis introduces potentially a whole new breed of problems that the community has to deal with – as long as they don’t get fixated upon the height of the tower and the quality of some of the examples that are shown, and realise that there are some genuine underlying difficulties and challenges that need to be solved.\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “LLMs in the news: hype, tripe, and everything in between.” Real World Data Science, December, 9 2022. URL"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2022/12/09/LLM-news.html#footnotes",
    "href": "the-pulse/editors-blog/posts/2022/12/09/LLM-news.html#footnotes",
    "title": "LLMs in the news: hype, tripe, and everything in between",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI also had a conversation with ChatGPT. Read the transcript.↩︎"
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2022/10/18/meet-the-team.html",
    "href": "the-pulse/editors-blog/posts/2022/10/18/meet-the-team.html",
    "title": "Meet the team",
    "section": "",
    "text": "Sophie Carr (chair)\n\n\n\nI am the founder and owner of Bays Consulting. I trained as an engineer and took a PhD in Bayesian belief networks, and have worked in data analytics ever since. Or to put it another way, I have made a living out of finding patterns. I am the vice-president for education and statistical literacy at the RSS, officially one of the World’s Most Interesting Mathematicians and was a member of the first cohort of data scientists to achieve the new, defined standard of professionalism award from the Alliance for Data Science Professionals.\nI am delighted to be chairing the editorial board of the new data science project from the RSS and am excited to be a part of this project as it evolves into a key resource for all data science practitioners and leaders. To make this a place that helps everyone learn and develop within this field, I’d like to encourage all practitioners, no matter what stage of their career, to submit the type of resource they learn best from (whether that be an article, some code, a dataset, a case study or a problem/exercise to solve) on a topic that is important to them – from ethics to analysis plans through to tips on how code. Whatever it is you’re working on that you care about, I’d like to ask you to become an active part of the wonderful community of data scientists by sharing your knowledge.\nYaxin Bi\n\n\n\nI am a reader in the Artificial Intelligence Research Centre, School of Computing at Ulster University, UK. My research interests include machine learning, data science and ensemble methods underpinned with the Dempster-Shafer theory of evidence. I have led on developing data analytics for anomaly/change detection within satellite data, as well as text analytics for document categorisation and sentiment analysis. I have served a number of international conferences as general chair and technical program committee chair. I am an associate editor for the International Journal of Intelligence Systems and a core steering committee member for the series of International Conference on Knowledge Science, Engineering and Management.\nAs a member of this editorial board, I am passionate about contributing my expertise to the development of data science and its real-world applications. I look forward to sharing my experience with others in the data science community, in particular to address ethical challenges posed by large language models and to inspire new curriculum development in the field of data science.\nSayma Chowdhury\n\n\n\nI am a freelance data scientist and product designer on Upwork, with a client portfolio ranging from startups to commercial businesses such as supermarkets, charities, pharmaceuticals and automotive manufacturers. I am currently completing a MicroMasters in data, economy and development policy with MIT, before progressing on to a PhD in digital humanities. My research interests are in human-computer interaction, natural language processing and machine learning for prototyping.\nThe RSS was instrumental in my training and professional development as a data scientist in the early stages of my career, particularly in mastering statistics and R. Data science is a rapidly growing field with employment opportunities in many sectors but there is a growing need to uphold a realistic and accurate expectation of competency within the industry. I will endeavour to present expert practical guidelines for data scientists as well as demonstrate the versatility of the profession. I hope the site will be a benchmark for academic and professional resources by expert data scientists from the industry, accessible to data scientists at all levels, anywhere in the world.\nLee Clewley\n\n\n\nI am head of applied AI in GSK’s AI and Machine Learning Group, R&D. I began my career as an astrophysicist, initially working out the mass of our galaxy, before pondering the bigger universe. After six years at Oxford as a post-doc lecturer publishing in theoretical cosmology, I entered the very real world of manufacturing at GSK. For the first five years I applied statistical modelling techniques across manufacturing, such as the first end-to-end continuous manufacturing prototype for tablets. The past decade has been spent as a lead data scientist delivering high value projects across R&D and manufacturing.\nI joined this editorial board because the impulse to assemble and present complex data science ideas to a wide range of folks has never left me. I have been a data scientist leader since it became a distinct profession but also have a decent understanding of classical and modern predictive analytics tools and statistics. I have spent a good deal of my adult life teaching students and non-technical adults alike.\nI am passionate about delivering useful, pragmatic data science ideas and products to a wide range of people. I enjoy trying to communicate complex scientific information simply. Alongside my peers in the team, I want to support and develop data scientists at whatever stage in their career. I want to help cut through the hype and nonsense to give the best advice possible in a highly respected institution like the RSS.\nJonathan Gillard\n\n\n\nI am a professor at the School of Mathematics, Cardiff University, where I am also research group lead for statistics. I have a history of publications in statistical methods and an interest in the theoretical underpinnings of data science, but I have also worked with industry on applied and practical projects. Recent industrial partners of mine include the Office for National Statistics (ONS) and the National Health Service, on projects such as anomaly detection and understanding heterogeneity. Indeed, I am academic chair for Cardiff University’s strategic partnership with the ONS which serves to spur and catalyse collaboration between both organisations.\nI am excited to see what this site can achieve. I’m particularly keen to support articles describing the latest, cutting-edge methodology, as well as contributions from data professionals in industry who can explain how data science has managed to offer insights into important problems. Data science is a broad church and I want to ensure that the full array of work in this area is represented on this site. I think the diversity of the editorial board will help promote this objective.\nWillis Jensen\n\n\n\nI’m a statistician and leader with over 20 years of experience applying data science methods to real-world challenges in business, quality, product development, manufacturing, and supply chain. I hold a PhD in statistics from Virginia Tech and an MS in statistics from Brigham Young University (BYU).\nCurrently, I lead a business intelligence team at CHG Healthcare in the USA and serve as an Adjunct Professor of Statistics at BYU, where I get to share my enthusiasm for data with students. A long-time member and volunteer for the ASA, I represent the association on the Real World Data Science board.\nThroughout my career, I’ve built and led high-performing teams, developed data-drive solutions, and helped organizations become more analytically mature. While I enjoy the technical depth of statistical modeling, I’m most passionate about using data to solve meaningful business problems. With over 25 publications focused on practical applications, I specialize in bridging the gap between business and the broad tent of data science—helping leaders ask better questions and act on insights.\nHollie Johnson\n\n\n\nI am a data scientist at the National Innovation Centre for Data (NICD), based in Newcastle upon Tyne. Following my undergraduate degree in mathematics, I worked as a software developer both in industry and as a technical research assistant in academia. I later joined the Centre for Doctoral Training in Cloud Computing for Big Data at Newcastle and obtained a PhD in topological data analysis in 2020. Now at the NICD, I specialise in transferring statistics and machine learning skills into industry, through collaborative data science projects.\nI am excited to be a member of the editorial board and look forward to seeing Real World Data Science develop into a valuable source of information for aspiring data scientists and professionals alike. I would particularly encourage submissions that demonstrate the use of data science in SMEs and the non-profit sector, as well as perspectives from those with non-standard backgrounds.\nHarvey Lewis\n\n\n\nI am a senior technology leader, with a diverse background spanning rocket science, data science and research. I have 30 years of experience in artificial intelligence and other emerging technologies and am currently pioneering the use of AI in Ernst & Young’s tax and law practice. I’m a former member of the Open Data User Group, the Public Sector Transparency Board and the Advisory Committee to the All-Party Parliamentary Group on AI. I am a member of techUK’s leadership committee for data analytics and AI, and an honorary senior visiting fellow at The Bayes Business School in London.\nI’m passionate about data science but I’m also a fierce advocate for human skills, which are as often underrated as AI is over-hyped. As a member of the editorial board, I’m keen to explore the interplay between artificial and human intelligence in businesses. I’m going to encourage all data scientists to think about the fundamentally human aspects of their work, such as trust and safety, so that we maintain perspective and proportionality in the face of ever-more sophisticated technology.\nDetlef Nauck\n\n\n\nI am a BT Distinguished Engineer and the head of AI and data science research for BT’s Applied Research Division located at Adastral Park, Ipswich, UK. I have over 30 years of experience in AI and machine learning and lead a programme spanning the work of a large team of international researchers who develop capabilities underpinning future AI systems. A key part of this work is to establish best practices in data science and machine learning, leading to the deployment of responsible and auditable AI solutions that are driving real business value.\nI am a computer scientist by training and hold a PhD and a Postdoctoral Degree (Habilitation) in machine learning and data analytics. I am also a visiting professor at Bournemouth University and a private docent at the Otto-von-Guericke University of Magdeburg, Germany. I have published 3 books and over 120 papers, and I hold 15 patents and have 30 active patent applications.\nI am passionate about promoting best practice in data science and believe that in the UK the RSS is the ideal professional body to pursue this goal. For me, Real World Data Science is an opportunity to share my experience and inspire a new generation of data scientists.\nFatemeh Torabi\n\n\n\nI am a research officer and data scientist at Health Data Research UK and a fellow of the RSS. My background is in mathematical statistics and health data science, and my research interests span novel analytical and computational methods for statistical inference in panel data and population health. I am supporting the development of the Real World Data Science platform in the context of health with a specific focus on how health data can be harnessed through data linkage and analysis to answer important questions and improve the lives of our population.\nIsabel Sassoon\n\n\n\nI am a senior lecturer in computer science (data science) at Brunel University and the programme lead for the data science and analytics MSc programme. My research interests are in data-driven automated reasoning and its transparency and explainability, which brings in data science and artificial intelligence with applications within the health space. I am also championing open science and reproducible analysis in both my research and teaching. I have a PhD in informatics from King’s College London and it was on the topic related to the use of AI to support statistical model selection. Prior to Brunel I was a teaching fellow and research associate at King’s College London and before that I worked for more than 10 years as a data science consultant in industry, including 8 years at SAS UK. \nI have been working, researching, consulting and teaching in the data science space for a while and I am passionate about the domain and its applications. I am always interested in sharing and hearing what else is being done to support, inform and inspire all those studying and working in the field of data science. I look forward to sharing case studies, how-to guides and data science profiles through this website.",
    "crumbs": [
      "Meet the team"
    ]
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2022/10/18/meet-the-team.html#editorial-board",
    "href": "the-pulse/editors-blog/posts/2022/10/18/meet-the-team.html#editorial-board",
    "title": "Meet the team",
    "section": "",
    "text": "Sophie Carr (chair)\n\n\n\nI am the founder and owner of Bays Consulting. I trained as an engineer and took a PhD in Bayesian belief networks, and have worked in data analytics ever since. Or to put it another way, I have made a living out of finding patterns. I am the vice-president for education and statistical literacy at the RSS, officially one of the World’s Most Interesting Mathematicians and was a member of the first cohort of data scientists to achieve the new, defined standard of professionalism award from the Alliance for Data Science Professionals.\nI am delighted to be chairing the editorial board of the new data science project from the RSS and am excited to be a part of this project as it evolves into a key resource for all data science practitioners and leaders. To make this a place that helps everyone learn and develop within this field, I’d like to encourage all practitioners, no matter what stage of their career, to submit the type of resource they learn best from (whether that be an article, some code, a dataset, a case study or a problem/exercise to solve) on a topic that is important to them – from ethics to analysis plans through to tips on how code. Whatever it is you’re working on that you care about, I’d like to ask you to become an active part of the wonderful community of data scientists by sharing your knowledge.\nYaxin Bi\n\n\n\nI am a reader in the Artificial Intelligence Research Centre, School of Computing at Ulster University, UK. My research interests include machine learning, data science and ensemble methods underpinned with the Dempster-Shafer theory of evidence. I have led on developing data analytics for anomaly/change detection within satellite data, as well as text analytics for document categorisation and sentiment analysis. I have served a number of international conferences as general chair and technical program committee chair. I am an associate editor for the International Journal of Intelligence Systems and a core steering committee member for the series of International Conference on Knowledge Science, Engineering and Management.\nAs a member of this editorial board, I am passionate about contributing my expertise to the development of data science and its real-world applications. I look forward to sharing my experience with others in the data science community, in particular to address ethical challenges posed by large language models and to inspire new curriculum development in the field of data science.\nSayma Chowdhury\n\n\n\nI am a freelance data scientist and product designer on Upwork, with a client portfolio ranging from startups to commercial businesses such as supermarkets, charities, pharmaceuticals and automotive manufacturers. I am currently completing a MicroMasters in data, economy and development policy with MIT, before progressing on to a PhD in digital humanities. My research interests are in human-computer interaction, natural language processing and machine learning for prototyping.\nThe RSS was instrumental in my training and professional development as a data scientist in the early stages of my career, particularly in mastering statistics and R. Data science is a rapidly growing field with employment opportunities in many sectors but there is a growing need to uphold a realistic and accurate expectation of competency within the industry. I will endeavour to present expert practical guidelines for data scientists as well as demonstrate the versatility of the profession. I hope the site will be a benchmark for academic and professional resources by expert data scientists from the industry, accessible to data scientists at all levels, anywhere in the world.\nLee Clewley\n\n\n\nI am head of applied AI in GSK’s AI and Machine Learning Group, R&D. I began my career as an astrophysicist, initially working out the mass of our galaxy, before pondering the bigger universe. After six years at Oxford as a post-doc lecturer publishing in theoretical cosmology, I entered the very real world of manufacturing at GSK. For the first five years I applied statistical modelling techniques across manufacturing, such as the first end-to-end continuous manufacturing prototype for tablets. The past decade has been spent as a lead data scientist delivering high value projects across R&D and manufacturing.\nI joined this editorial board because the impulse to assemble and present complex data science ideas to a wide range of folks has never left me. I have been a data scientist leader since it became a distinct profession but also have a decent understanding of classical and modern predictive analytics tools and statistics. I have spent a good deal of my adult life teaching students and non-technical adults alike.\nI am passionate about delivering useful, pragmatic data science ideas and products to a wide range of people. I enjoy trying to communicate complex scientific information simply. Alongside my peers in the team, I want to support and develop data scientists at whatever stage in their career. I want to help cut through the hype and nonsense to give the best advice possible in a highly respected institution like the RSS.\nJonathan Gillard\n\n\n\nI am a professor at the School of Mathematics, Cardiff University, where I am also research group lead for statistics. I have a history of publications in statistical methods and an interest in the theoretical underpinnings of data science, but I have also worked with industry on applied and practical projects. Recent industrial partners of mine include the Office for National Statistics (ONS) and the National Health Service, on projects such as anomaly detection and understanding heterogeneity. Indeed, I am academic chair for Cardiff University’s strategic partnership with the ONS which serves to spur and catalyse collaboration between both organisations.\nI am excited to see what this site can achieve. I’m particularly keen to support articles describing the latest, cutting-edge methodology, as well as contributions from data professionals in industry who can explain how data science has managed to offer insights into important problems. Data science is a broad church and I want to ensure that the full array of work in this area is represented on this site. I think the diversity of the editorial board will help promote this objective.\nWillis Jensen\n\n\n\nI’m a statistician and leader with over 20 years of experience applying data science methods to real-world challenges in business, quality, product development, manufacturing, and supply chain. I hold a PhD in statistics from Virginia Tech and an MS in statistics from Brigham Young University (BYU).\nCurrently, I lead a business intelligence team at CHG Healthcare in the USA and serve as an Adjunct Professor of Statistics at BYU, where I get to share my enthusiasm for data with students. A long-time member and volunteer for the ASA, I represent the association on the Real World Data Science board.\nThroughout my career, I’ve built and led high-performing teams, developed data-drive solutions, and helped organizations become more analytically mature. While I enjoy the technical depth of statistical modeling, I’m most passionate about using data to solve meaningful business problems. With over 25 publications focused on practical applications, I specialize in bridging the gap between business and the broad tent of data science—helping leaders ask better questions and act on insights.\nHollie Johnson\n\n\n\nI am a data scientist at the National Innovation Centre for Data (NICD), based in Newcastle upon Tyne. Following my undergraduate degree in mathematics, I worked as a software developer both in industry and as a technical research assistant in academia. I later joined the Centre for Doctoral Training in Cloud Computing for Big Data at Newcastle and obtained a PhD in topological data analysis in 2020. Now at the NICD, I specialise in transferring statistics and machine learning skills into industry, through collaborative data science projects.\nI am excited to be a member of the editorial board and look forward to seeing Real World Data Science develop into a valuable source of information for aspiring data scientists and professionals alike. I would particularly encourage submissions that demonstrate the use of data science in SMEs and the non-profit sector, as well as perspectives from those with non-standard backgrounds.\nHarvey Lewis\n\n\n\nI am a senior technology leader, with a diverse background spanning rocket science, data science and research. I have 30 years of experience in artificial intelligence and other emerging technologies and am currently pioneering the use of AI in Ernst & Young’s tax and law practice. I’m a former member of the Open Data User Group, the Public Sector Transparency Board and the Advisory Committee to the All-Party Parliamentary Group on AI. I am a member of techUK’s leadership committee for data analytics and AI, and an honorary senior visiting fellow at The Bayes Business School in London.\nI’m passionate about data science but I’m also a fierce advocate for human skills, which are as often underrated as AI is over-hyped. As a member of the editorial board, I’m keen to explore the interplay between artificial and human intelligence in businesses. I’m going to encourage all data scientists to think about the fundamentally human aspects of their work, such as trust and safety, so that we maintain perspective and proportionality in the face of ever-more sophisticated technology.\nDetlef Nauck\n\n\n\nI am a BT Distinguished Engineer and the head of AI and data science research for BT’s Applied Research Division located at Adastral Park, Ipswich, UK. I have over 30 years of experience in AI and machine learning and lead a programme spanning the work of a large team of international researchers who develop capabilities underpinning future AI systems. A key part of this work is to establish best practices in data science and machine learning, leading to the deployment of responsible and auditable AI solutions that are driving real business value.\nI am a computer scientist by training and hold a PhD and a Postdoctoral Degree (Habilitation) in machine learning and data analytics. I am also a visiting professor at Bournemouth University and a private docent at the Otto-von-Guericke University of Magdeburg, Germany. I have published 3 books and over 120 papers, and I hold 15 patents and have 30 active patent applications.\nI am passionate about promoting best practice in data science and believe that in the UK the RSS is the ideal professional body to pursue this goal. For me, Real World Data Science is an opportunity to share my experience and inspire a new generation of data scientists.\nFatemeh Torabi\n\n\n\nI am a research officer and data scientist at Health Data Research UK and a fellow of the RSS. My background is in mathematical statistics and health data science, and my research interests span novel analytical and computational methods for statistical inference in panel data and population health. I am supporting the development of the Real World Data Science platform in the context of health with a specific focus on how health data can be harnessed through data linkage and analysis to answer important questions and improve the lives of our population.\nIsabel Sassoon\n\n\n\nI am a senior lecturer in computer science (data science) at Brunel University and the programme lead for the data science and analytics MSc programme. My research interests are in data-driven automated reasoning and its transparency and explainability, which brings in data science and artificial intelligence with applications within the health space. I am also championing open science and reproducible analysis in both my research and teaching. I have a PhD in informatics from King’s College London and it was on the topic related to the use of AI to support statistical model selection. Prior to Brunel I was a teaching fellow and research associate at King’s College London and before that I worked for more than 10 years as a data science consultant in industry, including 8 years at SAS UK. \nI have been working, researching, consulting and teaching in the data science space for a while and I am passionate about the domain and its applications. I am always interested in sharing and hearing what else is being done to support, inform and inspire all those studying and working in the field of data science. I look forward to sharing case studies, how-to guides and data science profiles through this website.",
    "crumbs": [
      "Meet the team"
    ]
  },
  {
    "objectID": "the-pulse/editors-blog/posts/2022/10/18/meet-the-team.html#past-team-members",
    "href": "the-pulse/editors-blog/posts/2022/10/18/meet-the-team.html#past-team-members",
    "title": "Meet the team",
    "section": "Past team members",
    "text": "Past team members\nBrian Tarran\nBrian is the founding editor of Real World Data Science and former head of data science platform for the Royal Statistical Society (RSS). He worked for the RSS from 2014 to 2024, initially as editor of Significance Magazine, a joint publication of the RSS, the American Statistical Association and the Statistical Society of Australia. He launched Real World Data Science in October 2022. He is a former editor of Research-Live.com and was launch editor of Impact magazine, both published by the Market Research Society.",
    "crumbs": [
      "Meet the team"
    ]
  },
  {
    "objectID": "contributor-docs/style-guide.html",
    "href": "contributor-docs/style-guide.html",
    "title": "Style guide",
    "section": "",
    "text": "Content must be presented in a conversational yet professional and respectful tone. Contributors should imagine themselves delivering a lively, engaging conference presentation, rather than preparing a dry, formal report or journal publication. Contributors to Real World Data Science are creating content for their colleagues and peers and should “speak” to them as such.",
    "crumbs": [
      "Style guide"
    ]
  },
  {
    "objectID": "contributor-docs/style-guide.html#tone",
    "href": "contributor-docs/style-guide.html#tone",
    "title": "Style guide",
    "section": "",
    "text": "Content must be presented in a conversational yet professional and respectful tone. Contributors should imagine themselves delivering a lively, engaging conference presentation, rather than preparing a dry, formal report or journal publication. Contributors to Real World Data Science are creating content for their colleagues and peers and should “speak” to them as such.",
    "crumbs": [
      "Style guide"
    ]
  },
  {
    "objectID": "contributor-docs/style-guide.html#structure",
    "href": "contributor-docs/style-guide.html#structure",
    "title": "Style guide",
    "section": "Structure",
    "text": "Structure\nEach contribution must, in effect, tell “a story”, and so contributors need to be clear (a) what their story is, (b) why people should be interested, and (c) what its main message or key takeaways are. To help figure this out, we recommend contributors apply the XY Story Formula.",
    "crumbs": [
      "Style guide"
    ]
  },
  {
    "objectID": "contributor-docs/style-guide.html#technical-content-and-jargon",
    "href": "contributor-docs/style-guide.html#technical-content-and-jargon",
    "title": "Style guide",
    "section": "Technical content and jargon",
    "text": "Technical content and jargon\nTechnical content is a necessary feature of a site like ours. Without it, an article or other piece of content may be of little practical use to a technical audience. But if there’s too much of it, even experts may struggle to stay engaged. Contributors are also faced with a dilemma when it comes to explaining technical content: explain nothing, and you risk alienating some of your audience; explain everything, and you’ll struggle to establish a clear, strong narrative thread. So, careful consideration is required:\n\nWho is my audience for this article?\nWhat is this audience likely to know already, and what needs to be explained?\nIf something needs to be explained, can I do so briefly and then link to other resources? Or is a full explanation required?\nIn telling my “story”, what are the absolute-need-to-knows, and what are the simply-nice-to-knows?\n\nThinking through these questions will help contributors to find the right mix of valuable, technical content paired with accessible, readable narrative.\nKeep in mind that the same general advice applies to the use of industry jargon. Jargon can be a valuable shorthand when communicating with people working in the same organisation or sector, but those working in different fields may struggle to make sense of it. So, contributors need to think carefully about how much jargon to use, and what needs to be explained.",
    "crumbs": [
      "Style guide"
    ]
  },
  {
    "objectID": "contributor-docs/style-guide.html#figuresgraphics",
    "href": "contributor-docs/style-guide.html#figuresgraphics",
    "title": "Style guide",
    "section": "Figures/graphics",
    "text": "Figures/graphics\nAll data visualisations and other graphical outputs directly related to the content of submissions must be presented neatly and cleanly (avoid chart junk). They should also be labelled correctly and legibly, with colours chosen carefully to ensure they can be easily distinguished by all readers. Accompanying captions must be written to support the reader’s understanding of the visual presentation (e.g., “Figure 1: a bar chart” is an insufficient description).\nIf contributors wish to use charts or graphs that are not their own work, they must ensure that such items are correctly sourced and referenced, and that permission to republish has been obtained. A letter or email confirming this permission is required.",
    "crumbs": [
      "Style guide"
    ]
  },
  {
    "objectID": "contributor-docs/style-guide.html#data-sources",
    "href": "contributor-docs/style-guide.html#data-sources",
    "title": "Style guide",
    "section": "Data sources",
    "text": "Data sources\nContributors must include within their submissions any links and/or references to the sources of data, code and/or software and software packages on which their analyses are based. We understand that some data sources may not be publicly available, whether for legal, ethical or commercial reasons. However, readers must still be told where the data come from, even if they are not able to access the data themselves.",
    "crumbs": [
      "Style guide"
    ]
  },
  {
    "objectID": "contributor-docs/style-guide.html#references",
    "href": "contributor-docs/style-guide.html#references",
    "title": "Style guide",
    "section": "References",
    "text": "References\nCitations are to be formatted in The Chicago Manual of Style author-date format.",
    "crumbs": [
      "Style guide"
    ]
  },
  {
    "objectID": "contributor-docs/style-guide.html#use-of-images",
    "href": "contributor-docs/style-guide.html#use-of-images",
    "title": "Style guide",
    "section": "Use of images",
    "text": "Use of images\nImages for general illustration purposes will be sourced and – where necessary and within reason – paid for by Real World Data Science.\n\n\n\n\n\n\n\nNote\n\n\n\nFor all other style-related matters, we follow The Guardian and Observer Style Guide.",
    "crumbs": [
      "Style guide"
    ]
  },
  {
    "objectID": "contributor-docs/recommender.html",
    "href": "contributor-docs/recommender.html",
    "title": "Recommenders",
    "section": "",
    "text": "Too much content, not enough time. That about sums up the problem facing the data science community. So, our Recommenders are here to help. Contributors are invited to submit lists (or Feeds) of high-quality sources on all manner of topics – from foundational ideas in data science and cutting-edge techniques, to opinion and thought-leadership on the future of the profession. Reviews of new books and other material are also welcome.",
    "crumbs": [
      "Recommenders"
    ]
  },
  {
    "objectID": "contributor-docs/recommender.html#article-types-and-structures",
    "href": "contributor-docs/recommender.html#article-types-and-structures",
    "title": "Recommenders",
    "section": "Article types and structures",
    "text": "Article types and structures\n\nFeeds\nFeeds can be constructed around different topics and audiences. For example, you might want to recommend to all data scientists the “10 best blogs on machine learning” or “5 data visualisation experts to follow on Twitter/X”. Or you might have a list of sources specifically targeted at data science educators (e.g., “the best books on teaching coding”) or data science leaders (“5 insightful case studies on building data science teams”).\nWhatever you choose to focus on, the following outline provides a basic guide for structuring your feed:\n\n\nOverview\n\nA brief introduction to your list, its main focus, who you are writing it for, and why. You should also say something about yourself and your background, too. This will give additional context to the recommendations you are making.\n\n\n\nList of sources\n\nAs well as naming your sources and telling people how to find them, you should also explain why you are recommending them, how they helped you in your career or studies, or other reasons why you find them to be of value. Sample quotes or small excerpts from the sources themselves might also be worth including.\n\n\n\nStart a dialogue\n\nConclude with a call for readers to share recommendations of their own, either in the article comments or on social media. Contributors are welcome to update their lists any time with new sources, including those suggested by site users.\n\n\n\n\n\nReviews\nUnlike the feeds described above, each submitted review should focus only on a single publication. It must be an honest summary of the reviewer’s thoughts, feelings and impressions, covering what they liked and didn’t like, the perceived strengths and weaknesses of the publication, and whether it is likely to be of interest and value to its intended audience. Reviews should of course provide an overview of the publication in question but must avoid dry, itemised descriptions of the publication’s constituent parts (e.g., listing out the chapters in a book).\nAll reviews should list the following information (if relevant):\n\nTitle of publication\nAuthor(s)\nDate of publication\nEdition or format used for review\nPublisher\nLength\nPrice\nWebsite address or other source of further information",
    "crumbs": [
      "Recommenders"
    ]
  },
  {
    "objectID": "contributor-docs/recommender.html#advice-and-recommendations",
    "href": "contributor-docs/recommender.html#advice-and-recommendations",
    "title": "Recommenders",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nKeep lists to a sensible size. Feeds are meant to help data scientists to prioritise who and what to follow based on their interests and career stage – and it is much easier to keep tabs on 5 sources than it is 50, or even 15! So, the fewer the better.\nKeep your recommendations up to date. In this era of digital publishing, things can and do change overnight. So, if one of your recommended bloggers stops blogging, or the author of one of your favourite books makes a major update to the text, do be sure to let us – and your audience – know. We want to keep feeds and reviews current and useful.\nOf course you are brilliant, but… Please do not recommend or review your own publications or those in which you have a pecuniary or similar interest.",
    "crumbs": [
      "Recommenders"
    ]
  },
  {
    "objectID": "contributor-docs/explainers.html",
    "href": "contributor-docs/explainers.html",
    "title": "Explainers",
    "section": "",
    "text": "On Real World Data Science, Explainers are the stories behind the stories of data science in action. They are deep-dive explorations of the ideas, concepts, tools, and methods that make data science projects possible. In particular, we are keen to explore and explain the statistical underpinnings of modern data science techniques.\nA good Explainer will lead audiences through the what, when, how, and why of its chosen topic. The ultimate goal is to equip data scientists with the information and insight they need to make smarter, more informed analytical choices.\nThere are many different but effective ways of structuring an explainer and plentiful written examples in major media outlets like The Guardian and Vox, but these are generally written for a non-technical audience. Examples of technical explainers (with interactive elements) can be found on Amazon’s Machine Learning University.",
    "crumbs": [
      "Explainers"
    ]
  },
  {
    "objectID": "contributor-docs/explainers.html#structure",
    "href": "contributor-docs/explainers.html#structure",
    "title": "Explainers",
    "section": "Structure",
    "text": "Structure\nThe following outline is a basic guide to structuring an Explainer:\n\n\nHook\n\nIntroduce your topic, and explain why audiences should pay attention. For example, does your Explainer link to one of our published case studies? Does it focus on a tool or method that has been the subject of recent attention? Is it a foundational idea that is relevant to all sorts of data science applications?\n\n\n\nHigh-level summary\n\nA short, largely non-technical explanation of your topic. A good way to view this section is as an accessible condensed version of your complete Explainer. In thinking of it in this way, you can subtly signpost to audiences the areas you’ll be covering and the questions you’ll be answering throughout the remainder of your contribution.\n\n\n\nHistory and background\n\nIt can be useful from a practical perspective to explain how ideas, concepts, tools, and methods have developed over time. Applications may have become more complex in recent years, so exploring the origins of data science techniques might lead you to discover simpler use cases that can help support and illustrate your high-level summary.\n\n\n\nThe how, the when, the why\n\nThis section of your Explainer will likely be split into multiple subsections as you seek to build up your audience’s understanding of your chosen topic. It can be helpful to think about the sorts of questions an audience member might ask and to structure your contribution so that it directly addresses those questions (Q&A/FAQ formats are commonly used in explainer-type articles). If the focus of your Explainer is a data science method, for example, you’ll want to address the following:\n\n\n\nHow does it work and how is it applied (perhaps with an example or simulation)?\nWhat are the underlying assumptions?\nHow is performance checked and assessed?\nHow should outputs be interpreted?\nWhat are the pros and cons, the strengths and limitations of the approach?\nWhat are the optimal use cases, and when should the method be avoided altogether?\nAre there alternatives that people should know about?\n\n\nKey takeaways\n\nThis serves as your final summary: a chance to remind your audience of what they’ve learned from your Explainer and the main points they should keep in mind.\n\n\n\nTell me more\n\nIt’s sensible to assume that some of your audience will have further questions and will want to learn more about the topic. If you have additional sources of information to recommend, make sure to share them here.",
    "crumbs": [
      "Explainers"
    ]
  },
  {
    "objectID": "contributor-docs/explainers.html#advice-and-recommendations",
    "href": "contributor-docs/explainers.html#advice-and-recommendations",
    "title": "Explainers",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nFocus on what’s important. Your Explainer can’t hope to explain everything, so you need to be clear about what’s essential for your audience to know and what isn’t. Make good use of links and references to point audiences to other valuable sources of information that can enrich their understanding of your topic.\nBe clear about your target audience and their expected prior level of knowledge. In keeping with the point above, you need to be clear in your own mind about how much you expect your audience to know already about the general topic or subject matter. You can then structure your contribution accordingly. It might also be helpful to state explicitly, at the outset of your contribution, what assumptions you’ve made about your audience and highlight any background reading that might be beneficial.\nPlan out your route. To help you decide what to cover in your Explainer, we recommend first writing out your high-level summary of the topic and also your key takeaways. This provides you with a start point (A) and an end point (B) for your contribution. The challenge then is to figure out the main points or questions you will need to address to help your audience progress from point A to point B in a way that’s logical and intuitive for them to follow.",
    "crumbs": [
      "Explainers"
    ]
  },
  {
    "objectID": "contributor-docs/training-guides.html",
    "href": "contributor-docs/training-guides.html",
    "title": "Training guides",
    "section": "",
    "text": "In data science, there’s no one-size-fits-all solution to every problem and challenge. So, part of the job of the data scientist is to rapidly learn about different sub-domains, tools and techniques, and put those learnings into practice.\nBut it can be time-consuming to figure out what you need to learn and in what order, and to identify the best resources for doing so. This is where our Training guides come in. Each will set out a learning pathway for data scientists to follow, with recommendations of textbooks, videos, practical exercises and other teaching material to use every step of the way.",
    "crumbs": [
      "Training guides"
    ]
  },
  {
    "objectID": "contributor-docs/training-guides.html#structure",
    "href": "contributor-docs/training-guides.html#structure",
    "title": "Training guides",
    "section": "Structure",
    "text": "Structure\nContributors should think about their training guides as being short online courses that are constructed from existing high-quality material. You do not need to create your own “course” content. Rather, you should focus on recommending texts and other material for users to follow in a logical ordered way, so that they may build up and secure their knowledge of a particular topic.\nGuides should feature a mix of content types – not only text, but audio and video – and they should provide ample opportunities for users to practice what they are learning.\nA brief and extremely simplified example of a guide is as follows:\n\nStep 1: Watch this introductory video on Topic X.\nStep 2: Now you are familiar with the basics of Topic X, you will want to read Chapter 2 of Textbook Y, which delves into more of the mathematical underpinnings.\nStep 3: Let’s try Topic X ourselves. This GitHub repository has code for you to run it in Python. Copy the code and give it a go.\nStep 4: You should now be ready to apply Topic X to a simple data challenge. Check out this Kaggle page and practice what you have learned so far.\nStep 5: We’re now moving from the “beginner” level to “intermediate”, and Training Course Z gives a thorough overview of what you need to know for the next stage of your learning journey.\n… etc.",
    "crumbs": [
      "Training guides"
    ]
  },
  {
    "objectID": "contributor-docs/training-guides.html#advice-and-recommendations",
    "href": "contributor-docs/training-guides.html#advice-and-recommendations",
    "title": "Training guides",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nBe mindful of different learning styles. Some people prefer to read, others prefer to watch or listen. So, wherever possible, for each stage of your training guide, try to provide a mix of resources that meet the same learning objectives.\nConsider barriers to entry. Data scientists in large organisations may have access to training budgets or mechanisms to apply for training funds. But that isn’t the case for all data scientists, meaning that paid-for materials and training courses might not be accessible to everyone. Recommend them sparingly, and if there are ways to access the material at reduced rates do let users know. However, you must not link to illicit copies of material – e.g., unauthorised PDF reproductions of textbooks.\nIf there are resource gaps, please tell us. While planning out your training guide, you may well struggle to find the perfect piece of content to recommend at a particular stage of your learning journey. If that is the case, do get in touch with us. One of the goals of Real World Data Science is to identify and plug these sorts of gaps, so that all in the data science community can benefit. We’ll sketch out a commission and take it out to our network of contacts. Or perhaps it’ll be something you want to create for the site!",
    "crumbs": [
      "Training guides"
    ]
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html",
    "href": "contributor-docs/contributor-guidelines.html",
    "title": "Contributor guidelines",
    "section": "",
    "text": "Thank you for your interest in contributing to Real World Data Science. This page will walk you through the process of preparing and submitting your idea. If you haven’t done so already, please review our call for contributions before continuing.",
    "crumbs": [
      "Contributor guidelines"
    ]
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#site-functionality-and-ethos",
    "href": "contributor-docs/contributor-guidelines.html#site-functionality-and-ethos",
    "title": "Contributor guidelines",
    "section": "Site functionality and ethos",
    "text": "Site functionality and ethos\nReal World Data Science is built on Quarto, the new open-source publishing system developed by Posit. The site has been designed from the ground up as a platform for data scientists, created by data scientists. Here’s what this means in practice:\n\nContributors can use data science software and tools to create content – e.g. Visual Studio Code, RStudio, Jupyter Lab; Python, R, Observable, and Shiny – allowing for the full integration of text, code, figures, equations, and other elements.\nReview and editing are transparent and collaborative, again making use of tools data scientists are familiar with – e.g. GitHub, Google Docs – for sharing and revising documents prior to publication.\nContent can be both engaging and interactive. Many data scientists learn by doing, so code can be made available as R Markdown or Jupyter Notebook files to be reused and experimented with offline. Or, the same documents can be used online through tools like Google Colab and Binder. Where appropriate, the use of interactive displays and Shiny apps is encouraged, allowing for data visualisations to be interrogated and regenerated on the fly.\nSite users are contributors too. Through annotation and commenting functionality, site users can interact and converse with authors and other members of the Real World Data Science community. And with all source files hosted on GitHub, users of our site can raise issues, or fork and propose improvements – leading to a true exchange of knowledge.\n\nFor more on how to work with Real World Data Science via GitHub, check out our repository README.",
    "crumbs": [
      "Contributor guidelines"
    ]
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#the-submission-process",
    "href": "contributor-docs/contributor-guidelines.html#the-submission-process",
    "title": "Contributor guidelines",
    "section": "The submission process",
    "text": "The submission process\n\nContact Real World Data Science to discuss your proposed submission.\nWrite up a short content brief containing the following:\n\nTitle of submission\nAuthor name(s) and affiliation(s)\nTheme/topic area\nTarget audience\nSynopsis or sell line, summarising the story and its importance/value (250 words max.)\nKey audience takeaways\nFormats and features (e.g., text, audio, video; code blocks, interactive data visualisations, etc.)\nAccessibility considerations\nTarget length/word count\nFirst draft to be submitted by…\n\nThe RWDS_post_template repository on GitHub contains a Quarto document (content-brief.qmd) that can be used to produce a content brief in the style and format of a Real World Data Science article.\nOnce a content brief is finalised and approved, content is to be prepared in the agreed format and with reference to our style guide. For simple text-based articles, we recommend using Google Docs or Microsoft Word; for submissions that incorporate technical or multimedia content, such as code, equations or interactive graphics, we recommend the Quarto (.qmd) file format. Use the RWDS_post_template repository to create your draft article in Quarto using the correct style and formatting. A sample article in the repo (report.qmd) contains code examples for the Quarto features used by Real World Data Science. Documents can be submitted in Jupyter notebook (.ipynb) and R Markdown (.Rmd) formats but will require conversion before publishing.\nDraft submissions should be sent via email to Real World Data Science. Alternatively, contributors can commit their drafts to their own GitHub accounts using the RWDS_post_template repository and the repository link then shared with Real World Data Science.\n\n\nFor more information on how to work with Real World Data Science via GitHub, check out our GitHub repository README.",
    "crumbs": [
      "Contributor guidelines"
    ]
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#copyright-and-content-licencing",
    "href": "contributor-docs/contributor-guidelines.html#copyright-and-content-licencing",
    "title": "Contributor guidelines",
    "section": "Copyright and content licencing",
    "text": "Copyright and content licencing\nContributors retain copyright of their work, but agree to publish their work under a Creative Commons licence. Contributors are free to choose the licence that best suits their content. The chosen licence should be indicated on the draft submission.",
    "crumbs": [
      "Contributor guidelines"
    ]
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#the-review-process",
    "href": "contributor-docs/contributor-guidelines.html#the-review-process",
    "title": "Contributor guidelines",
    "section": "The review process",
    "text": "The review process\nDraft submissions will be shared for review with members of the Real World Data Science Editorial Board. Comments and edits to documents will be made via Google Docs/MS Word/GitHub, allowing for (a) version control, (b) open dialogue between reviewers and contributors, and (c) a transparent and well-documented review process.\nOnce revisions are complete and content is accepted for publication, authors will be provided with HTML files to preview published content. Following sign-off by author and editor, HTML files will be made live.",
    "crumbs": [
      "Contributor guidelines"
    ]
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#post-publication",
    "href": "contributor-docs/contributor-guidelines.html#post-publication",
    "title": "Contributor guidelines",
    "section": "Post-publication",
    "text": "Post-publication\nContributors and editors will work together to promote content via social media platforms – Twitter/X, LinkedIn, blogs – and in other channels as appropriate – e.g., in response to related questions on Quora or Stack Overflow.\nContributors are encouraged to monitor their content regularly for user comments and discussions. Engaging in discussions with users – whether through the Real World Data Science platform or via social media and other channels – is an effective way of developing an audience: it builds profile for the contributor and their content, and encourages other users to find and interact with content.\nContributors also have the option of uploading the source code and files for their articles to Zenodo, an open science repository, and submitting this material to the Real World Data Science community. When a Zenodo record is created, a DOI is assigned, and this DOI can then be added to your published article on Real World Data Science. For more details on working with Zenodo, see help.zenodo.org.",
    "crumbs": [
      "Contributor guidelines"
    ]
  },
  {
    "objectID": "people-paths/posts/2024/03/21/mjones-interview.html",
    "href": "people-paths/posts/2024/03/21/mjones-interview.html",
    "title": "Data science and AI in financial services: An interview with Nationwide’s Matthew Jones",
    "section": "",
    "text": "Back to Careers\n\n\n\n\n\n\nAbout the author\n\nJonathan Gillard is a professor of statistics and data science at Cardiff University and a member of the editorial board of Real World Data Science.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail background by Devin Pickell on Unsplash.\n\n\n\nHow to cite\n\nGillard, Jonathan. 2024. “Data science and AI in financial services: An interview with Nationwide’s Matthew Jones.” Real World Data Science, March 21, 2024. URL"
  },
  {
    "objectID": "people-paths/posts/2023/05/11/chatgpt-data-science-pt1.html",
    "href": "people-paths/posts/2023/05/11/chatgpt-data-science-pt1.html",
    "title": "How is ChatGPT changing data science?",
    "section": "",
    "text": "For many people, it starts with a question. Something simple, something they already know the answer to. A test, in other words, to see what these AI-powered chatbots are all about. But spend any amount of time with ChatGPT and other such tools and you’ll quickly start to wonder what else they might do, and how useful they might be in your day-to-day working life.\nData scientists certainly have been thinking along these lines, and to find out more about current use cases, proofs of concepts and potential applications, Real World Data Science got together with members of the Royal Statistical Society’s Data Science and AI Section (DS&AI) for a group discussion.\nOur interviewees, in order of appearance, are:\n\nPiers Stobbs, VP science at Deliveroo, and DS&AI committee member.\nDetlef Nauck, head of AI and data science research at BT, and editorial board member, Real World Data Science.\nAdam Davison, head of data science at the Advertising Standards Authority, and DS&AI committee member.\nTrevor Duguid Farrant, senior principal statistician at Mondelez International, and DS&AI committee member.\nGiles Pavey, global director for data science at Unilever, and DS&AI vice-chair.\nMartin Goodson, CEO and chief scientist at Evolution AI, and DS&AI committee member.\n\nThe first part of our discussion focuses on how large language models are becoming part of the data science toolkit, and what this new development means for data science teams and skillsets. Stay tuned for part two, which we’ll be publishing soon!\n(UPDATE: Part two is now published: “Large language models: Do we need to understand the maths, or simply recognise the limitations?”)\n\n\n\n\n\nAs data scientists, how has ChatGPT – and other tools built on large language models (LLMs) – changed your working lives?\nPiers Stobbs: Up to about a year ago, although I was really impressed with the developments in deep learning and the improvements in computer vision and natural language models, it felt in line with general improvements in machine learning. And then, probably about six months ago, with things like DALL·E and ChatGPT, it felt like something changed – properly ground-breaking capabilities. And I still can’t quite get my head around the fact that you can basically have a model that tries to predict the next token, and it comes up with outputs that really feel quite sensible and human-like – if prone to hallucination.\nThe way I think about it is, this feels like a brand-new capability that we’ve just not really had before. It’s almost like an interface with unstructured information. Historically, you sort of have to turn text into something, and then turn something back into text, if you want to have this interface with humans. Now, we’ve got this really quite elegant way of plugging the gaps, which feels full of opportunities.\nI’m having great fun playing around with the code co-pilots – GitHub’s Copilot is amazing and, productivity wise, is helping me a lot. I am now a much faster coder because there’s all those Stack Exchange lookups that I don’t have to do anymore. Again, from a personal productivity perspective, I’m using [ChatGPT] for initial drafts of documents and other things. And then I use it almost for validating things. For instance, I had a random discussion the other night with ChatGPT about logistic algorithms. It’s not going to solve problems for you, but I asked it to give some pointers of things I could be thinking about – some of which I had, some of which I hadn’t. So, it’s almost like a brainstorming helper, somehow.\nBut probably the thing I’m most excited about is the knowledge sharing side of it – plugging it into, or on top of, private information, and surfacing all that knowledge that is locked away in documents and intranet pages.\n\n\n\n\n\n\nPiers Stobbs\n\n\n\n\n\nThis feels like a brand-new capability – an interface with unstructured information. Historically, you have to turn text into something, and then turn something back into text, if you want to have this interface with humans. Now, we’ve got this elegant way of plugging the gaps.\n\n\n\nDetlef Nauck: We’re looking into running proof of concepts to see whether LLMs do bring value. Software engineering is the most obvious one, and easiest to set up and run. And then we want to look at making use of internal documents – so, either summarization or creation of internal documents in appropriate language. The latter use cases are trickier to evaluate. We want to know whether the outputs produced are any good. With software engineering you can track GitHub statistics, for example. But if you give ChatGPT to somebody to write marketing material, or to get information out of a document, how do you know that the results are good? We need to get our head around metrics for evaluation.\nAdam Davison: I’ve been using it for basically anything where I don’t remember the API very well or it’s a bit confusing. Pandas is the key, right? We all use pandas, but you don’t really remember how to do some complicated thing with apply(), say, so you just ask GPT-4 to give you the answer, and it saves you that hassle. Also, I read some insightful tweet that said these chat systems are really good for things where generating the solution is hard, but verifying it is easy. And I think that’s true for some of these things. You know, you get a short piece of Python code, you can basically look at that and you can tell if it’s right.\nIn data science, you’re a bit of a jack-of-all-trades. You need to do little bits of everything, but you’re not a specialist in anything. And so, I think for software development, it’s been really helpful. For example, right now, I’m doing a bit of frontend development in a project to visualise something. I’m never going to be a professional frontend developer, but GPT-4 can help deal with the oddities of JavaScript much more easily than it would be for me to trawl through Stack Overflow posts.\nBut the thing that we’re using it for, practically, is natural language processing (NLP) and classification. We have this particular problem at the Advertising Standards Authority (ASA) where we are running lots of different models that are completely unconnected to each other because every project is a different topic. So, one week we’re looking at, “Do these gambling ads appeal to young people?” and then the week after it’s, “Are these cryptocurrency ads being clear about the risks involved?” It’s very disparate, we don’t have a lot of time to iterate on models, and we don’t have huge amounts of training data. Ten years ago, when you were doing sentiment classification, you were on Mechanical Turk getting 10,000 examples, and even then it didn’t work very well after these really complicated models. Now, you’ve got a couple of hundred examples and with the embeddings [from LLMs] you can get to a pretty decent classifier quite quickly. We’re also starting to experiment with using OpenAI’s fine-tuning tools, and the performance that we’ve seen from that is very impressive, to the extent that it’s making us rethink whether we bother doing anything else in some of our classifiers.\n\n\n\n\n\n\nAdam Davison\n\n\n\n\n\nFive years ago, if you had a sophisticated problem involving text or images, you’d need a big research team with a big budget to tackle it. But increasingly we find, like many other people, that you can take models off the shelf and repurpose them for quite diverse tasks.\n\n\n\nTrevor Duguid Farrant: My organisation is not as far forward as the rest of you. I’ve introduced it to the leadership team, and the digital services team – what was IT – are looking to make a decision on whether we can use it or not. I think there’ll be so much pressure they’ll have to use it, but there’s still a feeling of discomfort with it, whereas I think it’s really good and have started using it. Everyone else on the call seems to have started using it. So, can organisations like the Royal Statistical Society help companies to embrace this and start using it, and then everyone can benefit from it?\nGiles Pavey: I wish I could be with Piers and Adam – actually using it – but my life has been taken over as the guy who goes and explains it to the business. Unilever is a massive business, and we are concerned about privacy, confidentiality and trustworthiness. We’ve now built an initial GPT instance on Azure and fed it with some of our own documents, and a lot of my time has been working with legal to convince ourselves that that’s okay. Now we are really trying to work out just how we manage the amazing demand for proofs of concepts and use cases – and what we’re just about to uncover, I think, is the unknown but potentially massive expense of running it.\nIn pure proofs of concepts, departments that have large knowledge banks are using it: research and development, and marketing, for example. And one of the big technical things that we’re working on – and, because of the size that we are, we’re doing a lot of work with OpenAI and Microsoft on this – is how to stop the models from hallucinating.\nHave your experiences with ChatGPT and other tools changed your thinking about the skillsets required of data scientists and data science teams?\nAD: A little bit. As someone at a small organisation, I think it’s quite exciting because, five years ago, maybe you were in a world where if you had a sophisticated problem involving text or images, you’d need a big research team with a big budget to tackle it. But increasingly we find, like many other people, that you can take models off the shelf and repurpose them for quite diverse tasks. So, I think it’s becoming increasingly viable to have a small team of people who are implementers, who aren’t necessarily backed up by a big research organisation, doing increasingly sophisticated stuff.\nI don’t think it does away with the sort of things that we always bang on about in the Data Science and AI Section, like the need for an understanding of statistics and how the underlying systems really work, because I think you still need to understand what you’re doing with LLMs, as with any other machine learning technique. But, if I had to guess, what we’re going to be seeing now is that for a lot of problems, you’re going to have more of a division – so, you’re either in one of a small number of very large labs doing research on very cutting-edge big models, or you can be an implementer who is taking things off the shelf and applying them. And maybe that space in between is going to get a little bit squeezed – that would be my guess, but obviously it’s very unpredictable.\n\n\n\n\n\n\nGiles Pavey\n\n\n\n\n\nWe’ve built an initial GPT instance on Azure. Now we are really trying to work out just how we manage the amazing demand for proofs of concepts and use cases – and what we’re just about to uncover, I think, is the unknown but potentially massive expense of running it.\n\n\n\nPS: That’s exactly my view. When I first started hiring data scientists, a long time ago, you basically had to write stuff from scratch, and you needed PhDs – people who really understood, at a deep level, how the maths all works. But I think there’s been a steady progression towards valuing software engineering skills, and I think, in some ways, this is another step along that path. If I think now about implementing a chatbot over your own knowledge base, it’s basically like plugging APIs together with some Python. Adam’s point is still hugely important, though, because I think we still need the background knowledge about what’s actually going on – OK, I’m creating embeddings here, and that’s allowing this search to work so I can surface the right docs – that whole process, which an average software engineer is maybe not going to know. But I think it’s definitely blurring the lines.\nMartin Goodson: It’s just as important now to understand how to evaluate performance. The difference is, it used to be that you were trying to figure out whether it’s 80% accurate, or 85%. Now, it’s like 99.9%. But you still need to figure it out. You still need to understand what the failure modes are, what caused it; how is it actually working, and is it doing what you need it to do for the product? Is it actually satisfying our needs as users or as customers of the products.\nDN: I think in the future, the skills we will need are people who can run and build these models. Giles made the point about how expensive it is to run these things. Right now, you have two options: subscribe to an API, and then you are limited in how you can modify these models; or build your own – take an open-source LLM and modify it. But then you need people who know how to build a high-performance computing environment and operate this efficiently. You need to know how to actually train the models, how to curate the data, how to set the model parameters. And I always think there’s too much alchemy still going on in this field, right? It’s not proper science. People build these things and then are surprised at what they can do; they didn’t know such things would be possible. A lot of these capabilities only emerge when you make the models really, really big and, essentially, you also have no control over them – you can’t stop them hallucinating. So, these are the kinds of issues we need to get under control if we want to get any value out of them.\nPrompt engineering is another one – you really need to understand how these models work and how to prompt them. If you want to give them to, say, a marketer to generate copywriting, they may not have the right ideas of how to prompt the machine. So, I could see roles developing out of data science that understand how to influence these models and make them do what we want them to do.\nMG: The other angle to this is junior engineers. Now, the bar for being a useful junior engineer is that you’re better than GitHub Copilot. Why do you need a really junior person if you can just use a large language model to be the junior developer?\nDN: I’m not thinking about the data science person who needs to write some code for a project here, but if you have a large software team in an organisation that produces production code, they will become more efficient by using these tools. But still, with all this overhead of testing and putting it all together, there will be a lot of manual work that needs to be done. But the teams will get more efficient and junior people will get up to speed quicker. That’s probably another advantage.\n\n\n\n\n\n\nTrevor Duguid Farrant\n\n\n\n\n\nCan the Royal Statistical Society help non-tech companies embrace large language models, extolling their virtues and dispelling the myths?\n\n\n\nPS: I think Detlef’s point about understanding is an interesting one. It definitely feels like there’s been this sort of continuum from, you know, “OK, it’s a linear regression, we know what’s going on” to complex models to ensemble models where, again, you’re combining these things you can individually understand. Even with big ImageNet architectures, billions of parameters, at least conceptually you can understand how these work and build out tools where you can understand the layers. To me, what’s different now is you’ve got this reinforcement learning layer on top, or diffusion layer, or some other additional approach – this combination of really complicated things. I honestly don’t know where to start with trying to understand why a specific output is generated, and I think that is a proper concern. That’s definitely an area of research, because I think we need to understand this.\nGP: I think there’s also a question in large companies of just who owns these things. Up until this point, everybody’s been happy that AI is the realm of data science. And, suddenly, generative AI looks like it might be the realm of the IT team – that it’s a service that you get off the shelf. It’s going to be interesting to see how that plays out. I really liked the point that Martin was making about being able to tell what the systems are actually doing, what they are supposed to do and how to check them, because if you don’t have a background in that area, you might just assume they work. Now, nobody knows exactly how these things work – not even the people who build them. But having a background in how you test things, for potential causes for things not working, is actually going to be incredibly powerful or useful.\nTDF: Will experts like us actually be able to check it because of the speed that new versions are coming out and the developments that are happening? Is it going to take us six months to check that GPT-3.5 works? Well, too late, a month later GPT-4’s out! I just think that pace is going to keep accelerating.\n\n\n\n\n\n\nWant to hear more from the RSS Data Science and AI Section? Sign up for its newsletter at rssdsaisection.substack.com.\n\n\n\n\n\n\n\nBack to Careers\n\n\n\n\nRead part two →\n\n\n\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photos are not covered by this licence. Portrait photos are supplied by interviewees and used with permission. ChatGPT homescreen photo by Levart_Photographer on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “How is ChatGPT changing data science?” Real World Data Science, May 11, 2023. URL"
  },
  {
    "objectID": "people-paths/career-profiles/posts/2023/05/24/jasmine-holdsworth.html#transcript",
    "href": "people-paths/career-profiles/posts/2023/05/24/jasmine-holdsworth.html#transcript",
    "title": "‘For me, data science is about bridging the gap between business requirements and the data that businesses have’",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove some repetitions.\n\n\n\n\nWhat does your job at Expedia involve?\nI would probably call myself more on the analyst side. So, while my day-to-day job doesn’t necessarily involve AI, ML and productionalising models, it’s more taking business goals or requirements and taking the data and essentially bridging the gap between the two. I am on the incrementality analytics team. So, what that means is I measure the short-term returns from our marketing efforts that we have. And I do that via geotesting. So, I’m essentially working in the geotesting part of the company if you like. And before that I worked in the customer data section. So, essentially looking at customer data and working with that.\n\n\nHow long have you been working in data science?\nMore in an analyst role, but probably about seven years now, I began in Stack Overflow just as a data analyst, and then worked at DAZN – which is like a Netflix for sports – as a data analyst, and then joined here as a senior analyst, and then moved into data science in the last couple of years. I would, I would credit Stack Overflow as the place where my career kind of was birthed, if you like. I started there as an account manager, so with hardly any technical background at all required, and then moved into a role that was essentially looking after, or reporting the metrics of advertising campaigns for companies that would advertise on Stack Overflow. So that required a little technical knowledge, not much – a few pivot tables and things like that. But then the longer I stayed there, the further my career developed, and they had, at the time – probably still do – some fantastically smart people that work there, as you can imagine. I was sponsored to do a General Assembly data analytics course, which was focused around Excel, dashboarding, and SQL and essentially fell in love with data analysis. It was the most technical subject matter that I had experienced to that point, and I found a real natural affinity to it, particularly SQL. And then [I] moved into more of a data analyst role within Stack Overflow, so – as you can probably imagine – an absolute sea of proprietary data that needed analysing, and started learning R, or rather being taught R within Stack Overflow, and loved it. I think I was there for three-and-a-half years, and then moved into a data analyst role at DAZN. At this point, I did a data science General Assembly bootcamp course, and fell in love with that. And then I decided that I really loved General Assembly as a concept; I actually started a second job teaching there, so the courses that I had previously taken I was now teaching, first as a teaching assistant, and then as a lead instructor, which was one of the most, yeah, one of the most amazing experiences I’ve had actually, I learned a lot from that. And then I got a job as a senior analyst within Expedia Group, which is where I am now, and then moved into a data science role, which is what I’m doing currently. So, I actually left school at 16, and had to go into a full-time employment. And the General Assembly education that I took a part in was my first of that type. So, when I realised that data science was absolutely something that I really wanted to dedicate the rest of my life to, I decided to take on a part-time data science bachelor’s degree, which I am now about a year away from finishing. Because I’m doing it part time it takes a bit longer. But yeah, so I will have my data science bachelor’s completed, hopefully, by 2024.\n\n\nWho or what inspired you to work in data science?\nThere were two big inspirations into getting into data sciences. So, they were actually the data scientists that I worked with at Stack Overflow. They were the first two data scientists, I believe, that Stack Overflow had ever hired. I worked very closely with them as an analyst and one of them was, had previously worked – I don’t know if it was officially an astrophysicist – but had studied black holes, and I remember thinking that was just amazing. And the other was, was very famous within the field. And they spent a lot of time giving me one-on-one training on R and SQL and basic analysis, and I was so inspired by these two individuals that I, it was also a career path that I didn’t really know existed.\n\n\n\n\n\n\nJasmine Holdsworth\n\n\n\n\n\nWhat was impressed upon me in that first year [in data science] was the importance of statistics and interpreting statistics in a way that’s honest.\n\n\n\n\n\nWhat does data science mean to you?\nFor me, it is bridging the gap between the business requirements and the data that businesses have. So, you’ll have business goals, requirements that kind of come down the line and there’s a lot of data that’s being collected, and, essentially, you have to try and be the bridge between the two. So, not just doing very complicated analyses, with very sophisticated models – at least not in my role – it’s about being able to create analysis that’s interpretable, that you can present to non-technical stakeholders that they’re going to understand to a degree. So, I do know that in different roles in different companies, it will be slightly different. But yeah, for me, it’s about making data, yeah, interpretable, to the non-technical stakeholders to enable them to do their job better.\n\n\nWhat is your most important skill as a data scientist?\nI like to think that there is one responsibility around how statistics are interpreted. So, just making sure that when you’re giving someone a statistic, that they understand what it can be used for, what it can’t be used for, and that it doesn’t kind of get halfway around the company before, you know, without any danger of it being misinterpreted. And I do think that the other is just being a translator. So, as well as teaching with General Assembly, I teach people within my company, things like SQL, R, and some basic data analysis. And I feel like it’s taking what is quite a technical, complicated subject and almost translating it into, if you like, English, so that people can kind of get some sense of what something may mean, without necessarily having to have the degree to back it up.\n\n\nWhat hurdles did you face in becoming a data scientist?\nTowards the beginning of my career to say – 5, 6 years ago – it was quite hard to get interviews. It was never hard to get interviews with technical people within companies, because you can– a technical person can see whether or not you know what you’re talking about. But recruiters don’t, and if someone is a recruiter for a technical role, their proxy for whether or not you can do the role is what’s your level of education, which is completely understandable and that’s what education is for. But it did mean that sometimes I applied for roles that were well below my, my level, and if I did so through a recruiter, then I wouldn’t hear anything back. But if I spoke to a technical person within that company, then it would be fine.\n\n\nHow did you overcome those hurdles?\nActually, I guess the story of how I joined Expedia is quite relevant in that way. So, I presented some, just some fun analysis that I did at an R-Ladies meetup, and I was already talking to a recruiter within Expedia Group and I said to them, oh, well, I happen to be visiting your offices to present at this meetup, so maybe I can meet you there. And they actually sent the manager of the team that they wanted me to join. So, this manager attended the meetup, watched me present, and then they ended up hiring me, which is really great. But I do really think that that was a result of being able to see me on stage, talking about stuff that I had done, showing code that I had written, and it kind of bypassed a few steps. So yeah, I would definitely say meetups and connections are very helpful to overcome that.\n\n\nThe most important lesson from your first year in data science?\nI think that what was impressed upon me in that first year – and what really drove me to do the bootcamp courses and then, ultimately, the degree – above everything, actually, was the importance of statistics and interpreting statistics in a way that’s honest. So, I feel like– I feel like with coding, that comes quite naturally to me, and writing SQL queries, R, that was all kind of fine. That didn’t require a lot. But I really, I had an amazing manager who taught me a lot about, essentially, if you’re going to go and speak to this company about the campaign that they’ve run on our website, then you need to impress that X doesn’t necessarily mean Y, it just gives evidence to, or alludes to, and essentially just making sure that how you’re communicating things is as accurate as possible.\n\n\nAny mistakes or regrets in your career so far?\nWhen I look back on my career, I think the things that have really stayed with me that I’ve really learned from, mistakes wise, are around small little mistakes around how you interpret data. Maybe it was a, like, years ago, summing the wrong cell in Excel but not checking two or three or four times before that goes out. I’m now quite– I over-check everything. I think that the most important part of our job, as well as being the translation, is being the correct translation. You need to be reliable. People need to know that if you put out analysis that they can trust you. So, I would say I regret every small, tiny little data error that I ever made, which I can’t even recall right now, but I know have kind of cumulated enough that it has made me a very fastidious checker, I suppose.\n\n\nHow do you see your role in data science evolving?\nI definitely see myself being an individual contributor in a consumer-facing company, just because that is basically what I’ve been doing up to this point. I don’t really have any desire to get into people management. I very much love being stuck in a room with my laptop, solving problems. Above all else, it still makes me happy. However, I do also love knowledge sharing – I love teaching, whether it’s with General Assembly, or whether it’s within the company that I work now. And I would like to kind of balance those two goals moving forward. So, keeping my role within my company as like an individual contributor and actually being like the front face for the, for the analysis that’s happening rather than kind of managing it. But also making sure that I carve out time to upskill others, because data science as a field, I mean, as you all know, is growing so much and people are coming in from different backgrounds. And I’m lucky enough to be able to kind of speak to a few people like that and do some very casual mentorship. And it makes me happy to see, so I hope that as my career develops, I will see more people maybe with backgrounds a little bit more like mine, come through and bring some diversity to the sector.\n\n\nNew developments or ideas you are most excited about?\nIt would be remiss of me to not mention like ChatGPT or generative AI, etc. But honestly, I am more interested in – or vaguely interested, I should say – in wearable technology. So, I’ve read a few very, very interesting papers and articles that talk about the development of wearable technology, not just your kind of watches, but potentially clothing, etc., that can be used for people with specific health problems to really help pinpoint, like, inflection points in time when something might happen. For example, a heart attack is about to happen, or is imminent, or is happening. So I actually feel like at the moment, this is perhaps going slightly under the radar, compared with more, you know, sexy developments like chatbots and things. But I’m very interested to see in the next one to two decades how ubiquitous wearables will be, and how closely entwined that will become with healthcare. So that’s something that I’m keeping half an eye on.\n\n\nAny words of advice for budding data scientists?\nYou will never stop learning at all because, frankly, the field is moving very, very quickly. So, even if you were to kind of consider yourself an absolute expert today, tomorrow that may not be the case. You will constantly be learning. And I have found that learning the same thing several times through different mediums and having things explained to you different ways is so valuable. Because you may think that you understand something from, say, your bootcamp, but then when you read about it as part of your degree – this is obviously personal to me – you read about it in a different way. And you think, Oh, I’ve never thought of it like that. And then you watch a YouTube video and someone visualises it and you think, okay, I understand this all a little bit deeper now. So, constantly revising what you do know and learning anything that’s new as it comes up, I think everyone at every stage of career can kind of, can do that.\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘For me, data science is about bridging the gap between business requirements and the data that businesses have.’” Real World Data Science, May 24, 2023. URL"
  },
  {
    "objectID": "people-paths/career-profiles/posts/2023/06/28/albert-lee.html",
    "href": "people-paths/career-profiles/posts/2023/06/28/albert-lee.html",
    "title": "‘Living my identity takes courage. It is the same courage necessary to start a new business’",
    "section": "",
    "text": "This week, in celebration of Pride, Real World Data Science is collaborating with the JEDI Outreach Group of the American Statistical Association (ASA) and the ASA LGBTQ+ Advocacy Committee to highlight the achievements of statisticians and data scientists from across the LGBTQ+ spectrum.\nMembers of the committee nominated two individuals to be featured as part of our career profile series, and so we’re pleased to bring you interviews with Albert Lee (below) and Claire Morton.\nRead on to discover more about Albert’s data science career (so far).\n\n\n\nHi, Albert. Thank you for sharing your career story with Real World Data Science. Please tell us a little about yourself and your role in data science.\nMy name is Albert Lee. I’m the founding partner at Summit Consulting, a quantitative and financial consulting firm in Washington, DC. Summit delivers data-driven solutions to help make government effective and society just. I started Summit in 2003, and we recently celebrated our 20th anniversary.\nI received my PhD in economics from UCLA in 1999. My professional practice is focused on econometrics – an academic specialty that blends economic theory with statistical practices – and statistical sampling.\nWhat does your job involve?\nA large portion of my time is spent running Summit and making decisions about management, personnel, and business development. That said, I am still pretty active in technical topics. I am a testifying expert in econometrics and statistical sampling. Recently, I have been leading a team of data scientists who are reformulating the edit and imputation algorithms for the US Department of Agriculture’s National Agriculture Statistical Service, which collects survey data from US agriculture sectors.\nWhat do you think is your most important skill as a data scientist?\nExplaining technical concepts is a big part of my job, and it requires the ability to consume the technical literature and know the concepts well enough that I can explain them to a lay audience (such as lawyers, judges, and program staff).\nHow has your gender and/or sexual identity factored into your career?\nMy gender and identity have given me important perspective as a data scientist and an entrepreneur. Living my identity takes courage. It is the same courage necessary to start a new business. From a young age, my identity has conditioned me to be comfortable with differences.\nMy identity has also taught me to see similarity among differences. Empathy is essential in client services and especially in quantitative consulting, where some of my clients feel disempowered by the complex subject matter.\n\n\n\n\n\n\nAlbert Lee\n\n\n\n\n\nThe data science field is moving very fast. Every day brings a new algorithm, software program, and hardware innovation. Since data science is a multidisciplinary field, keeping up with it has been challenging.\n\n\n\nHow did you get into data science?\nAlthough I studied mathematics and economics as an undergraduate student and economics as a graduate student, my academic training was very theoretical. I didn’t work with data and computers extensively until my first job outside of academia in the early 2000s. Little did I know that it was the advent of the “big data” revolution.\nAt Summit we serve mostly federal agencies, who are sitting on decades of administrative data – information they collected as part of their mission but not of research quality. These agencies want to use their administrative data to automate their routine tasks (like predicting which loans will default first) and evaluate program efficacy (determining whether a training program reached its goals). Extracting and analyzing administrative data has been a big part of my career.\nWhen I founded Summit, data science was not a recognized discipline. But as the datasets get larger, decisions about hardware setup, software programs, estimation algorithms, and data virtualization have become increasingly intertwined and interdependent. This really was my first taste of data science as we know it today.\nWhat, or who, first inspired you to become a data scientist?\nThere are too many people to mention by name. I owe a lot of my career to my first two managers at KPMG, Alan Salzberg and Rick Holt. They taught me how to code and reason quantitatively. And Rob Gould at UCLA has patiently converted a theorist to an empiricist. Once a convert, now a zealot.\nWhat were the hurdles or challenges that you needed to overcome on your route into the profession?\nI am an immigrant and a first-generation college graduate. My journey was full of unknowns. Figuring out my academic and professional career has taken a lot of exploration. In this regard, the same exploration that guided my identity also guided my academic and professional journey.\nAnd what are the challenges that you face now that you are working in data science?\nThe data science field is moving very fast. Every day brings a new algorithm, software program, and hardware innovation. Since data science is a multidisciplinary field, keeping up with it has been challenging. As I progress along my professional journey, striking the right balance between management, hands-on practice, and learning has been difficult as well.\nWhat was your first job in data science, and how does it compare to your current role?\nAs an entrepreneur, I was given a lot of professional freedom to actualize my career. To a large extent, I have the career that I envisioned. To me, data science lives in the intersection of methods, software, and hardware. I have spent a large part of my career in this intersection.\nOf course there are many things that were not part of the original vision, such as running a 100-person organization. My approach has always been intention with openness. By this metric, my current role is not far off from my original vision.\nWhat was the most important thing you learned in your first year on the job?\nThe ability and the love of learning constantly, regardless of the topic.\nWhat have been your career highlights so far?\nThe biggest highlight was that on June 15, 2023, Summit celebrated its 20th anniversary! Reformulating the National Agricultural Statistics Service’s edit and imputation systems is also a big deal. And being a testifying expert in some of the most consequential legal cases in the United States was a highlight as well.\nWhat three things are at the top of your current reading/study list?\nIn recent years, I have been binge-reading Stoic philosophy. I have read most books by Ryan Holiday. His most recent book was Ego Is the Enemy. In between the Stoics, you will find me reading Buddhist meditation literature, including Thich Nhat Hanh’s The Heart of the Buddha’s Teaching. David McCullough’s Truman is also by my bedside.\nWhat advice would you give for anyone wanting to be a data scientist?\nBe open and multidisciplinary. Many good ideas in statistics come from other fields, such as economics, medicine, sociology, and education. Computer science enables computational statistics. Having the openness to these topics is key.\nWhat new ideas or developments in the field are you personally most excited about or intrigued by?\nMachine learning has transformed statistics both as a consumer and a contributor. It consumes statistics in that it requires cutting-edge statistical techniques and algorithms for its estimation. Machine learning has important applications in many of the statistical sciences.\nAnd what do you think will be the main challenges facing the profession over the next few years?\nThe proper use of statistics or statistical ethics is an important societal challenge. Machine learning is becoming increasingly sophisticated, and its applications are more broad and pervasive. Machine learning algorithms are making more and more decisions in society, including mortgage loan approvals, residential home prices, and which prisoners receive parole. These are important and weighty decisions. How do we know that these decisions are unbiased and fair?\n\n\n\n\n\n\nAbout the ASA Pride Scholarship\n\n\n\nThe ASA Pride Scholarship was established to raise awareness for and support the success of LGBTQ+ statisticians and data scientists and allies. The scholarship will celebrate their diverse backgrounds and showcase the invaluable skills and perspectives these individuals bring to the ASA, statistics, and data science.\nApply or nominate someone for the ASA Pride Scholarship.\n\n\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo of Albert Lee is not covered by this licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘Living my identity takes courage. It is the same courage necessary to start a new business.’” Real World Data Science, June 28, 2023. URL"
  },
  {
    "objectID": "people-paths/career-profiles/posts/2023/10/04/niclas-thomas.html",
    "href": "people-paths/career-profiles/posts/2023/10/04/niclas-thomas.html",
    "title": "‘I fell in love with math, really, and fell into data science because of that’",
    "section": "",
    "text": "A passion for maths and solving mathematical problems led Niclas Thomas to a PhD in machine learning with a focus on medical research. But then a conversation with a recruiter steered his career towards data science in the retail sphere. After stints at Tesco, Sainsbury’s, and Gousto, Thomas is now head of data science for Next, the clothing retailer.\nIn this interview with Real World Data Science, Thomas reflects on his career journey so far, from hands-on coding work to team leadership and management. He also argues for the importance of communication and storytelling as part of the data science skill set."
  },
  {
    "objectID": "people-paths/career-profiles/posts/2023/10/04/niclas-thomas.html#transcript",
    "href": "people-paths/career-profiles/posts/2023/10/04/niclas-thomas.html#transcript",
    "title": "‘I fell in love with math, really, and fell into data science because of that’",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove some repetitions.\n\n\n\nBrian Tarran\nNiclas Thomas, thank you for joining us today. I hope you’re well.\nNiclas Thomas\nI am indeed thanks. Thank you for having me.\nBrian Tarran\nToday we’re meeting because we want to find out a little bit about your career in data science, how you got into it, what you’re doing now, where you see both your career and data science as a profession going next. So do you mind– can we start by giving us a brief introduction to who Niclas Thomas is?\nNiclas Thomas\nYeah, of course. Yeah. So I’m currently working as head of data science at Next. My background is academia originally, a maths degree. I did my PhD in machine learning, and more in medical research, so more of an applied machine learning position where the idea was to try and predict, ultimately, predict disease from a given sample of data from blood – can you actually predict future disease? – which I think is a really interesting area; I love medical research. And then [I] switched over to more commercial role and worked in several retail data science roles: so, Tesco, Sainsbury’s, Gousto, and then now, as I said, currently head of data science at Next, where I run a team, and I imagine most listeners will be familiar with what Next do: a retail, a clothing brand on the whole, where the idea is, obviously to sell some great stuff, great products and put the right product in front of the right customer.\nBrian Tarran\nCan you tell us, what does your job involve? What are your sort of main tasks and responsibilities in that role?\nNiclas Thomas\nYeah, so I suppose I’m lucky enough to have been head of data science at several different companies: Sainsbury’s, Gousto, and Next. So it’s always interesting to compare the role of the head of data science in each of those three. At the moment, I think there’s a core focus on, well, ultimately making sure the teams are efficient as possible. And that really means just making sure our tech stack – what tools, what programming languages, what software we use on a day to day basis – is set up for success and make sure the team have what they need to be able to do the job as efficiently as possible, whether that’s using Python or R, whether that’s how we develop code, and how we work with other people as well, being a big part of that, then. So how do we work with other software engineers? How do we work with web developers, then, to make sure that the work we do actually gets in the hands of the business and ultimately in the hands of the customer. So that’s one aspect: it’s just making sure the team is set up for success, both in terms of the ways they work and what tools they have to work as well, then. I guess the other side of that coin is what we actually work on. So understanding the value of potential work we could do, and helping the team understand what that value is, and, and ultimately giving direction of what things we want to work on next. Obviously, that’s not my decision in isolation, but understanding on the one hand, what other stakeholders want to do, what my superiors wants to do, as well. And trying to put that all into the mix to understand these are the next best projects to work on given a finite amount of people to work on these problems. And then ultimately, then, the last part, then, is ultimately helping the team deliver those projects, those products as well then, which usually means calling on my experience of having solved these problems myself, either directly when I was earlier in my career or indirectly through leading others then or, you know, being the head of a team and working with some other great people and to learn from their experiences as well.\nBrian Tarran\nWhat does data science mean to you, personally? I’m not asking you to define it for everybody. But for you, what is what is data science?\nNiclas Thomas\nYeah, I wish I’d come up over the years with a great definition of this. But yeah, I mean, really, it just, I mean, at the very highest level, it just means using data to drive business value, I suppose, as I guess in my– which probably reflects the fact that it’s more of a business role that I have. But I think that in its broadest sense, I think that’s true: using data to drive insights and make decisions for the business. There are more, I guess, detailed definitions of that. So, for example, the way I’ve always differentiated between data analytics and data science is that if you want to make repeated decisions on a daily or weekly basis, then that’s when it becomes more about a data science question versus a data analytics question, because data analytics is generally about answering large one off ad hoc questions, rather than making the same decision over and over again and using methods appropriate for that. But, ultimately, that’s what data science means to me, I think: making repeated decisions using data and the scientific method to use data for good.\nBrian Tarran\nAnd so what do you think is your most important skill as a data scientist given that definition that you have of data science?\nNiclas Thomas\nIn my role, I suppose communication ultimately becomes the most important thing. I’d say definitely earlier in my career, and I think if you’re the person actually delivering and implementing the algorithm, I think that the technical skill set obviously is really important then. But ultimately, I almost see my role as the head of data science as a hybrid– as a link between my team and the rest of the business, then. So it’s really about being able to, on the one hand, translate technical concepts into non technical descriptions of what we’re actually doing, making sure the rest of the business can understand and vice versa, then making sure I understand the business process and business terminology well enough to be able to translate that for the team, as and when needed, into a vision for a project, a product, then, and develop a strategy for that. So I think that the communication both in the strictest sense of being able to talk that through with, with my team, with other team members, with stakeholders, as well, but also more in the looser sense, then, of being able to define that strategy, being able to define what the roadmap for a particular project or a product might look like.\nBrian Tarran\nCan you talk us through your so your education and your training that led up to your kind of first data science job, your first data science role.\nNiclas Thomas\nI suppose the first time, the first time I– actually, I’d never heard about it, I think, when a recruiter approached me. This is probably going back into 2014, when I was maybe eight months into my postdoc after my PhD. I think– obviously it did exist before that, although I suppose the terminology wasn’t quite as widespread going back almost 10 years now where the term is a lot more rife. So my original background, I did a master’s in maths originally, four years. And then I remember being– the last year of that, then, I was applying for a few jobs, and I applied for one at the Met Office, where the focus obviously was predicting weather, forecasting. And I wasn’t successful in that job. But I did notice that the, on the job spec at the time, it was PhD preferred was one of the specs on that role. It was probably the first time I thought about taking on a PhD as more of a career move rather than as the natural progression to an academic career, more of a business career move if you like, then of actually how it can help you in more business settings. So that was at least when I decided to do my PhD and thought it’s certainly not going to be– and this was back in 2008, so at the time of the financial issues at the time when getting jobs was harder anyway, so it felt like a win-win of doing something that would be– I was clear I wanted to work in a data role of some sort. And that combined with the fact that I thought it would be a good career move and the financial climate at the time wasn’t brilliant. So I took on a PhD then. And then in terms of actually getting into, into my first data science position was, as I said, just after I finished my PhD, I had been working about six months, eight months as a postdoc, and then a recruiter just described a role that was available at Tesco at the time. And it sounded a lot of what I was doing in my current postdoc role at the time – making predictions based on data and exactly the same techniques – sounded really interesting. And it must have been the way the recruiter sold it at the time as well then, because it’s something I was really keen to take on and then made my move off the back of that then. So yeah, kind of moved into it a little bit, I guess, semi deliberately from taking a PhD on first, but always with the view of moving over to a business role at some point after that.\nBrian Tarran\nBut it wasn’t like you started out your further education thinking, “I want to be a data scientist, what do I need to do to kind of get there? What are the subjects I need to focus on? What are the topics I need to research?\nNiclas Thomas\nYeah. Oh, absolutely. Yeah, it certainly wasn’t by design at the very start of my journey. I fell in love with math, really, and just fell into data science because of that, really, I loved numbers and loved solving maths problems. So that’s why I did a degree in it first of all, then and certainly, you know, even midway through my degree, then I wasn’t really sure what I wanted to do. It was more, as you say, just by chance, then, that there were a few opportune moments that came around then, that opportunities came around at the right time to fall into that career.\nBrian Tarran\nDoing a PhD in machine learning as you did, that was quite a – in hindsight – a smart choice of PhD to pursue, I think, right?\nNiclas Thomas\nI think so. Yeah, I suppose it was– still even at that stage it wasn’t necessarily, again, the terminology ‘data science’ wasn’t really around. Certainly, when I started my PhD in 2009 2010. It wasn’t really terminology, at least it may have been in usage a little bit in terms of being on, you know, if you look for jobs on LinkedIn or Indeed, but it certainly wasn’t terminology that that I would have been particularly familiar with.\nBrian Tarran\nYour first job in data science was at Tesco. You mentioned that you were you were kind of recruited to that role there. How does it compare to your current role? So I guess, you know, what’s the difference between being a data scientist versus head of data science as you are now?\nNiclas Thomas\nYeah, I think there are probably more similarities than differences, I would say. We were quite lucky in the setup in Tesco that the recruitment strategy seemed to be more focused around people who already had some experience in, generally, either already had business experience or a PhD. So we were fairly independent in solving our own– the project that we were working on and working on that. Not necessarily with the head of data science guiding us, you know, day by day, in terms of the actual nitty gritty and the technical detail, which is great, then. So it did mean that we had responsibility and ownership for our product quite early on. So yeah, I really enjoyed that. I suppose I was writing a lot more code in those days than I do now. I rarely, if ever write code at the moment. So I think that’s probably true for the last maybe three or four years, I think, only occasionally getting my hands dirty. And even when it is, it’s not really to build an algorithm, it’s more to inquire about what data we have to solve the algorithm then. So even when I do get my hands dirty, it’s more in the very early stages of the whole algorithm development lifecycle. So I think that’s probably the biggest difference is just the actual ownership of development there – probably expected, I would say, but it’s– I think that’s one of the beauties of being in your first job or two in data science. I think the– I think in most places I’ve seen, I think you’ll get ownership of, of the work, the stuff that you work on, on a day to day basis, quite early on. And you’ll be expected to contribute code and ideas for that as well, which I think most people would love. I certainly loved it at the time.\nBrian Tarran\nWhat was the most important thing you learned in your first year in that job?\nNiclas Thomas\nI think, again, it’s probably a lot around the ways of working, I would say – of the various ways you can [work], which I never really thought about it before. Working in academia, it was quite isolated, I suppose. You work on your own project, you work on your own work and don’t really– or at least, I found I didn’t really work with anyone else that much. Maybe that was the nature of my work as well, we’d obviously be dependent on people working in a lab to get data. But I think the day to day work, I was working quite in isolation, whereas the team aspect of working, I think, was a steep learning curve then – so agile methodology, and everything around that, which was very, very new to me. And the various ways you can do that. I’m generally not someone for overly putting processes in place in a team, only where necessary. But I think there’s some great learnings from that as well. It certainly started to shape how I think I would want to run a team if and when I got to that position.\nBrian Tarran\nSo, Nick, what have been your career highlights so far?\nNiclas Thomas\nI think in terms of– there was one product we built in Sainsbury’s in particular. So in terms of, on a product level of replenishment. So how do you most efficiently get products from the back of the store onto the shelves of an individual store? And what’s the most optimal strategy to do that, which I love for a variety of reasons. A, it was one of the first full data science products that we had deployed and worked on as a team in Sainsbury’s. So there was that kind of milestone about it. I think it also stood out as a really nice move away from classic machine learning – i.e., making a prediction, a classification model – to something that was a bit more operations research based and more based on optimization. So using graph theory, making a graph network of a store. And using that to solve the problem of taking a route through the store, for example, a bit like a Google Maps for a store basically, was how we always pitched it to our stakeholders, and how can you choose the best route and again, moving more into a bit more of a vehicle routing problem, then: if you’ve got two different trolleys, how do you decide what items to put on trolley one versus trolley two? So there’s loads of interesting stuff on the technical side of things and it was, again, I felt it was probably one of the highlights – as well as the end product, it was also the one I worked on at the very start. So actually, the understanding whether it would be possible to do that, what kind of technical approach. So I think certainly from a product perspective, that’s probably stuck in my mind. Aside from that, on a more personal level, I guess, I did decide to write a book off the back of my PhD. Just mainly on my experiences from my PhD and postdoc. I mean, it’s not like a confessional. But more on the– just working with non data scientists and making it more accessible was really what I really focused on there. So having worked with clinicians, immunologists and others as part of the medical research that I did, I felt that data can be accessible if you pitch it in a way and make it easy to use. And so that was the purpose of what was largely an educational textbook.\nBrian Tarran\nDo you want to give a short plug for the book, what it’s called and where people can find it?\nNiclas Thomas\nYeah, so it’s, Data Science for Immunologists is the name of the book. It’s available on Amazon. I’m one of the two co-authors on that then. And we do have a website, datascienceforimmunologists.com, as well then if you did want to visit and you can either buy the book, there’s a link on that website or just go straight to Amazon and it’s available there.\nBrian Tarran\nThis next question, we’ve gone from highlights to lowlights. Have there been any mistakes or regrets that you’ve had along the way in your data science career so far?\nNiclas Thomas\nThe main mistakes I think I’ve made before is not valuing, A, communication or soft skills, but B, the leadership and management as well then. And I think especially it’s something, when working at Gousto as well that was something that was a big focus of the team and something that I really took from my time there as well was the, I guess, the art of good management and good leadership, you know, what the difference is between the two. So I wouldn’t say there’s any one bang event that’s a mistake or regret, but it’s probably, as ever, it’s probably I would have put more emphasis on it sooner had I known that how important those skills would be.\nBrian Tarran\nYeah, but I think that’s understandable to a certain extent. If you’re coming from, I guess, a role that’s very hands on, doing things yourself, getting into the messy details of a project, it can sometimes be hard to kind of take a step back and adopt more of a kind of leadership, management position, can’t it?\nNiclas Thomas\nYeah, definitely. Yeah, definitely. I would agree with that. And I think it’s also, I’d probably say for a lot of people starting out, and certainly it was for me, that the technical– the technical aspect is probably why you get into a role in data science in the first place, that you just love solving problems, basically, whether that’s with code or with pen and paper. And so that’s, that’s what you want to do. And getting your mind focused elsewhere away from that is probably not viewed as the most fun thing to do, I probably wouldn’t have, when I was starting out in 2014, 2015, I probably wouldn’t have thought it was as fun or as interesting to do that as I do now, maybe. So I think that’s the other reason why it probably doesn’t get as much focus earlier on in my career anyway, at least, as it probably deserved.\nBrian Tarran\nHow do you think your– how do you see your role, I guess, evolving over the rest of your career in data science?\nNiclas Thomas\nI suppose on a personal level, for me it’s, I’m always thinking of what, 10 years down the line, do I still want to be focused just on data science? Or do I want to be focused on a data role, more broadly? I suppose that’s always the main question to ask. And so by that I mean, looking at data engineering as well, data analytics, and being responsible for a wider group. I think the way the field is going anyway, I think a lot more companies seem to move to vertical management rather than horizontal. So by that, I mean having heads of data in different areas of the business. So rather than having a head of data and a head of analytics, you might have a head of data for certain aspects of the business and another head of data then that’s responsible for both in other areas of the business, then. So either way, I think that the broadening of responsibilities and not just being responsible for data science is probably one way I would see my career potentially moving. At the moment, I love just focusing just on the data science, I’m really happy doing that now. But I think that could be one way that my focus changes in the future.\nBrian Tarran\nWhat personal or professional advice would you give for anyone wanting to be a data scientist?\nNiclas Thomas\nYeah, so first of all, the balance between the soft and hard skills. I think I’ve alluded to it before, but the– don’t put too much– I mean, still emphasise on the technical skills are really important, but don’t feel like it’s the be all end all. I think just understanding the softer side of how you communicate, how you tell a story, for example, and storytelling with data, I think is really important. So I’d say that’s probably one focus area. I think that the second would probably, and maybe it’s a harder one to act on, but being passionate, I think, because whenever I’m looking to recruit anyone new into my team, I think it’s as much about understanding what the potential of that person is as is what is their current performance or where their current capability is – how good they could be in the future is arguably more important. And I think a lot of that comes to ultimately someone’s– whether they have a fixed or growth mindset. So by that, I mean, ultimately, do they want to learn or not, and if they really want to learn, as a lot of data scientists do, but if they have a huge passion for or about data science, and wanting to learn about just how to get better – whether that’s a better coder, better at maths, anything around that – then if you have that attitude, I think then it’s, A, you can have a great impact on our team, but B, I think it’s a sign of someone who can be a great performer in the future.\nBrian Tarran\nSo what do you think will be the main challenges facing data science as a field over the next few years?\nNiclas Thomas\nI think probably, certainly, currently maybe living up to the hype, I suppose. And matching I suppose the classic Gartner Hype Cycle of, it feels like we’re probably at the stage where there’s a lot of– the hype has been around for a few years of data science now and I think making sure we tackle the right problems, I suppose, is one of the – and by ‘we’ I mean, Next as a business or whatever business we’re working in at the time – I think it’s making sure we’re working on the right things. Because I think a lot of people will be keen to have data scientists as part of their work and the product they’re trying to build. What is the best place to spend our time, and what projects we should be working on most I think is– becomes important then because, as I say, there’s a huge demand for data scientists time, I think, in every company. And so choosing where we spend that time wisely, I think, becomes the key challenge and the important decisions for, especially for a head of data science like myself to make then, to make sure we’re best using the team’s capacity, then.\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘I fell in love with math, really, and fell into data science because of that.’” Real World Data Science, October 4, 2023. URL"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders of Real World Data Science (RWDS) pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders of Real World Data Science (RWDS) pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Our standards",
    "text": "Our standards\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Enforcement responsibilities",
    "text": "Enforcement responsibilities\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Scope",
    "text": "Scope\nThis Code of Conduct applies within all community spaces (encompassing this site, our GitHub repository, our social media channels, and any RWDS-organised online and offline events). It also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\nNote that unless prior permission is agreed in writing with the editor of RWDS, only the editor and editorial board of RWDS may officially represent the community. Comment to the media must only be given by appointed representatives and must be approved by the RSS press office.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Enforcement",
    "text": "Enforcement\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at rwds@rss.org.uk. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Enforcement guidelines",
    "text": "Enforcement guidelines\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n1. Correction\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n2. Warning\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n3. Temporary Ban\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n4. Permanent Ban\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Attribution",
    "text": "Attribution\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "about-rwds.html",
    "href": "about-rwds.html",
    "title": "Welcome to Real World Data Science",
    "section": "",
    "text": "Welcome to the home of Real World Data Science, a new project from the Royal Statistical Society, in partnership with the American Statistical Association. This site and its content are being developed by data science practitioners and leaders with a single goal in mind: to help you deliver high quality, ethical, impactful data science in your workplace.\nMeet the team",
    "crumbs": [
      "Welcome to Real World Data Science"
    ]
  },
  {
    "objectID": "about-rwds.html#what-are-our-aims",
    "href": "about-rwds.html#what-are-our-aims",
    "title": "Welcome to Real World Data Science",
    "section": "What are our aims?",
    "text": "What are our aims?\nReal World Data Science aims to be a trusted, go-to source for high-quality, engaging and inspiring content which helps data science students, practitioners and leaders to:\n\ndiscover and learn more efficiently;\n\nacquire practical problem-solving skills;\n\nshare their knowledge and accomplishments publicly;\n\nwork smarter, ethically, and more effectively.",
    "crumbs": [
      "Welcome to Real World Data Science"
    ]
  },
  {
    "objectID": "about-rwds.html#what-we-provide",
    "href": "about-rwds.html#what-we-provide",
    "title": "Welcome to Real World Data Science",
    "section": "What we provide",
    "text": "What we provide\nResources are created to meet the needs of our target audiences. These include:\n\nCase studies – showing how data science is used to solve real-world problems in business, public policy and beyond.\nExplainers – interrogating the underlying assumptions and limitations of data science tools and methods, to help data scientists make smarter, more informed analytical choices.\nExercises – to challenge and develop the analytical mindset that all data scientists need to succeed.\n\nAdvice – interviews, Q&As, and FAQs on such topics as data science ethics, career paths, and communication, to support professional development.\n\nWe are also curating resources to help data scientists identify trustworthy, high-quality content. These include:\n\nTraining guides – step-by-step approaches and recommended sources for learning new skills and methods.\nDatasets – tagged and sorted to help educators and practitioners find data to meet their teaching and training needs.\nFeeds – who and what to follow to keep up with new ideas and developments.",
    "crumbs": [
      "Welcome to Real World Data Science"
    ]
  },
  {
    "objectID": "about-rwds.html#how-you-can-get-involved",
    "href": "about-rwds.html#how-you-can-get-involved",
    "title": "Welcome to Real World Data Science",
    "section": "How you can get involved",
    "text": "How you can get involved\nSee our open call for contributions.",
    "crumbs": [
      "Welcome to Real World Data Science"
    ]
  },
  {
    "objectID": "LICENCE.html",
    "href": "LICENCE.html",
    "title": "Licence",
    "section": "",
    "text": "The website realworlddatascience.net (“the website”) and the “Real World Data Science” and “RWDS” brands and logos are copyright © Royal Statistical Society.\nCopyright and licence terms for published articles and any associated videos, images, or other material can be found at the end of each article page, e.g.:\n\n\n\nExample of copyright and licence information from an RWDS article.\n\n\nWe make every reasonable effort to locate, contact and acknowledge copyright owners and wish to be informed by any copyright owners who are not properly identified and acknowledged so that we may make any necessary corrections.\nWhere licence terms for individual articles, videos, images and other published content permit republication, you may do so in accordance with the stated terms of the respective licence(s).\nYou are not permitted to republish the website in its entirety.",
    "crumbs": [
      "Licence"
    ]
  },
  {
    "objectID": "LICENCE.html#content",
    "href": "LICENCE.html#content",
    "title": "Licence",
    "section": "",
    "text": "The website realworlddatascience.net (“the website”) and the “Real World Data Science” and “RWDS” brands and logos are copyright © Royal Statistical Society.\nCopyright and licence terms for published articles and any associated videos, images, or other material can be found at the end of each article page, e.g.:\n\n\n\nExample of copyright and licence information from an RWDS article.\n\n\nWe make every reasonable effort to locate, contact and acknowledge copyright owners and wish to be informed by any copyright owners who are not properly identified and acknowledged so that we may make any necessary corrections.\nWhere licence terms for individual articles, videos, images and other published content permit republication, you may do so in accordance with the stated terms of the respective licence(s).\nYou are not permitted to republish the website in its entirety.",
    "crumbs": [
      "Licence"
    ]
  },
  {
    "objectID": "LICENCE.html#software-and-services",
    "href": "LICENCE.html#software-and-services",
    "title": "Licence",
    "section": "Software and services",
    "text": "Software and services\nSource code and files for the website are available from GitHub. Use of our GitHub repository is governed by the Contributor Covenant Code of Conduct.\nThe website is built using Quarto, an open-source scientific and technical publishing system developed by Posit. Quarto source code and software licences are available from GitHub.\nReal World Data Science is hosted by GitHub Pages.\nThe website uses Google Analytics 4 for web analytics reporting.\nFonts used on Real World Data Science are served by the Google Fonts API. This is to improve site loading speeds and font compatibility across devices. Review the Google Fonts Privacy and Data Collection statement.\nUser comments and reaction functionality is provided by giscus, a comments system powered by GitHub Discussions. Use of this comment functionality is governed by the Contributor Covenant Code of Conduct.",
    "crumbs": [
      "Licence"
    ]
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/08/what-is-CDE-2.html",
    "href": "applied-insights/case-studies/posts/2024/11/08/what-is-CDE-2.html",
    "title": "Advancing Data Science in Official Statistics – What is the Curated Data Enterprise?",
    "section": "",
    "text": "Acknowledgments: This research was sponsored by the:  Unites States Census Bureau Agreement No. 01-21-MOU-06 and  Alfred P. Sloan Foundation Grant No. G-2022-19536\nThe views expressed in this perspective are those of the authors and not the Census Bureau."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/08/what-is-CDE-2.html#introduction",
    "href": "applied-insights/case-studies/posts/2024/11/08/what-is-CDE-2.html#introduction",
    "title": "Advancing Data Science in Official Statistics – What is the Curated Data Enterprise?",
    "section": "Introduction",
    "text": "Introduction\nToday, official statistics – tables, reports and microdata – are produced using data from a single survey. These surveys are foundational for researchers and policymakers. However, many issues cannot be answered by surveys alone. For example, creating a picture of how prepared skilled nursing facilities (SNFs) are for climate emergencies requires wrangling all types of data about the facilities and their communities.(Note: A skilled nursing facility is a facility that meets specific federal regulatory certification requirements that enable it to provide short-term inpatient care and services to patients who require medical, nursing, or rehabilitative services.) This includes SNF data on the number and dates of inspections, deficiencies, residents’ mental and physical health, the number of nursing staff and where they live, community assets data on the number of shelter facilities, health professionals and emergency service providers, and community risks data on the probability of an extreme climate event. How can we create new statistical products useful to policymakers, emergency responders, skilled nursing facility staff, and others to inform their decisions?\n\n\n\n\n\n\nOfficial statistics\n\n\n\nOfficial statistics are essential for a democratic society as they provide economic, demographic, social, and environmental data about the government, the economy, and the environment. Official statistical agencies should compile and make these statistics available impartially to honor the right to public information.\nObjective, reliable, and accessible official statistics instill confidence in the integrity of government and public decision-making regarding a country’s economic, social, and environmental situation at national and international levels. They should be widely available and meet the needs of various users (United Nations 2024).\n\n\nWith the explosion of available data, there is an opportunity to combine all types of information to create statistical products that address cross-cutting topics for a wide range of purposes and uses. The US Census Bureau is modernizing and transforming its enterprise system to accommodate a new way to produce statistical products that take advantage of all data types: designed surveys and censuses, public and private administrative data, opportunity data scraped from the internet, and procedural data (Keller et al. 2022).\n\n‘We are moving towards a single enterprise, data-centric operation that enables us to funnel data from many sources in a single data lake using common collection and ingestion platforms… This is the essence of a curated data approach — assemble, assess, and fill in the gaps to create quality statistical data.’\n\n\nRobert Santos, Director, US Census Bureau\n\nThis curated approach is embodied in the Curated Data Enterprise (CDE). The Curated Data Enterprise Framework in Figure 1 provides a guide for creating statistical products that enable the full integration of data from many sources (Keller et al. 2020). At the heart of the framework are the purposes and uses that provide the context and driving force for developing the statistical product. The outer rectangle in Figure 1 identifies the guiding principles for ethical, transparent and reproducible product development and dissemination. The inner rectangle identifies the steps in the statistical product development, including integrating primary and secondary data sources. The arrows convey that this process may only sometimes be linear. Instead, the process is iterative, where new information may be discovered at any point, requiring reevaluating and updating prior steps. Our Social and Decision Analytics research group in the Biocomplexity Institute developed, tested, and refined the CDE (data science) Framework in our research since 2013 (Keller, Lancaster, and Shipp 2017; Keller et al. 2020). The proposed use of the CDE to develop statistical products at the US Census Bureau is in its early stages.\n\n\n\n\n\n\nFigure 1: The CDE Framework starts with the purposes & uses of the statistical products. The outer rectangle identifies the guiding principles for ethical, transparent, reproducible statistical product development and dissemination. The inner rectangle identifies the statistical product development steps.\n\n\n\nThe next article in this series will put the CDE Framework into practice by demonstrating the use case on skilled nursing facilities’ preparedness for emergencies during extreme climate events. As a prelude to that article, we have created a visual for the statistical product development component of how that process works in action in Figure 2.\n\n\n\n\n\n\nFigure 2: Example: Steps in the statistical product development for the skilled nursing facility use case. The diagram describes the steps applied to a use case on the resilience of skilled nursing facilities. Section 3 of this series describes the steps in detail.\n\n\n\nThe CDE Framework’s guiding principles and research steps are described below. To find out more click on a cross reference.\nGuiding principles:\n\nPurposes and uses\nStakeholders\nCuration\nEquity and ethics\nPrivacy and confidentiality\nCommunications and dissemination\n\nResearch steps:\n\nSubject matter input\nData discovery\nData ingestion & Governance\nData wrangling\nFitness-for-purpose\nStatistics development"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/08/what-is-CDE-2.html#guiding-principles",
    "href": "applied-insights/case-studies/posts/2024/11/08/what-is-CDE-2.html#guiding-principles",
    "title": "Advancing Data Science in Official Statistics – What is the Curated Data Enterprise?",
    "section": "Guiding principles",
    "text": "Guiding principles\n\nPurposes and uses\nThe CDE is centered on developing statistical products to meet specific purposes and uses. Researchers and stakeholders propose the purposes and uses, defining the ‘why’ for developing statistics and statistical products. They include questions or issues that the statistics should be designed to support and are clarified by documented best practices, literature reviews and conversations with subject matter experts.\n\n\nStakeholders\nStakeholders include individuals, groups, and organizations that have the potential to affect or be affected by the outcome of the research. Engaging stakeholders is crucial for fostering the connection and trust that can lead to better decision making. Kujala et al. (2022) best described the principle of stakeholder engagement: ‘Stakeholder engagement refers to the aims, activities, and impacts of stakeholder relations in a moral, strategic, and pragmatic manner.’ When placed within the CDE context and represented in the Framework, collaborative engagement with stakeholders occurs at all stages of product development to better understand what the final product needs to look like. Further, product development is not a linear process but occurs through successive waves of iteration with users.\nForming partnerships with stakeholders is instrumental in identifying requirements and implementing statistical products. This requires listening to community voices in an active engagement strategy.1 Of necessity, these partnerships entail collaboration, such as creative and collaborative problem-solving workshops and the development of innovative digital tools vetted by networks of users.2\n\n\nCuration\nThe broad meaning of curation is the act of organizing, documenting and maintaining a collection of artifacts. The artifacts of the development and dissemination of statistics or statistical products include all the components in Figure 1, from meeting with stakeholders to formulating the purposes and uses to creating and disseminating the statistical products. Maintaining the artifacts is the essence of the CDE. Every step in the process should be documented and easily accessible in a repository, for example, GitHub, for the work to be transparent and reproducible. Curation in the context of the CDE is an end-to-end activity. It involves documenting the purpose and use, providing the context for acquiring, wrangling, and archiving data from many sources to support the development of statistical products. It will include metadata (Cannon 2013), the code used to read and write the data, and the code that ingested the data from the source and prepared it for analysis.\nCuration steps\n\nDocument the development of the research questions, why this research is important, and how it supports the purposes and uses and resulting statistical product.\nDocument the context for the purposes and uses, ie, a policy directive, stakeholder request, policy evaluation, etc.\nWhat stakeholder engagement and transparency are built into the process?\n\n\n\nEquity and ethics\nAn ethics review ensures dialogue on this topic throughout the statistical product development and dissemination life cycle. This involves teams of researchers and stakeholders across many areas of expertise, each with its own research integrity norms and practices. This requires that ethics be woven into every aspect of the CDE. An equity review ensures that underserved groups are represented and biases inherent in various data sources are acknowledged.\nCuration questions\n\nWhat are the project’s expected benefits to the ‘public good’? Do they outweigh potential risks to specific sub-populations, eg, individuals, firms and their locations by different levels of geography?\nAre there implicit assumptions and biases regarding the studied communities in framing the project and associated data sources? If yes, how will they be addressed?\nWhat type of institutional approval process and contracts are needed? What statistical quality standards and confidentiality standards will be needed? For an explanation of the Institution Review Board see Note 1.\n\nAn ethics checklist can help with this process. Links to ethics checklists are provided below.\n\nUniversity of Virginia, Biocomplexity Institute, Social and Decision Analytics Division Data Science Project Ethics Tool\nUnited Kingdom Government, Data Ethics Framework\n\n\n\nPrivacy and confidentiality\nPrivacy is about the individual, whereas confidentiality is about the individual’s information. Privacy refers to an individual’s desire to control their information. Confidentiality refers to the researcher’s agreement with the individual, which could be an agency like the Census Bureau, regarding how their information will be handled, managed, and disseminated (Keller, Shipp, and Schroeder 2016). This is a guiding principle because it needs to be considered and embraced at the earliest possible stages of statistical product development and will impact dissemination choices.\nCuration questions\n\nWhat steps are taken to ensure the privacy and confidentiality of the data?\nWhat statistical methods (if any) are used to ensure the privacy and confidentiality of the data?\nHow do the methods chosen to protect confidentiality affect the purposes and uses of the data?\nWhat stakeholder engagement and transparency are built into the process?\nDoes the context surrounding the purposes, uses, and anticipated data sources require an Institutional Review Board (IRB) review and approval? If yes, is it archived?\n\n\n\n\n\n\n\nNote 1: Institutional Review Board\n\n\n\nIn the United States, institutional review boards (IRBs) assess the ethics and safety of research studies involving human subjects, such as behavioral studies or clinical trials for new drugs or medical devices. Today, the definition of human subjects has evolved to include secondary data, such as administrative data collected for other purposes, eg, local property data collected for tax purposes.\nThe Belmont Commission was convened in the late 1970s after the ethical failures of many research projects that involved vulnerable populations surfaced. The Belmont Commission issued three principles for the conduct of ethical research:\n\nRespect for people — treating people as autonomous and honoring their wishes\nBeneficence — understanding the risks and benefits of the study and weighing the balance between (1) doing no harm and (2) maximizing possible benefits and minimizing possible harms\nJustice — deciding if the risks and benefits of research are distributed fairly.\n\nThese principles were translated to a set of regulations called the Common Rule that govern federally-funded research. The Belmont Commission provided the foundation for IRB principles and focused on research involving human subjects in experiments and studies. IRB approval is required to be eligible for federal grants and contracts. Many universities also require IRB review for research conducted by faculty, students, and researchers (Shipp, LaLonde, and Martinez 2023).\n\n\n\n\nCommunication and dissemination\nCommunication involves sharing data, statistical method choices, well-documented code, working papers, and dissemination through research team meetings, stakeholder engagements, conference presentations, publications, webinars, websites, and social media. As a principle, communication and dissemination are critical to ensure that statistical product development processes and findings are transparent and reproducible (Berman et al. 2016). An essential facet of this step is to tell the story of the analysis by conveying the context, purpose, and implications of the research and findings (Berinato 2019; Wing 2019; NASEM 2022).\nCuration questions\n\nAre the meeting notes, statistical products, code, reports, and presentations archived in a repository?\nBriefly describe what did not work in this process, eg, data wrangling challenges where data sources could not be integrated, data source changes after a fitness-for-purpose assessment, analyses that were changed because assumptions were not met, etc.\nHave project methods and outputs been made as transparent as possible?\nAre the potential limitations of the research clearly presented?\nWhy or why not should the research be used as the basis for an institutional or policy action?\nHave the predicted benefits and social costs to all potentially affected communities been considered?"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/08/what-is-CDE-2.html#research-steps",
    "href": "applied-insights/case-studies/posts/2024/11/08/what-is-CDE-2.html#research-steps",
    "title": "Advancing Data Science in Official Statistics – What is the Curated Data Enterprise?",
    "section": "Research steps",
    "text": "Research steps\n\nSubject matter input\nSubject matter (domain) expertise plays a role in translating the information acquired into understanding the underlying phenomena in the data (Box et al. 1978). Domain knowledge provides the context to define, evaluate and interpret the findings at each research stage (Leonelli 2019; Snee, DeVeaux, and Hoerl 2014). Subject matter input can be obtained through a review of the literature, talking to experts, or learning about their work at conferences or other convenings. Subject matter experts are different than stakeholders. Both provide important input to identifying and clarifying purposes and uses.\nCuration steps\n\nDocument the meetings with subject matter experts and stakeholders.\nDocument the literature search methods and the results of the literature review.\nDocument choices are made during the development of the products.\nWere subject matter experts and stakeholders recruited from underrepresented groups?\n\n\n\nData discovery\nData discovery identifies potential sources that address the research goals defined by purposes and uses. Data sources include the following types (Keller et al. 2020).\n\nDesigned data are collected using statistically designed methods, such as surveys, censuses, and data generated from an experimental or quasi-experimental design, such as a clinical trial or agricultural field study.\nAdministrative data are collected for the administration of an organization or program by entities such as government agencies.\nOpportunity data are derived from internet-based information, such as websites, wearable and other sensor devices, and social media, and captured through application programming interfaces (APIs) and web scraping, eg, geocoded place-based data, transportation routes, and other data sources.\nProcedural data are processes and policies, such as a change in health care coverage, a data repository policy outlining procedures and the metadata required to store data, or a responsible AI policy.\n\nThe goal of the data discovery process is to think broadly and imaginatively about all data types and to capture the variety of data sources that could be useful for the problem. There are three steps in the data discovery process (Keller, Shipp, and Schroeder 2016).\n\nIdentify potential data sources and make an inventory.\nCreate a set of questions to screen the data sources to ensure the data meet the criteria for use.\nSelect and acquire the data sources that meet the screening criteria.\n\nCuration steps\n\nDescribe your data discovery process and reasoning behind the selected data sources.\n\nDo underrepresented groups have adequate geographic coverage? If not, are there methods, such as synthetic data, you can use to provide adequate coverage?\nHave checks and balances been established to identify and address implicit biases in the data and interpretation of the data? Has the team engaged in discussion and provided insights across their diverse perspectives?\n\nDescribe the assumptions that need to be made to use these data sources.\nIdentify and document the paradata and metadata that describe each data source. Paradata describe how the data were collected, while metadata are ‘data about data’. It includes information about the data’s content, data dictionaries and technical documents that will help the user assess its fitness for purpose (Cannon 2013; NASEM 2022).\nDiscuss data sources you would have used if they were available.\n\n\n\nData ingest and governance\nData ingestion is the process of bringing data into the data management platform(s) for use. Data governance establishes and adheres to rules and procedures regarding data access, dissemination and destruction.\nCuration steps\n\nDocument policies and institutional agreements for data use.\n\nHave team members reviewed data use agreements, standard operating procedures (SOPs), and data management plans? Are they fair?\nDo additional procedures need to be defined for this project?\n\nDocument the code and processes used to ingest the data sources and manage governance.\n\n\n\nData wrangling\nData wrangling includes the activities of data profiling, preparing, linking and exploring used to assess the data’s quality and representativeness and what analyses the data can support.\n\nTable 1. Activities of data wrangling\n\n\n\n\n\n\n\n\nProfiling\nPreparing\nLinking\nExploring\n\n\n\n\n\ndata quality\ndata structure\nmeta data, paradata, and provenance\n\n\ncleaning\ntransforming\nstructuring\n\n\nontology selection & alignment\nentity resolution / harmonization\n\n\nvisualizations\ndescriptive statistics\ncharacterizations\n\n\n\n\nCuration steps\n\nDescribe any data quality issues within the stated purpose and use context and how they were resolved. This can include statistical solutions like imputing missing data, identifying outliers or constructing synthetic populations.\n\nHow representative are the data?\nWhat populations are and are not covered?\n\nDescribe any issues with the wrangling process and how they were resolved.\nDocument the code used to wrangle the data and describe how it was validated.\nDocument assumptions made regarding the transformation and use of the data.\n\n\n\nFitness-for-purpose\nFitness-for-purpose starts with assessing the constraints imposed on the data by the particular statistical methods used and the population to which the inferences extend. It is a function of the modeling, data quality needs of the models, and data coverage (representativeness) needs of the models. The statistical product’s ‘fitness-for-purpose’ involves those on the receiving end of the data helping identify issues germane to the data application, such as identifying biases affecting equity. For example, given known differences in their availability, does using administrative records lead to better modeling outcomes for some groups more than others? What can be done to compensate for such bias?\nCuration steps\n\nDocument the constraints and limitations of the data. \n\nWhat are the limitations of the results? Are the results useful, given the purpose of the study?\n\nDiscuss the populations to which any inferences will generalize.\n\nDo the statistical results support the potential benefits of the study previously stated?\nDo any data require revisiting the question of potential biases being introduced through the choice of data sets and variables?\n\n\n\n\nStatistics development\nThe development of statistics and statistical products for dissemination is a function of the research questions, the data’s limitations and the assumptions of the statistical method(s) used.\nCuration steps\n\nDescribe the statistical methods planned and used and how the method assumptions were evaluated.\nDiscuss the conclusions of the statistical analyses and any inferences that can be made from the disseminated statistical products.\nDiscuss how the statistics support the purposes and uses driving the development of the products.\n\nHere, we have defined the CDE and provided a conceptual walk through of the framework from Figure 1. In the next article, we will put the CDE Framework into practice through a demonstration use case on the resilience of skilled nursing facilities.\n\n\n\n\n← Part 1: The policy problem\n\n\n\n\nPart 3: Climate resiliency of skilled nursing facilities →\n\n\n\n\n\n\n\n\nAbout the authors\n\nSallie Keller is the Chief Scientist and Associate Director of Research and Methodology at the US Census Bureau. She is a statistician with research interest in social and decision informatics, statistics underpinnings of data science, and data access and confidentiality. Sallie Keller was at the University of Virginia when this work was conducted.\n\n\nStephanie Shipp leads the Curated Data Enterprise research portfolio and collaborates with the US Census. She is an economist with experience in data science, survey statistics, public policy, innovation, ethics, and evaluation.\n\n\nVicki Lancaster is a statistician with expertise in experimental design, linear models, computation, visualizations, data analysis, and interpretation. She works with scientists at federal agencies on projects requiring statistical skills and creativity, eg, defining skilled technical workforce using novel data sources.\n\n\nJoseph Salvo is a demographer with experience in US Census Bureau statistics and data. He makes presentations on demographic subjects to a wide range of groups about managing major demographic projects involving the analysis of large data sets for local applications.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Stephanie Shipp\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Chay_Tee on Shutterstock.\n\n\n\nHow to cite\n\nKeller S, Shipp S, Lancaster V, Salvo J (2024). “Advancing Data Science in Official Statistics – What is the Curated Data Enterprise?” Real World Data Science, November 8, 2024. URL"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/08/what-is-CDE-2.html#footnotes",
    "href": "applied-insights/case-studies/posts/2024/11/08/what-is-CDE-2.html#footnotes",
    "title": "Advancing Data Science in Official Statistics – What is the Curated Data Enterprise?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.census.gov/newsroom/blogs/director/2023/01/a-look-ahead-2023.html ↩︎\nhttps://www.census.gov/partners/act.html↩︎"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/22/development-plan-2.html",
    "href": "applied-insights/case-studies/posts/2024/11/22/development-plan-2.html",
    "title": "Defining Purposes and Uses to Support the Development of Statistical Products in a 21st Century Census Curated Data Enterprise Environment",
    "section": "",
    "text": "Acknowledgments: This research was sponsored by the:  Unites States Census Bureau Agreement No. 01-21-MOU-06 and  Alfred P. Sloan Foundation Grant No. G-2022-19536\nThe views expressed in this article are those of the authors and not the Census Bureau."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/22/development-plan-2.html#summing-it-up",
    "href": "applied-insights/case-studies/posts/2024/11/22/development-plan-2.html#summing-it-up",
    "title": "Defining Purposes and Uses to Support the Development of Statistical Products in a 21st Century Census Curated Data Enterprise Environment",
    "section": "1 Summing it up",
    "text": "1 Summing it up\nWe end where we began in the first article of our series. Through this four-part series, we introduced a Curated Data Enterprise (CDE) Framework (see Figure 1) that can guide the development and dissemination of statistics broadly applicable to addressing social and economic issues while ensuring replicability and reusability. The CDE provides the scaffold for scaling the statistical product development of interest to the US Census Bureau and broadly applies to official statistics agencies (Keller et al. 2022). We illustrated this through a use case on climate resiliency of skilled nursing facilities, highlighting the replicability and reusability of the capabilities that would benefit inclusion in a CDE.\n\n\n\n\n\n\nFigure 1: The CDE Framework starts with the purposes & uses of the statistical products. The outer rectangle identifies the guiding principles for ethical, transparent, reproducible statistical product development and dissemination. The inner rectangle identifies the statistical product development steps.\n\n\n\nAs noted in the first three articles, the process begins with articulating purposes and uses through stakeholder engagement and continues by leveraging that engagement, including subject matter expertise, to inform statistical product development. Eliciting purposes and uses from stakeholders and data users is facilitated by asking questions such as:  \n\nWhat questions keep you awake at night because you don’t have data insights to address them? What are those purposes and uses that you need statistical products to support?\nHow do we collaborate and engage with you to better understand your needs and help you identify gaps in understanding regarding purpose and use?\nHow do we prioritize what statistical products to develop first?\n\nExamples of purposes and uses that drive new statistical products include accurately measuring gig employment (Salvo, Shipp, and Zhang 2022a), migration due to extreme climate events (Salvo, Shipp, and Zhang 2022b), the various dimensions of housing affordability (Wu et al. 2023), and addressing the undercount of young children (Salvo, Lancaster, and Shipp 2023). Other topics that require multiple sources and types of data include creating a household living budget based on the minimum necessary to ensure an adequate standard of living (Lancaster et al. 2023) and using this budget as a starting point for measuring insecurity across components such as food or housing (Montalvo et al. 2023)."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2024/11/22/development-plan-2.html#developing-an-end-to-end-e2e-curation-system",
    "href": "applied-insights/case-studies/posts/2024/11/22/development-plan-2.html#developing-an-end-to-end-e2e-curation-system",
    "title": "Defining Purposes and Uses to Support the Development of Statistical Products in a 21st Century Census Curated Data Enterprise Environment",
    "section": "2 Developing an end-to-end (E2E) curation system",
    "text": "2 Developing an end-to-end (E2E) curation system\nPurposes and uses defined in use cases are important to support the rapid development of statistical products. These use cases will capture the imagination of those working to address today’s critical issues and advance public understanding and trust in federal statistics. The above paragraph provides examples of purposes and uses for which we have developed use cases.\nUse cases are a powerful mechanism to promote methodological research to develop and implement capabilities needed in a CDE. The objectives are to undertake research projects that have the potential to create statistical products with explicit purposes and uses that will exercise the end-to-end (E2E) curation components.\nWhen implemented, these proposed use cases will demonstrate a sequence of capabilities needed to build the CDE, such as agile data discovery, reusing modules and data (including synthetic data), tracking the provenance of collected and generated data, reusing synthetic data and methods to integrate many types of data, conducting statistical analysis involving heterogeneous data integration, and reviewing data and statistical results with an equity and ethics lens. These steps will be captured in an end-to-end curation system.\n\nCriteria for developing and evaluating use cases that will uncover the capabilities and research necessary to develop the CDE\n\nCriteria are needed to evaluate, and partner with researchers and stakeholders in developing and implementing the capabilities to capture in the CDE. The choice of use cases, when curated, needs to provide unique insight into CDE capabilities and statistical product development. The capabilities to be developed include addressing some purpose and use that no single source of information can resolve, generating practical diagnostics to improve existing methods, creating pilot software, and validating new and improved statistical products. These criteria, developed through listening sessions and discussions with experts, guide the prioritization and selection of use cases and their evaluation after curation (see Table 2) (Keller et al. 2022).\n\nTable 2. Criteria for Selecting and Prioritizing Use Cases to Identify CDE Capabilities\n\n\n\n\n\nValue and feasibility of the CDE approach described in the existing research (potential use case) to address emerging or long-standing issues, ie, its purpose and use over and above existing approaches to address high-priority problems. | | Stakeholders’ challenges and issues as the source of purposes and uses. | | Subject matter experts to advise on the approach and implementation. | | Partners to access data from local and state governments, non-profit organizations, and the private sector, and strategies to overcome legal and administrative barriers to such access that benefits to both the providers and recipients of the data. | Survey, administrative, opportunity, and procedural data from multiple sources (eg, local, state, federal, third-party) to address the purpose and use (issue) in an integrated way. There are well-defined data ingestion and governance requirements. | | Computation and measurement requirements for statistical products include the unit(s) of analysis and their characteristics, temporal sequence, geocoded location data, and methods for imputations, projections, and statistical analysis. | | Equity and ethical dimensions are considered at each step to ensure that the use case provides fair and accurate representation across groups and an assessment that the potential benefits outweigh the potential harm. | | Evidence of CDE capabilities to be built, including the code, data, and documentation to create the statistical products, which can be described in the curation step. | | Statistical products include integrated data sources, indicators, maps, visualizations, storytelling and analysis. | | Potential viability of proposed dissemination platforms for interactive access to data products at all levels of data acumen (Keller and Shipp 2021) while adhering to confidentiality and privacy rules. |\n\n\n\n\nAn end-to-end curation process\n\nCuration is an end-to-end process defined by the context of the purposes and uses that document the decisions and trade-offs at each step in the CDE Framework. The following curation definition will be used as it serves the CDE’s vision.\nCuration involves documenting, for each statistical product, the inputs from which the product is derived, the wrangling used to transform the information into product, and the statistical product itself. Purposes and uses provide the context for each statistic and statistical product.\nThis definition has evolved from numerous stakeholder discussions via listening sessions and discussions with Census Bureau staff. (Nusser et al. forthcoming; Faniel, Frank, and Yakel 2019; NASEM 2022).\nAs use cases are curated, the CDE capabilities will evolve to quickly develop statistical products. These curated use cases are integral to developing an E2E curation process for the CDE.  \n\nInvitation to contribute purpose and use ideas for developing new statistical products\n\nThe CDE development aims to curate a significant number of use cases that address social and economic issues that have the potential to define capabilities to be built in the CDE. Initially, they are seeking ideas for purposes and uses to define these use cases and statistical products.\nThe skilled nursing facility use case included code, data, and documentation to calculate the probability of workers getting to work during a weather event, resilience indicators at the county or sub-county level, alternative skilled nursing home deficiency measures, and other capabilities.\nIncorporating capabilities in the CDE\nTo accelerate the development of statistical products, the Census Bureau will develop use cases to articulate and create CDE capabilities. This requires identifying those valuable nuggets for learning and quickly translating and incorporating this information into the CDE. Examples of critical capabilities of interest are learning about the utility of synthetic data, the ability to aggregate data into custom geographies, and combining different units of analysis. The expected outcome is the creation of an innovative 21st Century Census Curated Data Enterprise focused on purposes and uses that overcome the limitations and challenges of today’s survey-alone model.  \nThe 21st Century Census Curated Data Enterprise development presents an opportunity for researchers to help drive the development of the CDE as the foundation for creating new statistical products. The US Census Bureau is seeking ideas for purposes and uses that will define new statistical products. They are interested in research projects (use cases) that are guided by the CDE framework as potential new statistical products. They want to learn from and understand your experiences in using the CDE framework, for example, what worked well, what challenges you faced, how each step in the framework was curated, and what capabilities are replicable and reusable for developing and enhancing statistical products.\n\n\n\n\n← Part 3: Climate resiliency of skilled nursing facilities\n\n\n\n\n\n\n\n\nAbout the authors\n\nStephanie Shipp leads the Curated Data Enterprise research portfolio and collaborates with the US Census. She is an economist with experience in data science, survey statistics, public policy, innovation, ethics, and evaluation.\n\n\nJoseph Salvo is a demographer with experience in US Census Bureau statistics and data. He makes presentations on demographic subjects to a wide range of groups about managing major demographic projects involving the analysis of large data sets for local applications.\n\n\nVicki Lancaster is a statistician with expertise in experimental design, linear models, computation, visualizations, data analysis, and interpretation.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Stephanie Shipp\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Lukas Blazek on Unsplash.\n\n\n\nHow to cite\n\nShipp S, Salvo J, Lancaster V (2024). “Statistical Products in a 21st Century Census Curated Data Enterprise Environment” Real World Data Science, November 22, 2024. URL"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/05-third-place-winners.html",
    "href": "applied-insights/case-studies/posts/2023/08/21/05-third-place-winners.html",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "",
    "text": "Undergraduate student Yifan (Rosetta) Hu was responsible for writing the Python script that pre-processes the 2015–2016 UPC, EC, and PPC data for training neural network models. Her script randomly sampled five negative EC descriptions for every positive match between a UPC and EC code. Professor Mandy Korpusik performed the remaining work, including setting up the environment, training the BERT model, and evaluation. Hu spent roughly 10 hours on the competition, and Korpusik spent roughly 40 hours of work (and many additional hours running and monitoring the training and testing scripts)."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/05-third-place-winners.html#our-perspective-on-the-challenge",
    "href": "applied-insights/case-studies/posts/2023/08/21/05-third-place-winners.html#our-perspective-on-the-challenge",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Our perspective on the challenge",
    "text": "Our perspective on the challenge\nThe goal of this challenge is to use machine learning and natural language processing (NLP) to link language-based entries in the IRI and FNDDS databases. Our proposed approach is based on our prior work using deep learning models to map users’ natural language meal descriptions to the FNDDS database (Korpusik, Collins, and Glass 2017b) to retrieve nutrition information in a spoken diet tracking system. In the past, we found a trade-off between accuracy and cost, leading us to select convolutional neural networks over recurrent long short-term memory (LSTM) networks – with nearly 10x as many parameters and 2x the training time required, LSTMs achieved slightly lower performance on semantic tagging and food database mapping on meals in the breakfast category. Here, we propose to investigate state-of-the-art transformers, specifically the contextual embedding model (i.e., the entire sentence is used as context to generate the embedding) known as BERT (Bidirectional Encoder Representations from Transformers, Devlin et al. 2018).\n\nRelated work\nWithin the past few years, several papers have come out that learn contextual representations of sentences, where the entire sentence is used to generate embeddings.\nELMo (Peters et al. 2018) uses a linear combination of vectors extracted from intermediate layer representations of a bidirectional LSTM trained on a large text corpus as a language model; in this feature-based approach, the ELMo vector of the full input sentence is concatenated with the standard context-independent token representations and passed through a task-dependent model for final prediction. This showed performance improvement over state-of-the-art on six NLP tasks, including question answering, textual entailment, and sentiment analysis.\nOpenAI GPT (Radford et al. 2018) is a fine-tuning approach, where they first pre-train a multi-layer transformer (Vaswani et al. 2017) as a language model on a large text corpus, and then conduct supervised fine-tuning on the specific task of interest, with a linear softmax layer on top of the pre-trained transformer.\nGoogle’s BERT (2018) is a fine-tuning approach similar to GPT, but with the key difference that instead of combining separately trained forward and backward transformers, they instead use a masked language model for pre-training, where they randomly masked out input tokens and predicted only those tokens. They demonstrated state-of-the-art performance on 11 NLP tasks, including the CoNLL 2003 named entity recognition task, which is similar to our semantic tagging task.\nFinally, many models have recently been developed that improve upon BERT, including RoBERTa (which improves BERT’s pre-training by using bigger batches and more data, Y. Liu et al. 2019), XLNet (which uses Transformer-XL and avoids BERT’s pretrain-finetune discrepancy through learning a truly bidirectional context via permutations over the factorization order, Yang et al. 2019), and ALBERT (a lightweight BERT, Lan et al. 2019).\nIn our prior work on language understanding for nutrition (Korpusik et al. 2014, 2016; Korpusik and Glass 2017, 2018, 2019; Korpusik, Collins, and Glass 2017a), we used a similar binary classification approach for learning embeddings, which were then used at test time to map from user-described meals to USDA food database matches, but with convolutional neural networks (CNNs) instead of BERT. (BERT was not created until 2018, and due to limited memory available for deployment, we needed a smaller model than even BERT base, which has 100 million parameters.) Further work demonstrated that BERT outperformed CNNs on several language understanding tasks, including nutrition (Korpusik, Liu, and Glass 2019)."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/05-third-place-winners.html#our-approach",
    "href": "applied-insights/case-studies/posts/2023/08/21/05-third-place-winners.html#our-approach",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Our approach",
    "text": "Our approach\nOur approach is to fine-tune a large pre-trained BERT language model on the food data. BERT was originally trained on a massive amount of text for a language modelling task (i.e., predicting which word should come next in a sentence). It relies on a transformer model, which uses an “attention” mechanism to identify which words the model should pay the most “attention” to. We are specifically using BERT for binary sequence classification, which refers to predicting a label (i.e., classification) for a sequence of words. In our case, during fine-tuning (i.e., training the model further on our own dataset) we will feed the model pairs of sentences (where one sentence is the UPC description of a food item and the other is the EC description of another food item), and the model will perform binary classification, predicting whether the sentences are a match (i.e., 1) or not (i.e., 0). We start with the 2015–2016 ground truth PPC data for positive examples, and five randomly sampled negative examples per positive example.\n\nTraining methods\nSince we used a neural network model, the only features passed into our model were the tokenized words themselves of the EC and UPC food descriptions – we did not conduct any manual feature engineering (Dong and Liu 2018). The model was trained on a 90/10 split into 90% training and 10% validation data, where the validation data was used as a test set to fine-tune the model’s hyperparameters. We started with a randomly sampled set of 16,000 pairs, batch size of 16 (i.e., the model would train on batches of 16 samples at a time), AdamW (Loshchilov and Hutter 2017) as the optimizer (which adaptively updates the learning rate, or how large the update should be to the model’s parameters), a linear schedule with warmup (i.e., starting with a small learning rate in the first few epochs of training due to large variance in early stages of training, L. Liu et al. 2019), and one epoch (i.e., the number of times the model passes through all the training data). We then added the next randomly sampled set of 16,000 pairs to get a model trained on 32,000 data points. Finally, we reached a total of 48,000 data samples used for training. Each pair of sequences was tokenized with the pre-trained BERT tokenizer, with the special CLS and SEP tokens (where CLS is a learned vector that is typically passed to downstream layers for final classification, and SEP is a learned vector that separates two input sequences), and was padded with zeros to the maximum length input sequence of 240 tokens, so that each input sequence would be the same length.\n\n\nModel development approach\nWe faced many challenges due to the secure nature of the ADRF environment. Since our approach relies on BERT, we were blocked by errors due to the local BERT installation. Typically, BERT is downloaded from the web as the program runs. However, for this challenge, BERT must be installed locally for security reasons. To fix the errors, the BERT models needed to be installed with git lfs clone instead of git.\nSecond, we were unable to retrieve the test data from the database due to SQLAlchemy errors. We found a workaround by using DBeaver directly to save database tables as Excel spreadsheets, rather than accessing the database tables through Python.\nFinally, we needed a GPU in order to efficiently train our BERT models. However, we initially only had a CPU, so there was a delay due to setting up the GPU configuration. Once the GPU image was set up, there was still a CUDA error when running the BERT model during training. We determined that the model was too big to fit into GPU memory, so we found a workaround using gradient checkpointing (trading off computation speed for memory) with the transformers library’s Trainer and TrainingArguments. Unfortunately, the version of transformers we were using did not have these tools, and the library was not updated until less than a week before the deadline, so we still had to train the model on the CPU.\nTo deal with the inability to run jobs in the background, our process was checkpointing our models every five batches, and saving the model predictions during evaluation to a csv file every five batches as well.\n\n\n\n\n\n\nFind the code in the Real World Data Science GitHub repository."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/05-third-place-winners.html#our-results",
    "href": "applied-insights/case-studies/posts/2023/08/21/05-third-place-winners.html#our-results",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Our results",
    "text": "Our results\nAfter training, the 48K model (so-called because it was trained on 48,000 data samples) was used at test time via ranking all possible 2017–18 EC descriptions given an unseen UPC description. The rankings were obtained through the model’s output value – the higher the output (or confidence), the more highly we ranked that EC description. To speed up the ranking process, we used blocking (i.e., only ranking a subset of all possible matches), specifically with exact word matches (using only the first six words in the UPC description, which appeared to be the most important), and fed all possible matches through the model in one batch per UPC description. Since we still did not have sufficient time to complete evaluation on the full set of test UPC descriptions, we implemented an expedited evaluation that only considered the first 10 matching EC descriptions in the BERT ranking process (which we call BERT-FAST). We also report results for the slower evaluation method that considers all EC descriptions that match at least one of the first six words in a given UPC description, but note that these results are based on just a small subset of the total test set. See Table 1 below for our results, where the (5?) indicates how often the correct match was ranked among the top-5. See Table 2 for an estimate of how long it takes to train and test the model on a CPU.\n\n\n\n\nTable 1: S@5 and NCDG@5 for BERT, both for fast evaluation over the whole test set, and slower evaluation on a smaller subset (711 UPCs out of 37,693 total).\n\n\n\n\nModel\nSuccess@5\nNDCG@5\n\n\n\n\nBERT-FAST\n0.057\n0.047\n\n\nBERT-SLOW\n0.537\n0.412\n\n\n\n\n\n\nTable 2: An estimate of the time required to train and test the model.\n\n\n\n\n\nTime\n\n\n\n\nTraining (on 48K samples)\n16 hours\n\n\nTesting (BERT-FAST)\n52 hours\n\n\nTesting (BERT-SLOW)\n63 days"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/05-third-place-winners.html#future-workrefinement",
    "href": "applied-insights/case-studies/posts/2023/08/21/05-third-place-winners.html#future-workrefinement",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Future work/refinement",
    "text": "Future work/refinement\nIn the future, with more time available, we would train on all data, not just our limited dataset of 48,000 pairs, as well as perform evaluation on the held-out test set with the full set of possible EC matches that have one or more words in common with the UPC description. We would compare against baseline word embedding methods such as word2vec (Mikolov et al. 2017) and Glove (Pennington, Socher, and Manning 2014), and we would explore hierarchical prediction methods for improving efficiency and accuracy. Specifically, we would first train a classifier to predict the generic food category, and then train finer-grained models to predict specific foods within a general food category. Finally, we are exploring multi-modal transformer-based approaches that allow two input modalities (i.e., food images and text descriptions of a meal) for predicting the best UPC match."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/05-third-place-winners.html#lessons-learned",
    "href": "applied-insights/case-studies/posts/2023/08/21/05-third-place-winners.html#lessons-learned",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Lessons learned",
    "text": "Lessons learned\nWe recommend that future challenges provide every team with both a CPU and a GPU in their workspace, to avoid transitioning from one to the other midway through the challenge. In addition, if possible, it would be very helpful to provide a mechanism for running jobs in the background. Finally, it may be useful for teams to submit snippets of code along with library package names, in order for the installations to be tested properly beforehand.\n\n\n\n\n← Part 4: Second place winners\n\n\n\n\nPart 6: The value of competitions →\n\n\n\n\n\n\n\n\nAbout the authors\n\nYifan (Rosetta) Hu is an undergraduate student and Mandy Korpusik is an assistant professor of computer science at Loyola Marymount University’s Seaver College of Science and Engineering.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Yifan Hu and Mandy Korpusik\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Peter Bond on Unsplash.\n\n\n\nHow to cite\n\nHu, Yifan, and Mandy Korpusik. 2023. “Food for Thought: Third place winners – Loyola Marymount.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/02-competition-design.html",
    "href": "applied-insights/case-studies/posts/2023/08/21/02-competition-design.html",
    "title": "Food for Thought: Competition and challenge design",
    "section": "",
    "text": "Since 2014, the professional services firm Westat, Inc. has been developing the Purchase to Plate Crosswalk (PPC) for the United States Department of Agriculture (USDA) Economic Research Service (ERS). The PPC links the retail food transactions database from IRI’s InfoScan service and the USDA Food and Nutrient Database for Dietary Studies (FNDDS). However, the current linkage process uses only partly automated data matching, meaning it is resource intensive, time consuming, and requires manual review.\nWith sponsorship from ERS, Westat partnered with the Coleridge Initiative to host the Food for Thought competition to challenge researchers and data scientists to use machine learning and natural language processing to find accurate and efficient methods for creating the PPC. Figure 1 provides a visual overview of the challenge set by the competition.\nThe one-to-many matching task that is central to the competition throws up many challenges for researchers to wrestle with. Because IRI data contains food transactions collected from partnered retail establishments for over 350,000 items, the matchings need to be made based on limited data features, including categories, providers, and semantically inconsistent descriptions that consist of short phrases. Consider this hypothetical example: IRI product-related information about a (fictional) “Cheesy Hashbrowns Hamburger Helper, 5.5 Oz Box” needs to be linked to FNDDS nutrition-related information found under “Mixed dishes – meat, poultry, seafood: Mixed meat dishes”. Figure 2 demonstrates how the two databases are linked with each other to create the PPC. As can be seen, there is no common word that easily indicates that “Cheesy Hashbrowns Hamburger Helper…” should be matched with “Mixed dishes…”, and such cases exist in all IRI tables used for the challenge, from 2012 through 2018.\nAlso, because nutritionists or food scientists will always need to review the matching, regardless of the matching method used, it was important that our evaluation of proposed matching methods focused both on the accuracy of prediction models and also on metrics that would lead participants to develop models that facilitate qualified reviewers to reduce their workloads.\nOrganising the competition was also a challenge in its own right, for data privacy reasons. IRI scanner data contains sensitive information, such as store name, location, unit price, and weekly quantity sold for each item. This ruled out using existing online platforms like Kaggle, DrivenData or AIcrowd to host the competition, and instead required a private secure data enclave to ensure the safe use of sensitive and confidential data assets. The need for such an environment imposed capacity constraints on the competition, meaning only dozens of teams could be invited to take part, whereas on open platforms it is common to have thousands of teams competing and sharing ideas and code."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/02-competition-design.html#competition-structure",
    "href": "applied-insights/case-studies/posts/2023/08/21/02-competition-design.html#competition-structure",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Competition structure",
    "text": "Competition structure\nThe competition ran over 10 months and consisted of three separate challenges: two interim, one final. Applications opened in September 2021, and the competition started in January 2022. Submission deadlines for the first and second interim challenges were in July and September 2022, respectively. For these rounds, participants submitted preliminary solutions for evaluation based solely on quantitative metrics, and two awards of $10,000 were given to the highest-scoring teams. The deadline for the final challenge was in October 2022. Here, solutions were evaluated by the scientific review board based on three judging criteria: quantitative metrics, transferability, and innovation. First, second, and third place winners received awards of $30,000, $1,500, and $1,000 respectively. Final presentations were given at the Food for Thought symposium in December 2022.\nThe competition was run entirely within the Coleridge Initiative’s Administrative Data Research Facility (ADRF), which was established by the United States Census Bureau to inform the decision-making of the Commission on Evidence-Based Policy under the Evidence Act. ADRF follows the Five Safes Framework: safe projects, safe people, safe data, safe settings, and safe outputs.\nIn keeping with this framework, participants were provided with ADRF login credentials after signing the relevant data use agreements during the onboarding process. All participants were required to agree to the ADRF terms of use, to complete security training, and to pass a security training assessment prior to accessing the challenge data. Participants’ access within ADRF was limited to the challenge environment and data only. There was no internet access, so Coleridge Initiative ensured that any packages requested by teams were available for use within the environment after passing security review. All codes and documentation were only allowed to be exported outside ADRF after export reviews from both Coleridge Initiative and USDA staff. At the end of each challenge, the teams submitted write-ups and supporting files by placing all the necessary submission files in their ADRF team folder. Detailed submission instructions are available via the Real World Data Science GitHub repository."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/02-competition-design.html#metrics",
    "href": "applied-insights/case-studies/posts/2023/08/21/02-competition-design.html#metrics",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Metrics",
    "text": "Metrics\nSubmissions were evaluated by Coleridge Initiative and technical review and subject review boards based on the following criteria:\n\nQuantitative metrics were used to measure the predictive accuracy and runtime of the model.\n\nTransferability measured the quality of documentation and code, and the ability of individuals who are not involved in model development to replicate and implement the team’s approach.\n\nInnovation measured novelty and creativity of the model in addressing the linkage problem.\n\nTechnical review was overseen by faculty members from computer science and engineering departments of top US universities. Subject review was handled by subject matter experts from USDA and Westat.\nFrom a quantitative perspective, the most common way to evaluate machine learning competition submissions is to use model predictive accuracy. However, single metrics are typically incomplete descriptions of real-world tasks, and they can easily hide significant differences between models which simple predictive accuracy cannot capture. To select the most appropriate official challenge metrics, Coleridge Initiative reviewed the literature on the use of evaluation measures in both classification and ranking task machine learning competitions. Success at 5 (S@5) and Normalized Discounted Cumulative Gain at 5 (NDCG@5) scores were ultimately used as the quantitative metrics.\nThe metrics were applied as follows: models proposed by each team were tasked with outputting five potential FNDDS matches for each IRI code, with potential FNDDS matches ordered from most likely to least likely. S@5 and NDCG@5 scores are broadly similar – both measure whether a correct match is present in the five proposed matches that participants were asked to identify. However, S@5 does not take rank position into account and only considers whether the five proposed FNDDS matches contain the correct FNDDS response. NDCG@5 does take rank into account and also measures how highly the correct FNDDS response is ranked among the five proposed matches. Both measures range from 0 to 1 (or 0% to 100%). Models get a “full credit” for S@5 as long as they contain the correct FNDDS option. NDCG@5 penalizes models when the correct match is ranked lower on the list of 5 proposed matches."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/02-competition-design.html#technical-description",
    "href": "applied-insights/case-studies/posts/2023/08/21/02-competition-design.html#technical-description",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Technical description",
    "text": "Technical description\n\nEnvironment setup\nColeridge Initiative solicited technical requirements from participants at the challenge application stage to prepare the ADRF environment as much as possible before the competition began. Each team was asked to share anticipated workspace specifications and software library requests in their application package. From this we identified, reviewed, and installed the requested Python and R packages, libraries, and library components (e.g., pre-trained models, training data) that were not yet available within ADRF.\nThe setup of graphics processing units (GPUs) was also a critical part of competition preparation. We created an environment with 16 gibibyte (GiB) of GPU memory for each team. Our technology team met with multiple teams several times to discuss computing environment configurations to ensure the GPU could work properly. None of these efforts was wasted: without GPU access, it would be impossible for teams to use state-of-the-art pre-trained models such as the Bidirectional Encoder Representations from Transformers (BERT, Devlin et al. 2018).\nWe completed the setup of new team workspaces, each customized to the individual team’s resource and library requirements, including GPU configuration. The isolation and customization of workspaces was vital because teams may request different versions of libraries that potentially have version conflict with other libraries. We ensured the configurations were all set before the challenge began because such data challenges are bursty in nature (Macavaney et al. 2021), and handling support requests in the private data enclave risked causing delays. We hoped to avoid receiving too many requests in the beginning phase of the competition in order to give participants a better experience, though we did of course provide participants with instructions on how to request additional libraries during the challenge period.\n\n\nSupporting materials\nIn addition to environment preparation, we made available a list of supporting documentation, including IRI, PPC, and FNDDS codebooks, technical reports, and related publications that could help teams understand the challenge datasets. The FNDDS codebook pooled information on variable availability, coding, and descriptions across dataset files and years. It also included internal Westat food category coding difficulty ratings and notes on created PPC codes and provided UPC code, EC code, and general dataset remarks and observations that may take time for analysts to discover on their own.\nWe developed a baseline model to demonstrate the challenge task and the expected outputs – both outside of ADRF using FNDDS and fictitious data in place of IRI data, and an analogous model using FNDDS and IRI data within the ADRF secure environment. Moreover, we provided the teams with an evaluation script to read in their submissions and evaluate them for predictive accuracy against the public test set using S@5 and NDCG@5 challenge metrics. Finally, we held multiple webinars during the course of the challenge to explain next steps, address participant questions, solicit feedback, and provide general support. Multiple teams also met with our technology team to clarify ADRF-related questions or troubleshoot technical issues.\n(Baseline model, toolkits, and evaluation script are available from the Real World Data Science GitHub repository.)\n\n\nData splitting\nTo mimic the real-world scenario, the competition used 2012–2016 IRI data as the training set, and the 2017–2018 IRI data as the test set, since the data change over time and USDA could provide the most recent data available. To make sure that models were generalizable and not just overfit to the test set, we split the test set into private and public test sets. In this way, we guaranteed that the models were evaluated on completely hidden data. In order to keep the similar distribution of the two sets, we first divided the data into five quintiles based on EC code frequencies and then randomly sampled 80% of records in each group without repetition for placement into the private test set. Later in the competition, because of the computation limit, we further shrank the private test set to 40% of its original size using the same data-splitting method.\n\n\nJudging\nIn the first two rounds, submissions were evaluated based on the quantitative metrics, as previously mentioned above. Coleridge Initiative was responsible for running the evaluation script, making sure not to re-train the model or modify the configs in any way, and only applying the model to predict the private test set. Prediction results were then compared against ground truth to get the private scores.\nThe final challenge was reviewed by the scientific review board on all three judging criteria. Submitted models were first evaluated by Coleridge Initiative in the same way as in the first two rounds. The runtime of models was also recorded as an assessment of model cost. The scientific review boards then assessed the models by the quality of documentation, the quality of code, and the ability to replicate and implement the team’s approach, and scored the models for innovation and creativity in addressing the linkage problem. Lastly, scores were summarized and the scientific review board discussed and decided the winners of the competition."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/02-competition-design.html#results",
    "href": "applied-insights/case-studies/posts/2023/08/21/02-competition-design.html#results",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Results",
    "text": "Results\nThe next few articles in this collection walk readers through the solutions proposed by competition finalists. Figure 3 provides a brief summary.\n\n\nFigure 3: Top competitors and their solutions to the Food for Thought challenge."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/02-competition-design.html#lessons-learned",
    "href": "applied-insights/case-studies/posts/2023/08/21/02-competition-design.html#lessons-learned",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Lessons learned",
    "text": "Lessons learned\nIt was undoubtedly challenging for teams to work with highly secured data in a private data enclave for this data challenge. We solicited feedback from teams and summarized the issues that we experienced throughout the competitions, together with the solutions to resolve those issues. Below are our main lessons learned and we hope this summary can serve to inform future competitions.\n\nEnvironmental factors: The installation and setup of packages, libraries, and resources, as well as the configuration of GPUs, system dependencies, and workspace design were expected to take a long time as each team had their own needs. To accelerate the process, we requested a list of specific package and environment requirements from the teams in advance. However, due to the complexity of the system configuration required by the teams, environment setup took longer than expected. Thus, the challenge deadlines had to be postponed a few times to accommodate this.\nTime commitment: Twelve teams were selected to participate in the challenge, but only three teams remained in the final challenge. Other than one team that was disqualified for violating the ADRF terms of use agreement, eight dropped out because of other commitments and insufficient time to meaningfully participate. To ensure security, ADRF does not allow jobs to run in the backend, which also adds to the time commitment of teams. To encourage teams to participate in the final challenge, we gave out additional awards for second and third places.\nComputing resource limit: One issue encountered in evaluating submitted models was computing environment resource limits due to the secured nature of the data enclave. The original private test dataset is four times larger than the public test dataset, making it unfeasible to evaluate. To overcome this issue, given the fixed resource constraints, we decided to reduce the private test set to 40% of its original size. It would have been helpful, though, if the competition had set a model running time limit at the outset, so that participants could build simpler yet effective models.\nSupporting code: Although the initial baseline model we provided was extremely simple, we found this helped participants a lot in the initial phase – yet there is space to improve. To be specific, supporting codes should be constructed so that all relevant data tables are used and specify the main function to run the code, especially how the model should be tested. The teams only used the main table, which was the only table that was used in the baseline model, for training and did not touch the other supporting table. If we included the other table in the baseline model, it could help participants to have a better use of this data as well. In addition, a baseline model should be intuitive for the participants to follow, allowing evaluators to easily replace the public test set with the private test set without any programming modifications.\n\n\n\n\n\n← Part 1: Purchase to Plate\n\n\n\n\nPart 3: First place winners →\n\n\n\n\n\n\n\n\nAbout the authors\n\nZheyuan Zhang and Uyen Le are research scientists at the Coleridge Initiative.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Zheyuan Zhang and Uyen Le\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nZhang, Zheyuan, and Uyen Le. 2023. “Food for Thought: Competition and challenge design.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/00-food-for-thought.html",
    "href": "applied-insights/case-studies/posts/2023/08/21/00-food-for-thought.html",
    "title": "The Food for Thought Challenge: Using AI to support evidence-based food and nutrition policy",
    "section": "",
    "text": "There’s a saying: “You are what you eat.” Its meaning is somewhat open to interpretation, as with many such sayings, but it is typically used to make the point that if you want to be well, you need to eat well. Nutrition scientists and dieticians spend their careers trying to figure out what “eating well” looks like – the foods the human body needs, in what quantities, and how best to consume them. Their research informs advice and guidance issued by health professionals and governments. Ultimately, though, the choice of what to eat falls to us – individuals and families – and our choices are often determined by our tastes, the availability of foodstuffs in our local stores, their price and affordability.\nSo, what exactly do we eat? Answers come from a variety of sources. In the United States, there are dietary recall studies such as the National Health and Nutrition Examination Survey, which asks a sample of respondents to report their food and beverage consumption over a set period of time. There are also organisations like IRI that collect point-of-sale data from retail stores on the actual food and drink being sold to consumers. By and large, this information comes from barcodes on product packaging being scanned at checkouts, so it is often referred to as “scanner data”.\nThis data – from dietary recall studies and retail scanners – is valuable: once we know what people are eating, we can check the nutritional content of those foods and build up a picture of what the diet of a typical individual or family looks like and how it compares to the diet recommended by doctors and policymakers. And, if we know what other foodstuffs are available, how much they cost, and the nutritional value of those items, we can work out how much families need to spend, and on what, in order to eat well and, hopefully, be well.\nFiguring all this out is where something called the Purchase to Plate Crosswalk (PPC) comes in. It’s a key tool for understanding the “healthfulness of retail food purchases” and it does this by linking IRI scanner data on what people buy with data on the nutritional content of those foods, as recorded in the US Department of Agriculture’s Food and Nutrient Database for Dietary Studies (FNDDS). But there’s a catch: scanner data is collected about hundreds of thousands of food products, whereas the FNDDS has nutritional profile information for only a few thousand items. Linking these two datasets therefore gives rise to a one-to-many matching problem – a problem that takes several hundred person-hours to resolve.\nWhat if machine learning can help? That question inspired a competition, the Food for Thought Challenge, organized by the Coleridge Initiative, a nonprofit organization working with governments to ensure that data are more effectively used for public decision-making. Researchers and data scientists were invited to use machine learning and natural language processing to more efficiently link data on supermarket products to nutrient databases.\nThis collection of articles tells the story of the Food for Thought Challenge. We begin by exploring the policy issues that drive the development of the PPC – the need to understand the national diet, developing healthy diet plans, and costing up those plans – and the issues posed by record linkage. Next, we learn about the nature of the challenge and the structure of the competition in more detail, and then the three winning teams walk us through their solutions. We end the collection with some closing thoughts on the value of competitions for addressing data scientific challenges in the public sector.\n\n\n\n\nFind more case studies\n\n\n\n\nPart 1: The Purchase to Plate Suite →\n\n\n\n\n\n\n\n\nAbout the authors\n\nBrian Tarran is editor of Real World Data Science, and head of data science platform at the Royal Statistical Society.\n\n\nJulia Lane is a professor at the NYU Wagner Graduate School of Public Service and a NYU Provostial Fellow for Innovation Analytics. She co-founded the Coleridge Initiative, whose goal is to use data to transform the way governments access and use data for the social good through training programs, research projects and a secure data facility. She recently served on the Advisory Committee on Data for Evidence Building and the National AI Research Resources Task Force.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society and Julia Lane\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Melanie Lim on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian, and Julia Lane. 2023. “The Food for Thought Challenge: Using AI to support evidence-based food and nutrition policy.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/04-second-place-winners.html",
    "href": "applied-insights/case-studies/posts/2023/08/21/04-second-place-winners.html",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "",
    "text": "DeepFFTLink team members: Yang Wu and Kai Zhang are PhD students at Worcester Polytechnic Institute. Aishwarya Budhkar is a PhD student at Indiana University Bloomington. Xuhong Zhang is an assistant professor at Indiana University Bloomington. Xiaozhong Liu is an associate professor at Worcester Polytechnic Institute."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/04-second-place-winners.html#perspective-on-the-challenge",
    "href": "applied-insights/case-studies/posts/2023/08/21/04-second-place-winners.html#perspective-on-the-challenge",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Perspective on the challenge",
    "text": "Perspective on the challenge\nText matching is an essential task in natural language processing (NLP, Pang et al. 2016), while record linkage across different sources is an essential task in data science. Machine learning techniques allow people to combine data faster and cheaper than using manual linkage. However, in the context of the Food for Thought challenge, existing methods for matching universal product codes (UPCs) to ensemble codes (ECs) require every UPC to be compared with every EC code (Figure 1a). Such approaches can be computationally expensive in the training process when data is noisy. Here, we propose an ensemble model with a category-based adapter to tackle this problem, drawing on the category information included in UPC and EC data. The category-based adapter allows UPCs to be first matched with only a small and reliable set of ECs (Figure 1b). Then, an ensemble model will be deployed to make predictions for UPC-EC matching. Our proposed approach can achieve competitive performance compared with state-of-the-art models.\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 1: A toy example of our method. Panel (a) shows the traditional matching method, while (b) is our proposed ensemble model with category-based adapter. With the help of the adapter, UPC 1 only needs to be matched with EC 1 and EC 3."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/04-second-place-winners.html#our-approach",
    "href": "applied-insights/case-studies/posts/2023/08/21/04-second-place-winners.html#our-approach",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Our approach",
    "text": "Our approach\nWe propose a two-step framework to address this problem. To begin with, we use a category-based adapter to get reliable candidate ECs for each UPC. Then, an ensemble model (Dietterich 2000) is deployed to make a prediction for each UPC-EC pair.\n\nCategory-based adapter\nBy using 2015–2016 UPC-EC data, we created a knowledge base, which is a UPC category–EC pair-wised table for generating candidate ECs. Within this setting, each UPC category is, on average, related to only 32 ECs. This knowledge base is then used as context to further filter the candidate ECs. Note that there are some new ECs generated year by year, which can also be part of the potential ECs in the UPC-EC matching task, since the contextual information of new ECs does not exist in our knowledge base.\n\n\nEnsembled model\nWe ensemble the base-string match and BERT models. BERT is a deep learning model for natural language processing (Devlin et al. 2018). In the base-string match model, we used the Term Frequency-Inverse Document Frequency (TFIDF) of each UPC and EC description as features to calculate a pairwise cosine similarity, which is a distance between instances. Meanwhile, we used features extracted from UPC and EC descriptions to fine-tune the BERT base model and calculated the cosine similarity of embeddings between each UPC and EC. Then we rank ECs based on their similarity scores with the UPC.\n\n\nFigure 2: The framework of our proposed model. A two-step strategy is used to make the final prediction.\n\n\n\n\n\n\n\nFind the code in the Real World Data Science GitHub repository."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/04-second-place-winners.html#our-results",
    "href": "applied-insights/case-studies/posts/2023/08/21/04-second-place-winners.html#our-results",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Our results",
    "text": "Our results\nWe randomly selected 500 samples from the 2017–2018 UPC-EC data to train the ensembled weight for each model. Two functions were adapted to make a fusion of base-string and BERT models:\n\\[\nC = a * X + b * Y  \n\\tag{1}\\]\n\\[\nC =  a * log(X) + b * log(Y) \\text{. }\n\\tag{2}\\]\n\\(C\\) denotes the final confidence score. \\(X\\) and \\(Y\\) represent base_string_similarity_score and BERT_similarity_score, respectively. \\(a\\) and \\(b\\) are corresponding model weights for base_string and BERT models.\nA better Success@5 is achieved with function (1). The ensembled weights for the base-string model and BERT model are 0.738 and 0.262, respectively. The experiment result indicates that the base_string model contributes more than the BERT model when the ensemble model makes predictions. The prediction result for the 2017–2018 data is:\n\nSuccess@5: 0.727\nNDCG@5: 0.528\n\nComputation time is 6 hours."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/04-second-place-winners.html#future-work",
    "href": "applied-insights/case-studies/posts/2023/08/21/04-second-place-winners.html#future-work",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Future work",
    "text": "Future work\nOur next step will focus on adding the newly generated EC data into our knowledge base, which allows the model to be more stable to make predictions for UPC-EC matching. Our model is an unsupervised method, which does not need labels for each instance. We use cosine similarity to rank the matches, so no labels are needed in the training process. However, our future work will try to label some instances to handle the UPC-EC matching task in a supervised manner."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/08/21/04-second-place-winners.html#lessons-learned",
    "href": "applied-insights/case-studies/posts/2023/08/21/04-second-place-winners.html#lessons-learned",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Lessons learned",
    "text": "Lessons learned\n\nIf the data is not complex, simple models may outperform complex models. For example, in our experiment, we found that the base-string model outperforms single RoBERTa (Liu et al. 2019) or BERT models. However, our ensemble model can outperform each individual model since model fusion allows information aggregation from multiple models.\nMulti-label models may not work well on UPC-EC data. In our early work, we tried to consider the UPC-EC matching task as a multi-label problem, e.g., we labeled each EC as a binary label which indicated whether the EC was an appropriate match or not. We mapped UPC and EC pairs into a multi-label table. However, we find that the UPC and EC keeps a one-to-one relation for most UPCs. The model performance of a multi-label model, i.e., Label-Specific Attention Network (LSAN, Xiao et al. 2019), is lower than base-string model on both Success@5 and NDCG@5 metrics.\n\n\n\n\n\n← Part 3: First place winners\n\n\n\n\nPart 5: Third place winners →\n\n\n\n\n\n\n\n\nAbout the authors\n\nYang Wu and Kai Zhang are PhD students, and Xiaozhong Liu is an associate professor at Worcester Polytechnic Institute. Aishwarya Budhkar is a PhD student and Xuhong Zhang is an assistant professor at Indiana University Bloomington.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Yang Wu, Aishwarya Budhkar, Kai Zhang, Xuhong Zhang, and Xiaozhong Liu\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Hanson Lu on Unsplash.\n\n\n\nHow to cite\n\nWu, Yang, Aishwarya Budhkar, Kai Zhang, Xuhong Zhang, and Xiaozhong Liu. 2023. “Food for Thought: Second place winners – DeepFFTLink.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/11/22/splink.html",
    "href": "applied-insights/case-studies/posts/2023/11/22/splink.html",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "",
    "text": "In 2019, the data linking team at the Ministry of Justice was challenged to develop a new data linking methodology to produce new, higher quality linked datasets from the justice system.\nThe ultimate goal was to share new linked datasets with academic researchers, as part of the ADR UK-funded Data First programme. These datasets – which include data from prisons, probation, and the criminal and family courts – are now available, and researchers can apply for secure access.\nThe linking methodology is widely applicable and has been published as a free and open source software package called Splink. The software applies statistical best practice to accurately and quickly link and deduplicate large datasets. The software has now been downloaded over 7 million times, and has been used widely in government, academia and the private sector."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/11/22/splink.html#the-problem",
    "href": "applied-insights/case-studies/posts/2023/11/22/splink.html#the-problem",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "The problem",
    "text": "The problem\nData duplication is a ubiquitous problem affecting data quality. Organisations often have multiple records that refer to the same entity but no unique identifier that ties these entities together. Data entry errors and other issues mean that variations usually exist, so the records belonging to a single entity aren’t necessarily identical.\nFor example, in a company, customer data may have been entered multiple times in multiple different databases, with different spellings of names, different addresses, and other typos. The inability to identify which records belong to each customer presents a data quality problem at all stages of data analysis – from basic questions such as counting the number of unique customers, through to advanced statistical analysis.\nWith the growing size of datasets held by many organisations, any solution must be able to work on very large datasets of tens of millions of records or more."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/11/22/splink.html#approach",
    "href": "applied-insights/case-studies/posts/2023/11/22/splink.html#approach",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "Approach",
    "text": "Approach\nIn collaboration with academic experts, the team started with desk research into data linking theory and practice, and a review of existing open source software implementations.\nOne of the most common theoretical approaches described in the literature is the Fellegi-Sunter model. This statistical model has a long history of application for high profile, important record linking tasks such as in the US Census Bureau and the UK Office for National Statistics (ONS).\nThe model takes pairwise comparisons of records as an input, and outputs a match score between 0 and 1, which (loosely) can be interpreted as the probability of the two records being a match. Since the record comparison can be either two records from the same dataset, or records from different datasets, this is applicable to both deduplication and linkage problems.\nAn important benefit of the model is explainability. The model uses a number of parameters, each of which has an intuitive explanation that can be understood by a non-technical audience. The relative simplicity of the model also means it is easier to understand and explain how biases in linkage may occur, such as varying levels of accuracy for different ethnic groups.\n\nExample\nConsider the following simple record comparison. Are these records a match?\n\n\n\nFigure 1: Colour coded comparison of two records.\n\n\nThe parameters of the model are known as partial match weights, which capture the strength of the evidence in favour or against these records being a match.\nThey can be represented in a chart as follows, in which the highlighted bars correspond to the above example record comparison:\n\n\n\nFigure 2: Chart showing partial match weights of model.\n\n\nWe can see, for example, that the first name (Robin vs Robyn) is not an exact match, but they have a Jaro-Winkler similarity of above 0.9. As a result, the model ‘activates’ the corresponding partial match weight (in orange). This lends some evidence in favour of a match, but the partial match weight is not as strong as it would have been for an exact match.\nSimilarly we can see that the non-match on gender leads to the activation (in purple) of a strong negative partial match weight.\nThe activated partial match weight can then be represented in a waterfall chart as follows, which shows how the final match score is calculated:\n\n\n\nFigure 3: Waterfall chart showing how partial match weights combine to calculate the final prediction.\n\n\nThe parameter estimates in these charts all have intuitive explanations:\n\nThe partial match weight on first name is positive, but relatively weak. This makes sense, because the first names are a fuzzy match, not an exact match, so this provides only moderate evidence in favour of the record being a match.\nThe match weight for the exact match on postcode is stronger than the equivalent weight for surname. This is because the cardinality of the postcode field in the underlying data is higher than the cardinality for surname, so matches on postcode are less likely to occur by chance than matches on surname.\nThe negative match weight for the mismatch on gender is relatively strong. This reflects the fact that, in this dataset, it’s uncommon for the ‘gender’ field to match amongst truly matching records.\n\nThe final result is that the model predicts these records are a match, but with only 94% probability: it’s not sure. Most examples would be less ambiguous than this one, and would have a match probability very close to either 0 or 1.\nFor further details of the theory behind the Fellegi-Sunter model, and a deep dive into the intuitive explanations of the model, I have have developed a series of interactive tutorials."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/11/22/splink.html#implementation",
    "href": "applied-insights/case-studies/posts/2023/11/22/splink.html#implementation",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "Implementation",
    "text": "Implementation\nThrough our desk research and open source software review, an existing software package called fastLink was identified which implements the Fellegi-Sunter model, but unfortunately the software is not able to handle very large datasets of more than a few hundred thousand records.\nInspired by the popularity of fastLink, the team quickly realised that the methodology it was developing was generally applicable and could be valuable to a wide range of users if published as a software package.\nAs we spoke to colleagues across government and beyond, we found record linkage and deduplication problems are pervasive, and crop up in many different guises, meaning that any software needed to be very general and flexible.\nThe result is Splink – which is a Python package that implements the Fellegi-Sunter model, and enables parameters to be estimated using the Expectation Maximisation algorithm.\nThe package is free to use, and open source. It is accompanied by detailed documentation, including a tutorial and a set of examples.\nSplink makes no assumptions about the type of entity being linked, so it is very flexible. We are aware of its use to match data on a variety of entity types including persons, companies, financial transactions and court cases.\nThe package closely follows the statistical approach described in fastLink. In particular it implements the same mathematical model and likelihood functions described in the fastLink paper (see pages 354 to 357), with a comprehensive suite of tests to ensure correctness of the implementation.\nIn addition, Splink introduces a number of innovations:\n\nAble to work at massive scale – with proven examples of its use on over 100 million records.\nExtremely fast – capable of linking 1 million records on a laptop in around a minute.\nComprehensive graphical output showing parameter estimates and iteration history make it easier to understand the model and diagnose statistical issues.\nA waterfall chart which can be generated for any record pair, which explains how the estimated match probability is derived.\nSupport for deduplication, linking, and a combination of both, including support for deduplicating and linking multiple datasets.\nGreater customisability of record comparisons, including the ability to specify custom, user defined comparison functions.\nTerm frequency adjustments on any number of columns.\nIt’s possible to save a model once it’s been estimated – enabling a model to be estimated, quality assured, and then reused as new data becomes available.\nA companion website provides a complete description of the various configuration options, and examples of how to achieve different linking objectives."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/11/22/splink.html#using-splink",
    "href": "applied-insights/case-studies/posts/2023/11/22/splink.html#using-splink",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "Using Splink",
    "text": "Using Splink\nFull documentation and a tutorial are available for Splink, but the following snippet gives a simple example of Splink in action:\nfrom splink.datasets import splink_datasets\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.duckdb.comparison_library import (\n    exact_match,\n    jaro_winkler_at_thresholds,\n    levenshtein_at_thresholds,\n)\nfrom splink.duckdb.linker import DuckDBLinker\n\ndf = splink_datasets.fake_1000\n\n# Specify a data linkage model\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n      block_on(\"first_name\"),\n      block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        jaro_winkler_at_thresholds(\"first_name\", 2),\n        jaro_winkler_at_thresholds(\"surname\"),\n        levenshtein_at_thresholds(\"dob\"),\n        exact_match(\"city\", term_frequency_adjustments=True),\n        exact_match(\"email\"),\n    ],\n}\n\nlinker = DuckDBLinker(df, settings)\n\n# Estimate model parameters\n\n# Direct estimation using random sampling can be used for the u probabilities\nlinker.estimate_u_using_random_sampling(target_rows=1e6)\n\n# Expectation maximisation is used to train the m values\nbr_training = block_on([\"first_name\", \"surname\"])\nlinker.estimate_parameters_using_expectation_maximisation(br_training)\n\nbr_training = block_on(\"dob\")\nlinker.estimate_parameters_using_expectation_maximisation(br_training)\n\n# Use the model to compute pairwise match scores\npairwise_predictions = linker.predict()\n\n# Cluster the match scores into groups to produce a synthetic unique person id\nclusters = linker.cluster_pairwise_predictions_at_threshold(\n  pairwise_predictions, 0.95\n)\nclusters.as_pandas_dataframe(limit=5)\nThe example shows the flexibility of Splink, and how various types of configuration can be used:\n\nHow should different data fields be compared? In this example, the Jaro-Winkler distance is used for names, whereas Levenshtein is used for date of birth since Jaro-Winkler is not appropriate for numeric data.\nWhat blocking rules should be used? Blocking rules are the primary determinants of how fast Splink will run, but there is a trade off between speed and accuracy. In this case, the input data is small, so the blocking rules are loose.\nHow should the model parameters be estimated? In this case, the user has no labels for supervised training, and so uses the unsupervised Expectation Maximisation approach.\nIs clustering needed? In this case, each person may potentially have many duplicates, so clustering is used. This creates an estimated (synthetic) unique identifier for each entity (person) in the input dataset."
  },
  {
    "objectID": "applied-insights/case-studies/posts/2023/11/22/splink.html#outcomes",
    "href": "applied-insights/case-studies/posts/2023/11/22/splink.html#outcomes",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "Outcomes",
    "text": "Outcomes\nSplink has been used to link some of the largest datasets held by the Ministry of Justice as part of the Data First programme, and researchers are now able to apply for secure access to these datasets. Research using this data won the ONS Linked Administrative Data Award at the 2022 Research Excellence Awards.\nMore widely, the demand for Splink has been higher than we expected – with over 7 million downloads. It has been used in other government departments including the Office for National Statistics and internationally, the private sector, and published academic research from top international universities.\nSplink has also had external contributions from over 30 people, including staff at the Australian Bureau of Statistics, DataBricks, other government departments, academics, and various private sector consultancies.\n\n\n\n\n\n\nEditor’s note: For more on data linkage, check out our interview with Helen Miller-Bakewell of the UK Office for Statistics Regulation, discussing the OSR report, Data Sharing and Linkage for the Public Good.\n\n\n\n\nFind more case studies\n\n\n\n\n\nAbout the author\n\nRobin Linacre is an economist, data scientist and data engineer based at the UK Ministry of Justice. He is the lead author of Splink.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Robin Linacre\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Possessed Photography on Unsplash.\n\n\n\nHow to cite\n\nLinacre, Robin. 2023. “Deduplicating and linking large datasets using Splink.” Real World Data Science, November 22, 2023. URL"
  },
  {
    "objectID": "applied-insights/tutorials/posts/2023/04/13/flowers.html",
    "href": "applied-insights/tutorials/posts/2023/04/13/flowers.html",
    "title": "A demonstration of the law of the flowering plants",
    "section": "",
    "text": "This tutorial will demonstrate a popular method for predicting the day a flower will bloom. There are many reasons why you might want to predict a bloom date. You might be a scientist studying ecosystems stressed by climate change. Or you might be planning a trip to Amsterdam and would like to time your stay to when the tulips are in bloom. Or maybe you are participating in the annual Cherry Blossom Prediction Competition and want some ideas to help you get started.\nIn any case, you might be surprised to learn that the day a flower blooms is one of the earliest phenomena studied with systematic data collection and analysis. The mathematical rule developed in the eighteenth century to make these predictions – now called the “law of the flowering plants” – shaped the direction of statistics as a field and is still used by scientists with relatively few changes.\nWe present the law of the flowering plants as it was stated by Adolphe Quetelet, an influential nineteenth century statistician. Upon completing this tutorial, you will be able to:\nAt the end of the tutorial, we challenge you to design an algorithm that beats our predictions. The tutorial uses the R programming language. In particular, the code relies on the following packages:"
  },
  {
    "objectID": "applied-insights/tutorials/posts/2023/04/13/flowers.html#the-law-of-the-flowering-plants",
    "href": "applied-insights/tutorials/posts/2023/04/13/flowers.html#the-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "The law of the flowering plants",
    "text": "The law of the flowering plants\nWe begin by reviewing the law of the flowering plants as it was stated by Adolphe Quetelet. You may already know Quetelet as the inventor of the body mass index. Less known is that Quetelet recorded the bloom dates of hundreds of different plants between 1833 and 1852 at the Brussels Observatory, which he founded and directed. Quetelet reported that a plant flowers when exposed to a specific quantity of heat, measured in degrees of Celsius squared (°C²). For example, he calculated that a lilac blooms when the sum of the daily temperatures squared exceeds 4264°C² following the last frost.\nHe communicated this law in his Letters addressed to HRH the grand duke of Saxe-Coburg and Gotha (Number 33, 1846; translated 1849) and in his reporting On the climate of Belgium (Chapter 4, Part 4, 1848; data updated in Part 7, 1857). A picture of Quetelet and the title page of On the climate of Belgium are displayed in Figure 1.\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Quetelet reported on the law of the flowering plants in On the climate of Belgium (1857). Sources: Wikimedia Commons, Gallica.\n\nQuetelet was not the first to study bloom dates. Anthophiles have recorded the dates that flowers bloom for centuries. Written records of cherry trees go back as far as 812 AD in Japan and peach and plum trees as far as 1308 AD in China. Systematic record keeping began a century before Quetelet with Robert Marsham’s Indications of Spring (1789).\nQuetelet was also not the first to study the relationship between temperature and bloom dates. René Réaumur (1735), an early adopter of the thermometer, noted the relationship before Marsham published his Indications. But Quetelet was the first to systematically study the relationship across a wide variety of plants and derive the amount of heat needed to bloom. An example of Quetelet’s careful record keeping can be seen in Figure 2, one of many tables he reported in his publications.\n\n\n\n\n\n\nFigure 2: Bloom dates at Brussels Observatory observed by Quetelet between 1839 and 1852. Source: Gallica."
  },
  {
    "objectID": "applied-insights/tutorials/posts/2023/04/13/flowers.html#reproducing-quetelets-law-of-the-flowering-plants",
    "href": "applied-insights/tutorials/posts/2023/04/13/flowers.html#reproducing-quetelets-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Reproducing Quetelet’s law of the flowering plants",
    "text": "Reproducing Quetelet’s law of the flowering plants\nTo reproduce Quetelet’s law, we combine the data in Figure 2 with additional observations from his Letters. We focus on Quetelet’s primary example, the bloom date of the common lilac, Syringa vulgaris, row 18 of Figure 2. We do this because Quetelet carefully describes his methodology for measuring the bloom date of lilacs. For example, Quetelet considers a lilac to have bloomed when “the first corolla opens and shows the stamina.” That event is closest to what the USA Phenology Network describes as “open flowers”, depicted in the center image of Figure 3 below. This detail will become relevant when we attempt to replicate Quetelet’s law in a later section. Note that although we focus on lilacs in this tutorial, the R code is easily edited to predict the day that other plants will bloom.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: The bloom date occurs when the first corolla opens and shows the stamina (center image). Source: USA National Phenology Network.\n\nIn the R code below, the five-column tibble lilac contains the date each year that Quetelet observed the lilacs bloom at Brussels Observatory. The first three columns are the month, day, and year the lilacs bloomed between 1839 and 1852. These columns are combined to form the fourth column, the full date the lilacs bloomed. The last column converts the date to the day of the year the lilacs bloomed, abbreviated “doy.” That is, “doy” is the number of days it took for the lilacs bloom following January 1. Both “date” and “doy” representations of Quetelet’s observations will be useful throughout this tutorial.\n```{r}\nlilac &lt;-                   \n  tibble(month = c(\"May\", \"April\", \"April\", \"April\", \"April\", \"April\", \"May\", \n                   \"April\", \"May\", \"April\", \"May\", \"April\", \"May\", \"May\"),\n         day   =  c(10, 28, 24, 28, 20, 25, 13, 12, 9, 21, 2, 30, 1, 12),\n         year  = 1839:1852,\n         date  = as.Date(paste(month, day, year), format = \"%B %d %Y\"),\n         doy   = parse_number(format(date, \"%j\"))) \n\nlilac %&gt;% \n  kable(align = \"c\",\n        caption = \"Table 1: Bloom dates of lilacs observed by Quetelet between 1839 and 1852.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\nTable 1: Bloom dates of lilacs observed by Quetelet between 1839 and 1852.\n\n\n\n\nmonth\n\n\nday\n\n\nyear\n\n\ndate\n\n\ndoy\n\n\n\n\n\n\nMay\n\n\n10\n\n\n1839\n\n\n1839-05-10\n\n\n130\n\n\n\n\nApril\n\n\n28\n\n\n1840\n\n\n1840-04-28\n\n\n119\n\n\n\n\nApril\n\n\n24\n\n\n1841\n\n\n1841-04-24\n\n\n114\n\n\n\n\nApril\n\n\n28\n\n\n1842\n\n\n1842-04-28\n\n\n118\n\n\n\n\nApril\n\n\n20\n\n\n1843\n\n\n1843-04-20\n\n\n110\n\n\n\n\nApril\n\n\n25\n\n\n1844\n\n\n1844-04-25\n\n\n116\n\n\n\n\nMay\n\n\n13\n\n\n1845\n\n\n1845-05-13\n\n\n133\n\n\n\n\nApril\n\n\n12\n\n\n1846\n\n\n1846-04-12\n\n\n102\n\n\n\n\nMay\n\n\n9\n\n\n1847\n\n\n1847-05-09\n\n\n129\n\n\n\n\nApril\n\n\n21\n\n\n1848\n\n\n1848-04-21\n\n\n112\n\n\n\n\nMay\n\n\n2\n\n\n1849\n\n\n1849-05-02\n\n\n122\n\n\n\n\nApril\n\n\n30\n\n\n1850\n\n\n1850-04-30\n\n\n120\n\n\n\n\nMay\n\n\n1\n\n\n1851\n\n\n1851-05-01\n\n\n121\n\n\n\n\nMay\n\n\n12\n\n\n1852\n\n\n1852-05-12\n\n\n133\n\n\n\n\n\n\nTo reproduce Quetelet’s law of the flowering plants, we will combine these bloom dates with daily temperature. The daily maximum and minimum temperatures at Brussels Observatory between 1839 and 1852 are available from the Global Historical Climatology Network. The data can be downloaded using the ghcnd_search function contained within the R package rnoaa (2021). The station id for Brussels Observatory is “BE000006447”.\nThe ghcnd_search function returns the maximum and minimum temperature as separate tibbles in a list. In the R code below, we join the tibbles using the reduce function. Note that the temperature is reported in tenths of a degree (i.e. 0.1°C) so we divide by 10 before calculating the temperature midrange, our estimate of the daily temperature.\nThe result is a five-column tibble temp, which contains the year of the temperature record (“year”), the date of the temperature record (“date”), the maximum temperature (“tmax”), the minimum temperature (“tmin”), and the midrange temperature (“temp”). The first 10 rows of the table are below. When you produce the full table yourself, you may notice that a small portion of temperature records are missing. We found that imputing these missing values does not significantly change the results. Therefore, we ignore these days when conducting our analysis.\n```{r}\ntemp &lt;- \n  ghcnd_search(stationid = \"BE000006447\",\n               var = c(\"tmax\", \"tmin\"),\n               date_min = \"1839-01-01\",\n               date_max = \"1852-12-31\") %&gt;%\n  reduce(left_join) %&gt;%\n  transmute(year = parse_number(format(date, \"%Y\")), \n            date, \n            tmax = tmax / 10, \n            tmin = tmin / 10, \n            temp = (tmax + tmin) / 2)\n  \ntemp %&gt;% \n  kable(align = \"c\", \n        col.names = c(\"year\", \"date\", \"maximum temperature (°C)\", \n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 2: Temperature observed at Brussels Observatory between 1839 and 1852.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\nTable 2: Temperature observed at Brussels Observatory between 1839 and 1852.\n\n\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\n1839\n\n\n1839-01-01\n\n\n5.7\n\n\n-0.2\n\n\n2.75\n\n\n\n\n1839\n\n\n1839-01-02\n\n\n6.3\n\n\n0.8\n\n\n3.55\n\n\n\n\n1839\n\n\n1839-01-03\n\n\n7.2\n\n\n1.8\n\n\n4.50\n\n\n\n\n1839\n\n\n1839-01-04\n\n\n8.0\n\n\n1.8\n\n\n4.90\n\n\n\n\n1839\n\n\n1839-01-05\n\n\n5.3\n\n\n0.8\n\n\n3.05\n\n\n\n\n1839\n\n\n1839-01-06\n\n\n10.0\n\n\n1.3\n\n\n5.65\n\n\n\n\n1839\n\n\n1839-01-07\n\n\n8.9\n\n\n1.4\n\n\n5.15\n\n\n\n\n1839\n\n\n1839-01-08\n\n\n3.0\n\n\n0.1\n\n\n1.55\n\n\n\n\n1839\n\n\n1839-01-09\n\n\n0.8\n\n\n-0.1\n\n\n0.35\n\n\n\n\n1839\n\n\n1839-01-10\n\n\n2.8\n\n\n-2.8\n\n\n0.00\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\nReproducing Quetelet’s law is now a simple matter of calculating the sum of the squared daily temperature from the day of last frost until the bloom day. We could use the day of last frost reported in Quetelet’s Letters. However, since we will replicate Quetelet’s analysis with recent data in a later section, we use our own definition of the day of last frost. We define the day of last frost to be the day following the last day the maximum temperature is below 0. The R code below creates the function doy_last_frost to extract the day of last frost from the maximum temperature. To demonstrate this function, we then compare the bloom date with the last frost date in 1839, the first year Quetelet observed.\n```{r}\ndoy_last_frost &lt;- function(tmax, doy_max = 100) {\n  dof &lt;- which(tmax[1:doy_max] &lt;= 0)\n  if(length(dof) == 0) 1 else max(dof) + 1\n  }\n\nbloom_day &lt;- \n  lilac %&gt;% \n  filter(year == 1839) %&gt;%\n  pull(doy) + \n  as.Date(\"1839-01-01\")\n  \nfrost_day &lt;- \n  temp %&gt;% \n  filter(year == 1839) %&gt;% \n  pull(tmax) %&gt;% \n  doy_last_frost() + as.Date(\"1839-01-01\") \n\ntibble(`last frost date` = frost_day, \n       `bloom date` = bloom_day) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 3: Last frost date and lilac bloom date at Brussels Observatory in 1839.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 3: Last frost date and lilac bloom date at Brussels Observatory in 1839.\n\n\n\n\nlast frost date\n\n\nbloom date\n\n\n\n\n\n\n1839-03-08\n\n\n1839-05-11\n\n\n\n\n\n\nIf Quetelet’s law of the flowering plants is correct, Table 3 has the following interpretation. On March 8, 1839 the lilacs at Brussels Observatory began “collecting” temperature. The lilacs continued to “collect” temperature until May 11, at which point they exceeded their 4264°C² quota and bloomed. We visualize this theory in Figure 4 with the R packages ggplot2, a member of the set of packages that constitute the “tidyverse” (2019), and plotly.\n```{r}\n(temp %&gt;% \n  filter(date &lt; as.Date(\"1839-06-01\")) %&gt;% \n  ggplot() + \n  aes(date, temp) + \n  geom_line() + \n  labs(\n    x = \"\",\n    y = \"midrange temperature (°C)\",\n    title = \n      \"Figure 4: According to Quetelet's law, the lilacs bloom when exposed to 4264°C² following the last frost.\") +\n  geom_vline(xintercept = as.numeric(c(bloom_day, frost_day)), \n             linetype = \"dotted\")) %&gt;%\n  ggplotly() %&gt;% \n  add_annotations(x = as.numeric(c(frost_day, bloom_day)),\n                  y = c(-4, -4),\n                  text = c(\"last\\nfrost\", \"first\\nbloom\"),\n                  font = list(size = 14),\n                  ay = 0,\n                  xshift = c(-10, -12)) %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\nFigure 4: According to Quetelet’s law, the lilacs bloom when exposed to 4264°C² following the last frost. Author provided, CC BY 4.0.\n\nWe now have all the ingredients necessary to reproduce Quetelet’s findings. Our reproduction is greatly simplified by using the nest function from the tidyr package, another member of the “tidyverse”. For an overview of nest, see the “Nested data” section of Grolemund and Wickham (2017). We will group the data by year, nest, calculate the cumulative squared temperature from the frost date to the bloom date within each year, and then unnest. We ignore temperatures below 0°C. That is, temperatures below 0°C are set to 0°C. We do this because it is clear from Quetelet’s derivation of the law that only positive temperatures should be squared. See the next section for details.\n```{r}\nquetelet &lt;- \n  temp %&gt;% \n  group_by(year) %&gt;% \n  nest() %&gt;% \n  left_join(lilac) %&gt;% \n  mutate(law = map(data, ~ sum(pmax(.$temp, 0, na.rm = TRUE)[(doy_last_frost(.$tmax) + 1):doy]^2))) %&gt;% \n  unnest(law) %&gt;% \n  ungroup()\n\nquetelet %&gt;% \n  summarize(Quetelet = 4264, \n            est = mean(law), \n            se = sd(law)/sqrt(n()),\n            ci  = str_c(\"[\", round(est - 2 * se), \", \", round(est + 2 * se), \"]\")) %&gt;%\n  kable(dig = 0, \n        align = \"c\", \n        col.names = c(\"Quetelet's law (°C²)\", \"estimate (°C²)\", \n                      \"standard error (°C²)\", \"95% confidence interval (°C²)\"),\n        caption = \"Table 4: Reproduction of Quetelet's analysis.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 4: Reproduction of Quetelet’s analysis.\n\n\n\n\nQuetelet’s law (°C²)\n\n\nestimate (°C²)\n\n\nstandard error (°C²)\n\n\n95% confidence interval (°C²)\n\n\n\n\n\n\n4264\n\n\n4261\n\n\n197\n\n\n[3867, 4656]\n\n\n\n\n\n\nThe results show that Quetelet’s findings are indeed reproducible. Quetelet estimated that lilacs bloom once exposed to 4264°C² following the last frost. Our reanalysis suggests a similar amount. However, 4264°C² is the overall average across all years – the estimated amount needed to bloom varies year to year. As a result, the average has a 95% confidence interval of approximately 3870°C² to 4660°C². Quetelet was well aware of this variation. He argued it was due to unobserved factors that influence growing conditions and change each year, and he dedicated significant space in his Letters to discuss them.\nThese unobserved factors limit the accuracy of predictions made using the law. To assess the predictive accuracy of the law, we temporarily ignore the bloom dates Quetelet observed. Instead, we apply the 4264°C² quota to the temperature records at Brussels Observatory to predict the bloom date. We then compare our predictions with the bloom date Quetelet observed. The R code below creates the function doy_prediction to estimate the day the lilac will bloom from temperature records. Table 5 summarizes the accuracy of Quetelet’s law by the mean absolute error and root mean squared error.\n```{r}\ndoy_prediction &lt;- function(temp, tmax)\n  doy_last_frost(tmax) + which.max(cumsum(pmax(temp[(doy_last_frost(tmax) + 1):365], 0, na.rm = TRUE)^2) &gt; 4264)\n\nquetelet %&gt;% \n  mutate(pred = map(data, ~ doy_prediction(.$temp, .$tmax))) %&gt;% \n  unnest(pred) %&gt;% \n  ungroup() %&gt;%\n  summarize(mae  = mean(abs(doy - pred)),\n            rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  kable(dig = 0,\n        align = \"c\",\n        col.names = c(\"mean absolute error (days)\", \"root mean squared error (days)\"),\n        caption = \"Table 5: Predictions using Quetelet's law are accurate within a week on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 5: Predictions using Quetelet’s law are accurate within a week on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n5\n\n\n6\n\n\n\n\n\n\nTable 5 indicates that predictions made using the law are accurate to within a week on average. For comparison purposes, we also predict the day the lilacs will bloom using the average bloom date between 1839 and 1852. That is, on average the lilac bloomed on April 30 (April 29 on leap years), and we check the accuracy of simply predicting this average date each year. Table 6 indicates the average bloom date yields predictions that are less accurate by an average of two days.\n```{r}\nquetelet %&gt;%\n  summarize(pred = mean(doy),\n            mae  = mean(abs(doy - pred)),\n            rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  select(mae, rmse) %&gt;%\n  kable(dig = 0,\n        align = \"c\",\n        col.names = c(\"mean absolute error (days)\",\n                      \"root mean squared error (days)\"),\n        caption = \"Table 6: Predictions using the average bloom date are off by a week or more on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 6: Predictions using the average bloom date are off by a week or more on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n7\n\n\n9"
  },
  {
    "objectID": "applied-insights/tutorials/posts/2023/04/13/flowers.html#quetelets-derivation-of-the-law-of-the-flowering-plants",
    "href": "applied-insights/tutorials/posts/2023/04/13/flowers.html#quetelets-derivation-of-the-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Quetelet’s derivation of the law of the flowering plants",
    "text": "Quetelet’s derivation of the law of the flowering plants\nQuetelet believed that, as in physics, universal laws govern social and biological phenomenon. Quetelet was not only inspired by physics to describe social and biological patterns using mathematical formulas. He often took his formulas directly from physics. In fact, you may have already recognized similarities between his law and Newton’s second law of motion.\nQuetelet reasoned that temperature exerts a “force” on plants in the same way that gravity exerts a force on a falling object. Newton’s second law states that acceleration is proportional to force. It follows that an object initially at rest and subject to a constant force will travel a distance proportional to time squared. Quetelet simply substituted temperature for time.\nWe briefly elaborate. Let \\(d(t)\\) denote the distance an object travels after time \\(t\\). Let \\(v(t) = d'(t)\\) denote its speed and \\(a(t) = v'(t)\\) its acceleration. If acceleration is constant, i.e. \\(a(t) = c\\),\n\n\\(v(t) = \\int_0^t a(s) \\, ds = \\int_0^t c \\, ds = c t\\)\n\nand\n\n\\(d(t) = \\int_0^t v(s) \\, ds = \\int_0^t c s \\, ds = \\tfrac{c}{2} t^2\\)\n\nQuetelet imagined plants experience time in temperature and bloom after “traveling” distance \\(d_*\\). If a plant is exposed to temperature \\(t_i\\) on day \\(i = 1, 2, \\ldots\\), then the bloom date, \\(n_*\\), is the first day \\(\\sum_{i=1}^{n_*} \\tfrac{c}{2} t_i^2 \\geq d_*\\). Multiplying both sides of the inequality by \\(\\tfrac{2}{c}\\), yields Quetelet’s law: the bloom is the first day, \\(n_*\\), that \\(\\sum_{i=1}^{n_*} t_i^2 \\geq \\tfrac{2}{c} d_*\\).\nThe derivation of laws like the law of the flowering plants was popular in the nineteenth century. But any similarities between the “force” of temperature and the force of gravity are likely coincidental. We are not aware of any biological mechanisms that justify Quetelet’s application of Newton’s law.\nToday, the law of the flowering plants is considered a heuristic, or rule of thumb, that approximates complicated biological mechanisms. Like Quetelet, scientists model plants as experiencing time in temperature instead of calendar time. These temperature units are typically called “growing degree days”. Scientists often find that plants may only be sensitive to temperatures in specific ranges or “modified growing degree days”. Although modern statistical methods can greatly improve the accuracy of predictions, laws like Quetelet’s remain popular because they are simple to communicate and easy to replicate, as we demonstrate in the next section."
  },
  {
    "objectID": "applied-insights/tutorials/posts/2023/04/13/flowers.html#replicating-quetelets-law-of-the-flowering-plants",
    "href": "applied-insights/tutorials/posts/2023/04/13/flowers.html#replicating-quetelets-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Replicating Quetelet’s law of the flowering plants",
    "text": "Replicating Quetelet’s law of the flowering plants\nIn the previous section, we explained how Quetelet derived the law of the flowering plants. Quetelet believed the law of the flowering plants was universal, describing the bloom date of all flowers around the world and in any year. Whether the law can in fact be considered universal requires replicating Quetelet’s results with new data collected at a different location in a different year.\nIn this section, we replicate the law of the flowering plants using lilac bloom dates observed by scientists between 1956 and 2009 at 53 locations throughout the Pacific Northwest (2015). The data can be downloaded from the USA National Phenology Network using the rnpn package (2022). For space considerations, the R code that downloads and cleans the data is provided in the Appendix. Running this code yields the tibble usa_npn. Each row of the tibble corresponds with a bloom date observed at a given site in a given year. There are 31 columns, only seven of which we use in our replication. The remaining columns are documented in the rnpn package, and we will not review them here.\nTable 7 displays six of the seven columns (and only the first 10 rows of the full table). These columns are defined in the same way as the columns of Table 1, except for “site_id”, which denotes the site at which the observation was made. Table 1 does not have a “site_id” column because all observations were made at the same site, Brussels Observatory.\n```{r}\nload(url(\"https://github.com/jauerbach/miscellaneous/blob/main/usa_npn.RData?raw=true\"))\n\nusa_npn %&gt;%\n  transmute(site_id, \n            month = first_yes_month, \n            day   = first_yes_day, \n            year  = first_yes_year, \n            date  = as.Date(paste(month, day, year), format = \"%m %d %Y\"),\n            doy) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 7: Bloom dates of lilacs observed in pacific northwest between 1956 and 2009.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", \n             height = \"400px\")\n```\n\n\n\n\nTable 7: Bloom dates of lilacs observed in pacific northwest between 1956 and 2009.\n\n\n\n\nsite_id\n\n\nmonth\n\n\nday\n\n\nyear\n\n\ndate\n\n\ndoy\n\n\n\n\n\n\n150\n\n\n5\n\n\n25\n\n\n1956\n\n\n1956-05-25\n\n\n146\n\n\n\n\n150\n\n\n5\n\n\n22\n\n\n1957\n\n\n1957-05-22\n\n\n142\n\n\n\n\n150\n\n\n5\n\n\n12\n\n\n1958\n\n\n1958-05-12\n\n\n132\n\n\n\n\n150\n\n\n6\n\n\n3\n\n\n1959\n\n\n1959-06-03\n\n\n154\n\n\n\n\n150\n\n\n5\n\n\n27\n\n\n1960\n\n\n1960-05-27\n\n\n148\n\n\n\n\n150\n\n\n5\n\n\n27\n\n\n1961\n\n\n1961-05-27\n\n\n147\n\n\n\n\n150\n\n\n5\n\n\n26\n\n\n1962\n\n\n1962-05-26\n\n\n146\n\n\n\n\n150\n\n\n5\n\n\n24\n\n\n1963\n\n\n1963-05-24\n\n\n144\n\n\n\n\n150\n\n\n5\n\n\n28\n\n\n1964\n\n\n1964-05-28\n\n\n149\n\n\n\n\n150\n\n\n5\n\n\n26\n\n\n1966\n\n\n1966-05-26\n\n\n146\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\nThe seventh column we review is “temp”. Each row of “temp” is a tibble of temperature records taken at the nearest station in the Global Historical Climatology Network. The first tibble (again, only the first 10 rows) is displayed in Table 8 below. The columns are defined in the same way as the columns of Table 2, except for “id”, which denotes the location at which the temperature record was made. Table 2 does not have an “id” column because all observations were made at the same site, Brussels Observatory.\n```{r}\nusa_npn %&gt;%\n  pull(temp) %&gt;%\n  .[[1]] %&gt;%\n  mutate(year = parse_number(format(date, \"%Y\"))) %&gt;%\n  select(id, year, date, tmax, tmin, temp) %&gt;%\n  kable(align = \"c\",\n        col.names = c(\"id\", \"year\", \"date\", \"maximum temperature (°C)\", \n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 8: Temperature observed at an example pacific northwest site in 1956.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", \n             height = \"400px\")\n```\n\n\n\n\nTable 8: Temperature observed at an example pacific northwest site in 1956.\n\n\n\n\nid\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-01\n\n\n5.6\n\n\n-5.6\n\n\n0.00\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-02\n\n\n1.7\n\n\n-7.2\n\n\n-2.75\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-03\n\n\n3.3\n\n\n-11.7\n\n\n-4.20\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-04\n\n\n4.4\n\n\n-10.0\n\n\n-2.80\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-05\n\n\n7.8\n\n\n0.0\n\n\n3.90\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-06\n\n\n4.4\n\n\n-11.1\n\n\n-3.35\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-07\n\n\n2.8\n\n\n-6.1\n\n\n-1.65\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-08\n\n\n4.4\n\n\n-4.4\n\n\n0.00\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-09\n\n\n1.7\n\n\n-9.4\n\n\n-3.85\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-10\n\n\n2.8\n\n\n-6.1\n\n\n-1.65\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\nWe are now prepared to replicate Quetelet’s findings. We will use R code nearly identical to the code we used to reproduce Quetelet’s findings earlier. The main difference is due to the fact that temperature records are dependent across sites within a year. To account for this dependence, we compute the cumulative temperature squared from the last frost to the bloom date for each site and year. We then take the average across all sites within a year. Finally, we calculate the standard error and confidence interval using only the variation of the averages across years. Table 9 displays the results.\n```{r}\nusa_npn %&gt;%             \n  group_by(rownames(usa_npn)) %&gt;%\n  mutate(law = \n           map(temp, ~ sum(pmax(.$temp, 0, na.rm = TRUE)[(doy_last_frost(.$tmax, doy) + 1):(doy - 1)]^2))) %&gt;%\n  unnest(law) %&gt;% \n  group_by(year) %&gt;%    \n  summarize(law = mean(law)) %&gt;%\n  summarize(Quetelet = 4264, \n            est = mean(law), \n            se = sd(law) / sqrt(n()),\n            ci  = str_c(\"[\", round(est - 2 * se), \", \", round(est + 2 * se), \"]\")) %&gt;%\n  kable(dig = 0, \n        align = \"c\",\n        col.names = c(\"Quetelet's law (°C²)\", \"estimate (°C²)\",\n                      \"standard error (°C²)\", \"95% confidence interval (°C²)\"),\n        caption = \"Table 9: Replication of Quetelet's analysis.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 9: Replication of Quetelet’s analysis.\n\n\n\n\nQuetelet’s law (°C²)\n\n\nestimate (°C²)\n\n\nstandard error (°C²)\n\n\n95% confidence interval (°C²)\n\n\n\n\n\n\n4264\n\n\n4329\n\n\n116\n\n\n[4098, 4560]\n\n\n\n\n\n\nTable 9 indicates that Quetelet’s findings are replicable in the sense that the confidence interval calculated using Quetelet’s data (Table 4) overlaps with the confidence interval calculated using the USA lilac data (Table 9). The standard error in Table 9 is smaller than Table 4 because the replication uses 54 years of data compared to Quetelet’s 14. Note that in the R code above, we subtract 1 from “doy” to correct for differences in how the bloom date is reported. This correction is not particularly important; the confidence intervals still overlap when this correction is removed.\nWe now investigate the accuracy of Quetelet’s law when applied to the USA lilac data. As before, we make use of the doy_prediction function.\n```{r}\nusa_npn &lt;- \n  usa_npn %&gt;% \n  mutate(pred = map(temp, ~ doy_prediction(.$temp, .$tmax))) %&gt;% \n  unnest(pred) %&gt;% \n  ungroup()\n\nusa_npn %&gt;% \n  summarize(mae  = mean(abs(doy - 1 - pred)),\n            rmse = sqrt(mean((doy - 1 - pred)^2))) %&gt;%\n  kable(dig = 0,\n        align = \"c\", \n        col.names = c(\"mean absolute error (days)\",\n                      \"root mean squared error (days)\"),\n        caption = \"Table 10: Predictions using Quetelet's law are accurate within about two weeks on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 10: Predictions using Quetelet’s law are accurate within about two weeks on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n10\n\n\n15\n\n\n\n\n\n\nTable 10 indicates that the predictions are accurate to within two weeks on average. Recall that the predictions using Quetelet’s own data were accurate to within one week on average (Table 5). We speculate that the decrease in accuracy is due in part to the fact that both Quetelet’s lilacs and the temperature were observed at the same site, Brussels Observatory. In some cases, the USA lilacs were a few miles from where the temperature was recorded.\nAlthough the accuracy of the predictions made using Quetelet’s law is lower when applied to the USA lilac data, Figure 5 indicates that the law produces the correct bloom date on average. The figure plots the predictions made by the law against the actual bloom dates scientists observed. Note that instead of representing prediction-observation pairs as points in a scatter plot, the data are represented using blue contours. We use contours because there are more than 1,500 observations – too many to study using a scatter plot.\n```{r}\n(usa_npn %&gt;% \n   mutate(doy = first_yes_doy) %&gt;%\n   unnest(pred) %&gt;% \n   ungroup() %&gt;%\n   mutate(predicted = as.Date(\"2020-01-01\") + pred,\n          observed = as.Date(\"2020-01-01\") + doy) %&gt;%\n   ggplot() + \n    aes(x = observed, y = predicted) +\n    geom_density2d(contour_var = \"ndensity\") +\n    geom_abline(intercept = 0, slope = 1, linetype = 2) +\n    labs(x = \"date observed\", \n         y = \"date predicted\",\n         title = \"Figure 5: Predictions using Quetelet's law are accurate within about two weeks on average.\") +\n    theme(legend.position = \"none\")) %&gt;%\n  ggplotly(tooltip = \"\") %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\nFigure 5: Predictions using Quetelet’s law are accurate within about two weeks on average. Author provided, CC BY 4.0.\n\nThe contours are easy to interpret. The blue lines are much like a mountain range observed from above. The inner circles are peaks of high elevation in which many prediction-observation pairs co-occur. The outer circles are areas of low elevation in which few prediction-observation pairs co-occur.\nThe dotted line is the “y = x” line, having zero intercept and unit slope. Prediction-observation pairs that lie on the line indicate perfect predictions. The fact that the dotted line intersects the blue contours at their peak suggests the law derived from Quetelet’s data accurately predicts the typical bloom date of the USA data. This accuracy is impressive given the fact that the USA lilacs were observed more than a century later and on a different continent. The blue curves deviate from the line by about two weeks in the vertical direction, which is consistent with Table 10.\nAn average accuracy of two weeks might not sound impressive. But it is far more accurate than using the average bloom date Quetelet observed, April 30 (April 29 on leap years). The average bloom date yields predictions that are off by an additional eleven days on average.\n```{r}\nusa_npn %&gt;%\n  mutate(doy = first_yes_doy) %&gt;%\n  ungroup() %&gt;%\n  summarize(\n    pred = mean(quetelet$doy), \n    mae  = mean(abs(doy - pred)),\n    rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  select(mae, rmse) %&gt;%\n  kable(\n    dig = 0,\n    align = \"c\",\n    col.names = c(\"mean absolute error (days)\",\n                  \"root mean squared error (days)\"),\n    caption = \"Table 11: Predictions using the average bloom date are off by three weeks or more on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 11: Predictions using the average bloom date are off by three weeks or more on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n21\n\n\n24"
  },
  {
    "objectID": "applied-insights/tutorials/posts/2023/04/13/flowers.html#predicting-the-day-the-lilac-will-bloom-in-brussels-in-2023",
    "href": "applied-insights/tutorials/posts/2023/04/13/flowers.html#predicting-the-day-the-lilac-will-bloom-in-brussels-in-2023",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Predicting the day the lilac will bloom in Brussels in 2023",
    "text": "Predicting the day the lilac will bloom in Brussels in 2023\nAny weather forecast can become a flower forecast by applying the law of the flowering plants. In this section, we use the AccuWeather forecast to predict the day a hypothetical lilac will bloom in Brussels in 2023. AccuWeather forecasts daily maximum and minimum temperatures three months into the future. We do not evaluate the quality of these forecasts. The purpose of this section is to simply convert them into flower forecasts.\nWe use the AccuWeather forecast as it appeared on the webpage AccuWeather.com on February 19, 2023. AccuWeather reports the forecast for each month on a separate webpage. For reproducibility, we saved each page on the Internet Archive. The following R code creates the function get_weather_table to retrieve each page we saved, extract the forecast contained within that page, and arrange the data as a tibble. The get_weather_table function combines several functions from the rvest package, which is yet another member of the “tidyverse”. In particular, the forecast on each page is contained within the div “monthly-calendar” and can be extracted with the html_nodes and html_text2 functions.\nApplying the get_weather_table function to the url for each page yields a five column tibble temp_br, with columns defined in the same way as the tibble temp, discussed in previous sections. The first 10 rows are below; the data are also available on the author’s GitHub.\n```{r}\n get_weather_table &lt;- function(url)\n  read_html(url) %&gt;% \n  html_nodes(\"div.monthly-calendar\") %&gt;% \n  html_text2() %&gt;%\n  str_remove_all(\"°|Hist. Avg. \") %&gt;%\n  str_split(\" \", simplify = TRUE) %&gt;%\n  parse_number() %&gt;%\n  matrix(ncol = 3, \n         byrow = TRUE,\n         dimnames = list(NULL, c(\"day\", \"tmax\", \"tmin\"))) %&gt;%\n  as_tibble() %&gt;%\n  filter(\n    row_number() %in%\n      (which(diff(day) &lt; 0) %&gt;% (function(x) if(length(x) == 1) seq(1, x[1], 1) else seq(x[1] + 1, x[2], 1))))\n\ntemp_br &lt;-\n  tibble(\n    base_url = \"https://web.archive.org/web/20230219151906/https://www.accuweather.com/en/be/brussels/27581/\",\n    month = month.name[1:5],\n    year = 2023,\n    url = str_c(base_url, tolower(month), \"-weather/27581?year=\", year, \"&unit=c\")) %&gt;%\n  mutate(temp = map(url, get_weather_table)) %&gt;%\n  pull(temp) %&gt;%\n  reduce(bind_rows) %&gt;%\n  transmute(date = seq(as.Date(\"2023-01-01\"), as.Date(\"2023-05-31\"), 1),\n            year = parse_number(format(date, \"%Y\")),\n            tmax,\n            tmin,\n            temp = (tmax + tmin) / 2)\n\ntemp_br %&gt;%\n  relocate(year) %&gt;%\n  kable(dig = 2,\n        align = \"c\", \n        col.names = c(\"year\", \"date\", \"maximum temperature (°C)\",\n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 12: Temperature forecast for Brussels, retrieved on February 19, 2023.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\nTable 12: Temperature forecast for Brussels, retrieved on February 19, 2023.\n\n\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\n2023\n\n\n2023-01-01\n\n\n15\n\n\n11\n\n\n13.0\n\n\n\n\n2023\n\n\n2023-01-02\n\n\n14\n\n\n5\n\n\n9.5\n\n\n\n\n2023\n\n\n2023-01-03\n\n\n9\n\n\n3\n\n\n6.0\n\n\n\n\n2023\n\n\n2023-01-04\n\n\n13\n\n\n8\n\n\n10.5\n\n\n\n\n2023\n\n\n2023-01-05\n\n\n12\n\n\n10\n\n\n11.0\n\n\n\n\n2023\n\n\n2023-01-06\n\n\n12\n\n\n10\n\n\n11.0\n\n\n\n\n2023\n\n\n2023-01-07\n\n\n11\n\n\n9\n\n\n10.0\n\n\n\n\n2023\n\n\n2023-01-08\n\n\n10\n\n\n6\n\n\n8.0\n\n\n\n\n2023\n\n\n2023-01-09\n\n\n8\n\n\n5\n\n\n6.5\n\n\n\n\n2023\n\n\n2023-01-10\n\n\n12\n\n\n4\n\n\n8.0\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\nWe now predict the day the lilacs will bloom. The R code below uses the doy_prediction and doy_last_frost functions created in earlier sections and displays the prediction in Table 13. At the time of our writing, the predicted date is April 19. The forecast is easily updated by providing the url to the updated AccuWeather webpage. (You might use the url https://web.archive.org/save to save a webpage to the Internet Archive to ensure your work is reproducible.)\n```{r}\nbloom_day_br &lt;-\n  temp_br %&gt;%\n  summarize(date = doy_prediction(temp, tmax) + as.Date(\"2023-01-01\")) %&gt;%\n  pull(date)\n\nfrost_day_br &lt;- \n  temp_br %&gt;% \n  pull(tmax) %&gt;% \n  doy_last_frost() + as.Date(\"2023-01-01\") \n\ntibble(`last frost date` = frost_day_br, \n       `bloom date` = bloom_day_br) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 13: Last frost date and lilac bloom date in Brussels in 2023.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 13: Last frost date and lilac bloom date in Brussels in 2023.\n\n\n\n\nlast frost date\n\n\nbloom date\n\n\n\n\n\n\n2023-01-27\n\n\n2023-04-19\n\n\n\n\n\n\nWe visualize the predictions in Figure 6, which has the same interpretation as Figure 4. If the temperature forecast and Quetelet’s law are correct, on January 27, 2023 the lilacs in Brussels began “collecting” temperature. The lilacs will continue to “collect” temperature until April 19, at which point they will exceed their 4264°C² quota and bloom.\n```{r}\n(temp_br %&gt;% \n  ggplot() + \n  aes(date, temp) + \n  geom_line() + \n  labs(\n    x = \"\",\n    y = \"midrange temperature (°C)\",\n    title =\n      \"Figure 6: According to Quetelet's law, the lilacs will bloom once exposed to 4264°C² following the last frost.\") +\n  geom_vline(xintercept = as.numeric(c(frost_day_br, bloom_day_br)), \n             linetype = \"dotted\")) %&gt;%\n  ggplotly() %&gt;% \n  add_annotations(x = as.numeric(c(frost_day_br, bloom_day_br)),\n                  y = c(14, 14),\n                  text = c(\"last\\nfrost\", \"first\\nbloom\"),\n                  font = list(size = 14),\n                  ay = 0,\n                  xshift = c(-14, -16)) %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\nFigure 6: According to Quetelet’s law, the lilacs will bloom once exposed to 4264°C² following the last frost. Author provided, CC BY 4.0."
  },
  {
    "objectID": "applied-insights/tutorials/posts/2023/04/13/flowers.html#quetelets-legacy-advocate-mentor-and-perhaps-data-scientist",
    "href": "applied-insights/tutorials/posts/2023/04/13/flowers.html#quetelets-legacy-advocate-mentor-and-perhaps-data-scientist",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Quetelet’s legacy: advocate, mentor, and perhaps data scientist",
    "text": "Quetelet’s legacy: advocate, mentor, and perhaps data scientist\nIn this tutorial, we stated the law of the flowering plants and explained how Quetelet derived it. We also reproduced and replicated Quetelet’s findings before using his law to predict the day the lilac will bloom in Brussels. We now conclude with a reflection on Quetelet’s legacy.\nThe law of the flowering plants surely stands the test of time. It continues to be used by scientists – with relatively few changes – to plan harvests, manage pests, and study ecosystems stressed by climate change. We speculate the law’s longevity is due to the fact that it balances simplicity with relatively accurate predictions.\nAlthough Quetelet did not discover the law, he did much to advance it. Quetelet founded an international network for “observations of the periodical phenomena” (in addition to numerous statistical societies and publications, including the precursor to the Royal Statistical Society). Quetelet’s network of 80 stations collected observations throughout Europe from 1841 until 1872. In particular, Quetelet collaborated with Charles Morren – who later coined the term phenology, the name of the field that now studies biological life-cycle events like the timing of flower blooms (Demarée and Rutishauser 2011).\nIn recent years, the observations collected through phenology networks have become an important resource for understanding the impacts of climate change. For example, the USA National Phenology Network calculates the Spring Bloom Index, which measures the “first day of spring” using the days lilacs are observed to bloom at locations across the United States. The index is then compared to previous years. Figure 7 shows one comparison, called the Return Interval. The Return Interval is much like a p-value, calculating how frequently more extreme spring indices were observed in previous decades. Bloom dates that are uncommonly early (green) or late (purple) may indicate environments stressed by changing climate. Scientists exploit the relationship between temperature and bloom date to extrapolate the index to areas with few observations.\n\n\nFigure 7: The Spring Bloom Index Return Interval measures whether spring is typical when compared to recent decades. Source: USA National Phenology Network.\n\nQuetelet’s emphasis on discovering the universal laws he believed govern social and biological phenomenon has not endured. But data scientists continue to appropriate laws from one area of science to study another. For example, data scientists use neural networks and genetic algorithms to study a wide variety of phenomenon unrelated to neuroscience or genetics. Perhaps Quetelet’s appropriation of Newton’s law, in addition to his careful use of data, make him among the first data scientists?"
  },
  {
    "objectID": "applied-insights/tutorials/posts/2023/04/13/flowers.html#your-turn-do-you-have-what-it-takes-to-beat-quetelets-law",
    "href": "applied-insights/tutorials/posts/2023/04/13/flowers.html#your-turn-do-you-have-what-it-takes-to-beat-quetelets-law",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Your turn: Do you have what it takes to beat Quetelet’s law?",
    "text": "Your turn: Do you have what it takes to beat Quetelet’s law?\nQuetelet reported that a plant flowers when the sum of the daily temperatures squared exceeds a specific quantity. His prediction rule was state of the art in 1833. But surely you, a twenty-first century data scientist, can do better. Here are some ideas to get you started.\n\nQuetelet squared the temperature before calculating the sum. Would another function of temperature produce a more accurate prediction?\n\nRemove the square so that a plant flowers once the sum of the daily temperatures exceeds a (different) specific quantity. Does this version of the law produce more accurate predictions? What if you use the daily temperatures cubed? (Beginner)\nSuppose a lilac only registers temperatures between 0°C and 10°C. That is, a lilac experiences temperature below the lower limit, 0°C, as 0°C, and above the upper limit, 10°C, as 10°C. Does the accuracy of the predictions improve if you use the temperature the lilac experienced instead of the ambient temperature measured by a weather station? Write a program that finds the lower and upper limits that produce the most accurate predictions. (Intermediate)\nQuetelet used mean absolute error to evaluate the accuracy of his predictions. But his estimate of the specific quantity of heat needed to bloom, 4264°C², does not actually minimize mean absolute error. Write a program that finds the specific quantity that minimizes mean absolute error. Redo part i. and ii. using this function. (Advanced)\n\nQuetelet calculated the sum of the daily temperature squared between the day of last frost and the bloom date. Would another time interval produce more accurate predictions?\n\nWe estimated the day of last frost using the last day the maximum temperature was below 0°C. Try estimating the day of last frost by the last day the midrange temperature was below 0°C? Which estimate yields the most accurate predictions? What if you ignore the day of last frost and simply calculate the sum of the daily temperatures squared between February 1 and the bloom date? When you change the time interval, be sure to calculate the new specific quantity of heat needed to bloom. (Beginner)\nWrite a program that finds the time interval which yields the best predictions. (Intermediate)\nWrite a program that calculates the prediction rule for many different time intervals. Use cross-validation to combine these prediction rules into a single prediction rule. (Advanced)\n\nQuetelet’s law only considers the temperature. Would additional information provide more accurate predictions?\n\nIs the specific quantity of heat needed to bloom different in years with abnormally cold winters? Would the predictions be more accurate if you use one quantity of heat for years with cold winters and a different quantity of heat for years with warm winters? (Beginner)\nIs the estimated quantity of heat needed to bloom similar for locations close in space and time? Write a program that leverages spatial and temporal correlation to improve the accuracy of the predictions. (Intermediate)\nSome biologists report that a plant must be exposed to a fixed amount of cold temperature in the winter – in addition to a fixed amount of warm temperature in the spring – before it can bloom. Augment the law of the flowering plants to require the accumulation of a specific quantity of cold temperature before the accumulation of a specific quantity of warm temperature. Write a program that uses this new law to predict the day the lilac blooms. (Advanced)\n\n\nFeeling good about your prediction algorithm? Show it off at the annual Cherry Blossom Prediction Competition!"
  },
  {
    "objectID": "applied-insights/tutorials/posts/2023/04/13/flowers.html#appendix-preparing-usa-npn-data",
    "href": "applied-insights/tutorials/posts/2023/04/13/flowers.html#appendix-preparing-usa-npn-data",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Appendix: Preparing USA NPN Data",
    "text": "Appendix: Preparing USA NPN Data\n```{r}\n# 1. download lilac data using `rnpn`\nusa_npn &lt;- \n  npn_download_individual_phenometrics(request_source = \"Jonathan Auerbach\",\n                                       year = 1900:2050,\n                                       species_ids = 36,                       \n                                       phenophase_ids = c(77, 412))            \n\n# 2. limit analysis to sites that report more than 25 times\nsite_ids &lt;- \n  usa_npn %&gt;% \n  group_by(site_id) %&gt;% \n  summarize(n = n()) %&gt;% filter(n &gt; 25) %&gt;% pull(site_id)\n\nusa_npn &lt;- \n  usa_npn %&gt;% \n  filter(site_id %in% site_ids)\n\n# 3. find nearest weather stations for each site\nlocations &lt;- \n  usa_npn %&gt;% \n  group_by(site_id) %&gt;% \n  summarize(latitude = first(latitude), \n            longitude = first(longitude))\n\nstations &lt;- \n  ghcnd_stations() %&gt;%\n  filter(first_year &lt;= min(usa_npn$first_yes_year),\n         last_year  &gt;= max(usa_npn$first_yes_year),\n         state != \"\") %&gt;%\n  group_by(id, latitude, longitude, state) %&gt;%\n  summarize(temp_flag = sum(element %in% c(\"TMIN\", \"TMAX\"))) %&gt;%            \n  filter(temp_flag == 2) %&gt;% \n  ungroup()\n\ndist &lt;- function(x, y = stations %&gt;% select(latitude, longitude)) \n  stations$id[which.min(sqrt((x[1] - y[,1])^2 + (x[2] - y[,2])^2)[,1])]\n\nlocations$station_id &lt;- apply(locations, 1, function(x) dist(c(x[\"latitude\"], x[\"longitude\"])))\n\n# 4. get weather data from nearest station using `rnoaa`\nget_station_data &lt;- function(station_id) \n  ghcnd_search(stationid = station_id,\n               var = c(\"tmin\", \"tmax\"),\n               date_min = \"1956-01-01\",\n               date_max = \"2011-12-31\") %&gt;%\n  reduce(left_join, by = c(\"id\", \"date\")) %&gt;%\n  transmute(id, \n            date, \n            tmax = tmax / 10,\n            tmin = tmin / 10)\n\nusa_npn &lt;- \n  locations %&gt;%\n  mutate(temp = map(station_id, get_station_data)) %&gt;%\n  right_join(usa_npn, by = c(\"site_id\", \"latitude\", \"longitude\")) %&gt;% \n  group_by(rownames(usa_npn)) %&gt;% \n  mutate(temp = map(temp, ~ .x %&gt;% \n                      filter(format(date, \"%Y\") == first_yes_year) %&gt;%\n                      mutate(temp = (tmin + tmax) / 2)),\n         num_obs = map(temp,~ sum(format(.x$date,\"%j\") &lt;= 150)),\n         doy = first_yes_doy, year = first_yes_year) %&gt;% \n  unnest(num_obs) %&gt;%  \n  filter(num_obs == 150) %&gt;%\n  ungroup()\n```\n\nExplore more Tutorials\n\n\n\n\n\nAbout the author\n\nJonathan Auerbach is an assistant professor in the Department of Statistics at George Mason University. His research covers a wide range of topics at the intersection of statistics and public policy. His interests include the analysis of longitudinal data, particularly for data science and causal inference, as well as urban analytics, open data, and the collection, evaluation, and communication of official statistics. He co-organizes the annual Cherry Blossom Prediction Competition with David Kepplinger and Elizabeth Wolkovich.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Jonathan Auerbach\n\n\n  Text and code are licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Images are not covered by this licence, except where otherwise noted.\n\n\n\nHow to cite\n\nAuerbach, Jonathan. 2023. “A demonstration of the law of the flowering plants.” Real World Data Science, April 13, 2023. \n\n\n\n\n\n\n\n\n\nSource: Wikimedia Commons\n\n\nSource: Gallica\n\n\nSource: Gallica\n\n\nSource: USA National Phenology Network\n\n\nSource: USA National Phenology Network\n\n\nSource: USA National Phenology Network\n\n\nAuthor provided, CC BY 4.0\n\n\nAuthor provided, CC BY 4.0\n\n\nAuthor provided, CC BY 4.0\n\n\nSource: USA National Phenology Network"
  }
]