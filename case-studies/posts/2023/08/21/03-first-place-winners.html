<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alex Knipper, Naman Bansal, Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker">
<meta name="dcterms.date" content="2023-08-21">
<meta name="description" content="Auburn University’s team of PhD students and faculty describe their winning solution to the Food for Thought challenge: random forest classifiers.">

<title>Real World Data Science - Food for Thought: First place winners – Auburn Big Data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../">
<link href="../../../../../images/rwds-favicon.png" rel="icon" type="image/png">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../../../../../site_libs/quarto-contrib/academicons-1.9.2/all.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-contrib/academicons-1.9.2/size.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-1TTWB7YTR6"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-1TTWB7YTR6', { 'anonymize_ip': true});
</script>
<!-- Thank you to Ben Ruijl for the progress bar code!
Ben is on GitHub here: https://github.com/benruijl
And you can see the original code here: https://github.com/quarto-dev/quarto-cli/discussions/3842#discussioncomment-4591721 -->

<meta property="og:title" content="Real World Data Science - Food for Thought: First place winners – Auburn Big Data">
<meta property="og:description" content="Auburn University’s team of PhD students and faculty describe their winning solution to the Food for Thought challenge: random forest classifiers.">
<meta property="og:image" content="https://realworlddatascience.net/case-studies/posts/2023/08/21/images/03-auburn.png">
<meta property="og:site_name" content="Real World Data Science">
<meta property="og:image:height" content="724">
<meta property="og:image:width" content="991">
<meta property="og:image:alt" content="Fresh vegetables on grocery store shelves.">
<meta name="twitter:title" content="Real World Data Science - Food for Thought: First place winners – Auburn Big Data">
<meta name="twitter:description" content="Auburn University’s team of PhD students and faculty describe their winning solution to the Food for Thought challenge: random forest classifiers.">
<meta name="twitter:image" content="https://realworlddatascience.net/case-studies/posts/2023/08/21/images/03-auburn.png">
<meta name="twitter:site" content="@rwdatasci">
<meta name="twitter:image-height" content="724">
<meta name="twitter:image-width" content="991">
<meta name="twitter:image:alt" content="Fresh vegetables on grocery store shelves.">
<meta name="twitter:card" content="summary_large_image">
</head><body class="nav-fixed"><div id="progress-bar" style="width: 0%; height:4px; background-color: #939bc9;; position: fixed; top: 0px; z-index: 2000;"></div>

<script id="progressbar" type="text/javascript">

document.addEventListener("DOMContentLoaded", function() {

    const bar = document.querySelector('#progress-bar');
    const post = document.querySelector('#quarto-content');
    const html = document.documentElement;
    
    const height = post.scrollHeight + post.offsetTop;
    
    window.addEventListener('scroll', () => {
        bar.style.width = (html.scrollTop / (height- html.clientHeight)) * 100 + '%';
    });
});
</script>


<link rel="stylesheet" href="../../../../../rwds.css">




<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../../images/rwds-logo-150px.png" alt="Real World Data Science brand" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../case-studies/index.html"> 
<span class="menu-text">Case studies</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../ideas/index.html"> 
<span class="menu-text">Ideas</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../careers/index.html"> 
<span class="menu-text">Careers</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../viewpoints/index.html"> 
<span class="menu-text">Viewpoints</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-about-rwds" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">About RWDS</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-about-rwds">    
        <li>
    <a class="dropdown-item" href="../../../../../about-rwds.html">
 <span class="dropdown-text">Who we are</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../../contributor-docs/call-for-contributions.html">
 <span class="dropdown-text">How to contribute</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../../contact.html">
 <span class="dropdown-text">Contact us</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Food for Thought: First place winners – Auburn Big Data</h1>
                  <div>
        <div class="description">
          <p>Auburn University’s team of PhD students and faculty describe their winning solution to the Food for Thought challenge: random forest classifiers.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Machine learning</div>
                <div class="quarto-category">Natural language processing</div>
                <div class="quarto-category">Public policy</div>
                <div class="quarto-category">Health and wellbeing</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alex Knipper, Naman Bansal, Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 21, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#our-perspective-on-the-challenge" id="toc-our-perspective-on-the-challenge" class="nav-link active" data-scroll-target="#our-perspective-on-the-challenge">Our perspective on the challenge</a></li>
  <li><a href="#our-approach" id="toc-our-approach" class="nav-link" data-scroll-target="#our-approach">Our approach</a></li>
  <li><a href="#our-results" id="toc-our-results" class="nav-link" data-scroll-target="#our-results">Our results</a></li>
  <li><a href="#future-workrefinement" id="toc-future-workrefinement" class="nav-link" data-scroll-target="#future-workrefinement">Future work/refinement</a></li>
  <li><a href="#lessons-learned" id="toc-lessons-learned" class="nav-link" data-scroll-target="#lessons-learned">Lessons learned</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/realworlddatascience/realworlddatascience.github.io/edit/main/case-studies/posts/2023/08/21/03-first-place-winners.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/realworlddatascience/realworlddatascience.github.io/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>The Auburn Big Data team from Auburn University consists of five members, including three assistant professors: Dr Wenying Li of the Department of Agricultural Economics and Rural Sociology, Dr Jingyi Zheng of the Department of Mathematics and Statistics, and Dr Shubhra Kanti Karmaker of the Department of Computer Science and Software Engineering. Additionally, the team comprises two PhD students, Naman Bansal and Alex Knipper, who are affiliated with Dr Karmaker’s big data lab at Auburn University.</p>
<p>It is estimated that our team has spent approximately 1,400 hours on this project.</p>
<section id="our-perspective-on-the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="our-perspective-on-the-challenge">Our perspective on the challenge</h2>
<p>At the start of this competition, we decided to test three general approaches, in the order listed:</p>
<ol type="1">
<li><p>A heuristic approach, where we use only the data and a defined similarity metric to predict which FNDDS label a given IRI item should have.</p></li>
<li><p>A simpler modeling approach, where we train a simple statistical classifier, like a random forest <span class="citation" data-cites="10.1007/978-3-030-03146-6_86">(<a href="#ref-10.1007/978-3-030-03146-6_86" role="doc-biblioref">Parmar, Katariya, and Patel 2019</a>)</span>, logistic regression, etc., to predict the FNDDS label for a given IRI item. For this method, we opted to use a random forest as our statistical model, as it was a simpler model to use as a baseline, having shown decent performance in a wide range of classification tasks. As it turned out, this approach was quite robust and accurate, so we kept it as our main model for this approach.</p></li>
<li><p>A large language modeling approach, where we train a model like BERT <span class="citation" data-cites="DBLP:journals/corr/abs-1810-04805">(<a href="#ref-DBLP:journals/corr/abs-1810-04805" role="doc-biblioref">Devlin et al. 2018</a>)</span> to map the descriptions for given IRI and FNDDS items to the FNDDS category the supplied IRI item belongs to.</p></li>
</ol>
</section>
<section id="our-approach" class="level2">
<h2 class="anchored" data-anchor-id="our-approach">Our approach</h2>
<p>As we explored the data provided, we opted to use the given 2017–2018 PPC dataset as our primary dataset for both training and testing. To ensure a fair evaluation of the model, we randomly split the dataset into 60% training samples and 40% testing samples, making sure our training process never sees the testing dataset. For evaluating our models, we adopted the competition’s metrics: Success@5 and NDCG@5. After months of testing, our statistical classifier (approach #2) proved itself to be the model that both processes the data fastest and achieves the highest performance on our testing metrics.</p>
<p>This approach, at a high level, takes in the provided data (among other configuration parameters), formats the data in a computer-readable format – converting the IRI and FNDDS descriptions to a numerical representation with word embeddings <span class="citation" data-cites="DBLP:journals/corr/abs-1810-04805 mikolov2013efficient pennington-etal-2014-glove">(<a href="#ref-DBLP:journals/corr/abs-1810-04805" role="doc-biblioref">2018</a>; <a href="#ref-mikolov2013efficient" role="doc-biblioref">Mikolov et al. 2013</a>; <a href="#ref-pennington-etal-2014-glove" role="doc-biblioref">Pennington, Socher, and Manning 2014</a>)</span> and then using that numerical representation to calculate the distances between each description – and then trains a classification model (random forest <span class="citation" data-cites="10.1007/978-3-030-03146-6_86">(<a href="#ref-10.1007/978-3-030-03146-6_86" role="doc-biblioref">2019</a>)</span>/neural network <span class="citation" data-cites="SCHMIDHUBER201585">(<a href="#ref-SCHMIDHUBER201585" role="doc-biblioref">Schmidhuber 2015</a>)</span>) that can predict an FNDDS label for a given IRI item.</p>
<p>In terms of data, our approach uses the FNDDS/IRI descriptions, combining them into a single “description” field, and the IRI item’s categorical items – department, aisle, category, product, brand, manufacturer, and parent company – to further discern between items.</p>
<p>While most industrial methods require use of a graphics processing unit (graphics card, or GPU) to perform this kind of processing, our primary method only requires the computer’s internal processor (CPU) to function properly. With that in mind, to achieve the best possible performance on our test metrics, the most time-consuming operations are run in parallel. The time taken to train our primary model can likely be further improved if we parallelize these operations across a GPU, with the only downside being the imposition of a GPU requirement for systems aiming to run this method.</p>
<p>In addition to our primary method, our team has worked with alternate approaches on the GPU (using BERT <span class="citation" data-cites="DBLP:journals/corr/abs-1810-04805">(<a href="#ref-DBLP:journals/corr/abs-1810-04805" role="doc-biblioref">2018</a>)</span>, neural networks <span class="citation" data-cites="SCHMIDHUBER201585">(<a href="#ref-SCHMIDHUBER201585" role="doc-biblioref">2015</a>)</span>, etc.) to either: 1) speed up the time it takes to process and make inferences for the data, achieving similar performance on our test metrics, or 2) achieve higher performance, likely at a cost to the time it takes to process everything. Our reasoning behind doing so is that if a simple statistical model performs well, then a larger language model should be able to demonstrate a higher performance on our test metrics without much of an increase in training time. At the current time, these methods are still unable to match the performance/efficiency tradeoff of our primary method.</p>
<p>After exploring alternate methods to no avail, our team then decided to focus again on our primary method, the random forest <span class="citation" data-cites="10.1007/978-3-030-03146-6_86">(<a href="#ref-10.1007/978-3-030-03146-6_86" role="doc-biblioref">2019</a>)</span>, and a secondary method, feed-forward neural network mapping our input features (X) to the FNDDS labels (Y) <span class="citation" data-cites="SCHMIDHUBER201585">(<a href="#ref-SCHMIDHUBER201585" role="doc-biblioref">2015</a>)</span>, to optimize their training hyperparameters for the dataset. Our aim in this is to see which of our already-implemented, easier-to-run downstream methods would better optimize the performance/efficiency tradeoff after having its training parameters optimized to the fullest. This has resulted in a marginal increase in training time (+20-30 minutes) and a roughly 5% increase in performance for our still-highest performing model, the random forest.</p>
<p>Overall, our primary method – the random forest – gave us an approximate training time (including data pre-processing) of 4 hours 30 minutes for our ~38,000 IRI item training set, and an approximate inference time of 15 minutes on our testing set of ~15,000 IRI items. Furthermore, our method gave us a Success@5 score of .789 and an NDCG@5 score of .705 on our testing set.</p>
<section id="key-features" class="level5">
<h5 class="anchored" data-anchor-id="key-features">Key features</h5>
<p>Here is a list of the key features we utilize, along with what type of data we treat it as.</p>
<ul>
<li>FNDDS
<ul>
<li>food_code – identifier</li>
<li>main_food_description – text</li>
<li>additional_food_description – text</li>
<li>ingredient_description – text</li>
</ul></li>
<li>IRI
<ul>
<li>upc – identifier</li>
<li>upcdesc – text</li>
<li>dept – categorical</li>
<li>aisle – categorical</li>
<li>category – categorical</li>
<li>product – categorical</li>
<li>brand – categorical</li>
<li>manufacturer – categorical</li>
<li>parent – categorical</li>
</ul></li>
</ul>
<p>The intuition behind using these particular features is that the text-based descriptions provide the majority of the “meaning” of the item. By converting each description to a numerical representation <span class="citation" data-cites="mikolov2013efficient pennington-etal-2014-glove">(<a href="#ref-mikolov2013efficient" role="doc-biblioref">2013</a>; <a href="#ref-pennington-etal-2014-glove" role="doc-biblioref">2014</a>)</span>, we can then calculate the similarity between each “meaning” to determine which FNDDS label is most similar to the IRI item provided. However, that alone is not enough. The categorical features on the IRI item help to further enhance the model’s classifications using the logic and categories people use in places like grocery stores. For example, if given an item whose aisle was “fruit” and brand was “Dole”, the item could be reasonably expected to be something like “peaches” over something like “broccoli”.</p>
</section>
<section id="feature-selection" class="level5">
<h5 class="anchored" data-anchor-id="feature-selection">Feature selection</h5>
<p>Aforementioned intuition aside, our feature selection was rather naive, in that we manually examined the data and removed any redundant text features before doing anything else. After that, we decided to use description fields as “text” data to comprise the main “meaning” of the item, represented numerically after converting the text using a word embedding <span class="citation" data-cites="mikolov2013efficient pennington-etal-2014-glove">(<a href="#ref-mikolov2013efficient" role="doc-biblioref">2013</a>; <a href="#ref-pennington-etal-2014-glove" role="doc-biblioref">2014</a>)</span>. We also decided to use the non-description fields (aisle, category, etc.) as “categorical” data that would be turned into its own numerical representation, allowing our model to more easily discern between items using similar systems to people.</p>
</section>
<section id="feature-transformations" class="level5">
<h5 class="anchored" data-anchor-id="feature-transformations">Feature transformations</h5>
<p>Our feature transformations are also relatively simple. First, we combine all description fields for each item to make one large description, and then use a word embedding method (like GloVe <span class="citation" data-cites="pennington-etal-2014-glove">(<a href="#ref-pennington-etal-2014-glove" role="doc-biblioref">2014</a>)</span> or BERT <span class="citation" data-cites="DBLP:journals/corr/abs-1810-04805">(<a href="#ref-DBLP:journals/corr/abs-1810-04805" role="doc-biblioref">2018</a>)</span>) to convert the description into a numerical representation, resulting in a 300-dimensional GloVe or 768-dimensional BERT vector of numbers for each description. Then, for each IRI item, we calculate the cosine and Euclidean distances from each FNDDS item, resulting in two vectors, both equal in length to the original FNDDS data (in this case, two vectors of length ~7,300). The intuition behind this is that while cosine and Euclidean distances can tell us similar things, providing both of these sets of distances to the model should allow it to pick up on a more nuanced set of relationships between the IRI and FNDDS items.</p>
<p>For categorical data, we take all unique values in each field and assign them an ID number. While that is often not the best practice for making a numerical representation out of categorical data <span class="citation" data-cites="10.5120/ijca2017915495">(<a href="#ref-10.5120/ijca2017915495" role="doc-biblioref">Potdar, Pardawala, and Pai 2017</a>)</span>, it seemed to work for the downstream model.</p>
<p>Regardless, the aforementioned feature transformations give us (ad hoc) ~14,900 features if we use GloVe and ~15,300 features if we use BERT. Both feature sets can then be sent to the downstream random forest/neural network to start classifying items.</p>
<p>It should be noted that processing the data is by far the most time-consuming part of our method. The data processing times for each embedding are as follows:</p>
<ul>
<li>GloVe: ~3 hours</li>
<li>BERT: ~6 hours</li>
</ul>
<p>Due to BERT both taking so long to process data and performing lower than our GloVe embeddings on the classification task, we opt to use GloVe embeddings for our primary method. Our only theoretical explanation here is that since BERT is better at context-dependent tasks <span class="citation" data-cites="10.1145/3443279.3443304">(<a href="#ref-10.1145/3443279.3443304" role="doc-biblioref">Wang, Nulty, and Lillis 2021</a>)</span>, it likely will expect something similar to well-structured sentences as input, which is not what the IRI/FNDDS descriptions are. Rather, GloVe – being a method that depends less on context <span class="citation" data-cites="mikolov2013efficient pennington-etal-2014-glove">(<a href="#ref-mikolov2013efficient" role="doc-biblioref">2013</a>; <a href="#ref-pennington-etal-2014-glove" role="doc-biblioref">2014</a>)</span> – should excel better when the input text is not a well-formed sentence.</p>
</section>
<section id="training-methods" class="level5">
<h5 class="anchored" data-anchor-id="training-methods">Training methods</h5>
<p>Once the data has been processed, we collect the following data for each IRI item:</p>
<ul>
<li>UPC code</li>
<li>Description (converted to numerical representation)</li>
<li>Categorical variables (converted to numerical representation)</li>
<li>Distances to each FNDDS item</li>
</ul>
<p>Once that has been collected for each IRI item, we can finally use our classification model. We initialize our model and begin the training process with the IRI data mentioned above and the target FNDDS labels for each one, so the model knows what the “correct” answer is for the given data. Once the model has trained on our training dataset, we save the model and it is ready for use.</p>
<p>This part of training takes much less time than preparing the data, since calculating the embeddings takes a lot more computation than a random forest model. The training times for each method are as follows:</p>
<ul>
<li>Random Forest: ~1 hour 15 minutes</li>
<li>Neural Network: ~25 minutes</li>
</ul>
<p>Despite the neural network taking far less time to train than the random forest, it still scores lower on the scoring metrics than the random forest, so we opt to continue using the random forest model as our primary method.</p>
</section>
<section id="general-approach-to-developing-the-model" class="level5">
<h5 class="anchored" data-anchor-id="general-approach-to-developing-the-model">General approach to developing the model</h5>
<p>Since the linkage problem involves mapping tens of thousands of items to a smaller category set of a few thousand items, we decided to frame this problem as a multi-class classification problem <span class="citation" data-cites="aly2005survey">(<a href="#ref-aly2005survey" role="doc-biblioref">Aly 2005</a>)</span>, where we then rank the top “k” most probable class mappings, as requested by the competition ruleset.</p>
<p>Most of the usable data available to us is text data, so we need a method that can use that text-based information to accurately map classes based on the aforementioned text information. To best accomplish this, we opt to use word embedding techniques to calculate an average numerical representation for each text description (both IRI and FNDDS), so we can calculate distances between each description, giving our model a sense of how similar each description is.</p>
</section>
<section id="the-key-trick-to-the-model" class="level5">
<h5 class="anchored" data-anchor-id="the-key-trick-to-the-model">The key “trick” to the model</h5>
<p>Since text descriptions hold the most information that can be used to link between an IRI item and an FNDDS item, finding a way to calculate the similarity between each description is paramount to making this method work.</p>
<p>Both distance calculation methods used in this work, cosine and Euclidean distance, are very similar in the type of information encoded, the only major difference being that cosine distance is implicitly normalized and Euclidean distance is not <span class="citation" data-cites="10.1145/967900.968151">(<a href="#ref-10.1145/967900.968151" role="doc-biblioref">Qian et al. 2004</a>)</span>.</p>
</section>
<section id="notable-observations" class="level5">
<h5 class="anchored" data-anchor-id="notable-observations">Notable observations</h5>
<p>Just by building the ranking using the cosine similarities between each IRI item and all FNDDS items, we can achieve a Success@5 performance of 0.234 and an NDCG@5 performance of 0.312. The other features are provided and the random forest classifier is used to add some extra discriminative power to the model.</p>
</section>
<section id="data-disclaimer" class="level5">
<h5 class="anchored" data-anchor-id="data-disclaimer">Data disclaimer</h5>
<p>Our current method only uses the data readily available from the 2017–2018 dataset, which we acknowledge is intended for testing. To remedy this, we further split this dataset into train/test sets and report results on our unseen test subset for our primary performance metrics. This gives a decent look into how the model will perform on unseen data.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>Find the code in the <a href="https://github.com/realworlddatascience/realworlddatascience.github.io/tree/main/case-studies/posts/2023/08/21/_code">Real World Data Science GitHub repository</a>.</p>
</div>
</div>
</div>
</section>
</section>
<section id="our-results" class="level2">
<h2 class="anchored" data-anchor-id="our-results">Our results</h2>
<section id="approximate-training-time" class="level5">
<h5 class="anchored" data-anchor-id="approximate-training-time">Approximate training time</h5>
<p>Overall, our approximate training time for our primary method is 4 hours 30 minutes broken down (approximately) as follows:</p>
<ol type="1">
<li>Reading data from database: 30 seconds</li>
<li>Calculating ~7,300 FNDDS description embeddings: 15 minutes 45 seconds</li>
<li>Calculating ~38,000 IRI description embeddings and similarity scores: 2 hours 20 minutes 45 seconds</li>
<li>Formatting calculated data for the random forest classifier: 35 minutes</li>
<li>Training the random forest classifier: 1 hour 15 minutes</li>
</ol>
</section>
<section id="approximate-inference-time" class="level5">
<h5 class="anchored" data-anchor-id="approximate-inference-time">Approximate inference time</h5>
<p>Our approximate inference time for our primary method is 15 minutes to make inferences for ~15,000 IRI items.</p>
</section>
<section id="s5-ndcg5-performance" class="level5">
<h5 class="anchored" data-anchor-id="s5-ndcg5-performance">S@5 &amp; NDCG@5 performance</h5>
<p>This is how our best-performing model (GloVe + random forest) performs at the current time on the testing set:</p>
<ul>
<li>NDCG@5: 0.705</li>
<li>Success@5: 0.789</li>
</ul>
<p>When we evaluate that same model on the full PPC dataset we were provided (~38,000 items), we get the following scores:</p>
<ul>
<li>NDCG@5: 0.879</li>
<li>Success@5: 0.916</li>
</ul>
<p>(Note: The full PPC dataset contains approximately 15,000 items that we used to train the model, so these scores are not as representative of our method’s performance as the previous scores.)</p>
</section>
</section>
<section id="future-workrefinement" class="level2">
<h2 class="anchored" data-anchor-id="future-workrefinement">Future work/refinement</h2>
<p>As mentioned previously, we only used the given 2017–2018 PPC dataset as our primary dataset for both training and testing. Going forward, we would like to include datasets from previous years as well, which we believe would further increase our model performance. Additionally, the datasets generated from this research have the potential to inform and support additional studies from a variety of perspectives, including nutrition, consumer research, and public health. Further research utilizing these datasets has the potential to make significant contributions to our understanding of consumer behavior and the role of food and nutrient consumption in overall health and well-being.</p>
</section>
<section id="lessons-learned" class="level2">
<h2 class="anchored" data-anchor-id="lessons-learned">Lessons learned</h2>
<p>It was interesting that the random forest model performed better than the vanilla neural network model. This shows that a simple solution can work better, depending on the application. This observation is in line with the well-established principle in machine learning that the choice of model should be guided by the nature of the problem and the characteristics of the data. In this case, the random forest model, being a simpler and more interpretable model, was better suited to the problem at hand and was able to outperform the more complex neural network model. These results underscore the importance of careful model selection and the need to consider both the complexity of the model and the specific requirements of the problem when choosing an algorithm for a particular application.</p>
<div class="nav-btn-container">
<div class="grid">
<div class="g-col-12 g-col-sm-6">
<div class="nav-btn">
<p><a href="../../../../../case-studies/posts/2023/08/21/02-competition-design.html">← Part 2: Competition design</a></p>
</div>
</div>
<div class="g-col-12 g-col-sm-6">
<div class="nav-btn">
<p><a href="../../../../../case-studies/posts/2023/08/21/04-second-place-winners.html">Part 4: Second place winners →</a></p>
</div>
</div>
</div>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the authors</dt>
<dd>
<strong>Alex Knipper</strong> and <strong>Naman Bansal</strong> are PhD students, and <strong>Jingyi Zheng</strong>, <strong>Wenying Li</strong>, and <strong>Shubhra Kanti Karmaker</strong> are assistant professors at Auburn University.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Alex Knipper, Naman Bansal, Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" style="height:22px!important;vertical-align:text-bottom;"><img src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>. Thumbnail photo by <a href="https://unsplash.com/@nicotitto?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">nrd</a> on <a href="https://unsplash.com/photos/D6Tu_L3chLE?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Knipper, Alex, Naman Bansal, Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker. 2023. “Food for Thought: First place winners – Auburn Big Data.” Real World Data Science, August 21, 2023. <a href="https://realworlddatascience.net/viewpoints/case-studies/posts/2023/08/21/03-first-place-winners.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-aly2005survey" class="csl-entry" role="listitem">
Aly, M. 2005. <span>“Survey on Multiclass Classification Methods, Tech. Rep.”</span> <em>California Institute of Technology</em>.
</div>
<div id="ref-DBLP:journals/corr/abs-1810-04805" class="csl-entry" role="listitem">
Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2018. <span>“<span>BERT:</span> Pre-Training of Deep Bidirectional Transformers for Language Understanding.”</span> <em>CoRR</em> abs/1810.04805. <a href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</a>.
</div>
<div id="ref-mikolov2013efficient" class="csl-entry" role="listitem">
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. <span>“Efficient Estimation of Word Representations in Vector Space.”</span> <a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a>.
</div>
<div id="ref-10.1007/978-3-030-03146-6_86" class="csl-entry" role="listitem">
Parmar, A., R. Katariya, and V. Patel. 2019. <span>“A Review on Random Forest: An Ensemble Classifier.”</span> In <em>International Conference on Intelligent Data Communication Technologies and Internet of Things (ICICI) 2018</em>, edited by J. Hemanth, X. Fernando, P. Lafata, and Z. Baig, 758–63. Cham: Springer International Publishing.
</div>
<div id="ref-pennington-etal-2014-glove" class="csl-entry" role="listitem">
Pennington, J., R. Socher, and C. Manning. 2014. <span>“<span>G</span>lo<span>V</span>e: Global Vectors for Word Representation.”</span> In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (<span>EMNLP</span>)</em>, 1532–43. Doha, Qatar: Association for Computational Linguistics. <a href="https://doi.org/10.3115/v1/D14-1162">https://doi.org/10.3115/v1/D14-1162</a>.
</div>
<div id="ref-10.5120/ijca2017915495" class="csl-entry" role="listitem">
Potdar, K., T. S. Pardawala, and C. D. Pai. 2017. <span>“A Comparative Study of Categorical Variable Encoding Techniques for Neural Network Classifiers.”</span> <em>International Journal of Computer Applications</em> 175 (4): 7–9. <a href="https://doi.org/10.5120/ijca2017915495">https://doi.org/10.5120/ijca2017915495</a>.
</div>
<div id="ref-10.1145/967900.968151" class="csl-entry" role="listitem">
Qian, G., S. Sural, Y. Gu, and S. Pramanik. 2004. <span>“Similarity Between Euclidean and Cosine Angle Distance for Nearest Neighbor Queries.”</span> In <em>Proceedings of the 2004 ACM Symposium on Applied Computing</em>, 1232–37. SAC ’04. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/967900.968151">https://doi.org/10.1145/967900.968151</a>.
</div>
<div id="ref-SCHMIDHUBER201585" class="csl-entry" role="listitem">
Schmidhuber, J. 2015. <span>“Deep Learning in Neural Networks: An Overview.”</span> <em>Neural Networks</em> 61: 85–117. https://doi.org/<a href="https://doi.org/10.1016/j.neunet.2014.09.003">https://doi.org/10.1016/j.neunet.2014.09.003</a>.
</div>
<div id="ref-10.1145/3443279.3443304" class="csl-entry" role="listitem">
Wang, C., P. Nulty, and D. Lillis. 2021. <span>“A Comparative Study on Word Embeddings in Deep Learning for Text Classification.”</span> In <em>Proceedings of the 4th International Conference on Natural Language Processing and Information Retrieval</em>, 37–46. NLPIR ’20. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/3443279.3443304">https://doi.org/10.1145/3443279.3443304</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="realworlddatascience/realworlddatascience.github.io" data-repo-id="R_kgDOILnnig" data-category="General" data-category-id="DIC_kwDOILnnis4CSt2s" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright © 2024 <a href="https://rss.org.uk/">Royal Statistical Society</a></p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/realworlddatascience">
      <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://zenodo.org/communities/realworlddatascience">
<p><i class="ai  ai-zenodo ai-2xl"></i></p>
</a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/showcase/rss-real-world-data-science">
      <i class="bi bi-linkedin" role="img" aria-label="LinkedIn">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/rwdatasci">
      <i class="bi bi-twitter-x" role="img" aria-label="Twitter/X">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://fosstodon.org/@rwdatasci">
      <i class="bi bi-mastodon" role="img" aria-label="Mastodon">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="../../../../../feeds.html">
      <i class="bi bi-rss" role="img" aria-label="RWDS rss">
</i> 
    </a>
  </li>  
</ul>
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/realworlddatascience/realworlddatascience.github.io/edit/main/case-studies/posts/2023/08/21/03-first-place-winners.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/realworlddatascience/realworlddatascience.github.io/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p><a href="../../../../../ts-and-cs.html">Terms &amp; Conditions</a></p>
</div>
  </div>
</footer>




</body></html>